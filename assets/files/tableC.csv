node,PageRank,ltable_ID,ltable_title,ltable_authors,ltable_abstract,ltable_url,ltable_date,rtable_ID,rtable_title,rtable_authors,rtable_abstract,rtable_url,rtable_date,
1,0.000678989392134464,1,Mining for Unknown Unknowns,Bernard Sinclair-Desgagné,"Unknown unknowns are future relevant contingencies that lack an ex ante description. While there are numerous retrospective accounts showing that significant gains or losses might have been achieved or avoided had such contingencies been previously uncovered, getting hold of unknown unknowns still remains elusive, both in practice and conceptually. Using Formal Concept Analysis (FCA) - a subfield of lattice theory which is increasingly applied for mining and organizing data - this paper introduces a simple framework to systematically think out of the box and direct the search for unknown unknowns. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.05071,July 2023,1,Characterization of a Reconfigurable Free-Space Optical Channel for Embedded Computer Applications with Experimental Validation Using Rapid Prototyping Technology,"Rafael Gil-Otero, Theodore Lim and John F Snowdon","Free-space optical interconnects (FSOIs) are widely seen as a potential solution to current and future bandwidth bottlenecks for parallel processors. In this paper, an FSOI system called optical highway (OH) i...",https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/2007/67603,14 February 2007,
2,0.000678989392134464,2,A physics-constrained machine learning method for mapping gapless land surface temperature,"Jun Ma, Huanfeng Shen, Menghui Jiang, Liupeng Lin, Chunlei Meng, Chao Zeng, Huifang Li, Penghai Wu","More accurate, spatio-temporally, and physically consistent LST estimation has been a main interest in Earth system research. Developing physics-driven mechanism models and data-driven machine learning (ML) models are two major paradigms for gapless LST estimation, which have their respective advantages and disadvantages. In this paper, a physics-constrained ML model, which combines the strengths in the mechanism model and ML model, is proposed to generate gapless LST with physical meanings and high accuracy. The hybrid model employs ML as the primary architecture, under which the input variable physical constraints are incorporated to enhance the interpretability and extrapolation ability of the model. Specifically, the light gradient-boosting machine (LGBM) model, which uses only remote sensing data as input, serves as the pure ML model. Physical constraints (PCs) are coupled by further incorporating key Community Land Model (CLM) forcing data (cause) and CLM simulation data (effect) as inputs into the LGBM model. This integration forms the PC-LGBM model, which incorporates surface energy balance (SEB) constraints underlying the data in CLM-LST modeling within a biophysical framework. Compared with a pure physical method and pure ML methods, the PC-LGBM model improves the prediction accuracy and physical interpretability of LST. It also demonstrates a good extrapolation ability for the responses to extreme weather cases, suggesting that the PC-LGBM model enables not only empirical learning from data but also rationally derived from theory. The proposed method represents an innovative way to map accurate and physically interpretable gapless LST, and could provide insights to accelerate knowledge discovery in land surface processes and data mining in geographical parameter estimation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.04817,July 2023,2,Review of computational neuroaesthetics: bridging the gap between neuroaesthetics and computer science,Rui Li and Junsong Zhang,"The mystery of aesthetics attracts scientists from various research fields. The topic of aesthetics, in combination with other disciplines such as neuroscience and computer science, has brought out the burgeon...",https://www.springeropen.com//braininformatics.springeropen.com/articles/10.1186/s40708-020-00118-w,16 November 2020,
3,0.000678989392134464,3,Automatically detecting activities of daily living from in-home sensors as indicators of routine behaviour in an older population,"Claire M. Timon, Pamela Hussey, Hyowon Lee, Catriona Murphy, Harsh Vardan Rai, and Alan F. Smeaton","Objective: The NEX project has developed an integrated Internet of Things (IoT) system coupled with data analytics to offer unobtrusive health and wellness monitoring supporting older adults living independently at home. Monitoring {currently} involves visualising a set of automatically detected activities of daily living (ADLs) for each participant. The detection of ADLs is achieved {} to allow the incorporation of additional participants whose ADLs are detected without re-training the system. Methods: Following an extensive User Needs and Requirements study involving 426 participants, a pilot trial and a friendly trial of the deployment, an Action Research Cycle (ARC) trial was completed. This involved 23 participants over a 10-week period each with c.20 IoT sensors in their homes. During the ARC trial, participants each took part in two data-informed briefings which presented visualisations of their own in-home activities. The briefings also gathered training data on the accuracy of detected activities. Association rule mining was then used on the combination of data from sensors and participant feedback to improve the automatic detection of ADLs. Results: Association rule mining was used to detect a range of ADLs for each participant independently of others and was then used to detect ADLs across participants using a single set of rules {for each ADL}. This allows additional participants to be added without the necessity of them providing training data. Conclusions: Additional participants can be added to the NEX system without the necessity to re-train the system for automatic detection of the set of their activities of daily living. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.04563,July 2023,3,Through a computational lens: using dual computer-criminology degree programs to advance the study of criminology and criminal justice practice,"Colby L Valentine, Carter Hay, Kevin M Beaver and Thomas G Blomberg","Computational criminology seeks to address criminological and criminal justice problems through the use of applied mathematics, computer science, and criminology. The development of mathematical and computatio...",https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-2-2,16 January 2013,
4,0.000678989392134464,4,Learning Behavioral Representations of Routines From Large-scale Unlabeled Wearable Time-series Data Streams using Hawkes Point Process,"Tiantian Feng, Brandon M Booth, Shrikanth Narayanan","Continuously-worn wearable sensors enable researchers to collect copious amounts of rich bio-behavioral time series recordings of real-life activities of daily living, offering unprecedented opportunities to infer novel human behavior patterns during daily routines. Existing approaches to routine discovery through bio-behavioral data rely either on pre-defined notions of activities or use additional non-behavioral measurements as contexts, such as GPS location or localization within the home, presenting risks to user privacy. In this work, we propose a novel wearable time-series mining framework, Hawkes point process On Time series clusters for ROutine Discovery (HOT-ROD), for uncovering behavioral routines from completely unlabeled wearable recordings. We utilize a covariance-based method to generate time-series clusters and discover routines via the Hawkes point process learning algorithm. We empirically validate our approach for extracting routine behaviors using a completely unlabeled time-series collected continuously from over 100 individuals both in and outside of the workplace during a period of ten weeks. Furthermore, we demonstrate this approach intuitively captures daily transitional relationships between physical activity states without using prior knowledge. We also show that the learned behavioral patterns can assist in illuminating an individual's personality and affect. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.04445,July 2023,4,A comparative analysis of two computer science degree offerings,"Anna Carolina Finamore, Haydée G. Jiménez, Marco A. Casanova, Bernardo P. Nunes, Ana Moura Santos and António Pacheco Pires","This article presents an in-depth analysis and comparison of two computer science degree offerings, viz., the Bologna BSc in Information Systems and Computer Engineering, offered by the Instituto Superior Técn...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-020-00097-0,18 May 2020,
5,0.000678989392134464,5,Social Media Analytics in Disaster Response: A Comprehensive Review,Mohammadsepehr Karimiziarani,"Social media has emerged as a valuable resource for disaster management, revolutionizing the way emergency response and recovery efforts are conducted during natural disasters. This review paper aims to provide a comprehensive analysis of social media analytics for disaster management. The abstract begins by highlighting the increasing prevalence of natural disasters and the need for effective strategies to mitigate their impact. It then emphasizes the growing influence of social media in disaster situations, discussing its role in disaster detection, situational awareness, and emergency communication. The abstract explores the challenges and opportunities associated with leveraging social media data for disaster management purposes. It examines methodologies and techniques used in social media analytics, including data collection, preprocessing, and analysis, with a focus on data mining and machine learning approaches. The abstract also presents a thorough examination of case studies and best practices that demonstrate the successful application of social media analytics in disaster response and recovery. Ethical considerations and privacy concerns related to the use of social media data in disaster scenarios are addressed. The abstract concludes by identifying future research directions and potential advancements in social media analytics for disaster management. The review paper aims to provide practitioners and researchers with a comprehensive understanding of the current state of social media analytics in disaster management, while highlighting the need for continued research and innovation in this field. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.04046,July 2023,5,Joint Letter from the Editor-in-Chief and the Publications Chair of the Brazilian Computer Society,Maria Cristina Ferreira de Oliveira and Karin Breitman,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194508,December 2009,
6,0.000678989392134464,6,CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution,"Matthias Freiberger, Peter Kun, Anders Sundnes Løvlie, Sebastian Risi","Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained multi-modal networks. From the perspective of vulnerability to off-manifold attacks, we therefore argue for the mitigation of modality gaps in CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints are available at https://github.com/matfrei/CLIPMasterPrints. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.03798,July 2023,6,Finding time for computer science in the elementary school day: a quasi-experimental study of a transdisciplinary problem-based learning approach,"Jeanne Century, Kaitlyn A. Ferris and Huifang Zuo","As the number of computer science (CS) jobs become increasingly available in this country and computing skills become essential tools for managing all aspects of our personal lives, CS is quickly becoming an e...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00218-3,15 May 2020,
7,0.000678989392134464,7,GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest,"Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, Ping Luo","Instruction tuning large language model (LLM) on image-text pairs has achieved unprecedented vision-language multimodal abilities. However, their vision-language alignments are only built on image-level, the lack of region-level alignment limits their advancements to fine-grained multimodal understanding. In this paper, we propose instruction tuning on region-of-interest. The key design is to reformulate the bounding box as the format of spatial instruction. The interleaved sequences of visual features extracted by the spatial instruction and the language embedding are input to LLM, and trained on the transformed region-text data in instruction tuning format. Our region-level vision-language model, termed as GPT4RoI, brings brand new conversational and interactive experience beyond image-level understanding. (1) Controllability: Users can interact with our model by both language and spatial instructions to flexibly adjust the detail level of the question. (2) Capacities: Our model supports not only single-region spatial instruction but also multi-region. This unlocks more region-level multimodal capacities such as detailed region caption and complex region reasoning. (3) Composition: Any off-the-shelf object detector can be a spatial instruction provider so as to mine informative object attributes from our model, like color, shape, material, action, relation to other objects, etc. The code, data, and demo can be found at https://github.com/jshilong/GPT4RoI. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.03601,July 2023,7,VeriSIM: A model-based learning pedagogy for fostering software design evaluation skills in computer science undergraduates,Prajish Prasad and Sridhar Iyer,"Evaluating a software design is an important practice of expert software designers. They spend significant time evaluating their solution, by developing an integrated mental model of the software design and th...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00192-0,24 May 2022,
8,0.000678989392134464,8,Structure Guided Multi-modal Pre-trained Transformer for Knowledge Graph Reasoning,"Ke Liang, Sihang Zhou, Yue Liu, Lingyuan Meng, Meng Liu, Xinwang Liu","Multimodal knowledge graphs (MKGs), which intuitively organize information in various modalities, can benefit multiple practical downstream tasks, such as recommendation systems, and visual question answering. However, most MKGs are still far from complete, which motivates the flourishing of MKG reasoning models. Recently, with the development of general artificial architectures, the pretrained transformer models have drawn increasing attention, especially for multimodal scenarios. However, the research of multimodal pretrained transformer (MPT) for knowledge graph reasoning (KGR) is still at an early stage. As the biggest difference between MKG and other multimodal data, the rich structural information underlying the MKG still cannot be fully leveraged in existing MPT models. Most of them only utilize the graph structure as a retrieval map for matching images and texts connected with the same entity. This manner hinders their reasoning performances. To this end, we propose the graph Structure Guided Multimodal Pretrained Transformer for knowledge graph reasoning, termed SGMPT. Specifically, the graph structure encoder is adopted for structural feature encoding. Then, a structure-guided fusion module with two different strategies, i.e., weighted summation and alignment constraint, is first designed to inject the structural information into both the textual and visual features. To the best of our knowledge, SGMPT is the first MPT model for multimodal KGR, which mines the structural information underlying the knowledge graph. Extensive experiments on FB15k-237-IMG and WN18-IMG, demonstrate that our SGMPT outperforms existing state-of-the-art models, and prove the effectiveness of the designed strategies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.03591,July 2023,8,Student-centered case-based teaching and online–offline case discussion in postgraduate courses of computer science,"Xinhong Zhang, Boyan Zhang and Fan Zhang",This study explores a student-centered teaching method in postgraduate courses. Teacher-centered classroom teaching cannot fully stimulate learning initiative and enthusiasm of students. Student-centered means...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00374-2,31 January 2023,
9,0.000678989392134464,9,JSONoid: Monoid-based Enrichment for Configurable and Scalable Data-Driven Schema Discovery,Michael J. Mior,"Schema discovery is an important aspect to working with data in formats such as JSON. Unlike relational databases, JSON data sets often do not have associated structural information. Consumers of such datasets are often left to browse through data in an attempt to observe commonalities in structure across documents to construct suitable code for data processing. However, this process is time-consuming and error-prone. Existing distributed approaches to mining schemas present a significant usability advantage as they provide useful metadata for large data sources. However, depending on the data source, ad hoc queries for estimating other properties to help with crafting an efficient data pipeline can be expensive. We propose JSONoid, a distributed schema discovery process augmented with additional metadata in the form of monoid data structures that are easily maintainable in a distributed setting. JSONoid subsumes several existing approaches to distributed schema discovery with similar performance. Our approach also adds significant useful additional information about data values to discovered schemas with linear scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.03113,July 2023,9,"SAT patterns and engineering and computer science college majors: an intersectional, state-level study","Lin Tan, Isabel S. Bradburn, David B. Knight, Timothy Kinoshita and Jacob Grohs","Numerous efforts worldwide have been made to increase diversity in engineering and computer science (ECS), fields that pay well and promote upward mobility. However, in the United States (U.S.), females and st...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00384-6,5 November 2022,
10,0.000678989392134464,10,Applying Process Mining on Scientific Workflows: a Case Study,"Zahra Sadeghibogar, Alessandro Berti, Marco Pegoraro, Wil M. P. van der Aalst","Computer-based scientific experiments are becoming increasingly data-intensive. High-Performance Computing (HPC) clusters are ideal for executing large scientific experiment workflows. Executing large scientific workflows in an HPC cluster leads to complex flows of data and control within the system, which are difficult to analyze. This paper presents a case study where process mining is applied to logs extracted from SLURM-based HPC clusters, in order to document the running workflows and find the performance bottlenecks. The challenge lies in correlating the jobs recorded in the system to enable the application of mainstream process mining techniques. Users may submit jobs with explicit or implicit interdependencies, leading to the consideration of different event correlation techniques. We present a log extraction technique from SLURM clusters, completed with an experimental. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.02833,July 2023,10,"Application-based principles of islamic geometric patterns; state-of-the-art, and future trends in computer science/technologies: a review","Mohammadreza Ranjazmay Azari, Mohammadreza Bemanian, Mohammadjavad Mahdavinejad, Axel Körner and Jan Knippers","Currently, there is a tendency to use Islamic Geometric Patterns (IGPs) as important identities and cultural elements of building design in the Middle East. Despite high demand, lack of information about the p...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-022-00852-w,1 February 2023,
11,0.000678989392134464,11,CPDG: A Contrastive Pre-Training Method for Dynamic Graph Neural Networks,"Yuanchen Bei, Hao Xu, Sheng Zhou, Huixuan Chi, Mengdi Zhang, Zhao Li, Jiajun Bu","Dynamic graph data mining has gained popularity in recent years due to the rich information contained in dynamic graphs and their widespread use in the real world. Despite the advances in dynamic graph neural networks (DGNNs), the rich information and diverse downstream tasks have posed significant difficulties for the practical application of DGNNs in industrial scenarios. To this end, in this paper, we propose to address them by pre-training and present the Contrastive Pre-Training Method for Dynamic Graph Neural Networks (CPDG). CPDG tackles the challenges of pre-training for DGNNs, including generalization and long-short term modeling capability, through a flexible structural-temporal subgraph sampler along with structural-temporal contrastive pre-training schemes. Extensive experiments conducted on both large-scale research and industrial dynamic graph datasets show that CPDG outperforms existing methods in dynamic graph pre-training for various downstream tasks under three transfer settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.02813,July 2023,11,Efficient computer operation for users with a neuromuscular disease with OnScreenDualScribe,"Torsten Felzer, Ian Scott MacKenzie and Stephan Rinderknecht","We developed a tool based on a modified number pad to empower persons with certain diseases, in particular of neuromuscular origin, to efficiently operate a computer and enter text. As the keypad lies securely...",https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/s40166-014-0002-7,25 June 2014,
12,0.000678989392134464,12,Real-time Workload Pattern Analysis for Large-scale Cloud Databases,"Jiaqi Wang, Tianyi Li, Anni Wang, Xiaoze Liu, Lu Chen, Jie Chen, Jianye Liu, Junyang Wu, Feifei Li, Yunjun Gao","Hosting database services on cloud systems has become a common practice. This has led to the increasing volume of database workloads, which provides the opportunity for pattern analysis. Discovering workload patterns from a business logic perspective is conducive to better understanding the trends and characteristics of the database system. However, existing workload pattern discovery systems are not suitable for large-scale cloud databases which are commonly employed by the industry. This is because the workload patterns of large-scale cloud databases are generally far more complicated than those of ordinary databases. In this paper, we propose Alibaba Workload Miner (AWM), a real-time system for discovering workload patterns in complicated large-scale workloads. AWM encodes and discovers the SQL query patterns logged from user requests and optimizes the querying processing based on the discovered patterns. First, Data Collection & Preprocessing Module collects streaming query logs and encodes them into high-dimensional feature embeddings with rich semantic contexts and execution features. Next, Online Workload Mining Module separates encoded queries by business groups and discovers the workload patterns for each group. Meanwhile, Offline Training Module collects labels and trains the classification model using the labels. Finally, Pattern-based Optimizing Module optimizes query processing in cloud databases by exploiting discovered patterns. Extensive experimental results on one synthetic dataset and two real-life datasets (extracted from Alibaba Cloud databases) show that AWM enhances the accuracy of pattern discovery by 66% and reduce the latency of online inference by 22%, compared with the state-of-the-arts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.02626,July 2023,12,A Minimax Mutual Information Scheme for Supervised Feature Extraction and Its Application to EEG-Based Brain-Computer Interfacing,Farid Oveisi and Abbas Erfanian,"This paper presents a novel approach for efficient feature extraction using mutual information (MI). In terms of mutual information, the optimal feature extraction is creating a feature set from the data which...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2008/673040,10 July 2008,
13,0.000678989392134464,13,Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives,Moming Duan,"Traditional Federated Learning (FL) follows a server-domincated cooperation paradigm which narrows the application scenarios of FL and decreases the enthusiasm of data holders to participate. To fully unleash the potential of FL, we advocate rethinking the design of current FL frameworks and extending it to a more generalized concept: Open Federated Learning Platforms. We propose two reciprocal cooperation frameworks for FL to achieve this: query-based FL and contract-based FL. In this survey, we conduct a comprehensive review of the feasibility of constructing an open FL platform from both technical and legal perspectives. We begin by reviewing the definition of FL and summarizing its inherent limitations, including server-client coupling, low model reusability, and non-public. In the query-based FL platform, which is an open model sharing and reusing platform empowered by the community for model mining, we explore a wide range of valuable topics, including the availability of up-to-date model repositories for model querying, legal compliance analysis between different model licenses, and copyright issues and intellectual property protection in model reusing. In particular, we introduce a novel taxonomy to streamline the analysis of model license compatibility in FL studies that involve batch model reusing methods, including combination, amalgamation, distillation, and generation. This taxonomy provides a systematic framework for identifying the corresponding clauses of licenses and facilitates the identification of potential legal implications and restrictions when reusing models. Through this survey, we uncover the the current dilemmas faced by FL and advocate for the development of sustainable open FL platforms. We aim to provide guidance for establishing such platforms in the future, while identifying potential problems and challenges that need to be addressed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.02140,July 2023,13,Video-based learning ecosystem to support active learning: application to an introductory computer science course,"Michail N. Giannakos, John Krogstie and Trond Aalberg",The systematic use of technologies in order to orchestrate learning has become widely used in the past years. Diverse technologies have been applied in a variety of teaching practices, for instance learning to...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-016-0036-0,7 July 2016
14,0.000678989392134464,14,LØ: An Accountable Mempool for MEV Resistance,"Bulat Nasrulin, Georgy Ishmaev, Jérémie Decouchant, Johan Pouwelse","Possible manipulation of user transactions by miners in a permissionless blockchain systems is a growing concern. This problem is a pervasive and systemic issue, known as Miner Extractable Value (MEV), incurs highs costs on users of decentralised applications. Furthermore, transaction manipulations create other issues in blockchain systems such as congestion, higher fees, and system instability. Detecting transaction manipulations is difficult, even though it is known that they originate from the pre-consensus phase of transaction selection for a block building, at the base layer of blockchain protocols. In this paper we summarize known transaction manipulation attacks. We then present LØ, an accountable base layer protocol specifically designed to detect and mitigate transaction manipulations. LØ is built around accurate detection of transaction manipulations and assignment of blame at the granularity of a single mining node. LØ forces miners to log all the transactions they receive into a secure mempool data structure and to process them in a verifiable manner. Overall, LØ quickly and efficiently detects reordering, injection or censorship attempts. Our performance evaluation shows that LØ is also practical and only introduces a marginal performance overhead. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.02081,July 2023,14,Network of collaboration among PC members of Brazilian computer science conferences,Ana L. C. Bazzan and V. F. Argenta,"The structure, dynamics, and importance of the social network of collaboration among scientists has been already studied, sometimes yielding counter-intuitive conclusions. In this paper we investigate the role...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0033-7,17 May 2011,
15,0.000678989392134464,15,Vibronic fine structure in the nitrogen 1s photoelectron spectra from Franck-Condon simulations II: Indoles,"Minrui Wei, Lu Zhang, Guangjun Tian, Weijie Hua","The vibronic coupling effect in nitrogen 1s X-ray photoelectron spectra (XPS) was systematically studied for a family of 17 bicyclic indole molecules by combining Franck-Condon simulations (including the Duschinsky rotation effect) and density functional theory. The simulated vibrationally-resolved spectra of 4 molecules agree well with available experiments. Reliable predictions for this family further allowed us to summarize rules for spectral evolution in response to three types of common structural changes (side chain substitution, CH$\leftrightarrow$N replacement, and isomerization). Interestingly, vibronic properties of amine and imine nitrogen are clearly separated: they show negative and positive $Δ$ZPE (zero-point vibration energy of the core-ionized with respect to the ground state), respectively, indicating flatter and steeper PESs induced by the N 1s ionization; amine N's show stronger mode mixing effects than imine N's; the 1s ionizations on two types of nitrogens led to distinct changes in local bond lengths and angles. The rules are useful for a basic understanding of vibronic coupling in this family, and the precise spectra are useful for future reference and data mining studies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.01510,July 2023,15,Macro-void distribution analysis in strand-based wood composites using an X-ray computer tomography technique,Masatoshi Sugimori and Frank Lam,A database from a series of cross-sectional density distributions in a 0.16 × 0.34 × 1.28m strand-based wood composite specimen has been successfully developed using X-ray computer tomography (CT) techniques. ...,https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/BF01177735,1 June 1999,
16,0.000678989392134464,16,Spatio-Temporal Surrogates for Interaction of a Jet with High Explosives: Part II -- Clustering Extremely High-Dimensional Grid-Based Data,"Chandrika Kamath, Juliette S. Franzman","Building an accurate surrogate model for the spatio-temporal outputs of a computer simulation is a challenging task. A simple approach to improve the accuracy of the surrogate is to cluster the outputs based on similarity and build a separate surrogate model for each cluster. This clustering is relatively straightforward when the output at each time step is of moderate size. However, when the spatial domain is represented by a large number of grid points, numbering in the millions, the clustering of the data becomes more challenging. In this report, we consider output data from simulations of a jet interacting with high explosives. These data are available on spatial domains of different sizes, at grid points that vary in their spatial coordinates, and in a format that distributes the output across multiple files at each time step of the simulation. We first describe how we bring these data into a consistent format prior to clustering. Borrowing the idea of random projections from data mining, we reduce the dimension of our data by a factor of thousand, making it possible to use the iterative k-means method for clustering. We show how we can use the randomness of both the random projections, and the choice of initial centroids in k-means clustering, to determine the number of clusters in our data set. Our approach makes clustering of extremely high dimensional data tractable, generating meaningful cluster assignments for our problem, despite the approximation introduced in the random projections. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.01400,July 2023,16,Fixed point theorems in generalized metric spaces with applications to computer science,"Maryam A Alghamdi, Naseer Shahzad and Oscar Valero","In 1994, Matthews introduced the notion of a partial metric space in order to obtain a suitable mathematical tool for program verification (Matthews in Ann. N.Y. Acad. Sci. 728:183-197, 1994). He gave an appli...",https://www.springeropen.com//fixedpointtheoryandalgorithms.springeropen.com/articles/10.1186/1687-1812-2013-118,3 May 2013,
17,0.000678989392134464,17,Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking,"Qinyong Wang, Zhenxiang Gao, Rong Xu","The biomedical field relies heavily on concept linking in various areas such as literature mining, graph alignment, information retrieval, question-answering, data, and knowledge integration. Although large language models (LLMs) have made significant strides in many natural language processing tasks, their effectiveness in biomedical concept mapping is yet to be fully explored. This research investigates a method that exploits the in-context learning (ICL) capabilities of large models for biomedical concept linking. The proposed approach adopts a two-stage retrieve-and-rank framework. Initially, biomedical concepts are embedded using language models, and then embedding similarity is utilized to retrieve the top candidates. These candidates' contextual information is subsequently incorporated into the prompt and processed by a large language model to re-rank the concepts. This approach achieved an accuracy of 90.% in BC5CDR disease entity normalization and 94.7% in chemical entity normalization, exhibiting a competitive performance relative to supervised learning methods. Further, it showed a significant improvement, with an over 20-point absolute increase in F1 score on an oncology matching dataset. Extensive qualitative assessments were conducted, and the benefits and potential shortcomings of using large language models within the biomedical domain were discussed. were discussed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.01137,July 2023,17,Developing a computer-based assessment of complex problem solving in Chemistry,"Ronny Scherer, Jenny Meßinger-Koppelt and Rüdiger Tiemann","Complex problem-solving competence is regarded as a key construct in science education. But due to the necessity of using interactive and intransparent assessment procedures, appropriate measures of the constr...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/2196-7822-1-2,27 August 2014,
18,0.000678989392134464,18,Towards Suicide Prevention from Bipolar Disorder with Temporal Symptom-Aware Multitask Learning,"Daeun Lee, Sejung Son, Hyolim Jeon, Seungbae Kim, Jinyoung Han","Bipolar disorder (BD) is closely associated with an increased risk of suicide. However, while the prior work has revealed valuable insight into understanding the behavior of BD patients on social media, little attention has been paid to developing a model that can predict the future suicidality of a BD patient. Therefore, this study proposes a multi-task learning model for predicting the future suicidality of BD patients by jointly learning current symptoms. We build a novel BD dataset clinically validated by psychiatrists, including 14 years of posts on bipolar-related subreddits written by 818 BD patients, along with the annotations of future suicidality and BD symptoms. We also suggest a temporal symptom-aware attention mechanism to determine which symptoms are the most influential for predicting future suicidality over time through a sequence of BD posts. Our experiments demonstrate that the proposed model outperforms the state-of-the-art models in both BD symptom identification and future suicidality prediction tasks. In addition, the proposed temporal symptom-aware attention provides interpretable attention weights, helping clinicians to apprehend BD patients more comprehensively and to provide timely intervention by tracking mental state progression. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00995,July 2023,18,Focus on the notice: evidence of spatial skills’ effect on middle school learning from a computer simulation,"Colleen M. Epler-Ruths, Scott McDonald, Amy Pallant and Hee-Sun Lee",This article represents the findings from the qualitative portion of a mixed methods study that investigated the impact of middle school students’ spatial skills on their plate tectonics learning while using a...,https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-020-00263-0,25 November 2020,
19,0.000678989392134464,19,"The Building Data Genome Directory -- An open, comprehensive data sharing platform for building performance research","Xiaoyu Jin, Chun Fu, Hussain Kazmi, Atilla Balint, Ada Canaydin, Matias Quintana, Filip Biljecki, Fu Xiao, Clayton Miller","The building sector plays a crucial role in the worldwide decarbonization effort, accounting for significant portions of energy consumption and environmental effects. However, the scarcity of open data sources is a continuous challenge for built environment researchers and practitioners. Although several efforts have been made to consolidate existing open datasets, no database currently offers a comprehensive collection of building data types with all subcategories and time granularities (e.g., year, month, and sub-hour). This paper presents the Building Data Genome Directory, an open data-sharing platform serving as a one-stop shop for the data necessary for vital categories of building energy research. The data directory is an online portal (http://buildingdatadirectory.org/) that allows filtering and discovering valuable datasets. The directory covers meter, building-level, and aggregated community-level data at the spatial scale and year-to-minute level at the temporal scale. The datasets were consolidated from a comprehensive exploration of sources, including governments, research institutes, and online energy dashboards. The results of this effort include the aggregation of 60 datasets pertaining to building energy ontologies, building energy models, building energy and water data, electric vehicle data, weather data, building information data, text-mining-based research data, image data of buildings, fault detection diagnosis data and occupant data. A crowdsourcing mechanism in the platform allows users to submit datasets they suggest for inclusion by filling out an online form. This directory can fuel research and applications on building energy efficiency, which is an essential step toward addressing the world's energy and environmental challenges. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00793,July 2023,19,Analysing domain-specific problem-solving processes within authentic computer-based learning and training environments by using eye-tracking: a scoping review,"Christian W. Mayer, Andreas Rausch and Jürgen Seifried","Recently, many studies have been published on the use of eye-tracking to analyse complex problem-solving processes within authentic computer-based learning and training environments. This scoping review aims t...",https://www.springeropen.com//ervet-journal.springeropen.com/articles/10.1186/s40461-023-00140-2,18 April 2023,
20,0.00461519317491824,20,Numerical Association Rule Mining: A Systematic Literature Review,"Minakshi Kaushik, Rahul Sharma, Iztok Fister Jr., Dirk Draheim","Numerical association rule mining is a widely used variant of the association rule mining technique, and it has been extensively used in discovering patterns and relationships in numerical data. Initially, researchers and scientists integrated numerical attributes in association rule mining using various discretization approaches; however, over time, a plethora of alternative methods have emerged in this field. Unfortunately, the increase of alternative methods has resulted into a significant knowledge gap in understanding diverse techniques employed in numerical association rule mining -- this paper attempts to bridge this knowledge gap by conducting a comprehensive systematic literature review. We provide an in-depth study of diverse methods, algorithms, metrics, and datasets derived from 1,140 scholarly articles published from the inception of numerical association rule mining in the year 1996 to 2022. In compliance with the inclusion, exclusion, and quality evaluation criteria, 68 papers were chosen to be extensively evaluated. To the best of our knowledge, this systematic literature review is the first of its kind to provide an exhaustive analysis of the current literature and previous surveys on numerical association rule mining. The paper discusses important research issues, the current status, and future possibilities of numerical association rule mining. On the basis of this systematic review, the article also presents a novel discretization measure that contributes by providing a partitioning of numerical data that meets well human perception of partitions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00662,July 2023,20,Directly wireless communication of human minds via non-invasive brain-computer-metasurface platform,"Qian Ma, Wei Gao, Qiang Xiao, Lingsong Ding, Tianyi Gao, Yajun Zhou, Xinxin Gao, Tao Yan, Che Liu, Ze Gu, Xianghong Kong, Qammer H. Abbasi, Lianlin Li, Cheng-Wei Qiu, Yuanqing Li and Tie Jun Cui","Brain-computer interfaces (BCIs), invasive or non-invasive, have projected unparalleled vision and promise for assisting patients in need to better their interaction with the surroundings.  Inspired by the BCI...",https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-022-00019-x,11 June 2022,
21,0.000678989392134464,21,STG4Traffic: A Survey and Benchmark of Spatial-Temporal Graph Neural Networks for Traffic Prediction,"Xunlian Luo, Chunjiang Zhu, Detian Zhang, Qing Li","Traffic prediction has been an active research topic in the domain of spatial-temporal data mining. Accurate real-time traffic prediction is essential to improve the safety, stability, and versatility of smart city systems, i.e., traffic control and optimal routing. The complex and highly dynamic spatial-temporal dependencies make effective predictions still face many challenges. Recent studies have shown that spatial-temporal graph neural networks exhibit great potential applied to traffic prediction, which combines sequential models with graph convolutional networks to jointly model temporal and spatial correlations. However, a survey study of graph learning, spatial-temporal graph models for traffic, as well as a fair comparison of baseline models are pending and unavoidable issues. In this paper, we first provide a systematic review of graph learning strategies and commonly used graph convolution algorithms. Then we conduct a comprehensive analysis of the strengths and weaknesses of recently proposed spatial-temporal graph network models. Furthermore, we build a study called STG4Traffic using the deep learning framework PyTorch to establish a standardized and scalable benchmark on two types of traffic datasets. We can evaluate their performance by personalizing the model settings with uniform metrics. Finally, we point out some problems in the current study and discuss future directions. Source codes are available at https://github.com/trainingl/STG4Traffic. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00495,July 2023,21,"Comparing learners’ knowledge, behaviors, and attitudes between two instructional modes of computer programming in secondary education","Dan Sun, Fan Ouyang, Yan Li and Caifeng Zhu","Unplugged programming is proved to be an effective means to foster the learner-centered programming learning. In addition to the final tests, learners’ programming knowledge, skills, and capacities are primari...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00311-1,23 September 2021,
22,0.000678989392134464,22,Spatiotemporal Cluster Analysis of Gridded Temperature Data -- A Comparison Between K-means and MiSTIC,"E Ankitha Reddy, KS Rajan","The Earth is a system of numerous interconnected spheres, such as the climate. Climate's global and regional influence requires understanding its evolution in space and time to improve knowledge and forecasts. Analyzing and studying decades of climate data is a data mining challenge. Cluster analysis minimizes data volumes and analyzes behavior by cluster. Understanding invariant behavior is as crucial as understanding variable behavior. Gridded data from two sources: Grided IMD data and CMIP5 HadCM3 decadal experiments, are studied using K-Means and MiSTIC clustering techniques to explore spatiotemporal clustering of maximum and minimum temperatures. The boundaries of k-means clustering correspond with topography. The Indian subcontinent's physiographic, climatic, and topographical characteristics affect MiSTIC's core areas. Both techniques yield overlapping clusters. The datasets' MiSTIC cluster counts varied significantly. The impact of data on this technique is shown in how the datasets group the Himalayas. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00480,July 2023,22,Computational imaging without a computer: seeing through random diffusers at the speed of light,"Yi Luo, Yifan Zhao, Jingxi Li, Ege Çetintaş, Yair Rivenson, Mona Jarrahi and Aydogan Ozcan","Imaging through diffusers presents a challenging problem with various digital image reconstruction solutions demonstrated to date using computers. Here, we present a computer-free, all-optical image reconstruc...",https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-022-00012-4,26 January 2022,
23,0.000678989392134464,23,Equal Confusion Fairness: Measuring Group-Based Disparities in Automated Decision Systems,"Furkan Gursoy, Ioannis A. Kakadiaris","As artificial intelligence plays an increasingly substantial role in decisions affecting humans and society, the accountability of automated decision systems has been receiving increasing attention from researchers and practitioners. Fairness, which is concerned with eliminating unjust treatment and discrimination against individuals or sensitive groups, is a critical aspect of accountability. Yet, for evaluating fairness, there is a plethora of fairness metrics in the literature that employ different perspectives and assumptions that are often incompatible. This work focuses on group fairness. Most group fairness metrics desire a parity between selected statistics computed from confusion matrices belonging to different sensitive groups. Generalizing this intuition, this paper proposes a new equal confusion fairness test to check an automated decision system for fairness and a new confusion parity error to quantify the extent of any unfairness. To further analyze the source of potential unfairness, an appropriate post hoc analysis methodology is also presented. The usefulness of the test, metric, and post hoc analysis is demonstrated via a case study on the controversial case of COMPAS, an automated decision system employed in the US to assist judges with assessing recidivism risks. Overall, the methods and metrics provided here may assess automated decision systems' fairness as part of a more extensive accountability assessment, such as those based on the system accountability benchmark. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00472,July 2023,23,Schools overcoming the digital divide: in depth analyses towards organizational resilience in the computer and information literacy domain,"Kerstin Drossel, Birgit Eickelmann and Mario Vennemann","The ongoing digitalization poses new challenges for schools concerning students’ digital skills. In this context, the International Computer and Information Literacy Study (IEA-ICILS 2018) has identified subst...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00087-w,15 July 2020,
24,0.000678989392134464,24,Detection of River Sandbank for Sand Mining with the Presence of Other High Mineral Content Regions Using Multi-spectral Images,Jit Mukherjee,"Sand mining is a booming industry. The river sandbank is one of the primary sources of sand mining. Detection of potential river sandbank regions for sand mining directly impacts the economy, society, and environment. In the past, semi-supervised and supervised techniques have been used to detect mining regions including sand mining. A few techniques employ multi-modal analysis combining different modalities such as multi-spectral imaging, synthetic aperture radar (\emph{SAR}) imaging, aerial images, and point cloud data. However, the distinguishing spectral characteristics of river sandbank regions are yet to be fully explored. This paper provides a novel method to detect river sandbank regions for sand mining using multi-spectral images without any labeled data over the seasons. Association with a river stream and the abundance of minerals are the most prominent features of such a region. The proposed work uses these distinguishing features to determine the spectral signature of a river sandbank region, which is robust to other high mineral abundance regions. It follows a two-step approach, where first, potential high mineral regions are detected and next, they are segregated using the presence of a river stream. The proposed technique provides average accuracy, precision, and recall of 90.75%, 85.47%, and 73.5%, respectively over the seasons from Landsat 8 images without using any labeled dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00314,July 2023,24,A deep learning-based brain-computer interaction system for speech and motor impairment,Nader A. Rahman Mohamed,"Some people may experience accidents, strokes, or diseases that lead to both motor and speech disabilities, making it difficult to communicate with others. Those with paralysis face daily challenges in meetin...",https://www.springeropen.com//jeas.springeropen.com/articles/10.1186/s44147-023-00212-w,9 May 2023,
25,0.000678989392134464,25,Information Extraction in Domain and Generic Documents: Findings from Heuristic-based and Data-driven Approaches,"Shiyu Yuan, Carlo Lipizzi","Information extraction (IE) plays very important role in natural language processing (NLP) and is fundamental to many NLP applications that used to extract structured information from unstructured text data. Heuristic-based searching and data-driven learning are two main stream implementation approaches. However, no much attention has been paid to document genre and length influence on IE tasks. To fill the gap, in this study, we investigated the accuracy and generalization abilities of heuristic-based searching and data-driven to perform two IE tasks: named entity recognition (NER) and semantic role labeling (SRL) on domain-specific and generic documents with different length. We posited two hypotheses: first, short documents may yield better accuracy results compared to long documents; second, generic documents may exhibit superior extraction outcomes relative to domain-dependent documents due to training document genre limitations. Our findings reveals that no single method demonstrated overwhelming performance in both tasks. For named entity extraction, data-driven approaches outperformed symbolic methods in terms of accuracy, particularly in short texts. In the case of semantic roles extraction, we observed that heuristic-based searching method and data-driven based model with syntax representation surpassed the performance of pure data-driven approach which only consider semantic information. Additionally, we discovered that different semantic roles exhibited varying accuracy levels with the same method. This study offers valuable insights for downstream text mining tasks, such as NER and SRL, when addressing various document features and genres. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00130,July 2023,25,Domain multiplexed computer-generated holography by embedded wavevector filtering algorithm,Lin Wu and Ziyang Zhang,"Computer-generated holography can obtain the wavefront required for constructing arbitrary intensity distributions in space. Currently, speckle noises in holography remain an issue for most computational metho...",https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-020-00023-9,5 January 2021,
26,0.000678989392134464,26,Improving the Transferability of Time Series Forecasting with Decomposition Adaptation,"Yan Gao, Yan Wang, Qiang Wang","Due to effective pattern mining and feature representation, neural forecasting models based on deep learning have achieved great progress. The premise of effective learning is to collect sufficient data. However, in time series forecasting, it is difficult to obtain enough data, which limits the performance of neural forecasting models. To alleviate the data scarcity limitation, we design Sequence Decomposition Adaptation Network (SeDAN) which is a novel transfer architecture to improve forecasting performance on the target domain by aligning transferable knowledge from cross-domain datasets. Rethinking the transferability of features in time series data, we propose Implicit Contrastive Decomposition to decompose the original features into components including seasonal and trend features, which are easier to transfer. Then we design the corresponding adaptation methods for decomposed features in different domains. Specifically, for seasonal features, we perform joint distribution adaptation and for trend features, we design an Optimal Local Adaptation. We conduct extensive experiments on five benchmark datasets for multivariate time series forecasting. The results demonstrate the effectiveness of our SeDAN. It can provide more efficient and stable knowledge transfer. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00066,July 2023,26,Letter from the Guest Editors,Eduardo Miranda and Marcelo Pimenta,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192560,September 2008,
27,0.000678989392134464,27,SAHAAYAK 2023 -- the Multi Domain Bilingual Parallel Corpus of Sanskrit to Hindi for Machine Translation,"Vishvajitsinh Bakrola, Jitendra Nasariwala","The data article presents the large bilingual parallel corpus of low-resourced language pair Sanskrit-Hindi, named SAHAAYAK 2023. The corpus contains total of 1.5M sentence pairs between Sanskrit and Hindi. To make the universal usability of the corpus and to make it balanced, data from multiple domain has been incorporated into the corpus that includes, News, Daily conversations, Politics, History, Sport, and Ancient Indian Literature. The multifaceted approach has been adapted to make a sizable multi-domain corpus of low-resourced languages like Sanskrit. Our development approach is spanned from creating a small hand-crafted dataset to applying a wide range of mining, cleaning, and verification. We have used the three-fold process of mining: mining from machine-readable sources, mining from non-machine readable sources, and collation from existing corpora sources. Post mining, the dedicated pipeline for normalization, alignment, and corpus cleaning is developed and applied to the corpus to make it ready to use on machine translation algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2307.00021,July 2023,27,Crime forecasting: a machine learning and computer vision approach to crime prediction and prevention,"Neil Shah, Nandish Bhagat and Manan Shah","A crime is a deliberate act that can cause physical or psychological harm, as well as property damage or loss, and can lead to punishment by a state or other authority according to the severity of the crime. T...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00075-z,29 April 2021,
28,0.000678989392134464,28,Hashing-Based Distributed Clustering for Massive High-Dimensional Data,"Yifeng Xiao, Jiang Xue, Deyu Meng","Clustering analysis is of substantial significance for data mining. The properties of big data raise higher demand for more efficient and economical distributed clustering methods. However, existing distributed clustering methods mainly focus on the size of data but ignore possible problems caused by data dimension. To solve this problem, we propose a new distributed algorithm, referred to as Hashing-Based Distributed Clustering (HBDC). Motivated by the outstanding performance of hashing methods for nearest neighbor searching, this algorithm applies the learning-to-hash technique to the clustering problem, which possesses incomparable advantages for data storage, transmission and computation. Following a global-sub-site paradigm, the HBDC consists of distributed training of hashing network and spectral clustering for hash codes at the global site. The sub-sites use the learnable network as a hash function to convert massive HD original data into a small number of hash codes, and send them to the global site for final clustering. In addition, a sample-selection method and slight network structures are designed to accelerate the convergence of the hash network. We also analyze the transmission cost of HBDC, including the upper bound. Our experiments on synthetic and real datasets illustrate the superiority of HBDC compared with existing state-of-the-art algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.17417,June 2023,28,A computer programming hybrid MOOC for Greek secondary education,"Philippos Koutsakas, Charalampos Karagiannidis, Panagiotis Politis and Ilias Karasavvidis","The present paper presents the planning, implementation, observation and analysis of both quantitative and qualitative data of a participatory action research, aiming to study the potential role, value and use...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-0114-1,4 March 2020,
29,0.000678989392134464,29,RECAP-KG: Mining Knowledge Graphs from Raw GP Notes for Remote COVID-19 Assessment in Primary Care,"Rakhilya Lee Mekhtieva, Brandon Forbes, Dalal Alrajeh, Brendan Delaney, Alessandra Russo","Clinical decision-making is a fundamental stage in delivering appropriate care to patients. In recent years several decision-making systems designed to aid the clinician in this process have been developed. However, technical solutions currently in use are based on simple regression models and are only able to take into account simple pre-defined multiple-choice features, such as patient age, pre-existing conditions, smoker status, etc. One particular source of patient data, that available decision-making systems are incapable of processing is the collection of patient consultation GP notes. These contain crucial signs and symptoms - the information used by clinicians in order to make a final decision and direct the patient to the appropriate care. Extracting information from GP notes is a technically challenging problem, as they tend to include abbreviations, typos, and incomplete sentences. This paper addresses this open challenge. We present a framework that performs knowledge graph construction from raw GP medical notes written during or after patient consultations. By relying on support phrases mined from the SNOMED ontology, as well as predefined supported facts from values used in the RECAP (REmote COVID-19 Assessment in Primary Care) patient risk prediction tool, our graph generative framework is able to extract structured knowledge graphs from the highly unstructured and inconsistent format that consultation notes are written in. Our knowledge graphs include information about existing patient symptoms, their duration, and their severity. We apply our framework to consultation notes of COVID-19 patients in the UK COVID-19 Clinical Assesment Servcie (CCAS) patient dataset. We provide a quantitative evaluation of the performance of our framework, demonstrating that our approach has better accuracy than traditional NLP methods when answering questions about patients. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.17175,June 2023,29,Comparison of test performance on paper-based testing (PBT) and computer-based testing (CBT) by English-majored undergraduate students in China,Wenjing Yu and Noriko Iwashita,"Computer-based testing (CBT), which refers to delivering assessments with computers, has been widely used in large English proficiency tests worldwide. Despite an increasing CBT in China, limited research is a...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-021-00147-0,25 November 2021,
30,0.000678989392134464,30,MEMD-ABSA: A Multi-Element Multi-Domain Dataset for Aspect-Based Sentiment Analysis,"Hongjie Cai, Nan Song, Zengzhi Wang, Qiming Xie, Qiankun Zhao, Ke Li, Siwei Wu, Shijie Liu, Jianfei Yu, Rui Xia","Aspect-based sentiment analysis is a long-standing research interest in the field of opinion mining, and in recent years, researchers have gradually shifted their focus from simple ABSA subtasks to end-to-end multi-element ABSA tasks. However, the datasets currently used in the research are limited to individual elements of specific tasks, usually focusing on in-domain settings, ignoring implicit aspects and opinions, and with a small data scale. To address these issues, we propose a large-scale Multi-Element Multi-Domain dataset (MEMD) that covers the four elements across five domains, including nearly 20,000 review sentences and 30,000 quadruples annotated with explicit and implicit aspects and opinions for ABSA research. Meanwhile, we evaluate generative and non-generative baselines on multiple ABSA subtasks under the open domain setting, and the results show that open domain ABSA as well as mining implicit aspects and opinions remain ongoing challenges to be addressed. The datasets are publicly released at \url{https://github.com/NUSTM/MEMD-ABSA}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16956,June 2023,30,Toward computer-supported semi-automated timelines of future events,"Alan de Oliveira Lyra, Carlos Eduardo Barbosa, Yuri Oliveira de Lima, Herbert Salazar dos Santos, Matheus Argôlo and Jano Moreira de Souza","During a Futures Study, researchers analyze a significant quantity of information dispersed across multiple document databases to gather conjectures about future events, making it challenging for researchers t...",https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-023-00216-y,1 April 2023,
31,0.000678989392134464,31,SaaFormer: Spectral-spatial Axial Aggregation Transformer for Hyperspectral Image Classification,"Enzhe Zhao, Zhichang Guo, Yao Li, Dazhi Zhang","Hyperspectral images (HSI) captured from earth observing satellites and aircraft is becoming increasingly important for applications in agriculture, environmental monitoring, mining, etc. Due to the limited available hyperspectral datasets, the pixel-wise random sampling is the most commonly used training-test dataset partition approach, which has significant overlap between samples in training and test datasets. Furthermore, our experimental observations indicates that regions with larger overlap often exhibit higher classification accuracy. Consequently, the pixel-wise random sampling approach poses a risk of data leakage. Thus, we propose a block-wise sampling method to minimize the potential for data leakage. Our experimental findings also confirm the presence of data leakage in models such as 2DCNN. Further, We propose a spectral-spatial axial aggregation transformer model, namely SaaFormer, to address the challenges associated with hyperspectral image classifier that considers HSI as long sequential three-dimensional images. The model comprises two primary components: axial aggregation attention and multi-level spectral-spatial extraction. The axial aggregation attention mechanism effectively exploits the continuity and correlation among spectral bands at each pixel position in hyperspectral images, while aggregating spatial dimension features. This enables SaaFormer to maintain high precision even under block-wise sampling. The multi-level spectral-spatial extraction structure is designed to capture the sensitivity of different material components to specific spectral bands, allowing the model to focus on a broader range of spectral details. The results on six publicly available datasets demonstrate that our model exhibits comparable performance when using random sampling, while significantly outperforming other methods when employing block-wise sampling partition. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16759,June 2023,31,Impacts of computer-assisted diagnostic assessment on sustainability of L2 learners’ collaborative writing improvement and their engagement modes,Natasha Pourdana,"Diagnostic assessment (DIA) is under-researched in second/foreign language education despite its common practice across a wide range of professions such as medicine, mechanics, and information technology. This...",https://www.springeropen.com//sfleducation.springeropen.com/articles/10.1186/s40862-022-00139-4,18 April 2022,
32,0.000678989392134464,32,NCL++: Nested Collaborative Learning for Long-Tailed Visual Recognition,"Zichang Tan, Jun Li, Jinhao Du, Jun Wan, Zhen Lei, Guodong Guo","Long-tailed visual recognition has received increasing attention in recent years. Due to the extremely imbalanced data distribution in long-tailed learning, the learning process shows great uncertainties. For example, the predictions of different experts on the same image vary remarkably despite the same training settings. To alleviate the uncertainty, we propose a Nested Collaborative Learning (NCL++) which tackles the long-tailed learning problem by a collaborative learning. To be specific, the collaborative learning consists of two folds, namely inter-expert collaborative learning (InterCL) and intra-expert collaborative learning (IntraCL). In-terCL learns multiple experts collaboratively and concurrently, aiming to transfer the knowledge among different experts. IntraCL is similar to InterCL, but it aims to conduct the collaborative learning on multiple augmented copies of the same image within the single expert. To achieve the collaborative learning in long-tailed learning, the balanced online distillation is proposed to force the consistent predictions among different experts and augmented copies, which reduces the learning uncertainties. Moreover, in order to improve the meticulous distinguishing ability on the confusing categories, we further propose a Hard Category Mining (HCM), which selects the negative categories with high predicted scores as the hard categories. Then, the collaborative learning is formulated in a nested way, in which the learning is conducted on not just all categories from a full perspective but some hard categories from a partial perspective. Extensive experiments manifest the superiority of our method with outperforming the state-of-the-art whether with using a single model or an ensemble. The code will be publicly released. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16709,June 2023,32,Prevalence rate of attention deficit hyperactivity disorder (ADHD) and computer vision syndrome (CVS) symptoms predisposition among digital device users of Bangladesh,"Z Islam, M Rahman, A H Olive and MK Hasan","Around 5.29% of the world population is suffering from ADHD, and 60 million people are suffering from CVS, with an increasing rate of prevalence of these disorders. This study aimed to determine the prevalence...",https://www.springeropen.com//mecp.springeropen.com/articles/10.1186/s43045-022-00176-2,25 January 2022,
33,0.000678989392134464,33,Towards Blockchain-Assisted Privacy-Aware Data Sharing For Edge Intelligence: A Smart Healthcare Perspective,"Youyang Qu, Lichuan Ma, Wenjie Ye, Xuemeng Zhai, Shui Yu, Yunfeng Li, David Smith","The popularization of intelligent healthcare devices and big data analytics significantly boosts the development of smart healthcare networks (SHNs). To enhance the precision of diagnosis, different participants in SHNs share health data that contains sensitive information. Therefore, the data exchange process raises privacy concerns, especially when the integration of health data from multiple sources (linkage attack) results in further leakage. Linkage attack is a type of dominant attack in the privacy domain, which can leverage various data sources for private data mining. Furthermore, adversaries launch poisoning attacks to falsify the health data, which leads to misdiagnosing or even physical damage. To protect private health data, we propose a personalized differential privacy model based on the trust levels among users. The trust is evaluated by a defined community density, while the corresponding privacy protection level is mapped to controllable randomized noise constrained by differential privacy. To avoid linkage attacks in personalized differential privacy, we designed a noise correlation decoupling mechanism using a Markov stochastic process. In addition, we build the community model on a blockchain, which can mitigate the risk of poisoning attacks during differentially private data transmission over SHNs. To testify the effectiveness and superiority of the proposed approach, we conduct extensive experiments on benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16630,June 2023,33,Contributions of Jayme Luiz Szwarcfiter to graph theory and computer science,Cláudio Leonardo Lucchesi,"This is an account of Jayme’s contributions to graph theory and computer science. Due to restrictions in length, it is not possible to provide an in-depth coverage of every aspect of Jayme’s extensive scientif...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0054-2,25 January 2012,
34,0.000678989392134464,34,Feature Selection: A perspective on inter-attribute cooperation,"Gustavo Sosa-Cabrera, Santiago Gómez-Guerrero, Miguel García-Torres, Christian E. Schaerer","High-dimensional datasets depict a challenge for learning tasks in data mining and machine learning. Feature selection is an effective technique in dealing with dimensionality reduction. It is often an essential data processing step prior to applying a learning algorithm. Over the decades, filter feature selection methods have evolved from simple univariate relevance ranking algorithms to more sophisticated relevance-redundancy trade-offs and to multivariate dependencies-based approaches in recent years. This tendency to capture multivariate dependence aims at obtaining unique information about the class from the intercooperation among features. This paper presents a comprehensive survey of the state-of-the-art work on filter feature selection methods assisted by feature intercooperation, and summarizes the contributions of different approaches found in the literature. Furthermore, current issues and challenges are introduced to identify promising future research and development. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16559,June 2023,34,Temperature dependence of the short-range repulsion between hydrated phospholipid membranes: A computer simulation study,Alexander Pertsin and Michael Grunze,The temperature dependence of the short-range water-mediated repulsive pressure between supported phospholipid membranes is calculated at two intermembrane separations using the grand canonical Monte Carlo tec...,https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.2771541,21 August 2007,
35,0.000678989392134464,35,cuSLINK: Single-linkage Agglomerative Clustering on the GPU,"Corey J. Nolet, Divye Gala, Alex Fender, Mahesh Doijade, Joe Eaton, Edward Raff, John Zedlewski, Brad Rees, Tim Oates","In this paper, we propose cuSLINK, a novel and state-of-the-art reformulation of the SLINK algorithm on the GPU which requires only $O(Nk)$ space and uses a parameter $k$ to trade off space and time. We also propose a set of novel and reusable building blocks that compose cuSLINK. These building blocks include highly optimized computational patterns for $k$-NN graph construction, spanning trees, and dendrogram cluster extraction. We show how we used our primitives to implement cuSLINK end-to-end on the GPU, further enabling a wide range of real-world data mining and machine learning applications that were once intractable. In addition to being a primary computational bottleneck in the popular HDBSCAN algorithm, the impact of our end-to-end cuSLINK algorithm spans a large range of important applications, including cluster analysis in social and computer networks, natural language processing, and computer vision. Users can obtain cuSLINK at https://docs.rapids.ai/api/cuml/latest/api/#agglomerative-clustering △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16354,June 2023,35,Usability of mobile applications: literature review and rationale for a new usability model,"Rachel Harrison, Derek Flood and David Duce",The usefulness of mobile devices has increased greatly in recent years allowing users to perform more tasks in a mobile context. This increase in usefulness has come at the expense of the usability of these de...,https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/2194-0827-1-1,7 May 2013,
36,0.000678989392134464,36,Low-Confidence Samples Mining for Semi-supervised Object Detection,"Guandu Liu, Fangyuan Zhang, Tianxiang Pan, Bin Wang","Reliable pseudo-labels from unlabeled data play a key role in semi-supervised object detection (SSOD). However, the state-of-the-art SSOD methods all rely on pseudo-labels with high confidence, which ignore valuable pseudo-labels with lower confidence. Additionally, the insufficient excavation for unlabeled data results in an excessively low recall rate thus hurting the network training. In this paper, we propose a novel Low-confidence Samples Mining (LSM) method to utilize low-confidence pseudo-labels efficiently. Specifically, we develop an additional pseudo information mining (PIM) branch on account of low-resolution feature maps to extract reliable large-area instances, the IoUs of which are higher than small-area ones. Owing to the complementary predictions between PIM and the main branch, we further design self-distillation (SD) to compensate for both in a mutually-learning manner. Meanwhile, the extensibility of the above approaches enables our LSM to apply to Faster-RCNN and Deformable-DETR respectively. On the MS-COCO benchmark, our method achieves 3.54% mAP improvement over state-of-the-art methods under 5% labeling ratios. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.16201,June 2023,36,Feature Selection and Blind Source Separation in an EEG-Based Brain-Computer Interface,"David A. Peterson, James N. Knight, Michael J. Kirby, Charles W. Anderson and Michael H. Thaut","Most EEG-based BCI systems make use of well-studied patterns of brain activity. However, those systems involve tasks that indirectly map to simple binary commands such as ""yes"" or ""no"" or require many weeks of...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.3128,17 November 2005,
37,0.000678989392134464,37,Image-based Communication on Social Coding Platforms,"Maleknaz Nayebi, Bram Adams","Visual content in the form of images and videos has taken over general-purpose social networks in a variety of ways, streamlining and enriching online communications. We are interested to understand if and to what extent the use of images is popular and helpful in social coding platforms. We mined nine years of data from two popular software developers' platforms: the Mozilla issue tracking system, i.e., Bugzilla, and the most well-known platform for developers' Q/A, i.e., Stack Overflow. We further triangulated and extended our mining results by performing a survey with 168 software developers. We observed that, between 2013 and 2022, the number of posts containing image data on Bugzilla and Stack Overflow doubled. Furthermore, we found that sharing images makes other developers engage more and faster with the content. In the majority of cases in which an image is included in a developer's post, the information in that image is complementary to the text provided. Finally, our results showed that when an image is shared, understanding the content without the information in the image is unlikely for 86.9\% of the cases. Based on these observations, we discuss the importance of considering visual content when analyzing developers and designing automation tools. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.15851,June 2023,37,The effects of computer-based virtual learning environments on nursing students’ mathematical learning in medication processes,"Diana P. Zwart, Sui Lin Goei, Omid Noroozi and Johannes E. H. Van Luit",Computer-based virtual learning environments (CBVLEs) are potentially useful teaching tools for training nursing students in professional duties such as the mathematical tasks associated with medication proces...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-021-00147-x,12 February 2021,
38,0.000678989392134464,38,DMNER: Biomedical Entity Recognition by Detection and Matching,"Junyi Bian, Rongze Jiang, Weiqi Zhai, Tianyang Huang, Hong Zhou, Shanfeng Zhu","Biomedical named entity recognition (BNER) serves as the foundation for numerous biomedical text mining tasks. Unlike general NER, BNER require a comprehensive grasp of the domain, and incorporating external knowledge beyond training data poses a significant challenge. In this study, we propose a novel BNER framework called DMNER. By leveraging existing entity representation models SAPBERT, we tackle BNER as a two-step process: entity boundary detection and biomedical entity matching. DMNER exhibits applicability across multiple NER scenarios: 1) In supervised NER, we observe that DMNER effectively rectifies the output of baseline NER models, thereby further enhancing performance. 2) In distantly supervised NER, combining MRC and AutoNER as span boundary detectors enables DMNER to achieve satisfactory results. 3) For training NER by merging multiple datasets, we adopt a framework similar to DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the training. Through extensive experiments conducted on 10 benchmark datasets, we demonstrate the versatility and effectiveness of DMNER. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.15736,June 2023,38,Vasorelaxing and antihypertensive activities of synthesized peptides derived from computer-aided simulation of pepsin hydrolysis of yam dioscorin,"Yin-Shiou Lin, Yeh-Lin Lu, Guei-Jane Wang, Hong-Jen Liang and Wen-Chi Hou","We reported that yam dioscorin and its peptic hydrolysates exhibited ACE inhibition and antihypertensive effects on SHRs, however, the active peptides are not really isolated until now. Using ACE inhibitory sc...",https://www.springeropen.com//as-botanicalstudies.springeropen.com/articles/10.1186/s40529-014-0049-3,7 June 2014,
39,0.000678989392134464,39,MIMIC: Masked Image Modeling with Image Correspondences,"Kalyani Marathe, Mahtab Bigverdi, Nishat Khan, Tuhin Kundu, Aniruddha Kembhavi, Linda G. Shapiro, Ranjay Krishna","Many pixelwise dense prediction tasks-depth estimation and semantic segmentation in computer vision today rely on pretrained image representations. Therefore, curating effective pretraining datasets is vital. Unfortunately, the effective pretraining datasets are those with multi-view scenes and have only been curated using annotated 3D meshes, point clouds, and camera parameters from simulated environments. We propose a dataset-curation mechanism that does not require any annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with 3.1M multi-view image pairs from open-sourced video datasets and from synthetic 3D environments. We train multiple self-supervised models with different masked image modeling objectives to showcase the following findings: Representations trained on MIMIC-3M outperform those mined using annotations on multiple downstream tasks, including depth estimation, semantic segmentation, surface normals, and pose estimation. They also outperform representations that are frozen and when downstream training data is limited to few-shot. Larger dataset (MIMIC-3M) significantly improves performance, which is promising since our curation method can arbitrarily scale to produce even larger datasets. MIMIC code, dataset, and pretrained models are open-sourced at https://github.com/RAIVNLab/MIMIC. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.15128,June 2023,39,Accelerating quantum computer developments,"Garrelt J. N. Alberts, M. Adriaan Rol, Thorsten Last, Benno W. Broer, Cornelis C. Bultink, Matthijs S. C. Rijlaarsdam and Amber E. Van Hauwermeiren","Given the recent breakthroughs in quantum technology development in R& D labs all over the world, the perspective of high-tech companies has changed.Productdevelopment is initiated next to the existingresearch",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00107-w,8 July 2021,
40,0.000678989392134464,40,Hard Sample Mining Enabled Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis,"Zixuan Wang, Bo Qin, Mengxuan Li, Mark D. Butala, Haibo Wang, Peng Peng, Hongwei Wang","The efficient utilization of wind power by wind turbines relies on the ability of their pitch systems to adjust blade pitch angles in response to varying wind speeds. However, the presence of multiple fault types in the pitch system poses challenges in accurately classifying these faults. This paper proposes a novel method based on hard sample mining-enabled contrastive feature learning (HSMCFL) to address this problem. The proposed method employs cosine similarity to identify hard samples and subsequently leverages contrastive feature learning to enhance representation learning through the construction of hard sample pairs. Furthermore, a multilayer perceptron is trained using the learned discriminative representations to serve as an efficient classifier. To evaluate the effectiveness of the proposed method, two real datasets comprising wind turbine pitch system cog belt fracture data are utilized. The fault diagnosis performance of the proposed method is compared against existing methods, and the results demonstrate its superior performance. The proposed approach exhibits significant improvements in fault diagnosis accuracy, providing promising prospects for enhancing the reliability and efficiency of wind turbine pitch system fault diagnosis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.14701,June 2023,40,Activity Theory and e-Course Design: An Experience in Discrete Mathematics for Computer Science,"José Luis Ramírez, Manuel Juárez and Ana Remesal",The aim of this article is to present a distance e-learning experience of mathematics in higher education. The course is offered as a remedial program for master’s degree students of Computer Science. It was d...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.7238/rusc.v9i1.1264,15 January 2012,
41,0.00435011815751728,41,Interpretable Sparsification of Brain Graphs: Better Practices and Effective Designs for Graph Neural Networks,"Gaotang Li, Marlena Duda, Xiang Zhang, Danai Koutra, Yujun Yan","Brain graphs, which model the structural and functional relationships between brain regions, are crucial in neuroscientific and clinical applications involving graph classification. However, dense brain graphs pose computational challenges including high runtime and memory usage and limited interpretability. In this paper, we investigate effective designs in Graph Neural Networks (GNNs) to sparsify brain graphs by eliminating noisy edges. While prior works remove noisy edges based on explainability or task-irrelevant properties, their effectiveness in enhancing performance with sparsified graphs is not guaranteed. Moreover, existing approaches often overlook collective edge removal across multiple graphs. To address these issues, we introduce an iterative framework to analyze different sparsification models. Our findings are as follows: (i) methods prioritizing interpretability may not be suitable for graph sparsification as they can degrade GNNs' performance in graph classification tasks; (ii) simultaneously learning edge selection with GNN training is more beneficial than post-training; (iii) a shared edge selection across graphs outperforms separate selection for each graph; and (iv) task-relevant gradient information aids in edge selection. Based on these insights, we propose a new model, Interpretable Graph Sparsification (IGS), which enhances graph classification performance by up to 5.1% with 55.0% fewer edges. The retained edges identified by IGS provide neuroscientific interpretations and are supported by well-established literature. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.14375,June 2023,41,Prevalence of musculoskeletal pain among computer users working from home during the COVID-19 pandemic: a cross-sectional survey,"Lakshita Gosain, Irshad Ahmad, Moattar Raza Rizvi, Ankita Sharma and Shobhit Saxena","Office employees are at a greater risk for musculoskeletal disorders (MSD) due to their prolonged computer use. In the context of COVID-19, an unanticipated shift to working from home is likely to increase MSD...",https://www.springeropen.com//bfpt.springeropen.com/articles/10.1186/s43161-022-00110-x,30 December 2022,
42,0.00435011815751728,42,GPT-assisted learning of structure-property relationships by graph neural networks: Application to rare-earth doped phosphors,"Xiang Zhang, Zichun Zhou, Chen Ming, Yi-Yang Sun","Applications of machine learning techniques in materials science are often based on two key ingredients, a set of empirical descriptors and a database of a particular material property of interest. The advent of graph neural networks, such as the Crystal Graph Convolutional Neural Network (CGCNN), demonstrates the possibility of directly mapping the relationship between material structures and properties without employing empirical descriptors. Another exciting recent advancement is in large language models such as OpenAI's GPT-4, which demonstrates competency at reading comprehension tasks and holds great promise for accelerating the acquisition of databases on material properties. Here, we utilize the combination of GPT-4 and CGCNN to develop rare-earth doped phosphors for solid-state lighting. GPT-4 is applied to data-mine chemical formulas and emission wavelengths of 264 Eu(II)-doped phosphors from 274 papers. A CGCNN model is trained on the acquired dataset, achieving a test $R^2$ of 0.77. The model is then used to screen over 40,000 inorganic materials to make predictions on the emission wavelengths. We also demonstrate the possibility of leveraging transfer learning to fine-tune a bandgap-predicting CGCNN model towards the prediction of phosphor emission wavelengths. The workflow requires minimal human supervision, little domain knowledge about phosphors, and is generalizable to other material properties. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.14238,June 2023,42,Editorial: Journal of Interaction Science,Gisela Susanne Bahr,Unknown,https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/2194-0827-1-3,8 May 2013,
43,0.000678989392134464,43,Mining Stable Preferences: Adaptive Modality Decorrelation for Multimedia Recommendation,"Jinghao Zhang, Qiang Liu, Shu Wu, Liang Wang","Multimedia content is of predominance in the modern Web era. In real scenarios, multiple modalities reveal different aspects of item attributes and usually possess different importance to user purchase decisions. However, it is difficult for models to figure out users' true preference towards different modalities since there exists strong statistical correlation between modalities. Even worse, the strong statistical correlation might mislead models to learn the spurious preference towards inconsequential modalities. As a result, when data (modal features) distribution shifts, the learned spurious preference might not guarantee to be as effective on the inference set as on the training set. We propose a novel MOdality DEcorrelating STable learning framework, MODEST for brevity, to learn users' stable preference. Inspired by sample re-weighting techniques, the proposed method aims to estimate a weight for each item, such that the features from different modalities in the weighted distribution are decorrelated. We adopt Hilbert Schmidt Independence Criterion (HSIC) as independence testing measure which is a kernel-based method capable of evaluating the correlation degree between two multi-dimensional and non-linear variables. Our method could be served as a play-and-plug module for existing multimedia recommendation backbones. Extensive experiments on four public datasets and four state-of-the-art multimedia recommendation backbones unequivocally show that our proposed method can improve the performances by a large margin. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.14179,June 2023,43,"Computer-aided molecular modeling studies of some 2, 3-dihydro-[1, 4] dioxino [2, 3-f] quinazoline derivatives as EGFRWTinhibitors","Muhammad Tukur Ibrahim, Adamu Uzairu, Gideon Adamu Shallangwa and Sani Uba",Quinazoline are known to possess different biological activities which among is anti-cancer most especially NSCLC. Epidermal growth factor receptor (EGFR) belongs to the receptor tyrosine kinases (RTKs) family...,https://www.springeropen.com//bjbas.springeropen.com/articles/10.1186/s43088-020-00047-x,10 April 2020,
44,0.000678989392134464,44,Aircraft Environmental Impact Segmentation via Metric Learning,"Zhenyu Gao, Dimitri N. Mavris","Metric learning is the process of learning a tailored distance metric for a particular task. This advanced subfield of machine learning is useful to any machine learning or data mining task that relies on the computation of distances or similarities over objects. In recently years, machine learning techniques have been extensively used in aviation and aerospace engineering to make predictions, extract patterns, discover knowledge, etc. Nevertheless, metric learning, an element that can advance the performance of complex machine learning tasks, has so far been hardly utilized in relevant literature. In this study, we apply classic metric learning formulations with novel components on aviation environmental impact modeling. Through a weakly-supervised metric learning task, we achieve significant improvement in the newly emerged problem of aircraft characterization and segmentation for environmental impacts. The result will enable the more efficient and accurate modeling of aircraft environmental impacts, a focal topic in sustainable aviation. This work is also a demonstration that shows the potential and value of metric learning in a wide variety of similar studies in the transportation domain. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.13830,June 2023,44,In silico cell and tissue science,Liesbet Geris and Fred J Vermolen,Unknown,https://www.springeropen.com//in-silico-cell-and-tissue-science.springeropen.com/articles/10.1186/2196-050X-1-1,23 October 2014,
45,0.00673579331412596,45,Efficient Model Selection for Predictive Pattern Mining Model by Safe Pattern Pruning,"Takumi Yoshida, Hiroyuki Hanada, Kazuya Nakagawa, Kouichi Taji, Koji Tsuda, Ichiro Takeuchi","Predictive pattern mining is an approach used to construct prediction models when the input is represented by structured data, such as sets, graphs, and sequences. The main idea behind predictive pattern mining is to build a prediction model by considering substructures, such as subsets, subgraphs, and subsequences (referred to as patterns), present in the structured data as features of the model. The primary challenge in predictive pattern mining lies in the exponential growth of the number of patterns with the complexity of the structured data. In this study, we propose the Safe Pattern Pruning (SPP) method to address the explosion of pattern numbers in predictive pattern mining. We also discuss how it can be effectively employed throughout the entire model building process in practical data analysis. To demonstrate the effectiveness of the proposed method, we conduct numerical experiments on regression and classification problems involving sets, graphs, and sequences. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.13561,June 2023,45,A predictable smoothing evolution model for computer-controlled polishing,"Jing Hou, Pengli Lei, Shiwei Liu, Xianhua Chen, Jian Wang, Wenhui Deng and Bo Zhong",Quantitative prediction of the smoothing of mid-spatial frequency errors (MSFE) is urgently needed to realize process guidance for computer controlled optical surfacing (CCOS) rather than a qualitative analysi...,https://www.springeropen.com//jeos.springeropen.com/articles/10.1186/s41476-020-00145-4,25 November 2020,
46,0.000678989392134464,46,"Identification of the most important external features of highly cited scholarly papers through 3 (i.e., Ridge, Lasso, and Boruta) feature selection data mining methods","Sepideh Fahimifar, Khadijeh Mousavi, Fatemeh Mozaffari, Marcel Ausloos","Highly cited papers are influenced by external factors that are not directly related to the document's intrinsic quality. In this study, 50 characteristics for measuring the performance of 68 highly cited papers, from the Journal of the American Medical Informatics Association indexed in Web of Sciences (WoS), from 2009 to 2019 were investigated. In the first step, a Pearson correlation analysis is performed to eliminate variables with zero or weak correlation with the target (dependent) variable ([number of citations in WOS]). Consequently, 32 variables are selected for the next step. By applying the Ridge technique, 13 features show a positive effect on the number of citations. Using three different algorithms, i.e., Ridge, Lasso, and Boruta, 6 factors appear to be the most relevant ones. The [Number of citations by international researchers], [Journal self-citations in citing documents], and [Authors' self-citations in citing documents], are recognized as the most important features by all three methods here used. The [First author's scientific age], [Open-access paper], and [Number of first author's citations in WOS] are identified as the important features of highly cited papers by only two methods, Ridge and Lasso. Notice that we use specific machine learning algorithms as feature selection methods (Ridge, Lasso, and Boruta) to identify the most important features of highly cited papers, tools that had not previously been used for this purpose. In conclusion, we re-emphasize the performance resulting from such algorithms. Moreover, we do not advise authors to seek to increase the citations of their articles by manipulating the identified performance features. Indeed, ethical rules regarding these characteristics must be strictly obeyed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.13492,June 2023,46,Computer-assisted horizontal translational osseous genioplasty: a simple method to correct chin deviation,"Seied Omid Keyhan, Abbas Azari, Parisa Yousefi, Behzad Cheshmi, Hamid Reza Fallahi and Mohammad Amin Valipour",Different genioplasty techniques are applied for the adjustment of chin area deformities such as chin deviation.,https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-020-00278-z,20 October 2020,
47,0.000678989392134464,47,On Hate Scaling Laws For Data-Swamps,"Abeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti","`Scale the model, scale the data, scale the GPU-farms' is the reigning sentiment in the world of generative AI today. While model scaling has been extensively studied, data scaling and its downstream impacts remain under explored. This is especially of critical importance in the context of visio-linguistic datasets whose main source is the World Wide Web, condensed and packaged as the CommonCrawl dump. This large scale data-dump, which is known to have numerous drawbacks, is repeatedly mined and serves as the data-motherlode for large generative models. In this paper, we: 1) investigate the effect of scaling datasets on hateful content through a comparative audit of the LAION-400M and LAION-2B-en, containing 400 million and 2 billion samples respectively, and 2) evaluate the downstream impact of scale on visio-linguistic models trained on these dataset variants by measuring racial bias of the models trained on them using the Chicago Face Dataset (CFD) as a probe. Our results show that 1) the presence of hateful content in datasets, when measured with a Hate Content Rate (HCR) metric on the inferences of the Pysentimiento hate-detection Natural Language Processing (NLP) model, increased by nearly $12\%$ and 2) societal biases and negative stereotypes were also exacerbated with scale on the models we evaluated. As scale increased, the tendency of the model to associate images of human faces with the `human being' class over 7 other offensive classes reduced by half. Furthermore, for the Black female category, the tendency of the model to associate their faces with the `criminal' class doubled, while quintupling for Black male faces. We present a qualitative and historical analysis of the model audit results, reflect on our findings and its implications for dataset curation practice, and close with a summary of our findings and potential future work to be done in this area. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.13141,June 2023,47,Evaluating environmental factors using microclimate survey and computer fluid dynamics analysis of Korean traditional wooden architectural cultural heritage: focusing on the Kim Myeong-KwanGotaek,YunSang Kim,"Preservation of traditional wooden buildings is important to extend their life and to adapt them to modern society. Wood can be subject to decay, cracking, and discoloration when exposed to various climatic en...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-023-00951-2,23 May 2023,
48,0.000678989392134464,48,Generating Synergistic Formulaic Alpha Collections via Reinforcement Learning,"Shuo Yu, Hongyan Xue, Xiang Ao, Feiyang Pan, Jia He, Dandan Tu, Qing He","In the field of quantitative trading, it is common practice to transform raw historical stock data into indicative signals for the market trend. Such signals are called alpha factors. Alphas in formula forms are more interpretable and thus favored by practitioners concerned with risk. In practice, a set of formulaic alphas is often used together for better modeling precision, so we need to find synergistic formulaic alpha sets that work well together. However, most traditional alpha generators mine alphas one by one separately, overlooking the fact that the alphas would be combined later. In this paper, we propose a new alpha-mining framework that prioritizes mining a synergistic set of alphas, i.e., it directly uses the performance of the downstream combination model to optimize the alpha generator. Our framework also leverages the strong exploratory capabilities of reinforcement learning~(RL) to better explore the vast search space of formulaic alphas. The contribution to the combination models' performance is assigned to be the return used in the RL process, driving the alpha generator to find better alphas that improve upon the current set. Experimental evaluations on real-world stock market data demonstrate both the effectiveness and the efficiency of our framework for stock trend forecasting. The investment simulation results show that our framework is able to achieve higher returns compared to previous approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.12964,June 2023,48,Using response time to investigate students' test-taking behaviors in a NAEP computer-based study,Yi-Hsuan Lee and Yue Jia,"Large-scale survey assessments have been used for decades to monitor what students know and can do. Such assessments aim at providing group-level scores for various populations, with little or no consequence t...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-014-0008-1,17 September 2014,
49,0.000678989392134464,49,xSIM++: An Improved Proxy to Bitext Mining Performance for Low-Resource Languages,"Mingda Chen, Kevin Heffernan, Onur Çelebi, Alex Mourachko, Holger Schwenk","We introduce a new proxy score for evaluating bitext mining based on similarity in a multilingual embedding space: xSIM++. In comparison to xSIM, this improved proxy leverages rule-based approaches to extend English sentences in any evaluation set with synthetic, hard-to-distinguish examples which more closely mirror the scenarios we encounter during large-scale mining. We validate this proxy by running a significant number of bitext mining experiments for a set of low-resource languages, and subsequently train NMT systems on the mined data. In comparison to xSIM, we show that xSIM++ is better correlated with the downstream BLEU scores of translation systems trained on mined bitexts, providing a reliable proxy of bitext mining performance without needing to run expensive bitext mining pipelines. xSIM++ also reports performance for different error types, offering more fine-grained feedback for model development. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.12907,June 2023,49,The relationship between differences in students’ computer and information literacy and response times: an analysis of IEA-ICILS data,"Melanie Heldt, Corinna Massek, Kerstin Drossel and Birgit Eickelmann","Due to the increasing use of information and communication technology, computer-related skills are important for all students in order to participate in the digital age (Fraillon, J., Ainley, J., Schulz, W., F...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00090-1,23 October 2020,
50,0.000678989392134464,50,Constructing Colloquial Dataset for Persian Sentiment Analysis of Social Microblogs,"Mojtaba Mazoochi, Leyla Rabiei, Farzaneh Rahmani, Zeinab Rajabi","Introduction: Microblogging websites have massed rich data sources for sentiment analysis and opinion mining. In this regard, sentiment classification has frequently proven inefficient because microblog posts typically lack syntactically consistent terms and representatives since users on these social networks do not like to write lengthy statements. Also, there are some limitations to low-resource languages. The Persian language has exceptional characteristics and demands unique annotated data and models for the sentiment analysis task, which are distinctive from text features within the English dialect. Method: This paper first constructs a user opinion dataset called ITRC-Opinion by collaborative environment and insource way. Our dataset contains 60,000 informal and colloquial Persian texts from social microblogs such as Twitter and Instagram. Second, this study proposes a new deep convolutional neural network (CNN) model for more effective sentiment analysis of colloquial text in social microblog posts. The constructed datasets are used to evaluate the presented model. Furthermore, some models, such as LSTM, CNN-RNN, BiLSTM, and BiGRU with different word embeddings, including Fasttext, Glove, and Word2vec, investigated our dataset and evaluated the results. Results: The results demonstrate the benefit of our dataset and the proposed model (72% accuracy), displaying meaningful improvement in sentiment classification performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.12679,June 2023,50,"Motivating students’ participation in a computer networks course by means of magic, drama and games",Constantinos S Hilas and Anastasios Politis,The recent economic crisis has forced many universities to cut down expenses by packing students into large lecture groups. The problem with large auditoria is that they discourage dialogue between students an...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-3-362,16 July 2014,
51,0.000678989392134464,51,Design of Energy Harvesting based Hardware for IoT Applications,"Satyajaswanth Badri, Mukesh Saini, Neeraj Goel","Internet of Things (IoT) devices are rapidly expanding in many areas, including deep mines, space, industrial environments, and health monitoring systems. Most of the sensors and actuators are battery-powered, and these batteries have a finite lifespan. Maintaining and replacing these many batteries increases the maintenance cost of IoT systems and causes massive environmental damage. Energy-harvesting devices (EHDs) are the alternative and promising solution for these battery-operated IoT devices. These EHDs collect energy from the environment and use it for daily computations, like collecting and processing data from the sensors and actuators. Using EHDs in IoT reduces overall maintenance costs and makes the IoT system energy-sufficient. However, energy availability from these EHDs is unpredictable, resulting in frequent power failures. Most of these devices use volatile memories as storage elements, implying that all collected data and decisions made by the IoT devices are lost during frequent power failures, resulting in two possible overheads. First, the IoT device must execute the application from the beginning whenever power comes back. Second, IoT devices may make wrong decisions by considering incomplete data, i.e., data-inconsistency issues. To address these two challenges, a computing model is required that backs up the collected data during power failures and restores it for later computations; this type of computing is defined as intermittent computing. However, this computing model doesn't work with conventional processors or memories. Non-volatile memory and processors are required to design a battery-less IoT device that supports intermittent computing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.12019,June 2023,51,Assessing Japanese junior high school students’ English achievement through computer-based testing in the classroom: a case of integrated reading-into-writing continuous task,Noriyasu Niimi and Nobukazu Matsuura,This paper describes the exploratory case and initial evaluation of the computer-based testing (CBT) prototype. The advantage of CBT over paper-based testing (PBT) is that it allows us to control the order of ...,https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-022-00189-y,13 September 2022,
52,0.000678989392134464,52,Hypergraph Classification via Persistent Homology,"Mehmet Emin Aktas, Thu Nguyen, Rakin Riza, Muhammad Ifte Islam, Esra Akbas","Persistent homology is a mathematical tool used for studying the shape of data by extracting its topological features. It has gained popularity in network science due to its applicability in various network mining problems, including clustering, graph classification, and graph neural networks. The definition of persistent homology for graphs is relatively straightforward, as graphs possess distinct intrinsic distances and a simplicial complex structure. However, hypergraphs present a challenge in preserving topological information since they may not have a simplicial complex structure. In this paper, we define several topological characterizations of hypergraphs in defining hypergraph persistent homology to prioritize different higher-order structures within hypergraphs. We further use these persistent homology filtrations in classifying four different real-world hypergraphs and compare their performance to the state-of-the-art graph neural network models. Experimental results demonstrate that persistent homology filtrations are effective in classifying hypergraphs and outperform the baseline models. To the best of our knowledge, this study represents the first systematic attempt to tackle the hypergraph classification problem using persistent homology. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11484,June 2023,52,Computer-assisted implant placement and full-arch immediate loading with digitally prefabricated provisional prostheses without cast: a prospective pilot cohort study,"Nikolay Makarov, Giorgio Pompa and Piero Papi",Immediate loading of implant-supported full-arch rehabilitations has become routine practice when treating edentulous patients. The combination of static computer-aided implant surgery (s-CAIS) and digital pro...,https://www.springeropen.com//journalimplantdent.springeropen.com/articles/10.1186/s40729-021-00369-0,6 September 2021,
53,0.000678989392134464,53,A Collection of Simulated Event Logs for Fairness Assessment in Process Mining,"Timo Pohl, Alessandro Berti, Mahnaz Sadat Qafari, Wil M. P. van der Aalst","The analysis of fairness in process mining is a significant aspect of data-driven decision-making, yet the advancement in this field is constrained due to the scarcity of event data that incorporates fairness considerations. To bridge this gap, we present a collection of simulated event logs, spanning four critical domains, which encapsulate a variety of discrimination scenarios. By simulating these event logs with CPN Tools, we ensure data with known ground truth, thereby offering a robust foundation for fairness analysis. These logs are made freely available under the CC-BY-4.0 license and adhere to the XES standard, thereby assuring broad compatibility with various process mining tools. This initiative aims to empower researchers with the requisite resources to test and develop fairness techniques within process mining, ultimately contributing to the pursuit of equitable, data-driven decision-making processes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11453,June 2023,53,Computer-assisted analysis of functional internal rotation after reverse total shoulder arthroplasty: implications for component choice and orientation,"Bettina Hochreiter, Michel Meisterhans, Christoph Zindel, Anna-Katharina Calek and Christian Gerber",Functional internal rotation (IR) is a combination of extension and IR. It is clinically often limited after reverse total shoulder arthroplasty (RTSA) either due to loss of extension or IR in extension. It wa...,https://www.springeropen.com//jeo-esska.springeropen.com/articles/10.1186/s40634-023-00580-5,14 March 2023,
54,0.000678989392134464,54,Fingerprinting and Building Large Reproducible Datasets,"Romain Lefeuvre, Jessie Galasso, Benoit Combemale, Houari Sahraoui, Stefano Zacchiroli","Obtaining a relevant dataset is central to conducting empirical studies in software engineering. However, in the context of mining software repositories, the lack of appropriate tooling for large scale mining tasks hinders the creation of new datasets. Moreover, limitations related to data sources that change over time (e.g., code bases) and the lack of documentation of extraction processes make it difficult to reproduce datasets over time. This threatens the quality and reproducibility of empirical studies. In this paper, we propose a tool-supported approach facilitating the creation of large tailored datasets while ensuring their reproducibility. We leveraged all the sources feeding the Software Heritage append-only archive which are accessible through a unified programming interface to outline a reproducible and generic extraction process. We propose a way to define a unique fingerprint to characterize a dataset which, when provided to the extraction process, ensures that the same dataset will be extracted. We demonstrate the feasibility of our approach by implementing a prototype. We show how it can help reduce the limitations researchers face when creating or reproducing datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11391,June 2023,54,Computer or human: a comparative study of automated evaluation scoring and instructors’ feedback on Chinese college students’ English writing,Huimei Chen and Jie Pan,The role of internet technology in higher education and particularly in teaching English as a Foreign language is increasingly prominent because of the interest in the ways in which technology can be applied t...,https://www.springeropen.com//sfleducation.springeropen.com/articles/10.1186/s40862-022-00171-4,15 December 2022,
55,0.000678989392134464,55,Visually grounded few-shot word learning in low-resource settings,"Leanne Nortje, Dan Oneata, Herman Kamper","We propose a visually grounded speech model that learns new words and their visual depictions from just a few word-image example pairs. Given a set of test images and a spoken query, we ask the model which image depicts the query word. Previous work has simplified this few-shot learning problem by either using an artificial setting with digit word-image pairs or by using a large number of examples per class. Moreover, all previous studies were performed using English speech-image data. We propose an approach that can work on natural word-image pairs but with less examples, i.e. fewer shots, and then illustrate how this approach can be applied for multimodal few-shot learning in a real low-resource language, Yoruba. Our approach involves using the given word-image example pairs to mine new unsupervised word-image training pairs from large collections of unlabelledspeech and images. Additionally, we use a word-to-image attention mechanism to determine word-image similarity. With this new model, we achieve better performance with fewer shots than previous approaches on an existing English benchmark. Many of the model's mistakes are due to confusion between visual concepts co-occurring in similar contexts. The experiments on Yoruba show the benefit of transferring knowledge from a multimodal model trained on a larger set of English speech-image data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11371,June 2023,55,Computer simulation of short-range repulsion between supported phospholipid membranes,"Alexander Pertsin, Dmitry Platonov and Michael Grunze","The grand canonical Monte Carlo technique is used to calculate the water-mediated pressure between two supported 1,2-dilauroyl-dl-phosphatidylethanolamine (DLPE) membranes in the short separation range. The intra...",https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.2190699,March 2006,
56,0.000678989392134464,56,ChatGPT Chemistry Assistant for Text Mining and Prediction of MOF Synthesis,"Zhiling Zheng, Oufan Zhang, Christian Borgs, Jennifer T. Chayes, Omar M. Yaghi","We use prompt engineering to guide ChatGPT in the automation of text mining of metal-organic frameworks (MOFs) synthesis conditions from diverse formats and styles of the scientific literature. This effectively mitigates ChatGPT's tendency to hallucinate information -- an issue that previously made the use of Large Language Models (LLMs) in scientific fields challenging. Our approach involves the development of a workflow implementing three different processes for text mining, programmed by ChatGPT itself. All of them enable parsing, searching, filtering, classification, summarization, and data unification with different tradeoffs between labor, speed, and accuracy. We deploy this system to extract 26,257 distinct synthesis parameters pertaining to approximately 800 MOFs sourced from peer-reviewed research articles. This process incorporates our ChemPrompt Engineering strategy to instruct ChatGPT in text mining, resulting in impressive precision, recall, and F1 scores of 90-99%. Furthermore, with the dataset built by text mining, we constructed a machine-learning model with over 86% accuracy in predicting MOF experimental crystallization outcomes and preliminarily identifying important factors in MOF crystallization. We also developed a reliable data-grounded MOF chatbot to answer questions on chemical reactions and synthesis procedures. Given that the process of using ChatGPT reliably mines and tabulates diverse MOF synthesis information in a unified format, while using only narrative language requiring no coding expertise, we anticipate that our ChatGPT Chemistry Assistant will be very useful across various other chemistry sub-disciplines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11296,June 2023,56,Correction: Constant-depth circuits for dynamic simulations of materials on quantum computers,"Lindsay Bassman Oftelie, Roel Van Beeumen, Ed Younis, Ethan Smith, Costin Iancu and Wibe A. de Jong",Theoriginal articlewas published inMaterials Theory20226:13,https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-022-00048-6,14 November 2022,
57,0.00673579331412596,57,Using Motif Transitions for Temporal Graph Generation,"Penghang Liu, A. Erdem Sarıyüce","Graph generative models are highly important for sharing surrogate data and benchmarking purposes. Real-world complex systems often exhibit dynamic nature, where the interactions among nodes change over time in the form of a temporal network. Most temporal network generation models extend the static graph generation models by incorporating temporality in the generation process. More recently, temporal motifs are used to generate temporal networks with better success. However, existing models are often restricted to a small set of predefined motif patterns due to the high computational cost of counting temporal motifs. In this work, we develop a practical temporal graph generator, Motif Transition Model (MTM), to generate synthetic temporal networks with realistic global and local features. Our key idea is modeling the arrival of new events as temporal motif transition processes. We first calculate the transition properties from the input graph and then simulate the motif transition processes based on the transition probabilities and transition rates. We demonstrate that our model consistently outperforms the baselines with respect to preserving various global and local temporal graph statistics and runtime performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.11190,June 2023,57,Using deep learning to solve computer security challenges: a survey,"Yoon-Ho Choi, Peng Liu, Zitong Shang, Haizhou Wang, Zhilong Wang, Lan Zhang, Junwei Zhou and Qingtian Zou","Although using machine learning techniques to solve computer security challenges is not a new idea, the rapidly emerging Deep Learning technology has recently triggered a substantial amount of interests in the...",https://www.springeropen.com//cybersecurity.springeropen.com/articles/10.1186/s42400-020-00055-5,10 August 2020,
58,0.000678989392134464,58,Efficient Generalized Temporal Pattern Mining in Big Time Series Using Mutual Information,"Van Long Ho, Nguyen Ho, Torben Bach Pedersen, Panagiotis Papapetrou","Big time series are increasingly available from an ever wider range of IoT-enabled sensors deployed in various environments. Significant insights can be gained by mining temporal patterns from these time series. Temporal pattern mining (TPM) extends traditional pattern mining by adding event time intervals into extracted patterns, making them more expressive at the expense of increased time and space complexities. Besides frequent temporal patterns (FTPs), which occur frequently in the entire dataset, another useful type of temporal patterns are so-called rare temporal patterns (RTPs), which appear rarely but with high confidence. Mining rare temporal patterns yields additional challenges. For FTP mining, the temporal information and complex relations between events already create an exponential search space. For RTP mining, the support measure is set very low, leading to a further combinatorial explosion and potentially producing too many uninteresting patterns. Thus, there is a need for a generalized approach which can mine both frequent and rare temporal patterns. This paper presents our Generalized Temporal Pattern Mining from Time Series (GTPMfTS) approach with the following specific contributions: (1) The end-to-end GTPMfTS process taking time series as input and producing frequent/rare temporal patterns as output. (2) The efficient Generalized Temporal Pattern Mining (GTPM) algorithm mines frequent and rare temporal patterns using efficient data structures for fast retrieval of events and patterns during the mining process, and employs effective pruning techniques for significantly faster mining. (3) An approximate version of GTPM that uses mutual information, a measure of data correlation, to prune unpromising time series from the search space. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10994,June 2023,58,Saudi high school students’ attitudes and barriers toward the use of computer technologies in learning English,Ahmed Abdulateef Sabti and Rasha Sami Chaichan,This study examines the attitudes of Saudi Arabian high school students toward the use of computer technologies in learning English. The study also discusses the possible barriers that affect and limit the act...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-3-460,23 August 2014,
59,0.000678989392134464,59,Pattern Mining for Anomaly Detection in Graphs: Application to Fraud in Public Procurement,"Lucas Potin, Rosa Figueiredo, Vincent Labatut, Christine Largeron","In the context of public procurement, several indicators called red flags are used to estimate fraud risk. They are computed according to certain contract attributes and are therefore dependent on the proper filling of the contract and award notices. However, these attributes are very often missing in practice, which prohibits red flags computation. Traditional fraud detection approaches focus on tabular data only, considering each contract separately, and are therefore very sensitive to this issue. In this work, we adopt a graph-based method allowing leveraging relations between contracts, to compensate for the missing attributes. We propose PANG (Pattern-Based Anomaly Detection in Graphs), a general supervised framework relying on pattern extraction to detect anomalous graphs in a collection of attributed graphs. Notably, it is able to identify induced subgraphs, a type of pattern widely overlooked in the literature. When benchmarked on standard datasets, its predictive performance is on par with state-of-the-art methods, with the additional advantage of being explainable. These experiments also reveal that induced patterns are more discriminative on certain datasets. When applying PANG to public procurement data, the prediction is superior to other methods, and it identifies subgraph patterns that are characteristic of fraud-prone situations, thereby making it possible to better understand fraudulent behavior. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10857,June 2023,59,Letter from the Guest Editor,Paulo Cesar Masiero,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192396,March 2007,
60,0.000678989392134464,60,LaDe: The First Comprehensive Last-mile Delivery Dataset from Industry,"Lixia Wu, Haomin Wen, Haoyuan Hu, Xiaowei Mao, Yutong Xia, Ergang Shan, Jianbin Zhen, Junhong Lou, Yuxuan Liang, Liuqing Yang, Roger Zimmermann, Youfang Lin, Huaiyu Wan","Real-world last-mile delivery datasets are crucial for research in logistics, supply chain management, and spatio-temporal data mining. Despite a plethora of algorithms developed to date, no widely accepted, publicly available last-mile delivery dataset exists to support research in this field. In this paper, we introduce \texttt{LaDe}, the first publicly available last-mile delivery dataset with millions of packages from the industry. LaDe has three unique characteristics: (1) Large-scale. It involves 10,677k packages of 21k couriers over 6 months of real-world operation. (2) Comprehensive information. It offers original package information, such as its location and time requirements, as well as task-event information, which records when and where the courier is while events such as task-accept and task-finish events happen. (3) Diversity. The dataset includes data from various scenarios, including package pick-up and delivery, and from multiple cities, each with its unique spatio-temporal patterns due to their distinct characteristics such as populations. We verify LaDe on three tasks by running several classical baseline models per task. We believe that the large-scale, comprehensive, diverse feature of LaDe can offer unparalleled opportunities to researchers in the supply chain community, data mining community, and beyond. The dataset homepage is publicly available at https://huggingface.co/datasets/Cainiao-AI/LaDe. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10675,June 2023,60,Letter from the Guest Editors,"Carlos J. P. de Lucena, Marcelo Blois, Ricardo Choren and Viviane Torres da Silva",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192405,June 2007,
61,0.000678989392134464,61,Isabelle Formalisation of Original Representation Theorems,Marco B. Caminati,"In a recent paper, new theorems linking apparently unrelated mathematical objects (event structures from concurrency theory and full graphs arising in computational biology) were discovered by cross-site data mining on huge databases, and building on existing Isabelle-verified event structures enumeration algorithms. Given the origin and newness of such theorems, their formal verification is particularly desirable. This paper presents such a verification via Isabelle/HOL definitions and theorems, and exposes the technical challenges found in the process. The introduced formalisation completes the verification of Isabelle-verified event structure enumeration algorithms into a fully verified framework to link event structures to full graphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10558,June 2023,61,Computer-guided buccal cortical plate separation for removal of calcified benign odontogenic tumors affecting the mandibular angle region,"Mohammed Omara, Ayman Gouda and Sherif Ali","Surgical removal of intra-bony calcific benign lesions is technically challenging regarding its accessibility, proximity to vital structures, and deteriorating effect on the remaining bony structures.",https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-022-00354-6,22 September 2022,
62,0.000678989392134464,62,Rapid Image Labeling via Neuro-Symbolic Learning,"Yifeng Wang, Zhi Tu, Yiwen Xiang, Shiyuan Zhou, Xiyuan Chen, Bingxuan Li, Tianyi Zhang","The success of Computer Vision (CV) relies heavily on manually annotated data. However, it is prohibitively expensive to annotate images in key domains such as healthcare, where data labeling requires significant domain expertise and cannot be easily delegated to crowd workers. To address this challenge, we propose a neuro-symbolic approach called Rapid, which infers image labeling rules from a small amount of labeled data provided by domain experts and automatically labels unannotated data using the rules. Specifically, Rapid combines pre-trained CV models and inductive logic learning to infer the logic-based labeling rules. Rapid achieves a labeling accuracy of 83.33% to 88.33% on four image labeling tasks with only 12 to 39 labeled samples. In particular, Rapid significantly outperforms finetuned CV models in two highly specialized tasks. These results demonstrate the effectiveness of Rapid in learning from small data and its capability to generalize among different tasks. Code and our dataset are publicly available at https://github.com/Neural-Symbolic-Image-Labeling/ △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10490,June 2023,62,OpenIaC: open infrastructure as code - the network is my computer,"Chunming Rong, Jiahui Geng, Thomas J. Hacker, Haakon Bryhni and Martin G. Jaatun","Modern information systems are built fron a complex composition of networks, infrastructure, devices, services, and applications, interconnected by data flows that are often private and financially sensitive. ...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-022-00285-7,8 May 2022,
63,0.000678989392134464,63,Iterative Hierarchy and Ranking Process (IHRP): A Novel Effective Hierarchy Method for Densely Connected Systems and Case Study in Student Performance Assessment,"Suvojit Dhara, Adrijit Goswami","In real-life decision-making problems, determining the influences of the factors on the decision attribute is one of the primary tasks. To affect the decision attribute most, finding a proper hierarchy among the factors and determining their importance values in the system becomes quite important. Interpretive structural modeling (ISM) is a widely used hierarchy-building method that mines factor inter-influences based on expert opinions. This paper discusses one of the main drawbacks of the conventional ISM method in systems where the factors are densely interrelated. We refer to such systems as ""dense systems"". We propose a novel iterative hierarchy-building technique, called 'Iterative Hierarchy and Ranking Process'(IHRP) which performs effectively in such dense systems. To take the vagueness of the expert opinions into account, intuitionistic fuzzy linguistics has been used in the research work. In this paper, we propose a two-stage calculation of the relative importance of the factors in the system based on their hierarchical positions and rank the factors accordingly. We have performed a case study on student performance assessment by taking up novel Indian high-school administrative factors' data collected by surveying the experts in this field. A comparative study has been conducted in terms of the correlation of the factor ranking achieved by the proposed method and conventional ISM method with that of standard outranking methods like TOPSIS, and VIKOR. Our proposed IHRP framework achieves an 85-95% correlation compared to a 50-60% correlation for the conventional ISM method. This proves the effectiveness of the proposed method in determining a better hierarchy than the conventional method, especially in dense systems. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10409,June 2023,63,GaS_GeoT: A computer program for an effective use of newly improved gas geothermometers in predicting reliable geothermal reservoir temperatures,"A. Acevedo-Anicasio, E. Santoyo, D. Pérez-Zárate, Kailasa Pandarinath, M. Guevara and L. Díaz-González","A geochemometric study based on a multi-criteria decision analysis was applied, for the first time, for the optimal evaluation and selection of artificial neural networks, and the prediction of geothermal rese...",https://www.springeropen.com//geothermal-energy-journal.springeropen.com/articles/10.1186/s40517-020-00182-9,18 January 2021,
64,0.000678989392134464,64,Tailoring Machine Learning for Process Mining,"Paolo Ceravolo, Sylvio Barbon Junior, Ernesto Damiani, Wil van der Aalst","Machine learning models are routinely integrated into process mining pipelines to carry out tasks like data transformation, noise reduction, anomaly detection, classification, and prediction. Often, the design of such models is based on some ad-hoc assumptions about the corresponding data distributions, which are not necessarily in accordance with the non-parametric distributions typically observed with process data. Moreover, the learning procedure they follow ignores the constraints concurrency imposes to process data. Data encoding is a key element to smooth the mismatch between these assumptions but its potential is poorly exploited. In this paper, we argue that a deeper insight into the issues raised by training machine learning models with process data is crucial to ground a sound integration of process mining and machine learning. Our analysis of such issues is aimed at laying the foundation for a methodology aimed at correctly aligning machine learning with process mining requirements and stimulating the research to elaborate in this direction. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10341,June 2023,64,Recent advances in the reconstruction of cranio-maxillofacial defects using computer-aided design/computer-aided manufacturing,Ji-hyeon Oh,"With the development of computer-aided design/computer-aided manufacturing (CAD/CAM) technology, it has been possible to reconstruct the cranio-maxillofacial defect with more accurate preoperative planning, pr...",https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-018-0141-9,5 February 2018,
65,0.000678989392134464,65,PIMMiner: A High-performance PIM Architecture-aware Graph Mining Framework,"Jiya Su, Peng Jiang, Rujia Wang","Graph mining applications, such as subgraph pattern matching and mining, are widely used in real-world domains such as bioinformatics, social network analysis, and computer vision. Such applications are considered a new class of data-intensive applications that generate massive irregular computation workloads and memory accesses, which degrade the performance significantly. Leveraging emerging hardware, such as process-in-memory (PIM) technology, could potentially accelerate such applications. In this paper, we propose PIMMiner, a high-performance PIM architecture graph mining framework. We first identify that current PIM architecture cannot be fully utilized by graph mining applications. Next, we propose a set of optimizations and interfaces that enhance the locality, and internal bandwidth utilization and reduce remote bank accesses and load imbalance through cohesive algorithm and architecture co-designs. We compare PIMMiner with several state-of-the-art graph mining frameworks and show that PIMMiner is able to outperform all of them significantly. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10257,June 2023,65,Interaction in computer supported collaborative learning: an analysis of the implementation phase,"Núria Hernández-Sellés, Pablo-César Muñoz-Carril and Mercedes González-Sanmamed",There is extensive research on interaction frameworks in distance education and studies in Computer Supported Collaborative Learning (CSCL) have also focused on establishing interaction models. There is still ...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00202-5,1 July 2020,
66,0.000678989392134464,66,Promises and Perils of Mining Software Package Ecosystem Data,"Raula Gaikovina Kula, Katsuro Inoue, Christoph Treude","The use of third-party packages is becoming increasingly popular and has led to the emergence of large software package ecosystems with a maze of inter-dependencies. Since the reliance on these ecosystems enables developers to reduce development effort and increase productivity, it has attracted the interest of researchers: understanding the infrastructure and dynamics of package ecosystems has given rise to approaches for better code reuse, automated updates, and the avoidance of vulnerabilities, to name a few examples. But the reality of these ecosystems also poses challenges to software engineering researchers, such as: How do we obtain the complete network of dependencies along with the corresponding versioning information? What are the boundaries of these package ecosystems? How do we consistently detect dependencies that are declared but not used? How do we consistently identify developers within a package ecosystem? How much of the ecosystem do we need to understand to analyse a single component? How well do our approaches generalise across different programming languages and package ecosystems? In this chapter, we review promises and perils of mining the rich data related to software package ecosystems available to software engineering researchers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.10021,June 2023,66,Robust EEG Channel Selection across Subjects for Brain-Computer Interfaces,"Michael Schröder, Thomas Navin Lal, Thilo Hinterberger, Martin Bogdan, N. Jeremy Hill, Niels Birbaumer, Wolfgang Rosenstiel and Bernhard Schölkopf","Most EEG-based brain-computer interface (BCI) paradigms come along with specific electrode positions, for example, for a visual-based BCI, electrode positions close to the primary visual cortex are used. For n...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.3103,17 November 2005,
67,0.000678989392134464,67,The Perils of Advocacy,Joel Atkins,"Statisticians and data scientists find insights that help lead to better understanding and better outcomes. When clients and managers come to us for help (and even when they don't), we want to share our advice. While we should be free to share our recommendations, we need to be clear about what the data is telling us and what is based ""only on our judgment"". Gelman, et. al. wrote ""As we have learned from the replication crisis sweeping the biomedical and social sciences, it is frighteningly easy for motivated researchers working in isolation to arrive at favored conclusions-whether inadvertently or intentionally."" One senior business leader I know said, ""if you have data, great; if we're just going on intuition we can use mine"". However, having data isn't enough. We need to be rigorous in our analysis to avoid finding insights that aren't supported. This paper will go through a number of examples to illustrate common mistakes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.09492,June 2023,67,Computer and robotic – assisted total knee arthroplasty: a review of outcomes,Jobe Shatrov and David Parker,Total knee arthroplasty (TKA) is a successful treatment for tricompartmental knee arthritis. Computer navigation and robotic-assisted-surgery (RAS) have emerged as tools that aim to help plan and execute surge...,https://www.springeropen.com//jeo-esska.springeropen.com/articles/10.1186/s40634-020-00278-y,24 September 2020,
68,0.000678989392134464,68,TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting,"Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam","Transformers have gained popularity in time series forecasting for their ability to capture long-sequence interactions. However, their high memory and computing requirements pose a critical bottleneck for long-term forecasting. To address this, we propose TSMixer, a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules. TSMixer is designed for multivariate forecasting and representation learning on patched time series, providing an efficient alternative to Transformers. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X). △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.09364,June 2023,68,Computer-aided design of some quinazoline analogues as epidermal growth factor receptor inhibitors,"Muhammad Tukur Ibrahim, Adamu Uzairu, Gideon Adamu Shallangwa and Sani Uba",The treatment of epidermal growth factor receptor (EGFR)-muted non-small cell lung cancer (NSCLC) remains among the utmost important unachieved therapeutic need worldwide. Development of EGFR inhibitors to tre...,https://www.springeropen.com//jmhg.springeropen.com/articles/10.1186/s43042-021-00181-w,28 June 2021,
69,0.000678989392134464,69,A Survey of Some Density Based Clustering Techniques,"Rupanka Bhuyan, Samarjeet Borah","Density Based Clustering are a type of Clustering methods using in data mining for extracting previously unknown patterns from data sets. There are a number of density based clustering methods such as DBSCAN, OPTICS, DENCLUE, VDBSCAN, DVBSCAN, DBCLASD and ST-DBSCAN. In this paper, a study of these methods is done along with their characteristics, advantages and disadvantages and most importantly, their applicability to different types of data sets to mine useful and appropriate patterns. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.09256,June 2023,69,Framing the fallibility of Computer-Aided Detection aids cancer detection,Melina A. Kunar and Derrick G. Watson,"Computer-Aided Detection (CAD) has been proposed to help operators search for cancers in mammograms. Previous studies have found that although accurate CAD leads to an improvement in cancer detection, inaccura...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-023-00485-y,24 May 2023,
70,0.000678989392134464,70,FRIGATE: Frugal Spatio-temporal Forecasting on Road Networks,"Mridul Gupta, Hariprasad Kodamana, Sayan Ranu","Modelling spatio-temporal processes on road networks is a task of growing importance. While significant progress has been made on developing spatio-temporal graph neural networks (Gnns), existing works are built upon three assumptions that are not practical on real-world road networks. First, they assume sensing on every node of a road network. In reality, due to budget-constraints or sensor failures, all locations (nodes) may not be equipped with sensors. Second, they assume that sensing history is available at all installed sensors. This is unrealistic as well due to sensor failures, loss of packets during communication, etc. Finally, there is an assumption of static road networks. Connectivity within networks change due to road closures, constructions of new roads, etc. In this work, we develop FRIGATE to address all these shortcomings. FRIGATE is powered by a spatio-temporal Gnn that integrates positional, topological, and temporal information into rich inductive node representations. The joint fusion of this diverse information is made feasible through a novel combination of gated Lipschitz embeddings with Lstms. We prove that the proposed Gnn architecture is provably more expressive than message-passing Gnns used in state-of-the-art algorithms. The higher expressivity of FRIGATE naturally translates to superior empirical performance conducted on real-world network-constrained traffic data. In addition, FRIGATE is robust to frugal sensor deployment, changes in road network connectivity, and temporal irregularity in sensing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.08277,June 2023,70,Influence of bone condition on implant placement accuracy with computer-guided surgery,"Ramadhan Hardani Putra, Nobuhiro Yoda, Masahiro Iikubo, Yoshihiro Kataoka, Kensuke Yamauchi, Shigeto Koyama, Upul Cooray, Eha Renwi Astuti, Tetsu Takahashi and Keiichi Sasaki","The impact of the jaw bone condition, such as bone quantity and quality in the implant placement site, affecting the accuracy of implant placement with computer-guided surgery (CGS) remains unclear. Therefore,...",https://www.springeropen.com//journalimplantdent.springeropen.com/articles/10.1186/s40729-020-00249-z,20 September 2020,
71,0.000678989392134464,71,MOFI: Learning Image Representations from Noisy Entity Annotated Images,"Wentao Wu, Aleksei Timofeev, Chen Chen, Bowen Zhang, Kun Duan, Shuangning Liu, Yantao Zheng, Jon Shlens, Xianzhi Du, Zhe Gan, Yinfei Yang","We present MOFI, a new vision foundation model designed to learn image representations from noisy entity annotated images. MOFI differs from previous work in two key aspects: ($i$) pre-training data, and ($ii$) training recipe. Regarding data, we introduce a new approach to automatically assign entity labels to images from noisy image-text pairs. Our approach involves employing a named entity recognition model to extract entities from the alt-text, and then using a CLIP model to select the correct entities as labels of the paired image. The approach is simple, does not require costly human annotation, and can be readily scaled up to billions of image-text pairs mined from the web. Through this method, we have created Image-to-Entities (I2E), a new large-scale dataset with 1 billion images and 2 million distinct entities, covering rich visual concepts in the wild. Building upon the I2E dataset, we study different training recipes, including supervised pre-training, contrastive pre-training, and multi-task learning. For constrastive pre-training, we treat entity names as free-form text, and further enrich them with entity descriptions. Experiments show that supervised pre-training with large-scale fine-grained entity labels is highly effective for image retrieval tasks, and multi-task training further improves the performance. The final MOFI model achieves 86.66% mAP on the challenging GPR1200 dataset, surpassing the previous state-of-the-art performance of 72.19% from OpenAI's CLIP model. Further experiments on zero-shot and linear probe image classification also show that MOFI outperforms a CLIP model trained on the original image-text data, demonstrating the effectiveness of the I2E dataset in learning strong image representations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.07952,June 2023,71,Computer-aided analysis of quercetin mechanism of overcoming docetaxel resistance in docetaxel-resistant prostate cancer,"Victor Omoboyede, Ochapa Ibrahim, Haruna Isiyaku Umar, Grace Ayomide Oke, Olugbenga Samson Onile and Prosper Obed Chukwuemeka","Prostate cancer (PC) is a silent but potent killer among men. In 2018, PC accounted for more than 350, 000 death cases while more than 1.2 million cases were diagnosed. Docetaxel, a chemotherapeutic drug belon...",https://www.springeropen.com//jgeb.springeropen.com/articles/10.1186/s43141-023-00498-6,26 April 2023,
72,0.000678989392134464,72,A Survey of Densest Subgraph Discovery on Large Graphs,"Wensheng Luo, Chenhao Ma, Yixiang Fang, Laks V. S. Lakshmanan","With the prevalence of graphs for modeling complex relationships among objects, the topic of graph mining has attracted a great deal of attention from both academic and industrial communities in recent years. As one of the most fundamental problems in graph mining, the densest subgraph discovery (DSD) problem has found a wide spectrum of real applications, such as discovery of filter bubbles in social media, finding groups of actors propagating misinformation in social media, social network community detection, graph index construction, regulatory motif discovery in DNA, fake follower detection, and so on. Theoretically, DSD closely relates to other fundamental graph problems, such as network flow and bipartite matching. Triggered by these applications and connections, DSD has garnered much attention from the database, data mining, theory, and network communities. In this survey, we first highlight the importance of DSD in various real-world applications and the unique challenges that need to be addressed. Subsequently, we classify existing DSD solutions into several groups, which cover around 50 research papers published in many well-known venues (e.g., SIGMOD, PVLDB, TODS, WWW), and conduct a thorough review of these solutions in each group. Afterwards, we analyze and compare the models and solutions in these works. Finally, we point out a list of promising future research directions. It is our hope that this survey not only helps researchers have a better understanding of existing densest subgraph models and solutions, but also provides insights and identifies directions for future study. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.07927,June 2023,72,The optimal use of computer aided detection to find low prevalence cancers,Melina A. Kunar,People miss a high proportion of targets that only appear rarely. This low prevalence (LP) effect has implications for applied search tasks such as the clinical reading of mammograms. Computer aided detection ...,https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-022-00361-1,4 February 2022,
73,0.00435011815751728,73,Skill Disentanglement for Imitation Learning from Suboptimal Demonstrations,"Tianxiang Zhao, Wenchao Yu, Suhang Wang, Lu Wang, Xiang Zhang, Yuncong Chen, Yanchi Liu, Wei Cheng, Haifeng Chen","Imitation learning has achieved great success in many sequential decision-making tasks, in which a neural agent is learned by imitating collected human demonstrations. However, existing algorithms typically require a large number of high-quality demonstrations that are difficult and expensive to collect. Usually, a trade-off needs to be made between demonstration quality and quantity in practice. Targeting this problem, in this work we consider the imitation of sub-optimal demonstrations, with both a small clean demonstration set and a large noisy set. Some pioneering works have been proposed, but they suffer from many limitations, e.g., assuming a demonstration to be of the same optimality throughout time steps and failing to provide any interpretation w.r.t knowledge learned from the noisy set. Addressing these problems, we propose {\method} by evaluating and imitating at the sub-demonstration level, encoding action primitives of varying quality into different skills. Concretely, {\method} consists of a high-level controller to discover skills and a skill-conditioned module to capture action-taking policies, and is trained following a two-phase pipeline by first discovering skills with all demonstrations and then adapting the controller to only the clean set. A mutual-information-based regularization and a dynamic sub-demonstration optimality estimator are designed to promote disentanglement in the skill space. Extensive experiments are conducted over two gym environments and a real-world healthcare dataset to demonstrate the superiority of {\method} in learning from sub-optimal demonstrations and its improved interpretability by examining learned skills. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.07919,June 2023,73,Automatic Speechreading with Applications to Human-Computer Interfaces,"Xiaozheng Zhang, Charles C. Broun, Russell M. Mersereau and Mark A. Clements","There has been growing interest in introducing speech as a new modality into the human-computer interface (HCI). Motivated by the multimodal nature of speech, the visual component is considered to yield inform...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206137,28 November 2002,
74,0.000678989392134464,74,FIRE: An Optimization Approach for Fast Interpretable Rule Extraction,"Brian Liu, Rahul Mazumder","We present FIRE, Fast Interpretable Rule Extraction, an optimization-based framework to extract a small but useful collection of decision rules from tree ensembles. FIRE selects sparse representative subsets of rules from tree ensembles, that are easy for a practitioner to examine. To further enhance the interpretability of the extracted model, FIRE encourages fusing rules during selection, so that many of the selected decision rules share common antecedents. The optimization framework utilizes a fusion regularization penalty to accomplish this, along with a non-convex sparsity-inducing penalty to aggressively select rules. Optimization problems in FIRE pose a challenge to off-the-shelf solvers due to problem scale and the non-convexity of the penalties. To address this, making use of problem-structure, we develop a specialized solver based on block coordinate descent principles; our solver performs up to 40x faster than existing solvers. We show in our experiments that FIRE outperforms state-of-the-art rule ensemble algorithms at building sparse rule sets, and can deliver more interpretable models compared to existing methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.07432,June 2023,74,Letter from the editor-in-chief,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194497,June 2009,
75,0.000678989392134464,75,"Enabling Spatial Digital Twins: Technologies, Challenges, and Future Research Directions","Mohammed Eunus Ali, Muhammad Aamir Cheema, Tanzima Hashem, Anwaar Ulhaq, Muhammad Ali Babar","A Digital Twin (DT) is a virtual replica of a physical object or system, created to monitor, analyze, and optimize its behavior and characteristics. A Spatial Digital Twin (SDT) is a specific type of digital twin that emphasizes the geospatial aspects of the physical entity, incorporating precise location and dimensional attributes for a comprehensive understanding within its spatial environment. The current body of research on SDTs primarily concentrates on analyzing their potential impact and opportunities within various application domains. As building an SDT is a complex process and requires a variety of spatial computing technologies, it is not straightforward for practitioners and researchers of this multi-disciplinary domain to grasp the underlying details of enabling technologies of the SDT. In this paper, we are the first to systematically analyze different spatial technologies relevant to building an SDT in layered approach (starting from data acquisition to visualization). More specifically, we present the key components of SDTs into four layers of technologies: (i) data acquisition; (ii) spatial database management \& big data analytics systems; (iii) GIS middleware software, maps \& APIs; and (iv) key functional components such as visualizing, querying, mining, simulation and prediction. Moreover, we discuss how modern technologies such as AI/ML, blockchains, and cloud computing can be effectively utilized in enabling and enhancing SDTs. Finally, we identify a number of research challenges and opportunities in SDTs. This work serves as an important resource for SDT researchers and practitioners as it explicitly distinguishes SDTs from traditional DTs, identifies unique applications, outlines the essential technological components of SDTs, and presents a vision for their future development along with the challenges that lie ahead. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06600,June 2023,75,Cursor movement detection in brain-computer-interface systems using the K-means clustering method and LSVM,"Leila Mohammadi, Zahra Einalou, Hamidreza Hosseinzadeh and Mehrdad Dadgostar","In this study, we present the detection of the up-downward as well as the right- leftward motion of cursor based on feature extraction. In this algorithm, the K-means clustering method is used to recognize the...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00456-4,20 May 2021,
76,0.000678989392134464,76,TALENT: Targeted Mining of Non-overlapping Sequential Patterns,"Zefeng Chen, Wensheng Gan, Gengsen Huang, Zhenlian Qi, Yan Li, Philip S. Yu","With the widespread application of efficient pattern mining algorithms, sequential patterns that allow gap constraints have become a valuable tool to discover knowledge from biological data such as DNA and protein sequences. Among all kinds of gap-constrained mining, non-overlapping sequence mining can mine interesting patterns and satisfy the anti-monotonic property (the Apriori property). However, existing algorithms do not search for targeted sequential patterns, resulting in unnecessary and redundant pattern generation. Targeted pattern mining can not only mine patterns that are more interesting to users but also reduce the unnecessary redundant sequence generated, which can greatly avoid irrelevant computation. In this paper, we define and formalize the problem of targeted non-overlapping sequential pattern mining and propose an algorithm named TALENT (TArgeted mining of sequentiaL pattErN with consTraints). Two search methods including breadth-first and depth-first searching are designed to troubleshoot the generation of patterns. Furthermore, several pruning strategies to reduce the reading of sequences and items in the data and terminate redundant pattern extensions are presented. Finally, we select a series of datasets with different characteristics and conduct extensive experiments to compare the TALENT algorithm with the existing algorithms for mining non-overlapping sequential patterns. The experimental results demonstrate that the proposed targeted mining algorithm, TALENT, has excellent mining efficiency and can deal efficiently with many different query settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06470,June 2023,76,Computer-aided molecular modeling and structural analysis of the human centromere protein–HIKM complex,"Henrietta Onyinye Uzoeto, Samuel Cosmas, Judith Nnedimkpa Ajima, Amarachukwu Vivian Arazu, Chizoba Maryann Didiugwu, Daniel Emmanuel Ekpo, Glory Omini Ibiang and Olanrewaju Ayodeji Durojaye",Protein–peptide and protein–protein interactions play an essential role in different functional and structural cellular organizational aspects. While Cryo-EM and X-ray crystallography generate the most complet...,https://www.springeropen.com//bjbas.springeropen.com/articles/10.1186/s43088-022-00285-1,19 August 2022,
77,0.000678989392134464,77,Enjoy the Silence: Analysis of Stochastic Petri Nets with Silent Transitions,"Sander J. J. Leemans, Fabrizio M. Maggi, Marco Montali","Capturing stochastic behaviors in business and work processes is essential to quantitatively understand how nondeterminism is resolved when taking decisions within the process. This is of special interest in process mining, where event data tracking the actual execution of the process are related to process models, and can then provide insights on frequencies and probabilities. Variants of stochastic Petri nets provide a natural formal basis for this. However, when capturing processes, such nets need to be labelled with (possibly duplicated) activities, and equipped with silent transitions that model internal, non-logged steps related to the orchestration of the process. At the same time, they have to be analyzed in a finite-trace semantics, matching the fact that each process execution consists of finitely many steps. These two aspects impede the direct application of existing techniques for stochastic Petri nets, calling for a novel characterization that incorporates labels and silent transitions in a finite-trace semantics. In this article, we provide such a characterization starting from generalized stochastic Petri nets and obtaining the framework of labelled stochastic processes (LSPs). On top of this framework, we introduce different key analysis tasks on the traces of LSPs and their probabilities. We show that all such analysis tasks can be solved analytically, in particular reducing them to a single method that combines automata-based techniques to single out the behaviors of interest within a LSP, with techniques based on absorbing Markov chains to reason on their probabilities. Finally, we demonstrate the significance of how our approach in the context of stochastic conformance checking, illustrating practical feasibility through a proof-of-concept implementation and its application to different datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06376,June 2023,77,A survey on generative adversarial networks for imbalance problems in computer vision tasks,"Vignesh Sampath, Iñaki Maurtua, Juan José Aguilar Martín and Aitor Gutierrez","Any computer vision application development starts off by acquiring images and data, then preprocessing and pattern recognition steps to perform a task. When the acquired images are highly imbalanced and not a...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00414-0,29 January 2021,
78,0.000678989392134464,78,Using Auxiliary Data to Boost Precision in the Analysis of A/B Tests on an Online Educational Platform: New Data and New Results,"Adam C. Sales, Ethan B. Prihar, Johann A. Gagnon-Bartsch, Neil T. Heffernan","Randomized A/B tests within online learning platforms represent an exciting direction in learning sciences. With minimal assumptions, they allow causal effect estimation without confounding bias and exact statistical inference even in small samples. However, often experimental samples and/or treatment effects are small, A/B tests are underpowered, and effect estimates are overly imprecise. Recent methodological advances have shown that power and statistical precision can be substantially boosted by coupling design-based causal estimation to machine-learning models of rich log data from historical users who were not in the experiment. Estimates using these techniques remain unbiased and inference remains exact without any additional assumptions. This paper reviews those methods and applies them to a new dataset including over 250 randomized A/B comparisons conducted within ASSISTments, an online learning platform. We compare results across experiments using four novel deep-learning models of auxiliary data and show that incorporating auxiliary data into causal estimates is roughly equivalent to increasing the sample size by 20\% on average, or as much as 50-80\% in some cases, relative to t-tests, and by about 10\% on average, or as much as 30-50\%, compared to cutting-edge machine learning unbiased estimates that use only data from the experiments. We show that the gains can be even larger for estimating subgroup effects, hold even when the remnant is unrepresentative of the A/B test sample, and extend to post-stratification population effects estimators. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06273,June 2023,78,Letter from the Editor-in-Chief,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192572,March 2009,
79,0.000678989392134464,79,WePaMaDM-Outlier Detection: Weighted Outlier Detection using Pattern Approaches for Mass Data Mining,"Ravindrakumar Purohit, Jai Prakash Verma, Rachna Jain, Madhuri Bhavsar","Weighted Outlier Detection is a method for identifying unusual or anomalous data points in a dataset, which can be caused by various factors like human error, fraud, or equipment malfunctions. Detecting outliers can reveal vital information about system faults, fraudulent activities, and patterns in the data, assisting experts in addressing the root causes of these anomalies. However,creating a model of normal data patterns to identify outliers can be challenging due to the nature of input data, labeled data availability, and specific requirements of the problem. This article proposed the WePaMaDM-Outlier Detection with distinct mass data mining domain, demonstrating that such techniques are domain-dependent and usually developed for specific problem formulations. Nevertheless, similar domains can adapt solutions with modifications. This work also investigates the significance of data modeling in outlier detection techniques in surveillance, fault detection, and trend analysis, also referred to as novelty detection, a semisupervised task where the algorithm learns to recognize abnormality while being taught the normal class. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06139,June 2023,79,Factors affecting cyberloafing in computer laboratory teaching settings,Sacip Toker and Meltem Huri Baturay,"This correlational study investigated the factors affecting cyberloafing behavior in an educational environment, specifically that of a computer laboratory teaching setting. A total of 272 students selected us...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00250-5,26 March 2021,
80,0.000678989392134464,80,Demystifying Fraudulent Transactions and Illicit Nodes in the Bitcoin Network for Financial Forensics,"Youssef Elmougy, Ling Liu","Blockchain provides the unique and accountable channel for financial forensics by mining its open and immutable transaction data. A recent surge has been witnessed by training machine learning models with cryptocurrency transaction data for anomaly detection, such as money laundering and other fraudulent activities. This paper presents a holistic applied data science approach to fraud detection in the Bitcoin network with two original contributions. First, we contribute the Elliptic++ dataset, which extends the Elliptic transaction dataset to include over 822k Bitcoin wallet addresses (nodes), each with 56 features, and 1.27M temporal interactions. This enables both the detection of fraudulent transactions and the detection of illicit addresses (actors) in the Bitcoin network by leveraging four types of graph data: (i) the transaction-to-transaction graph, representing the money flow in the Bitcoin network, (ii) the address-to-address interaction graph, capturing the types of transaction flows between Bitcoin addresses, (iii) the address-transaction graph, representing the bi-directional money flow between addresses and transactions (BTC flow from input address to one or more transactions and BTC flow from a transaction to one or more output addresses), and (iv) the user entity graph, capturing clusters of Bitcoin addresses representing unique Bitcoin users. Second, we perform fraud detection tasks on all four graphs by using diverse machine learning algorithms. We show that adding enhanced features from the address-to-address and the address-transaction graphs not only assists in effectively detecting both illicit transactions and illicit addresses, but also assists in gaining in-depth understanding of the root cause of money laundering vulnerabilities in cryptocurrency transactions and the strategies for fraud detection and prevention. Released at github.com/git-disl/EllipticPlusPlus. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.06108,June 2023,80,Computer simulation algorithm of gymnastics formation change path based on wireless sensor,"Bingxin Chen, Lifei Kuang and Wei He","Today with the rapid development of the information age, the exchange of science and technology has brought closer the closeness of countries, and our country has also begun to conduct in-depth research on WSN...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00766-y,30 July 2021,
81,0.000678989392134464,81,Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application,"Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundarò, Marina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, Giuliano Binetti, Riccardo Bellazzi, Alberto Redolfi","The introduction of computerized medical records in hospitals has reduced burdensome operations like manual writing and information fetching. However, the data contained in medical records are still far underutilized, primarily because extracting them from unstructured textual medical records takes time and effort. Information Extraction, a subfield of Natural Language Processing, can help clinical practitioners overcome this limitation, using automated text-mining pipelines. In this work, we created the first Italian neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to develop a Large Language Model for this task. Moreover, we conducted several experiments with three external independent datasets to implement an effective multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall 86.44%. The lessons learned are: (i) the crucial role of a consistent annotation process and (ii) a fine-tuning strategy that combines classical methods with a ""few-shot"" approach. This allowed us to establish methodological guidelines that pave the way for future implementations in this field and allow Italian hospitals to tap into important research opportunities. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.05323,June 2023,81,The TIMSS 2019 Item Equivalence Study: examining mode effects for computer-based assessment and implications for measuring trends,"Bethany Fishbein, Michael O. Martin, Ina V. S. Mullis and Pierre Foy","TIMSS 2019 is the first assessment in the TIMSS transition to a computer-based assessment system, called eTIMSS. The TIMSS 2019 Item Equivalence Study was conducted in advance of the field test in 2017 to exam...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-018-0064-z,29 October 2018,
82,0.000678989392134464,82,ShuttleSet: A Human-Annotated Stroke-Level Singles Dataset for Badminton Tactical Analysis,"Wei-Yao Wang, Yung-Chang Huang, Tsi-Ui Ik, Wen-Chih Peng","With the recent progress in sports analytics, deep learning approaches have demonstrated the effectiveness of mining insights into players' tactics for improving performance quality and fan engagement. This is attributed to the availability of public ground-truth datasets. While there are a few available datasets for turn-based sports for action detection, these datasets severely lack structured source data and stroke-level records since these require high-cost labeling efforts from domain experts and are hard to detect using automatic techniques. Consequently, the development of artificial intelligence approaches is significantly hindered when existing models are applied to more challenging structured turn-based sequences. In this paper, we present ShuttleSet, the largest publicly-available badminton singles dataset with annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492 strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles and women's singles players. ShuttleSet is manually annotated with a computer-aided labeling tool to increase the labeling efficiency and effectiveness of selecting the shot type with a choice of 18 distinct classes, the corresponding hitting locations, and the locations of both players at each stroke. In the experiments, we provide multiple benchmarks (i.e., stroke influence, stroke forecasting, and movement forecasting) with baselines to illustrate the practicability of using ShuttleSet for turn-based analytics, which is expected to stimulate both academic and sports communities. Over the past two years, a visualization platform has been deployed to illustrate the variability of analysis cases from ShuttleSet for coaches to delve into players' tactical preferences with human-interactive interfaces, which was also used by national badminton teams during multiple international high-ranking matches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04948,June 2023,82,Computer 3D modeling of radiofrequency ablation of atypical cartilaginous tumours in long bones using finite element methods and real patient anatomy,"Ricardo Rivas Loya, Paul C. Jutte, Thomas C. Kwee and Peter M. A. van Ooijen","Radiofrequency ablation (RFA) is a minimally invasive technique used for the treatment of neoplasms, with a growing interest in the treatment of bone tumours. However, the lack of data concerning the size of t...",https://www.springeropen.com//eurradiolexp.springeropen.com/articles/10.1186/s41747-022-00271-3,28 April 2022,
83,0.000678989392134464,83,X-COBOL: A Dataset of COBOL Repositories,"Mir Sameed Ali, Nikhil Manjunath, Sridhar Chimalakonda","Despite being proposed as early as 1959, COBOL (Common Business-Oriented Language) still predominantly acts as an integral part of the majority of operations of several financial, banking, and governmental organizations. To support the inevitable modernization and maintenance of legacy systems written in COBOL, it is essential for organizations, researchers, and developers to understand the nature and source code of COBOL programs. However, to the best of our knowledge, we are unaware of any dataset that provides data on COBOL software projects, motivating the need for the dataset. Thus, to aid empirical research on comprehending COBOL in open-source repositories, we constructed a dataset of 84 COBOL repositories mined from GitHub, containing rich metadata on the development cycle of the projects. We envision that researchers can utilize our dataset to study COBOL projects' evolution, code properties and develop tools to support their development. Our dataset also provides 1255 COBOL files present inside the mined repositories. The dataset and artifacts are available at https://doi.org/10.5281/zenodo.7968845. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04892,June 2023,83,Computer-aided design/computer-aided manufacturing of hydroxyapatite scaffolds for bone reconstruction in jawbone atrophy: a systematic review and case report,"Umberto Garagiola, Roberto Grigolato, Rossano Soldo, Marco Bacchini, Gianluca Bassi, Rachele Roncucci and Sandro De Nardi",We reviewed the biological and mechanical properties of porous hydroxyapatite (HA) compared to other synthetic materials. Computer-aided design/computer-aided manufacturing (CAD/CAM) was also evaluated to esti...,https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-015-0048-7,4 January 2016,
84,0.000678989392134464,84,Modeling Dual Period-Varying Preferences for Takeaway Recommendation,"Yuting Zhang, Yiqing Wu, Ran Le, Yongchun Zhu, Fuzhen Zhuang, Ruidong Han, Xiang Li, Wei Lin, Zhulin An, Yongjun Xu","Takeaway recommender systems, which aim to accurately provide stores that offer foods meeting users' interests, have served billions of users in our daily life. Different from traditional recommendation, takeaway recommendation faces two main challenges: (1) Dual Interaction-Aware Preference Modeling. Traditional recommendation commonly focuses on users' single preferences for items while takeaway recommendation needs to comprehensively consider users' dual preferences for stores and foods. (2) Period-Varying Preference Modeling. Conventional recommendation generally models continuous changes in users' preferences from a session-level or day-level perspective. However, in practical takeaway systems, users' preferences vary significantly during the morning, noon, night, and late night periods of the day. To address these challenges, we propose a Dual Period-Varying Preference modeling (DPVP) for takeaway recommendation. Specifically, we design a dual interaction-aware module, aiming to capture users' dual preferences based on their interactions with stores and foods. Moreover, to model various preferences in different time periods of the day, we propose a time-based decomposition module as well as a time-aware gating mechanism. Extensive offline and online experiments demonstrate that our model outperforms state-of-the-art methods on real-world datasets and it is capable of modeling the dual period-varying preferences. Moreover, our model has been deployed online on Meituan Takeaway platform, leading to an average improvement in GMV (Gross Merchandise Value) of 0.70%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04370,June 2023,84,The influence of ICT use and related attitudes on students’ math and science performance: multilevel analyses of the last decade’s PISA surveys,"Matthew Courtney, Mehmet Karakus, Zara Ersozlu and Kaidar Nurumov","This study analyzed the latest four PISA surveys, 2009, 2012, 2015, and 2018, to explore the association between students’ ICT-related use and math and science performance. Using ICT Engagement Theory as a the...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-022-00128-6,2 August 2022,
85,0.000678989392134464,85,CorrMatch: Label Propagation via Correlation Matching for Semi-Supervised Semantic Segmentation,"Boyuan Sun, Yuqi Yang, Weifeng Yuan, Le Zhang, Ming-Ming Cheng, Qibin Hou","In this paper, we present a simple but performant semi-supervised semantic segmentation approach, termed CorrMatch. Our goal is to mine more high-quality regions from the unlabeled images to leverage the unlabeled data more efficiently via consistency regularization. The key contributions of our CorrMatch are two novel and complementary strategies. First, we introduce an adaptive threshold updating strategy with a relaxed initialization to expand the high-quality regions. Furthermore, we propose to propagate high-confidence predictions through measuring the pairwise similarities between pixels. Despite its simplicity, we show that CorrMatch achieves great performance on popular semi-supervised semantic segmentation benchmarks. Taking the DeepLabV3+ framework with ResNet-101 backbone as our segmentation model, we receive a 76%+ mIoU score on the Pascal VOC 2012 segmentation benchmark with only 92 annotated images provided. We also achieve a consistent improvement over previous semi-supervised semantic segmentation models. Code will be made publicly available. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04300,June 2023,85,Students' Perceptions of Computerized TOEFL Test,Parvane Azadi Noubandegani,The present study investigated students' perception of computerized TOEFL test. Subject of this study were 100 adult male and female students who took the iBT TOEFL test in Iran. Participants were provided wit...,https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/2229-0443-2-2-73,15 May 2012,
86,0.000678989392134464,86,Timing Process Interventions with Causal Inference and Reinforcement Learning,"Hans Weytjens, Wouter Verbeke, Jochen De Weerdt","The shift from the understanding and prediction of processes to their optimization offers great benefits to businesses and other organizations. Precisely timed process interventions are the cornerstones of effective optimization. Prescriptive process monitoring (PresPM) is the sub-field of process mining that concentrates on process optimization. The emerging PresPM literature identifies state-of-the-art methods, causal inference (CI) and reinforcement learning (RL), without presenting a quantitative comparison. Most experiments are carried out using historical data, causing problems with the accuracy of the methods' evaluations and preempting online RL. Our contribution consists of experiments on timed process interventions with synthetic data that renders genuine online RL and the comparison to CI possible, and allows for an accurate evaluation of the results. Our experiments reveal that RL's policies outperform those from CI and are more robust at the same time. Indeed, the RL policies approach perfect policies. Unlike CI, the unaltered online RL approach can be applied to other, more generic PresPM problems such as next best activity recommendations. Nonetheless, CI has its merits in settings where online learning is not an option. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04299,June 2023,86,WISE: a computer system performance index scoring framework,"Lorenzo Luciano, Imre Kiss, Peter William Beardshear, Esther Kadosh and A. Ben Hamza",The performance levels of a computing machine running a given workload configuration are crucial for both users and providers of computing resources. Knowing how well a computing machine is running with a give...,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-020-00224-4,20 January 2021,
87,0.000678989392134464,87,"Data Mining for Faster, Interpretable Solutions to Inverse Problems: A Case Study Using Additive Manufacturing","Chandrika Kamath, Juliette Franzman, Ravi Ponmalai","Solving inverse problems, where we find the input values that result in desired values of outputs, can be challenging. The solution process is often computationally expensive and it can be difficult to interpret the solution in high-dimensional input spaces. In this paper, we use a problem from additive manufacturing to address these two issues with the intent of making it easier to solve inverse problems and exploit their results. First, focusing on Gaussian process surrogates that are used to solve inverse problems, we describe how a simple modification to the idea of tapering can substantially speed up the surrogate without losing accuracy in prediction. Second, we demonstrate that Kohonen self-organizing maps can be used to visualize and interpret the solution to the inverse problem in the high-dimensional input space. For our data set, as not all input dimensions are equally important, we show that using weighted distances results in a better organized map that makes the relationships among the inputs obvious. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04228,June 2023,87,Quantum pricing with a smile: implementation of local volatility model on quantum computer,"Kazuya Kaneko, Koichi Miyamoto, Naoyuki Takeda and Kazuyoshi Yoshino","Quantum algorithms for the pricing of financial derivatives have been discussed in recent papers. However, the pricing model discussed in those papers is too simple for practical purposes. It motivates us to c...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00125-2,12 February 2022,
88,0.000678989392134464,88,Revisiting Neural Retrieval on Accelerators,"Jiaqi Zhai, Zhaojie Gong, Yueming Wang, Xiao Sun, Zheng Yan, Fu Li, Xing Liu","Retrieval finds a small number of relevant candidates from a large corpus for information retrieval and recommendation applications. A key component of retrieval is to model (user, item) similarity, which is commonly represented as the dot product of two learned embeddings. This formulation permits efficient inference, commonly known as Maximum Inner Product Search (MIPS). Despite its popularity, dot products cannot capture complex user-item interactions, which are multifaceted and likely high rank. We hence examine non-dot-product retrieval settings on accelerators, and propose \textit{mixture of logits} (MoL), which models (user, item) similarity as an adaptive composition of elementary similarity functions. This new formulation is expressive, capable of modeling high rank (user, item) interactions, and further generalizes to the long tail. When combined with a hierarchical retrieval strategy, \textit{h-indexer}, we are able to scale up MoL to 100M corpus on a single GPU with latency comparable to MIPS baselines. On public datasets, our approach leads to uplifts of up to 77.3\% in hit rate (HR). Experiments on a large recommendation surface at Meta showed strong metric gains and reduced popularity bias, validating the proposed approach's performance and improved generalization. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.04039,June 2023,88,Computer-aided identification of a series of novel ligands showing high potency as hepatitis C virus NS3/4A protease inhibitors,"Stephen Ejeh, Adamu Uzairu, Gideon Adamu Shallangwa and Stephen E. Abechi",Hepatitis C virus (HCV) is a global medical condition that causes several life-threatening chronic diseases in the liver. The conventional interferon-free treatment regimens are currently in use by a blend of ...,https://www.springeropen.com//BNRC.springeropen.com/articles/10.1186/s42269-020-00467-w,6 January 2021,
89,0.000678989392134464,89,Subgraph Networks Based Contrastive Learning,"Jinhuan Wang, Jiafei Shao, Zeyu Wang, Shanqing Yu, Qi Xuan, Xiaoniu Yang","Graph contrastive learning (GCL), as a self-supervised learning method, can solve the problem of annotated data scarcity. It mines explicit features in unannotated graphs to generate favorable graph representations for downstream tasks. Most existing GCL methods focus on the design of graph augmentation strategies and mutual information estimation operations. Graph augmentation produces augmented views by graph perturbations. These views preserve a locally similar structure and exploit explicit features. However, these methods have not considered the interaction existing in subgraphs. To explore the impact of substructure interactions on graph representations, we propose a novel framework called subgraph network-based contrastive learning (SGNCL). SGNCL applies a subgraph network generation strategy to produce augmented views. This strategy converts the original graph into an Edge-to-Node mapping network with both topological and attribute features. The single-shot augmented view is a first-order subgraph network that mines the interaction between nodes, node-edge, and edges. In addition, we also investigate the impact of the second-order subgraph augmentation on mining graph structure interactions, and further, propose a contrastive objective that fuses the first-order and second-order subgraph information. We compare SGNCL with classical and state-of-the-art graph contrastive learning methods on multiple benchmark datasets of different domains. Extensive experiments show that SGNCL achieves competitive or better performance (top three) on all datasets in unsupervised learning settings. Furthermore, SGNCL achieves the best average gain of 6.9\% in transfer learning compared to the best method. Finally, experiments also demonstrate that mining substructure interactions have positive implications for graph contrastive learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.03506,June 2023,89,Do self-created metacognitive prompts promote short- and long-term effects in computer-based learning environments?,"Katharina Engelmann, Maria Bannert and Nadine Melzner","Students must engage in self-regulated learning in computer-based learning environments; however, many students experience difficulties in doing so. Therefore, this study aims to investigate self-created metac...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-021-00148-w,10 February 2021,
90,0.000678989392134464,90,Joint Pre-training and Local Re-training: Transferable Representation Learning on Multi-source Knowledge Graphs,"Zequn Sun, Jiacheng Huang, Jinghao Lin, Xiaozhou Xu, Qijin Chen, Wei Hu","In this paper, we present the ``joint pre-training and local re-training'' framework for learning and applying multi-source knowledge graph (KG) embeddings. We are motivated by the fact that different KGs contain complementary information to improve KG embeddings and downstream tasks. We pre-train a large teacher KG embedding model over linked multi-source KGs and distill knowledge to train a student model for a task-specific KG. To enable knowledge transfer across different KGs, we use entity alignment to build a linked subgraph for connecting the pre-trained KGs and the target KG. The linked subgraph is re-trained for three-level knowledge distillation from the teacher to the student, i.e., feature knowledge distillation, network knowledge distillation, and prediction knowledge distillation, to generate more expressive embeddings. The teacher model can be reused for different target KGs and tasks without having to train from scratch. We conduct extensive experiments to demonstrate the effectiveness and efficiency of our framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.02679,June 2023,90,Profiling the pausing behaviour of EFL learners in real-time computer-aided writing: a multi-method case study,Cuiping Shen and Ningyang Chen,Technologically enhanced means and devices in language education and research have enabled an in-depth exploration of the dynamics of writing. This study investigated the pausing behaviour of eight Chinese Eng...,https://www.springeropen.com//sfleducation.springeropen.com/articles/10.1186/s40862-021-00118-1,16 August 2021,
91,0.000678989392134464,91,Predicting Information Pathways Across Online Communities,"Yiqiao Jin, Yeon-Chang Lee, Kartik Sharma, Meng Ye, Karan Sikka, Ajay Divakaran, Srijan Kumar","The problem of community-level information pathway prediction (CLIPP) aims at predicting the transmission trajectory of content across online communities. A successful solution to CLIPP holds significance as it facilitates the distribution of valuable information to a larger audience and prevents the proliferation of misinformation. Notably, solving CLIPP is non-trivial as inter-community relationships and influence are unknown, information spread is multi-modal, and new content and new communities appear over time. In this work, we address CLIPP by collecting large-scale, multi-modal datasets to examine the diffusion of online YouTube videos on Reddit. We analyze these datasets to construct community influence graphs (CIGs) and develop a novel dynamic graph framework, INPAC (Information Pathway Across Online Communities), which incorporates CIGs to capture the temporal variability and multi-modal nature of video propagation across communities. Experimental results in both warm-start and cold-start scenarios show that INPAC outperforms seven baselines in CLIPP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.02259,June 2023,91,Research computer software technology auxiliary material dyeing process,Yan-li Hu,"In the context of large-scale industrial production, the traditional tie-dyeing process cannot accommodate large-scale mass production. The current representative computer software simulates the variation of t...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0314-6,5 September 2018,
92,0.000678989392134464,92,On the Generalized Mean Densest Subgraph Problem: Complexity and Algorithms,"Chandra Chekuri, Manuel R. Torres","Dense subgraph discovery is an important problem in graph mining and network analysis with several applications. Two canonical problems here are to find a maxcore (subgraph of maximum min degree) and to find a densest subgraph (subgraph of maximum average degree). Both of these problems can be solved in polynomial time. Veldt, Benson, and Kleinberg [VBK21] introduced the generalized $p$-mean densest subgraph problem which captures the maxcore problem when $p=-\infty$ and the densest subgraph problem when $p=1$. They observed that the objective leads to a supermodular function when $p \ge 1$ and hence can be solved in polynomial time; for this case, they also developed a simple greedy peeling algorithm with a bounded approximation ratio. In this paper, we make several contributions. First, we prove that for any $p \in (-\frac{1}{8}, 0) \cup (0, \frac{1}{4})$ the problem is NP-Hard and for any $p \in (-3,0) \cup (0,1)$ the weighted version of the problem is NP-Hard, partly resolving a question left open in [VBK21]. Second, we describe two simple $1/2$-approximation algorithms for all $p < 1$, and show that our analysis of these algorithms is tight. For $p > 1$ we develop a fast near-linear time implementation of the greedy peeling algorithm from [VBK21]. This allows us to plug it into the iterative peeling algorithm that was shown to converge to an optimum solution [CQT22]. We demonstrate the efficacy of our algorithms by running extensive experiments on large graphs. Together, our results provide a comprehensive understanding of the complexity of the $p$-mean densest subgraph problem and lead to fast and provably good algorithms for the full range of $p$. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.02172,June 2023,92,“I Broke Your Game!”: critique among middle schoolers designing computer games about climate change,"Eli Tucker-Raymond, Gillian Puttick, Michael Cassidy, Casper Harteveld and Giovanni M. Troiano","There have been increasing calls for integrating computational thinking and computing into school science, mathematics, and engineering classrooms. The learning goals of the curriculum in this study included l...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-019-0194-z,5 December 2019,
93,0.000678989392134464,93,Acoustic Word Embeddings for Untranscribed Target Languages with Continued Pretraining and Learned Pooling,"Ramon Sanabria, Ondrej Klejch, Hao Tang, Sharon Goldwater","Acoustic word embeddings are typically created by training a pooling function using pairs of word-like units. For unsupervised systems, these are mined using k-nearest neighbor (KNN) search, which is slow. Recently, mean-pooled representations from a pre-trained self-supervised English model were suggested as a promising alternative, but their performance on target languages was not fully competitive. Here, we explore improvements to both approaches: we use continued pre-training to adapt the self-supervised model to the target language, and we use a multilingual phone recognizer (MPR) to mine phone n-gram pairs for training the pooling function. Evaluating on four languages, we show that both methods outperform a recent approach on word discrimination. Moreover, the MPR method is orders of magnitude faster than KNN, and is highly data efficient. We also show a small improvement from performing learned pooling on top of the continued pre-trained representations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.02153,June 2023,93,Computer-assisted evaluation of plant-derived β-secretase inhibitors in Alzheimer’s disease,"Md. Asad Ullah, Fatema Tuz Johora, Bishajit Sarkar, Yusha Araf, Nafisa Ahmed, Abida Nurun Nahar and Tanzina Akter",Alzheimer’s disease (AD) is a progressive neurodegenerative age-related dementia that results in memory loss of elderly people. Many hypotheses have been formally articulated till now to decipher the pathogene...,https://www.springeropen.com//jmhg.springeropen.com/articles/10.1186/s43042-021-00150-3,8 April 2021,
94,0.000678989392134464,94,Painsight: An Extendable Opinion Mining Framework for Detecting Pain Points Based on Online Customer Reviews,"Yukyung Lee, Jaehee Kim, Doyoon Kim, Yookyung Kho, Younsun Kim, Pilsung Kang","As the e-commerce market continues to expand and online transactions proliferate, customer reviews have emerged as a critical element in shaping the purchasing decisions of prospective buyers. Previous studies have endeavored to identify key aspects of customer reviews through the development of sentiment analysis models and topic models. However, extracting specific dissatisfaction factors remains a challenging task. In this study, we delineate the pain point detection problem and propose Painsight, an unsupervised framework for automatically extracting distinct dissatisfaction factors from customer reviews without relying on ground truth labels. Painsight employs pre-trained language models to construct sentiment analysis and topic models, leveraging attribution scores derived from model gradients to extract dissatisfaction factors. Upon application of the proposed methodology to customer review data spanning five product categories, we successfully identified and categorized dissatisfaction factors within each group, as well as isolated factors for each type. Notably, Painsight outperformed benchmark methods, achieving substantial performance enhancements and exceptional results in human evaluations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.02043,June 2023,94,Letter from the editor-in-chief,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192567,December 2008,
95,0.00673579331412596,95,UADB: Unsupervised Anomaly Detection Booster,"Hangting Ye, Zhining Liu, Xinyi Shen, Wei Cao, Shun Zheng, Xiaofan Gui, Huishuai Zhang, Yi Chang, Jiang Bian","Unsupervised Anomaly Detection (UAD) is a key data mining problem owing to its wide real-world applications. Due to the complete absence of supervision signals, UAD methods rely on implicit assumptions about anomalous patterns (e.g., scattered/sparsely/densely clustered) to detect anomalies. However, real-world data are complex and vary significantly across different domains. No single assumption can describe such complexity and be valid in all scenarios. This is also confirmed by recent research that shows no UAD method is omnipotent. Based on above observations, instead of searching for a magic universal winner assumption, we seek to design a general UAD Booster (UADB) that empowers any UAD models with adaptability to different data. This is a challenging task given the heterogeneous model structures and assumptions adopted by existing UAD methods. To achieve this, we dive deep into the UAD problem and find that compared to normal data, anomalies (i) lack clear structure/pattern in feature space, thus (ii) harder to learn by model without a suitable assumption, and finally, leads to (iii) high variance between different learners. In light of these findings, we propose to (i) distill the knowledge of the source UAD model to an imitation learner (booster) that holds no data assumption, then (ii) exploit the variance between them to perform automatic correction, and thus (iii) improve the booster over the original UAD model. We use a neural network as the booster for its strong expressive power as a universal approximator and ability to perform flexible post-hoc tuning. Note that UADB is a model-agnostic framework that can enhance heterogeneous UAD models in a unified way. Extensive experiments on over 80 tabular datasets demonstrate the effectiveness of UADB. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.01997,June 2023,95,Steady-State VEP-Based Brain-Computer Interface Control in an Immersive 3D Gaming Environment,"E. C. Lalor, S. P. Kelly, C. Finucane, R. Burke, R. Smith, R. B. Reilly and G. McDarby",This paper presents the application of an effective EEG-based brain-computer interface design for binary control in a visually elaborate immersive 3D game. The BCI uses the steady-state visual evoked potential...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.3156,17 November 2005,
96,0.000678989392134464,96,Differential Privacy with Random Projections and Sign Random Projections,"Ping Li, Xiaoyun Li","In this paper, we develop a series of differential privacy (DP) algorithms from a family of random projections (RP) for general applications in machine learning, data mining, and information retrieval. Among the presented algorithms, iDP-SignRP is remarkably effective under the setting of ``individual differential privacy'' (iDP), based on sign random projections (SignRP). Also, DP-SignOPORP considerably improves existing algorithms in the literature under the standard DP setting, using ``one permutation + one random projection'' (OPORP), where OPORP is a variant of the celebrated count-sketch method with fixed-length binning and normalization. Without taking signs, among the DP-RP family, DP-OPORP achieves the best performance. Our key idea for improving DP-RP is to take only the signs, i.e., $sign(x_j) = sign\left(\sum_{i=1}^p u_i w_{ij}\right)$, of the projected data. The intuition is that the signs often remain unchanged when the original data ($u$) exhibit small changes (according to the ``neighbor'' definition in DP). In other words, the aggregation and quantization operations themselves provide good privacy protections. We develop a technique called ``smooth flipping probability'' that incorporates this intuitive privacy benefit of SignRPs and improves the standard DP bit flipping strategy. Based on this technique, we propose DP-SignOPORP which satisfies strict DP and outperforms other DP variants based on SignRP (and RP), especially when $ε$ is not very large (e.g., $ε= 5\sim10$). Moreover, if an application scenario accepts individual DP, then we immediately obtain an algorithm named iDP-SignRP which achieves excellent utilities even at small~$ε$ (e.g., $ε<0.5$). △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.01751,June 2023,96,The computational thinking for science (CT-S) framework: operationalizing CT-S for K–12 science education researchers and educators,"Timothy Hurt, Eric Greenwald, Sara Allan, Matthew A. Cannady, Ari Krakowski, Lauren Brodsky, Melissa A. Collins, Ryan Montgomery and Rena Dorph","Contemporary science is a field that is becoming increasingly computational. Today’s scientists not only leverage computational tools to conduct their investigations, they often must contribute to the design o...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00391-7,5 January 2023,
97,0.000678989392134464,97,Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations,"Pan Li, Yuyan Wang, Ed H. Chi, Minmin Chen","Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand. In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendation component, the extracted aspects are concatenated with the usual user and item features used by the recommendation model. The recommendation task mediates the learning of the user embeddings and item embeddings, which are used as soft prompts to generate aspects. Therefore, the extracted aspects are personalized and contextualized by the recommendation task. We showcase the effectiveness of our proposed method through extensive experiments on three industrial datasets, where our proposed framework significantly outperforms state-of-the-art baselines in both the personalized aspect extraction and aspect-based recommendation tasks. In particular, we demonstrate that it is necessary and beneficial to combine the learning of aspect extraction and aspect-based recommendation together. We also conduct extensive ablation studies to understand the contribution of each design component in our framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.01475,June 2023,97,Improved computer-aided detection of pulmonary nodules via deep learning in the sinogram domain,"Yongfeng Gao, Jiaxing Tan, Zhengrong Liang, Lihong Li and Yumei Huo","Computer aided detection (CADe) of pulmonary nodules plays an important role in assisting radiologists’ diagnosis and alleviating interpretation burden for lung cancer. Current CADe systems, aiming at simulati...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-019-0029-2,22 November 2019,
98,0.000678989392134464,98,Introduction of Medical Imaging Modalities,"S. K. M Shadekul Islam, MD Abdullah Al Nasim, Ismail Hossain, Dr. Md Azim Ullah, Dr. Kishor Datta Gupta, Md Monjur Hossain Bhuiyan","The diagnosis and treatment of various diseases had been expedited with the help of medical imaging. Different medical imaging modalities, including X-ray, Computed Tomography (CT), Magnetic Resonance Imaging (MRI), Nuclear Imaging, Ultrasound, Electrical Impedance Tomography (EIT), and Emerging Technologies for in vivo imaging modalities is presented in this chapter, in addition to these modalities, some advanced techniques such as contrast-enhanced MRI, MR approaches for osteoarthritis, Cardiovascular Imaging, and Medical Imaging data mining and search. Despite its important role and potential effectiveness as a diagnostic tool, reading and interpreting medical images by radiologists is often tedious and difficult due to the large heterogeneity of diseases and the limitation of image quality or resolution. Besides the introduction and discussion of the basic principles, typical clinical applications, advantages, and limitations of each modality used in current clinical practice, this chapter also highlights the importance of emerging technologies in medical imaging and the role of data mining and search aiming to support translational clinical research, improve patient care, and increase the efficiency of the healthcare system. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.01022,June 2023,98,Analysis of computer network attack based on the virus propagation model,"Yanshan He, Ting Wang, Jianli Xie and Ming Zhang","The conventional method can make a reasonable analysis of common network attacks, but the reliability of the analysis is low under the virus propagation model. This paper proposes a new research method of comp...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-020-1660-5,12 March 2020,
99,0.000678989392134464,99,Modeling and Analyzing Scorer Preferences in Short-Answer Math Questions,"Mengxue Zhang, Neil Heffernan, Andrew Lan","Automated scoring of student responses to open-ended questions, including short-answer questions, has great potential to scale to a large number of responses. Recent approaches for automated scoring rely on supervised learning, i.e., training classifiers or fine-tuning language models on a small number of responses with human-provided score labels. However, since scoring is a subjective process, these human scores are noisy and can be highly variable, depending on the scorer. In this paper, we investigate a collection of models that account for the individual preferences and tendencies of each human scorer in the automated scoring task. We apply these models to a short-answer math response dataset where each response is scored (often differently) by multiple different human scorers. We conduct quantitative experiments to show that our scorer models lead to improved automated scoring accuracy. We also conduct quantitative experiments and case studies to analyze the individual preferences and tendencies of scorers. We found that scorers can be grouped into several obvious clusters, with each cluster having distinct features, and analyzed them in detail. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00791,June 2023,99,Letter from the Editor-in-Chief,Jaelson F. B. Castro,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192384,February 2006,
100,0.000678989392134464,100,UNGOML: Automated Classification of unsafe Usages in Go,"Anna-Katharina Wickert, Clemens Damke, Lars Baumgärtner, Eyke Hüllermeier, Mira Mezini","The Go programming language offers strong protection from memory corruption. As an escape hatch of these protections, it provides the unsafe package. Previous studies identified that this unsafe package is frequently used in real-world code for several purposes, e.g., serialization or casting types. Due to the variety of these reasons, it may be possible to refactor specific usages to avoid potential vulnerabilities. However, the classification of unsafe usages is challenging and requires the context of the call and the program's structure. In this paper, we present the first automated classifier for unsafe usages in Go, UNGOML, to identify what is done with the unsafe package and why it is used. For UNGOML, we built four custom deep learning classifiers trained on a manually labeled data set. We represent Go code as enriched control-flow graphs (CFGs) and solve the label prediction task with one single-vertex and three context-aware classifiers. All three context-aware classifiers achieve a top-1 accuracy of more than 86% for both dimensions, WHAT and WHY. Furthermore, in a set-valued conformal prediction setting, we achieve accuracies of more than 93% with mean label set sizes of 2 for both dimensions. Thus, UNGOML can be used to efficiently filter unsafe usages for use cases such as refactoring or a security audit. UNGOML: https://github.com/stg-tud/ungoml Artifact: https://dx.doi.org/10.6084/m9.figshare.22293052 △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00694,June 2023,100,The promise of computer adaptive testing in collection of orthopaedic outcomes: an evaluation of PROMIS utilization,Liam H. Wong and James E. Meeker,"A crucial component to improving patient care is better clinician understanding of patients’ health-related quality of life (HRQoL). In orthopaedic surgery, HRQoL assessment instruments such as the NIH develop...",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-021-00407-w,4 January 2022,
101,0.000678989392134464,101,Representation Theorems Obtained by Miningacross Web Sources for Hints,"Marco B. Caminati, Juliana K. F. Bowles","A representation theorem relates different mathematical structures by providing an isomorphism between them: that is, a one-to-one correspondence preserving their original properties. Establishing that the two structures substantially behave in the same way, representation theorems typically provide insight and generate powerful techniques to study the involved structures, by cross-fertilising between the methodologies existing for each of the respective branches of mathematics. When the related structures have no obvious a priori connection, however, such results can be, by their own nature, elusive. Here, we show how data-mining across distinct web sources (including the Online Encyclopedia of Integer Sequences, OEIS), was crucial in the discovery of two original representation theorems relating event structures (mathematical structures commonly used to represent concurrent discrete systems) to families of sets (endowed with elementary disjointness and subset relations) and to full graphs, respectively. The latter originally emerged in the apparently unrelated field of bioinformatics. As expected, our representation theorems are powerful, allowing to capitalise on existing theorems about full graphs to immediately conclude new facts about event structures. Our contribution is twofold: on one hand, we illustrate our novel method to mine the web, resulting in thousands of candidate connections between distinct mathematical realms; on the other hand, we explore one of these connections to obtain our new representation theorems. We hope this paper can encourage people with relevant expertise to scrutinize these candidate connections. We anticipate that, building on the ideas presented here, further connections can be unearthed, by refining the mining techniques and by extending the mined repositories. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00584,June 2023,101,Quantum technology for military applications,Michal Krelina,"Quantum technology is an emergent and potentially disruptive discipline, with the ability to affect many human activities. Quantum technologies are dual-use technologies, and as such are of interest to the def...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00113-y,6 November 2021,
102,0.000678989392134464,102,Example-based Motion Synthesis via Generative Motion Matching,"Weiyu Li, Xuelin Chen, Peizhuo Li, Olga Sorkine-Hornung, Baoquan Chen","We present GenMM, a generative model that ""mines"" as many diverse motions as possible from a single or few example sequences. In stark contrast to existing data-driven methods, which typically require long offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, GenMM inherits the training-free nature and the superior quality of the well-known Motion Matching method. GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large skeletal structures. At the heart of our generative framework lies the generative motion matching module, which utilizes the bidirectional visual similarity as a generative cost function to motion matching, and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. In addition to diverse motion generation, we show the versatility of our generative framework by extending it to a number of scenarios that are not possible with motion matching alone, including motion completion, key frame-guided generation, infinite looping, and motion reassembly. Code and data for this paper are at https://wyysf-98.github.io/GenMM/ △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00378,June 2023,102,Computer-aided analysis in evaluation and grading of interstitial lung diseases in correlation with CT-based visual scoring and pulmonary function tests,"Mahmoud M. Higazi, Ehab Ali Abdelgawad, Ahmed H. Kaseem and Kerria Raif Adly",Interstitial lung diseases (ILDs) represent a large group of more than 200 different entities. High resolution computed tomography (HRCT) is accepted as the gold standard imaging modality in the diagnosis of I...,https://www.springeropen.com//ejrnm.springeropen.com/articles/10.1186/s43055-020-00201-6,18 May 2020,
103,0.000678989392134464,103,Explicit Feature Interaction-aware Uplift Network for Online Marketing,"Dugang Liu, Xing Tang, Han Gao, Fuyuan Lyu, Xiuqiang He","As a key component in online marketing, uplift modeling aims to accurately capture the degree to which different treatments motivate different users, such as coupons or discounts, also known as the estimation of individual treatment effect (ITE). In an actual business scenario, the options for treatment may be numerous and complex, and there may be correlations between different treatments. In addition, each marketing instance may also have rich user and contextual features. However, existing methods still fall short in both fully exploiting treatment information and mining features that are sensitive to a particular treatment. In this paper, we propose an explicit feature interaction-aware uplift network (EFIN) to address these two problems. Our EFIN includes four customized modules: 1) a feature encoding module encodes not only the user and contextual features, but also the treatment features; 2) a self-interaction module aims to accurately model the user's natural response with all but the treatment features; 3) a treatment-aware interaction module accurately models the degree to which a particular treatment motivates a user through interactions between the treatment features and other features, i.e., ITE; and 4) an intervention constraint module is used to balance the ITE distribution of users between the control and treatment groups so that the model would still achieve a accurate uplift ranking on data collected from a non-random intervention marketing scenario. We conduct extensive experiments on two public datasets and one product dataset to verify the effectiveness of our EFIN. In addition, our EFIN has been deployed in a credit card bill payment scenario of a large online financial platform with a significant improvement. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00315,June 2023,103,Computer-aided diagnosis of renal obstruction: utility of log-linear modeling versus standard ROC and kappa analysis,"Amita K Manatunga, José Nilo G Binongo and Andrew T Taylor","The accuracy of computer-aided diagnosis (CAD) software is best evaluated by comparison to a gold standard which represents the true status of disease. In many settings, however, knowledge of the true status o...",https://www.springeropen.com//ejnmmires.springeropen.com/articles/10.1186/2191-219X-1-5,20 June 2011,
104,0.000678989392134464,104,Broadband VLA Spectral Line Survey of a Sample of Ionized Jet Candidates,"E. Sanchez-Tovar, E. D. Araya, V. Rosero, P. Hofner, S. Kurtz","The study of the interaction between ionized jets, molecular outflows and their environments is critical to understanding high-mass star formation, especially because jets and outflows are thought to be key in the transfer of angular momentum outwards from accretion disks. We report a low-spectral resolution VLA survey for hydrogen radio recombination lines, OH, NH$_3$, and CH$_3$OH lines toward a sample of 58 high-mass star forming regions that contain numerous ionized jet candidates. The observations are from a survey designed to detect radio continuum; the novel aspect of this work is to search for spectral lines in broadband VLA data (we provide the script developed in this work to facilitate exploration of other datasets). We report detection of 25$\,$GHz CH$_3$OH transitions toward ten sources; five of them also show NH$_3$ emission. We found that most of the sources detected in CH$_3$OH and NH$_3$ have been classified as ionized jets or jet candidates and that the emission lines are coincident with, or very near ($\lesssim 0.1$ pc) these sources, hence, these molecular lines could be used as probes of the environment near the launching site of jets/outflows. No radio recombination lines were detected, but we found that the RMS noise of stacked spectra decreases following the radiometer equation. Therefore, detecting radio recombination lines in a sample of brighter free-free continuum sources should be possible. This work demonstrates the potential of broadband VLA continuum observations as low-resolution spectral line scans. △ Less",https://arxiv.orghttps://arxiv.org/abs/2306.00111,June 2023,104,Computer-assisted analysis of painting brushstrokes: digital image processing for unsupervised extraction of visible features from van Gogh’s works,"Fabrizio Lamberti, Andrea Sanna and Gianluca Paravati","The automatic extraction of objective features from paintings, like brushstroke distribution, orientation, and shape, could be particularly useful for different artwork analyses and management tasks. In fact, ...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/1687-5281-2014-53,17 December 2014,
105,0.000678989392134464,105,A Survey of Potential MPI Complex Collectives: Large-Scale Mining and Analysis of HPC Applications,"Pouya Haghi, Ryan Marshall, Po Hao Chen, Anthony Skjellum, Martin Herbordt","Offload of MPI collectives to network devices, e.g., NICs and switches, is being implemented as an effective mechanism to improve application performance by reducing inter- and intra-node communication and bypassing MPI software layers. Given the rich deployment of accelerators and programmable NICs/switches in data centers, we posit that there is an opportunity to further improve performance by extending this idea (of in-network collective processing) to a new class of more complex collectives. The most basic type of complex collective is the fusion of existing collectives. In previous work we have demonstrated the efficacy of this additional hardware and software support and shown that it can substantially improve the performance of certain applications. In this work we extend this approach. We seek to characterize a large number of MPI applications to determine overall applicability, both breadth and type, and so provide insight for hardware designers and MPI developers about future offload possibilities. Besides increasing the scope of prior surveys to include finding (potential) new MPI constructs, we also tap into new methods to extend the survey process. Prior surveys on MPI usage considered lists of applications constructed based on application developers' knowledge. The approach taken in this paper, however, is based on an automated mining of a large collection of code sources. More specifically, the mining is accomplished by GitHub REST APIs. We use a database management system to store the results and to answer queries. Another advantage is that this approach provides support for a more complex analysis of MPI usage, which is accomplished by user queries. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.19946,May 2023,105,A Time-Frequency Approach to Feature Extraction for a Brain-Computer Interface with a Comparative Analysis of Performance Measures,"Damien Coyle, Girijesh Prasad and T. M. McGinnity",The paper presents an investigation into a time-frequency (TF) method for extracting features from the electroencephalogram (EEG) recorded from subjects performing imagination of left- and right-hand movements...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.3141,17 November 2005,
106,0.000678989392134464,106,Fully Dynamic Submodular Maximization over Matroids,"Paul Dütting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, Morteza Zadimoghaddam","Maximizing monotone submodular functions under a matroid constraint is a classic algorithmic problem with multiple applications in data mining and machine learning. We study this classic problem in the fully dynamic setting, where elements can be both inserted and deleted in real-time. Our main result is a randomized algorithm that maintains an efficient data structure with an $\tilde{O}(k^2)$ amortized update time (in the number of additions and deletions) and yields a $4$-approximate solution, where $k$ is the rank of the matroid. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.19918,May 2023,106,A human-computer collaborative workflow for the acquisition and analysis of terrestrial insect movement in behavioral field studies,"Khairi Reda, Victor Mateevitsi and Catherine Offord","The study of insect behavior from video sequences poses many challenges. Despite the advances in image processing techniques, the current generation of insect tracking tools is only effective in controlled lab...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/1687-5281-2013-48,13 August 2013,
107,0.000678989392134464,107,Large language models improve Alzheimer's disease diagnosis using multi-modality data,"Yingjie Feng, Jun Wang, Xianfeng Gu, Xiaoyin Xu, Min Zhang","In diagnosing challenging conditions such as Alzheimer's disease (AD), imaging is an important reference. Non-imaging patient data such as patient information, genetic data, medication information, cognitive and memory tests also play a very important role in diagnosis. Effect. However, limited by the ability of artificial intelligence models to mine such information, most of the existing models only use multi-modal image data, and cannot make full use of non-image data. We use a currently very popular pre-trained large language model (LLM) to enhance the model's ability to utilize non-image data, and achieved SOTA results on the ADNI dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.19280,May 2023,107,Which STEM careers are most appealing? Examining high school students’ preferences and motivational beliefs for different STEM career choices,Emily Q. Rosenzweig and Xiao-Yin Chen,"Decades of research have examined what motivates students to pursue careers in science, technology, engineering, and mathematics (STEM) fields, but STEM careers are a broad category encompassing hundreds of di...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00427-6,9 June 2023,
108,0.000678989392134464,108,FakeSwarm: Improving Fake News Detection with Swarming Characteristics,"Jun Wu, Xuesong Ye","The proliferation of fake news poses a serious threat to society, as it can misinform and manipulate the public, erode trust in institutions, and undermine democratic processes. To address this issue, we present FakeSwarm, a fake news identification system that leverages the swarming characteristics of fake news. To extract the swarm behavior, we propose a novel concept of fake news swarming characteristics and design three types of swarm features, including principal component analysis, metric representation, and position encoding. We evaluate our system on a public dataset and demonstrate the effectiveness of incorporating swarm features in fake news identification, achieving an f1-score and accuracy of over 97% by combining all three types of swarm features. Furthermore, we design an online learning pipeline based on the hypothesis of the temporal distribution pattern of fake news emergence, validated on a topic with early emerging fake news and a shortage of text samples, showing that swarm features can significantly improve recall rates in such cases. Our work provides a new perspective and approach to fake news detection and highlights the importance of considering swarming characteristics in detecting fake news. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.19194,May 2023,108,Application of the SP theory of intelligence to the understanding of natural vision and the development of computer vision,J. Gerard Wolff,"TheSP theory of intelligenceaims to simplify and integrate concepts in computing and cognition, with information compression as a unifying theme. This article is about how the SP theory may, with advantage, be ...",https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-3-552,21 October 2014,
109,0.000678989392134464,109,PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series,Wenjie Du,"PyPOTS is an open-source Python library dedicated to data mining and analysis on multivariate partially-observed time series, i.e. incomplete time series with missing values, A.K.A. irregularlysampled time series. Particularly, it provides easy access to diverse algorithms categorized into four tasks: imputation, classification, clustering, and forecasting. The included models contain probabilistic approaches as well as neural-network methods, with a well-designed and fully-documented programming interface for both academic researchers and industrial professionals to use. With robustness and scalability in its design philosophy, best practices of software construction, for example, unit testing, continuous integration (CI) and continuous delivery (CD), code coverage, maintainability evaluation, interactive tutorials, and parallelization, are carried out as principles during the development of PyPOTS. The toolkit is available on both Python Package Index (PyPI) and Anaconda. PyPOTS is open-source and publicly available on GitHub https://github.com/WenjieDu/PyPOTS. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18811,May 2023,109,Periodic plane-wave electronic structure calculations on quantum computers,"Duo Song, Nicholas P. Bauman, Guen Prawiroatmodjo, Bo Peng, Cassandra Granade, Kevin M. Rosso, Guang Hao Low, Martin Roetteler, Karol Kowalski and Eric J. Bylaska","A procedure for defining virtual spaces, and the periodic one-electron and two-electron integrals, for plane-wave second quantized Hamiltonians has been developed, and it was validated using full configuration...",https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-022-00049-5,3 January 2023,
110,0.000678989392134464,110,Fast and Accurate Dual-Way Streaming PARAFAC2 for Irregular Tensors -- Algorithm and Application,"Jun-Gi Jang, Jeongyoung Lee, Yong-chan Park, U Kang","How can we efficiently and accurately analyze an irregular tensor in a dual-way streaming setting where the sizes of two dimensions of the tensor increase over time? What types of anomalies are there in the dual-way streaming setting? An irregular tensor is a collection of matrices whose column lengths are the same while their row lengths are different. In a dual-way streaming setting, both new rows of existing matrices and new matrices arrive over time. PARAFAC2 decomposition is a crucial tool for analyzing irregular tensors. Although real-time analysis is necessary in the dual-way streaming, static PARAFAC2 decomposition methods fail to efficiently work in this setting since they perform PARAFAC2 decomposition for accumulated tensors whenever new data arrive. Existing streaming PARAFAC2 decomposition methods work in a limited setting and fail to handle new rows of matrices efficiently. In this paper, we propose Dash, an efficient and accurate PARAFAC2 decomposition method working in the dual-way streaming setting. When new data are given, Dash efficiently performs PARAFAC2 decomposition by carefully dividing the terms related to old and new data and avoiding naive computations involved with old data. Furthermore, applying a forgetting factor makes Dash follow recent movements. Extensive experiments show that Dash achieves up to 14.0x faster speed than existing PARAFAC2 decomposition methods for newly arrived data. We also provide discoveries for detecting anomalies in real-world datasets, including Subprime Mortgage Crisis and COVID-19. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18376,May 2023,110,Motivating youth to learn STEM through a gender inclusive digital forensic science program,"Eoghan Casey, Jennifer Jocz, Karen A. Peterson, Daryl Pfeif and Cassy Soden","This paper describes the design, implementation and research of the Cyber Sleuth Science Lab (CSSL), an innovative educational program and supporting virtual learning environment, that combines pedagogical the...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-022-00213-x,6 January 2023,
111,0.000678989392134464,111,Pure Spectral Graph Embeddings: Reinterpreting Graph Convolution for Top-N Recommendation,"Edoardo D'Amico, Aonghus Lawlor, Neil Hurley","The use of graph convolution in the development of recommender system algorithms has recently achieved state-of-the-art results in the collaborative filtering task (CF). While it has been demonstrated that the graph convolution operation is connected to a filtering operation on the graph spectral domain, the theoretical rationale for why this leads to higher performance on the collaborative filtering problem remains unknown. The presented work makes two contributions. First, we investigate the effect of using graph convolution throughout the user and item representation learning processes, demonstrating how the latent features learned are pushed from the filtering operation into the subspace spanned by the eigenvectors associated with the highest eigenvalues of the normalised adjacency matrix, and how vectors lying on this subspace are the optimal solutions for an objective function related to the sum of the prediction function over the training data. Then, we present an approach that directly leverages the eigenvectors to emulate the solution obtained through graph convolution, eliminating the requirement for a time-consuming gradient descent training procedure while also delivering higher performance on three real-world datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18374,May 2023,111,Post-stroke aphasia rehabilitation using computer-based Arabic software program: a randomized controlled trial,"Engy Samy Elhakeem, Sabah Saeed Gommaa Mohamed Saeed, Ramy Nabil Abd-Elkader Elsalakawy, Reham Mohamed Elmaghraby and Ghada Abdel Hady Ossman Ashmawy","Aphasia is considered an acquired communication disorder. Language intervention in aphasia enhances the patient outcomes. Recently, computer programs are developed for the treatment of aphasia. It is an effect...",https://www.springeropen.com//ejo.springeropen.com/articles/10.1186/s43163-021-00144-3,28 July 2021,
112,0.000678989392134464,112,Assumption Generation for the Verification of Learning-Enabled Autonomous Systems,"Corina Pasareanu, Ravi Mangal, Divya Gopinath, Huafeng Yu","Providing safety guarantees for autonomous systems is difficult as these systems operate in complex environments that require the use of learning-enabled components, such as deep neural networks (DNNs) for visual perception. DNNs are hard to analyze due to their size (they can have thousands or millions of parameters), lack of formal specifications (DNNs are typically learnt from labeled data, in the absence of any formal requirements), and sensitivity to small changes in the environment. We present an assume-guarantee style compositional approach for the formal verification of system-level safety properties of such autonomous systems. Our insight is that we can analyze the system in the absence of the DNN perception components by automatically synthesizing assumptions on the DNN behaviour that guarantee the satisfaction of the required safety properties. The synthesized assumptions are the weakest in the sense that they characterize the output sequences of all the possible DNNs that, plugged into the autonomous system, guarantee the required safety properties. The assumptions can be leveraged as run-time monitors over a deployed DNN to guarantee the safety of the overall system; they can also be mined to extract local specifications for use during training and testing of DNNs. We illustrate our approach on a case study taken from the autonomous airplanes domain that uses a complex DNN for perception. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18372,May 2023,112,RETRACTED ARTICLE: A smart position optimization scheme for badminton doubles based on human–computer interactive training in wireless sensor networks,Bo Yao and Na Liang,"In the continuous development process, robot technology based on multimedia interactions has been widely used in aerospace, medical, education and service industries. The relationship between robots and humans...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-020-01847-6,12 November 2020,
113,0.000678989392134464,113,Mapping ChatGPT in Mainstream Media: Early Quantitative Insights through Sentiment Analysis and Word Frequency Analysis,Maya Karanouh,"The exponential growth in user acquisition and popularity of ChatGPT, an artificial intelligence(AI) powered chatbot, was accompanied by widespread mainstream media coverage. This article presents a quantitative data analysis of the early trends and sentiments revealed by conducting text mining and NLP methods onto a corpus of 10,902 mainstream news headlines related to the subject of ChatGPT and artificial intelligence, from the launch of ChatGPT in November 2022 to March 2023. The findings revealed in sentiment analysis, ChatGPT and artificial intelligence, were perceived more positively than negatively in the mainstream media. In regards to word frequency results, over sixty-five percent of the top frequency words were focused on Big Tech issues and actors while topics such as jobs, diversity, ethics, copyright, gender and women were poorly represented or completely absent and only accounted for six percent of the total corpus. This article is a critical analysis into the power structures and collusions between Big Tech and Big Media in their matrix of domination. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18340,May 2023,113,Computer-assisted coloring and illuminating based on a region-tree structure,"Renata Nascimento, Fabiane Queiroz, Allan Rocha, Tsang Ing Ren, Vinicius Mello and Adelailson Peixoto",Colorization and illumination are key processes for creating animated cartoons. Computer assisted methods have been incorporated in animation/illustration systems to reduce the artists' workload. This paper pr...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-1-1,6 March 2012,
114,0.000678989392134464,114,Representation Learning on Hyper-Relational and Numeric Knowledge Graphs with Transformers,"Chanyoung Chung, Jaejun Lee, Joyce Jiyoung Whang","A hyper-relational knowledge graph has been recently studied where a triplet is associated with a set of qualifiers; a qualifier is composed of a relation and an entity, providing auxiliary information for a triplet. While existing hyper-relational knowledge graph embedding methods assume that the entities are discrete objects, some information should be represented using numeric values, e.g., (J.R.R., was born in, 1892). Also, a triplet (J.R.R., educated at, Oxford Univ.) can be associated with a qualifier such as (start time, 1911). In this paper, we propose a unified framework named HyNT that learns representations of a hyper-relational knowledge graph containing numeric literals in either triplets or qualifiers. We define a context transformer and a prediction transformer to learn the representations based not only on the correlations between a triplet and its qualifiers but also on the numeric information. By learning compact representations of triplets and qualifiers and feeding them into the transformers, we reduce the computation cost of using transformers. Using HyNT, we can predict missing numeric values in addition to missing entities or relations in a hyper-relational knowledge graph. Experimental results show that HyNT significantly outperforms state-of-the-art methods on real-world datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.18256,May 2023,114,Automated computer quantification of breast cancer in small-animal models using PET-guided MR image co-segmentation,"Ulas Bagci, Gabriela Kramer-Marek and Daniel J Mollura",Care providers use complementary information from multiple imaging modalities to identify and characterize metastatic tumors in early stages and perform surveillance for cancer recurrence. These tasks require ...,https://www.springeropen.com//ejnmmires.springeropen.com/articles/10.1186/2191-219X-3-49,5 July 2013,
115,0.000678989392134464,115,"minOffense: Inter-Agreement Hate Terms for Stable Rules, Concepts, Transitivities, and Lattices","Animesh Chaturvedi, Rajesh Sharma","Hate speech classification has become an important problem due to the spread of hate speech on social media platforms. For a given set of Hate Terms lists (HTs-lists) and Hate Speech data (HS-data), it is challenging to understand which hate term contributes the most for hate speech classification. This paper contributes two approaches to quantitatively measure and qualitatively visualise the relationship between co-occurring Hate Terms (HTs). Firstly, we propose an approach for the classification of hate-speech by producing a Severe Hate Terms list (Severe HTs-list) from existing HTs-lists. To achieve our goal, we proposed three metrics (Hatefulness, Relativeness, and Offensiveness) to measure the severity of HTs. These metrics assist to create an Inter-agreement HTs-list, which explains the contribution of an individual hate term toward hate speech classification. Then, we used the Offensiveness metric values of HTs above a proposed threshold minimum Offense (minOffense) to generate a new Severe HTs-list. To evaluate our approach, we used three hate speech datasets and six hate terms lists. Our approach shown an improvement from 0.845 to 0.923 (best) as compared to the baseline. Secondly, we also proposed Stable Hate Rule (SHR) mining to provide ordered co-occurrence of various HTs with minimum Stability (minStab). The SHR mining detects frequently co-occurring HTs to form Stable Hate Rules and Concepts. These rules and concepts are used to visualise the graphs of Transitivities and Lattices formed by HTs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.17984,May 2023,115,Automatic Bone Surface Restoration for Markerless Computer-Assisted Orthopaedic Surgery,Xue Hu and Ferdinando Rodriguez y Baena,"An automatic markerless knee tracking and registration algorithm has been proposed in the literature to avoid the marker insertion required by conventional computer-assisted knee surgery, resulting in a shorte...",https://www.springeropen.com//cjme.springeropen.com/articles/10.1186/s10033-022-00684-6,4 March 2022,
116,0.000678989392134464,116,Revisiting the Alpha Algorithm To Enable Real-Life Process Discovery Applications -- Extended Report,"Aaron Küsters, Wil M. P. van der Aalst","The Alpha algorithm was the first process discovery algorithm that was able to discover process models with concurrency based on incomplete event data while still providing formal guarantees. However, as was stated in the original paper, practical applicability is limited when dealing with exceptional behavior and processes that cannot be described as a structured workflow net without short loops. This paper presents the Alpha+++ algorithm that overcomes many of these limitations, making the algorithm competitive with more recent process mining approaches. The different steps provide insights into the practical challenges of learning process models with concurrency, choices, sequences, loops, and skipping from event data. The approach was implemented in ProM and tested on various publicly available, real-life event logs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.17767,May 2023,116,A computer simulation study on the mode conversion process from slow X-mode to fast X-mode by the tunneling effect,Yuto Katoh and Masahide Iizima,"The mode conversion process from slow X-mode waves to fast X-mode waves by the tunneling effect has been studied by means of computer simulation. On the basis of the new approach by the simulation, we have con...",https://www.springeropen.com//earth-planets-space.springeropen.com/articles/10.1186/BF03352641,15 November 2006,
117,0.000678989392134464,117,Overlapping and Robust Edge-Colored Clustering in Hypergraphs,"Alex Crane, Brian Lavallee, Blair D. Sullivan, Nate Veldt","A recent trend in data mining has explored (hyper)graph clustering algorithms for data with categorical relationship types. Such algorithms have applications in the analysis of social, co-authorship, and protein interaction networks, to name a few. Many such applications naturally have some overlap between clusters, a nuance which is missing from current combinatorial models. Additionally, existing models lack a mechanism for handling noise in datasets. We address these concerns by generalizing Edge-Colored Clustering, a recent framework for categorical clustering of hypergraphs. Our generalizations allow for a budgeted number of either (a) overlapping cluster assignments or (b) node deletions. For each new model we present a greedy algorithm which approximately minimizes an edge mistake objective, as well as bicriteria approximations where the second approximation factor is on the budget. Additionally, we address the parameterized complexity of each problem, providing FPT algorithms and hardness results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.17598,May 2023,117,Computer Simulation of Water-Mediated Adhesion Between Phospholipid Bilayer and Solid Support Functionalized with Self-Assembled Monolayers,Alexander Pertsin and Michael Grunze,"An attempt is made to estimate, via computer simulation of the force–distance relation, the free energy of adhesion between a phosphatidylethanolamine bilayer and an alkanethiolate self-assembled monolayer (SA...",https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1007/s13758-012-0057-3,28 August 2012,
118,0.000678989392134464,118,A Framework For Refining Text Classification and Object Recognition from Academic Articles,"Jinghong Li, Koichi Ota, Wen Gu, Shinobu Hasegawa","With the widespread use of the internet, it has become increasingly crucial to extract specific information from vast amounts of academic articles efficiently. Data mining techniques are generally employed to solve this issue. However, data mining for academic articles is challenging since it requires automatically extracting specific patterns in complex and unstructured layout documents. Current data mining methods for academic articles employ rule-based(RB) or machine learning(ML) approaches. However, using rule-based methods incurs a high coding cost for complex typesetting articles. On the other hand, simply using machine learning methods requires annotation work for complex content types within the paper, which can be costly. Furthermore, only using machine learning can lead to cases where patterns easily recognized by rule-based methods are mistakenly extracted. To overcome these issues, from the perspective of analyzing the standard layout and typesetting used in the specified publication, we emphasize implementing specific methods for specific characteristics in academic articles. We have developed a novel Text Block Refinement Framework (TBRF), a machine learning and rule-based scheme hybrid. We used the well-known ACL proceeding articles as experimental data for the validation experiment. The experiment shows that our approach achieved over 95% classification accuracy and 90% detection accuracy for tables and figures. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.17401,May 2023,118,Total joint reconstruction using computer-assisted surgery with stock prostheses for a patient with bilateral TMJ ankylosis,"Seung-Hyun Rhee, Seung-Hak Baek, Sang-Hun Park, Jong-Cheol Kim, Chun-Gi Jeong and Jin-Young Choi",The purpose of this study is to discuss the total joint reconstruction surgery for a patient with recurrent ankylosis in bilateral temporomandibular joints (TMJs) using three-dimensional (3D) virtual surgical ...,https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-019-0225-1,10 October 2019,
119,0.000678989392134464,119,DotHash: Estimating Set Similarity Metrics for Link Prediction and Document Deduplication,"Igor Nunes, Mike Heddes, Pere Vergés, Danny Abraham, Alexander Veidenbaum, Alexandru Nicolau, Tony Givargis","Metrics for set similarity are a core aspect of several data mining tasks. To remove duplicate results in a Web search, for example, a common approach looks at the Jaccard index between all pairs of pages. In social network analysis, a much-celebrated metric is the Adamic-Adar index, widely used to compare node neighborhood sets in the important problem of predicting links. However, with the increasing amount of data to be processed, calculating the exact similarity between all pairs can be intractable. The challenge of working at this scale has motivated research into efficient estimators for set similarity metrics. The two most popular estimators, MinHash and SimHash, are indeed used in applications such as document deduplication and recommender systems where large volumes of data need to be processed. Given the importance of these tasks, the demand for advancing estimators is evident. We propose DotHash, an unbiased estimator for the intersection size of two sets. DotHash can be used to estimate the Jaccard index and, to the best of our knowledge, is the first method that can also estimate the Adamic-Adar index and a family of related metrics. We formally define this family of metrics, provide theoretical bounds on the probability of estimate errors, and analyze its empirical performance. Our experimental results indicate that DotHash is more accurate than the other estimators in link prediction and detecting duplicate documents with the same complexity and similar comparison time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.17310,May 2023,119,Joint Time-Frequency-Space Classification of EEG in a Brain-Computer Interface Application,"Gary N. Garcia Molina, Touradj Ebrahimi and Jean-Marc Vesin","Brain-computer interface is a growing field of interest in human-computer interaction with diverse applications ranging from medicine to entertainment. In this paper, we present a system which allows for class...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703302082,18 June 2003,
120,0.000678989392134464,120,Knowledge Extraction with Interval Temporal Logic Decision Trees,"Guido Sciavicco, Stan Ionel Eduard","Multivariate temporal, or time, series classification is, in a way, the temporal generalization of (numeric) classification, as every instance is described by multiple time series instead of multiple values. Symbolic classification is the machine learning strategy to extract explicit knowledge from a data set, and the problem of symbolic classification of multivariate temporal series requires the design, implementation, and test of ad-hoc machine learning algorithms, such as, for example, algorithms for the extraction of temporal versions of decision trees. One of the most well-known algorithms for decision tree extraction from categorical data is Quinlan's ID3, which was later extended to deal with numerical attributes, resulting in an algorithm known as C4.5, and implemented in many open-sources data mining libraries, including the so-called Weka, which features an implementation of C4.5 called J48. ID3 was recently generalized to deal with temporal data in form of timelines, which can be seen as discrete (categorical) versions of multivariate time series, and such a generalization, based on the interval temporal logic HS, is known as Temporal ID3. In this paper we introduce Temporal C4.5, that allows the extraction of temporal decision trees from undiscretized multivariate time series, describe its implementation, called Temporal J48, and discuss the outcome of a set of experiments with the latter on a collection of public data sets, comparing the results with those obtained by other, classical, multivariate time series classification methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.16864,May 2023,120,Editorial: Thematic series on best articles from IFIPTM and PST,Carmen Fernandez-Gago and Christian Damsgaard Jensen,Unknown,https://www.springeropen.com//journaloftrustmanagement.springeropen.com/articles/10.1186/2196-064X-1-2,20 May 2014,
121,0.000678989392134464,121,Clustering Method for Time-Series Images Using Quantum-Inspired Computing Technology,"Tomoki Inoue, Koyo Kubota, Tsubasa Ikami, Yasuhiro Egami, Hiroki Nagai, Takahiro Kashikawa, Koichi Kimura, Yu Matsuda","Time-series clustering serves as a powerful data mining technique for time-series data in the absence of prior knowledge about clusters. A large amount of time-series data with large size has been acquired and used in various research fields. Hence, clustering method with low computational cost is required. Given that a quantum-inspired computing technology, such as a simulated annealing machine, surpasses conventional computers in terms of fast and accurately solving combinatorial optimization problems, it holds promise for accomplishing clustering tasks that are challenging to achieve using existing methods. This study proposes a novel time-series clustering method that leverages an annealing machine. The proposed method facilitates an even classification of time-series data into clusters close to each other while maintaining robustness against outliers. Moreover, its applicability extends to time-series images. We compared the proposed method with a standard existing method for clustering an online distributed dataset. In the existing method, the distances between each data are calculated based on the Euclidean distance metric, and the clustering is performed using the k-means++ method. We found that both methods yielded comparable results. Furthermore, the proposed method was applied to a flow measurement image dataset containing noticeable noise with a signal-to-noise ratio of approximately 1. Despite a small signal variation of approximately 2%, the proposed method effectively classified the data without any overlap among the clusters. In contrast, the clustering results by the standard existing method and the conditional image sampling (CIS) method, a specialized technique for flow measurement data, displayed overlapping clusters. Consequently, the proposed method provides better results than the other two methods, demonstrating its potential as a superior clustering method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.16656,May 2023,121,Introduction to the special issue on computer vision in road safety and intelligent traffic,"Ankit Chaudhary, Reinhard Klette, J. L. Raheja and Xia Jin",Unknown,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-017-0166-5,14 February 2017,
122,0.000678989392134464,122,Automatic Extraction of Time-windowed ROS Computation Graphs from ROS Bag Files,"Zhuojun Chen, Michel Albonico, Ivano Malavolta","Robotic systems react to different environmental stimuli, potentially resulting in the dynamic reconfiguration of the software controlling such systems. One effect of such dynamism is the reconfiguration of the software architecture reconfiguration of the system at runtime. Such reconfigurations might severely impact the runtime properties of robotic systems, e.g., in terms of performance and energy efficiency. The ROS \emph{rosbag} package enables developers to record and store timestamped data related to the execution of robotic missions, implicitly containing relevant information about the architecture of the monitored system during its execution. In this study, we discuss about our approach for statically extracting (time-windowed) architectural information from ROS bag files. The proposed approach can support the robotics community in better discussing and reasoning the software architecture (and its runtime reconfigurations) of ROS-based systems. We evaluate our approach against hundreds of ROS bag files systematically mined from 4,434 public GitHub repositories. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.16405,May 2023,122,An efficient computer application of the sinc-Galerkin approximation for nonlinear boundary value problems,"Aydin Secer, Muhammet Kurulay, Mustafa Bayram and Mehmet Ali Akinlar",A powerful technique based on the sinc-Galerkin method is presented for obtaining numerical solutions of second-order nonlinear Dirichlet-type boundary value problems (BVPs). The method is based on approximati...,https://www.springeropen.com//boundaryvalueproblems.springeropen.com/articles/10.1186/1687-2770-2012-117,24 October 2012,
123,0.000678989392134464,123,A Conceptual Model for End-to-End Causal Discovery in Knowledge Tracing,"Nischal Ashok Kumar, Wanyong Feng, Jaewook Lee, Hunter McNichols, Aritra Ghosh, Andrew Lan","In this paper, we take a preliminary step towards solving the problem of causal discovery in knowledge tracing, i.e., finding the underlying causal relationship among different skills from real-world student response data. This problem is important since it can potentially help us understand the causal relationship between different skills without extensive A/B testing, which can potentially help educators to design better curricula according to skill prerequisite information. Specifically, we propose a conceptual solution, a novel causal gated recurrent unit (GRU) module in a modified deep knowledge tracing model, which uses i) a learnable permutation matrix for causal ordering among skills and ii) an optionally learnable lower-triangular matrix for causal structure among skills. We also detail how to learn the model parameters in an end-to-end, differentiable way. Our solution placed among the top entries in Task 3 of the NeurIPS 2022 Challenge on Causal Insights for Learning Paths in Education. We detail preliminary experiments as evaluated on the challenge's public leaderboard since the ground truth causal structure has not been publicly released, making detailed local evaluation impossible. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.16165,May 2023,123,Can computer simulators accurately represent the pathophysiology of individual COPD patients?,"Wenfei Wang, Anup Das, Tayyba Ali, Oanna Cole, Marc Chikhani, Mainul Haque, Jonathan G Hardman and Declan G Bates",Computer simulation models could play a key role in developing novel therapeutic strategies for patients with chronic obstructive pulmonary disease (COPD) if they can be shown to accurately represent the patho...,https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-014-0023-0,20 September 2014,
124,0.000678989392134464,124,Multijugate Dual Learning for Low-Resource Task-Oriented Dialogue System,"Shimin Li, Xiaotian Zhang, Yanjun Zheng, Linyang Li, Xipeng Qiu","Dialogue data in real scenarios tend to be sparsely available, rendering data-starved end-to-end dialogue systems trained inadequately. We discover that data utilization efficiency in low-resource scenarios can be enhanced by mining alignment information uncertain utterance and deterministic dialogue state. Therefore, we innovatively implement dual learning in task-oriented dialogues to exploit the correlation of heterogeneous data. In addition, the one-to-one duality is converted into a multijugate duality to reduce the influence of spurious correlations in dual training for generalization. Without introducing additional parameters, our method could be implemented in arbitrary networks. Extensive empirical analyses demonstrate that our proposed method improves the effectiveness of end-to-end task-oriented dialogue systems under multiple benchmarks and obtains state-of-the-art results in low-resource scenarios. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.16106,May 2023,124,Development of multi-pitch tool path in computer-controlled optical surfacing processes,"Jing Hou, Defeng Liao and Hongxiang Wang",Tool path in computer-controlled optical surfacing (CCOS) processes has a great effect on middle spatial frequency error in terms of residual ripples. Raster tool path of uniform path pitch is one of the mostl...,https://www.springeropen.com//jeos.springeropen.com/articles/10.1186/s41476-017-0050-z,9 September 2017,
125,0.000678989392134464,125,Entropy-Aware Similarity for Balanced Clustering: A Case Study with Melanoma Detection,"Seok Bin Son, Soohyun Park, Joongheon Kim","Clustering data is an unsupervised learning approach that aims to divide a set of data points into multiple groups. It is a crucial yet demanding subject in machine learning and data mining. Its successful applications span various fields. However, conventional clustering techniques necessitate the consideration of balance significance in specific applications. Therefore, this paper addresses the challenge of imbalanced clustering problems and presents a new method for balanced clustering by utilizing entropy-aware similarity, which can be defined as the degree of balances. We have coined the term, entropy-aware similarity for balanced clustering (EASB), which maximizes balance during clustering by complementary clustering of unbalanced data and incorporating entropy in a novel similarity formula that accounts for both angular differences and distances. The effectiveness of the proposed approach is evaluated on actual melanoma medial data, specifically the International Skin Imaging Collaboration (ISIC) 2019 and 2020 challenge datasets, to demonstrate how it can successfully cluster the data while preserving balance. Lastly, we can confirm that the proposed method exhibited outstanding performance in detecting melanoma, comparing to classical methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.15417,May 2023,125,"Validation of DM-Scan, a computer-assisted tool to assess mammographic density in full-field digital mammograms","Marina Pollán, Rafael Llobet, Josefa Miranda-García, Joaquín Antón, María Casals, Inmaculada Martínez, Carmen Palop, Francisco Ruiz-Perales, Carmen Sánchez-Contador, Carmen Vidal, Beatriz Pérez-Gómez and Dolores Salas-Trejo","We developed a semi-automated tool to assess mammographic density (MD), a phenotype risk marker for breast cancer (BC), in full-field digital images and evaluated its performance testing its reproducibility, c...",https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-2-242,24 May 2013,
126,0.000678989392134464,126,ARULESPY: Exploring Association Rules and Frequent Itemsets in Python,Michael Hahsler,"The R arules package implements a comprehensive infrastructure for representing, manipulating, and analyzing transaction data and patterns using frequent itemsets and association rules. The package also provides a wide range of interest measures and mining algorithms, including the code of Christian Borgelt's popular and efficient C implementations of the association mining algorithms Apriori and Eclat, and optimized C/C++ code for mining and manipulating association rules using sparse matrix representation. This document describes the new Python package arulespy, which makes this infrastructure available for Python users. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.15263,May 2023,126,Computer aided tool for diagnosis of ENT pathologies using digital signal processing of speech and stroboscopic images,"Amaia Méndez Zorrilla, Begoña García Zapirain and Agustín Pérez Izquierdo","The development of computer software and other technologies greatly facilitates the evaluation of pathological voice patients. This fact allows to reduce exploration time, improves the reproducibility of resul...",https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-1-64,13 December 2012,
127,0.000678989392134464,127,GPT4Graph: Can Large Language Models Understand Graph Structured Data ? An Empirical Evaluation and Benchmarking,"Jiayan Guo, Lun Du, Hengyu Liu, Mengyu Zhou, Xinyi He, Shi Han","Large language models~(LLM) like ChatGPT have become indispensable to artificial general intelligence~(AGI), demonstrating excellent performance in various natural language processing tasks. In the real world, graph data is ubiquitous and an essential part of AGI and prevails in domains like social network analysis, bioinformatics and recommender systems. The training corpus of large language models often includes some algorithmic components, which allows them to achieve certain effects on some graph data-related problems. However, there is still little research on their performance on a broader range of graph-structured data. In this study, we conduct an extensive investigation to assess the proficiency of LLMs in comprehending graph data, employing a diverse range of structural and semantic-related tasks. Our analysis encompasses 10 distinct tasks that evaluate the LLMs' capabilities in graph understanding. Through our study, we not only uncover the current limitations of language models in comprehending graph structures and performing associated reasoning tasks but also emphasize the necessity for further advancements and novel approaches to enhance their graph processing capabilities. Our findings contribute valuable insights towards bridging the gap between language models and graph understanding, paving the way for more effective graph mining and knowledge extraction. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.15066,May 2023,127,Principal component based covariate shift adaption to reduce non-stationarity in a MEG-based brain-computer interface,"Martin Spüler, Wolfgang Rosenstiel and Martin Bogdan",One of the biggest problems in today’s BCI research is the non-stationarity of the recorded signals. This non-stationarity can cause the BCI performance to deteriorate over time or drop significantly when tran...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2012-129,2 July 2012,
128,0.000678989392134464,128,Semi-Supervised and Long-Tailed Object Detection with CascadeMatch,"Yuhang Zang, Kaiyang Zhou, Chen Huang, Chen Change Loy","This paper focuses on long-tailed object detection in the semi-supervised learning setting, which poses realistic challenges, but has rarely been studied in the literature. We propose a novel pseudo-labeling-based detector called CascadeMatch. Our detector features a cascade network architecture, which has multi-stage detection heads with progressive confidence thresholds. To avoid manually tuning the thresholds, we design a new adaptive pseudo-label mining mechanism to automatically identify suitable values from data. To mitigate confirmation bias, where a model is negatively reinforced by incorrect pseudo-labels produced by itself, each detection head is trained by the ensemble pseudo-labels of all detection heads. Experiments on two long-tailed datasets, i.e., LVIS and COCO-LT, demonstrate that CascadeMatch surpasses existing state-of-the-art semi-supervised approaches -- across a wide range of detection architectures -- in handling long-tailed object detection. For instance, CascadeMatch outperforms Unbiased Teacher by 1.9 AP Fix on LVIS when using a ResNet50-based Cascade R-CNN structure, and by 1.7 AP Fix when using Sparse R-CNN with a Transformer encoder. We also show that CascadeMatch can even handle the challenging sparsely annotated object detection problem. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.14813,May 2023,128,Electronic evidence and its authenticity in forensic evidence,Ahmad Fekry Moussa,"The basis for criminal trials is the judge’s conviction of the evidence presented in a case. His belief is based on the context or evidence he is satisfactory with and understands. However, the law may establi...",https://www.springeropen.com//ejfs.springeropen.com/articles/10.1186/s41935-021-00234-6,27 August 2021,
129,0.000678989392134464,129,Confidential Truth Finding with Multi-Party Computation (Extended Version),"Angelo Saadeh, Pierre Senellart, Stéphane Bressan","Federated knowledge discovery and data mining are challenged to assess the trustworthiness of data originating from autonomous sources while protecting confidentiality and privacy. Truth-finding algorithms help corroborate data from disagreeing sources. For each query it receives, a truth-finding algorithm predicts a truth value of the answer, possibly updating the trustworthiness factor of each source. Few works, however, address the issues of confidentiality and privacy. We devise and present a secure secret-sharing-based multi-party computation protocol for pseudo-equality tests that are used in truth-finding algorithms to compute additions depending on a condition. The protocol guarantees confidentiality of the data and privacy of the sources. We also present variants of truth-finding algorithms that would make the computation faster when executed using secure multi-party computation. We empirically evaluate the performance of the proposed protocol on two state-of-the-art truth-finding algorithms, Cosine, and 3-Estimates, and compare them with that of the baseline plain algorithms. The results confirm that the secret-sharing-based secure multi-party algorithms are as accurate as the corresponding baselines but for proposed numerical approximations that significantly reduce the efficiency loss incurred. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.14727,May 2023,129,Computer Aided Modeling of Human Mastoid Cavity Biomechanics Using Finite Element Analysis,"Chia-Fone Lee, Peir-Rong Chen, Wen-Jeng Lee, Yuan-Fang Chou, Jyh-Horng Chen and Tien-Chen Liu",The aim of the present study was to analyze the human mastoid cavity on sound transmission using finite element method. Pressure distributions in the external ear canal and middle ear cavity at different frequ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2010/203037,4 August 2009,
130,0.000678989392134464,130,Knowledge Graphs Querying,Arijit Khan,"Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples -- that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases, query languages, join algorithms, graph patterns matching, to more sophisticated KG embedding and natural language questions (NLQs). We aim at uniting different interdisciplinary topics and concepts that have been developed for KG querying. Second, many recent advances on KG and query embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and computer vision domains. We identify important challenges of KG querying that received less attention by graph databases, and by the DB community in general, e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude by discussing interesting opportunities for the data management community, for instance, KG as a unified data model and vector-based query processing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.14485,May 2023,130,Computer wireless network integration technology in equipment state data collection,Lixin Ma,"The paper introduces a handheld integrated power data acquisition and analysis equipment based on a computer wireless network mobile platform, an intelligent transportation inspection box. The intelligent tran...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-021-02028-9,13 July 2021,
131,0.000678989392134464,131,"LIMIT: Language Identification, Misidentification, and Translation using Hierarchical Models in 350+ Languages","Milind Agarwal, Md Mahfuz Ibn Alam, Antonios Anastasopoulos","Knowing the language of an input text/audio is a necessary first step for using almost every natural language processing (NLP) tool such as taggers, parsers, or translation systems. Language identification is a well-studied problem, sometimes even considered solved; in reality, most of the world's 7000 languages are not supported by current systems. This lack of representation affects large-scale data mining efforts and further exacerbates data shortage for low-resource languages. We take a step towards tackling the data bottleneck by compiling a corpus of over 50K parallel children's stories in 350+ languages and dialects, and the computation bottleneck by building lightweight hierarchical models for language identification. Our data can serve as benchmark data for language identification of short texts and for understudied translation directions such as those between Indian or African languages. Our proposed method, Hierarchical LIMIT, uses limited computation to expand coverage into excluded languages while maintaining prediction quality. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.14263,May 2023,131,RETRACTED ARTICLE: Innovative design of wall painting pattern based on microprocessor system and evolutionary computer technology,Feng Xue,"With the improvement of people’s living standards, people pay more and more attention to the indoor living environment. This research mainly discusses the research and realization of the innovative design of w...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00810-x,21 October 2021,
132,0.000678989392134464,132,Fast Maximal Quasi-clique Enumeration: A Pruning and Branching Co-Design Approach,"Kaiqiang Yu, Cheng Long","Mining cohesive subgraphs from a graph is a fundamental problem in graph data analysis. One notable cohesive structure is $γ$-quasi-clique (QC), where each vertex connects at least a fraction $γ$ of the other vertices inside. Enumerating maximal $γ$-quasi-cliques (MQCs) of a graph has been widely studied. One common practice of finding all MQCs is to (1) find a set of QCs containing all MQCs and then (2) filter out non-maximal QCs. While quite a few algorithms have been developed (which are branch-and-bound algorithms) for finding a set of QCs that contains all MQCs, all focus on sharpening the pruning techniques and devote little effort to improving the branching part. As a result, they provide no guarantee on pruning branches and all have the worst-case time complexity of $O^*(2^n)$, where $O^*$ suppresses the polynomials and $n$ is the number of vertices in the graph. In this paper, we focus on the problem of finding a set of QCs containing all MQCs but deviate from further sharpening the pruning techniques as existing methods do. We pay attention to both the pruning and branching parts and develop new pruning techniques and branching methods that would suit each other better towards pruning more branches both theoretically and practically. Specifically, we develop a new branch-and-bound algorithm called FastQC based on newly developed pruning techniques and branching methods, which improves the worst-case time complexity to $O^*(α_k^n)$, where $α_k$ is a positive real number strictly smaller than 2. Furthermore, we develop a divide-and-conquer strategy for boosting the performance of FastQC. Finally, we conduct extensive experiments on both real and synthetic datasets, and the results show that our algorithms are up to two orders of magnitude faster than the state-of-the-art on real datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.14047,May 2023,132,Improving learning computer architecture through an educational mobile game,"Ahmed Tlili, Fathi Essalmi and Mohamed Jemni","Many studies have reported the efficiency of using educational games to teach a particular subject. With the rapid growth of mobile technology, mobile devices have become very popular and have reached a very h...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-016-0030-6,10 May 2016,
133,0.000678989392134464,133,Global Structure Knowledge-Guided Relation Extraction Method for Visually-Rich Document,"Xiangnan Chen, Juncheng Li, Duo Dong, Qian Xiao, Jun Lin, Xiaozhong Liu, Siliang Tang","Visual relation extraction (VRE) aims to extract relations between entities from visuallyrich documents. Existing methods usually predict relations for each entity pair independently based on entity features but ignore the global structure information, i.e., dependencies between entity pairs. The absence of global structure information may make the model struggle to learn long-range relations and easily predict conflicted results. To alleviate such limitations, we propose a GlObal Structure knowledgeguided relation Extraction (GOSE) framework, which captures dependencies between entity pairs in an iterative manner. Given a scanned image of the document, GOSE firstly generates preliminary relation predictions on entity pairs. Secondly, it mines global structure knowledge based on prediction results of the previous iteration and further incorporates global structure knowledge into entity representations. This ""generate-capture-incorporate"" schema is performed multiple times so that entity representations and global structure knowledge can mutually reinforce each other. Extensive experiments show that GOSE not only outperforms previous methods on the standard fine-tuning setting but also shows promising superiority in cross-lingual learning; even yields stronger data-efficient performance in the low-resource setting. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.13850,May 2023,133,The key technology of computer network vulnerability assessment based on neural network,Shaoqiang Wang,"With the wide application of computer network, network security has attracted more and more attention. The main reason why all kinds of attacks on the network can pose a great threat to the network security is...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-020-01841-y,31 October 2020,
134,0.000678989392134464,134,Online Open-set Semi-supervised Object Detection via Semi-supervised Outlier Filtering,"Zerun Wang, Ling Xiao, Liuyu Xiang, Zhaotian Weng, Toshihiko Yamasaki","Open-set semi-supervised object detection (OSSOD) methods aim to utilize practical unlabeled datasets with out-of-distribution (OOD) instances for object detection. The main challenge in OSSOD is distinguishing and filtering the OOD instances from the in-distribution (ID) instances during pseudo-labeling. The previous method uses an offline OOD detection network trained only with labeled data for solving this problem. However, the scarcity of available data limits the potential for improvement. Meanwhile, training separately leads to low efficiency. To alleviate the above issues, this paper proposes a novel end-to-end online framework that improves performance and efficiency by mining more valuable instances from unlabeled data. Specifically, we first propose a semi-supervised OOD detection strategy to mine valuable ID and OOD instances in unlabeled datasets for training. Then, we constitute an online end-to-end trainable OSSOD framework by integrating the OOD detection head into the object detector, making it jointly trainable with the original detection task. Our experimental results show that our method works well on several benchmarks, including the partially labeled COCO dataset with open-set classes and the fully labeled COCO dataset with the additional large-scale open-set unlabeled dataset, OpenImages. Compared with previous OSSOD methods, our approach achieves the best performance on COCO with OpenImages by +0.94 mAP, reaching 44.07 mAP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.13802,May 2023,134,Comparing the effects of dynamic computer visualization on undergraduate students’ understanding of osmosis with randomized posttest-only control group design,"Shannon Hsianghan-Huang Sung, Ji Shen, Shiyan Jiang and Guanhua Chen",This study describes the impact of embedding dynamic computer visualization (DCV) in an online instrument that was designed to assess students’ understanding of osmosis. The randomized posttest-only control gr...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-017-0067-3,28 December 2017,
135,0.000678989392134464,135,SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs,"Sheng Tian, Jihai Dong, Jintang Li, Wenlong Zhao, Xiaolong Xu, Baokun wang, Bowen Song, Changhua Meng, Tianyi Zhang, Liang Chen","Anomaly detection aims to distinguish abnormal instances that deviate significantly from the majority of benign ones. As instances that appear in the real world are naturally connected and can be represented with graphs, graph neural networks become increasingly popular in tackling the anomaly detection problem. Despite the promising results, research on anomaly detection has almost exclusively focused on static graphs while the mining of anomalous patterns from dynamic graphs is rarely studied but has significant application value. In addition, anomaly detection is typically tackled from semi-supervised perspectives due to the lack of sufficient labeled data. However, most proposed methods are limited to merely exploiting labeled data, leaving a large number of unlabeled samples unexplored. In this work, we present semi-supervised anomaly detection (SAD), an end-to-end framework for anomaly detection on dynamic graphs. By a combination of a time-equipped memory bank and a pseudo-label contrastive learning module, SAD is able to fully exploit the potential of large unlabeled samples and uncover underlying anomalies on evolving graph streams. Extensive experiments on four real-world datasets demonstrate that SAD efficiently discovers anomalies from dynamic graphs and outperforms existing advanced methods even when provided with only little labeled data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.13573,May 2023,135,On the analysis of the collaboration network of the Brazilian symposium on computer networks and distributed systems,"Guilherme Maia, Pedro O. S. Vaz de Melo, Daniel L. Guidoni, Fernanda S.H. Souza, Thiago H. Silva, Jussara M. Almeida and Antonio A. F. Loureiro",The Brazilian symposium on computer networks and distributed systems (SBRC) reached its 30th edition as the paramount scientific event in the area of computer networks and distributed systems in Brazil. Faced ...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-013-0109-7,26 March 2013,
136,0.000678989392134464,136,Multiclass classification for multidimensional functional data through deep neural networks,"Shuoyang Wang, Guanqun Cao","The intrinsically infinite-dimensional features of the functional observations over multidimensional domains render the standard classification methods effectively inapplicable. To address this problem, we introduce a novel multiclass functional deep neural network (mfDNN) classifier as an innovative data mining and classification tool. Specifically, we consider sparse deep neural network architecture with rectifier linear unit (ReLU) activation function and minimize the cross-entropy loss in the multiclass classification setup. This neural network architecture allows us to employ modern computational tools in the implementation. The convergence rates of the misclassification risk functions are also derived for both fully observed and discretely observed multidimensional functional data. We demonstrate the performance of mfDNN on simulated data and several benchmark datasets from different application domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.13349,May 2023,136,Letter from the guest editors,"Hugo Fuks, Stephan Lukosch and Gert-Jan de Vreede",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192541,September 2007,
137,0.000678989392134464,137,Imbalance-Agnostic Source-Free Domain Adaptation via Avatar Prototype Alignment,"Hongbin Lin, Mingkui Tan, Yifan Zhang, Zhen Qiu, Shuaicheng Niu, Dong Liu, Qing Du, Yanxia Liu","Source-free Unsupervised Domain Adaptation (SF-UDA) aims to adapt a well-trained source model to an unlabeled target domain without access to the source data. One key challenge is the lack of source data during domain adaptation. To handle this, we propose to mine the hidden knowledge of the source model and exploit it to generate source avatar prototypes. To this end, we propose a Contrastive Prototype Generation and Adaptation (CPGA) method. CPGA consists of two stages: Prototype generation and Prototype adaptation. Extensive experiments on three UDA benchmark datasets demonstrate the superiority of CPGA. However, existing SF.UDA studies implicitly assume balanced class distributions for both the source and target domains, which hinders their real applications. To address this issue, we study a more practical SF-UDA task, termed imbalance-agnostic SF-UDA, where the class distributions of both the unseen source domain and unlabeled target domain are unknown and could be arbitrarily skewed. This task is much more challenging than vanilla SF-UDA due to the co-occurrence of covariate shifts and unidentified class distribution shifts between the source and target domains. To address this task, we extend CPGA and propose a new Target-aware Contrastive Prototype Generation and Adaptation (T-CPGA) method. Specifically, for better prototype adaptation in the imbalance-agnostic scenario, T-CPGA applies a new pseudo label generation strategy to identify unknown target class distribution and generate accurate pseudo labels, by utilizing the collective intelligence of the source model and an additional contrastive language-image pre-trained model. Meanwhile, we further devise a target label-distribution-aware classifier to adapt the model to the unknown target class distribution. We empirically show that T-CPGA significantly outperforms CPGA and other SF-UDA methods in imbalance-agnostic SF-UDA. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.12649,May 2023,137,Do patients consider computer-adaptive measures more appropriate than static questionnaires?,"Eva-Maria Gamper, Caroline Martini, Morten Aagaard Petersen, Irene Virgolini, Bernhard Holzner and Johannes M. Giesinger","Computer-adaptive tests (CAT) use individualised sets of questions to assess patient-reported health states, whereas static (conventional) questionnaires present the same questions to all patients. CAT has bee...",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-019-0096-3,29 January 2019,
138,0.000678989392134464,138,WOT-Class: Weakly Supervised Open-world Text Classification,"Tianle Wang, Zihan Wang, Weitang Liu, Jingbo Shang","State-of-the-art weakly supervised text classification methods, while significantly reduced the required human supervision, still requires the supervision to cover all the classes of interest. This is never easy to meet in practice when human explore new, large corpora without complete pictures. In this paper, we work on a novel yet important problem of weakly supervised open-world text classification, where supervision is only needed for a few examples from a few known classes and the machine should handle both known and unknown classes in test time. General open-world classification has been studied mostly using image classification; however, existing methods typically assume the availability of sufficient known-class supervision and strong unknown-class prior knowledge (e.g., the number and/or data distribution). We propose a novel framework WOT-Class that lifts those strong assumptions. Specifically, it follows an iterative process of (a) clustering text to new classes, (b) mining and ranking indicative words for each class, and (c) merging redundant classes by using the overlapped indicative words as a bridge. Extensive experiments on 7 popular text classification datasets demonstrate that WOT-Class outperforms strong baselines consistently with a large margin, attaining 23.33% greater average absolute macro-F1 over existing approaches across all datasets. Such competent accuracy illuminates the practical potential of further reducing human effort for text classification. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.12401,May 2023,138,An empirical study of the effect that a computer graphics course has on visual-spatial abilities,"José Saúl González Campos, Jordi Sánchez-Navarro and Joan Arnedo-Moreno",Visual-spatial abilities are relevant for performing diverse everyday tasks as well as being successful in multiple fields. This work provides empirical evidence supporting the claim that studying a computer g...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-019-0169-7,5 November 2019,
139,0.000678989392134464,139,Study on Intelligent Forecasting of Credit Bond Default Risk,Kai Ren,"Credit risk in the China's bond market has become increasingly evident, creating a progressively escalating risk of default for credit bond investors. Given the current incomplete and inaccurate bond information disclosure, timely tracking and forecasting the individual credit bond default risks have become essential to maintain market stability and ensure healthy development. This paper proposes an Intelligent Forecasting Framework for Default Risk that provides precise day-by-day default risk prediction. In this framework, we first summarize the factors that impact credit bond defaults and construct a risk index system. Then, we employ a combined default probability annotation method based on the evolutionary characteristics of bond default risk. The method considers the weighted average of Variational Bayesian Gaussian Mixture estimation, Market Index estimation, and Default Trend Backward estimation for daily default risk annotation of matured or defaulted bonds according to the risk index system. Moreover, to mine time-series correlation and cross-sectional index correlation features efficiently, an intelligent prediction model for Chinese credit bond default risk is designed using the ConvLSTM neural network and trained with structured feature data. The experiments demonstrate that the predicted individual bond risk is slightly higher and substantially more responsive to fluctuations than the risk indicated by authoritative ratings, thereby improving on the inadequacies of inflated and untimely bond ratings. Consequently, this study's findings offer multiple insights for regulators, issuers, and investors. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.12142,May 2023,139,Towards early software reliability prediction for computer forensic tools (case study),Manar Abu Talib,"Versatility, flexibility and robustness are essential requirements for software forensic tools. Researchers and practitioners need to put more effort into assessing this type of tool. A Markov model is a robus...",https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/s40064-016-2539-0,22 June 2016,
140,0.00673579331412596,140,DisCo: Distilled Student Models Co-training for Semi-supervised Text Mining,"Weifeng Jiang, Qianren Mao, Jianxin Li, Chenghua Lin, Weiyi Yang, Ting Deng, Zheng Wang","Many text mining models are constructed by fine-tuning a large deep pre-trained language model (PLM) in downstream tasks. However, a significant challenge is maintaining performance when we use a lightweight model with limited labeled samples. We present DisCo, a semi-supervised learning (SSL) framework for fine-tuning a cohort of small student models generated from a large PLM using knowledge distillation. Our key insight is to share complementary knowledge among distilled student cohorts to promote their SSL effectiveness. DisCo employs a novel co-training technique to optimize multiple small student models by promoting knowledge sharing among students under diversified views: model views produced by different distillation strategies and data views produced by various input augmentations. We evaluate DisCo on both semi-supervised text classification and extractive summarization tasks. Experimental results show that DisCo can produce student models that are 7.6 times smaller and 4.8 times faster in inference than the baseline PLMs while maintaining comparable performance. We also show that DisCo-generated student models outperform the similar-sized models elaborately tuned in distinct tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.12074,May 2023,140,Computer-aided diagnosis for diagnostically challenging breast lesions in DCE-MRI based on image registration and integration of morphologic and dynamic characteristics,"Felix Retter, Claudia Plant, Bernhard Burgeth, Guillermo Botella, Thomas Schlossbauer and Anke Meyer-Bäse",Diagnostically challenging lesions comprise both foci (small lesions) and non-mass-like enhancing lesions and pose a challenge to current computer-aided diagnosis systems. Motion-based artifacts lead in dynami...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2013-157,17 October 2013,
141,0.000678989392134464,141,A Foray into Parallel Optimisation Algorithms for High Dimension Low Sample Space Generalized Distance Weighted Discrimination problems,"Srivathsan Amruth, Xin Yee Lam","In many modern data sets, High dimension low sample size (HDLSS) data is prevalent in many fields of studies. There has been an increased focus recently on using machine learning and statistical methods to mine valuable information out of these data sets. Thus, there has been an increased interest in efficient learning in high dimensions. Naturally, as the dimension of the input data increases, the learning task will become more difficult, due to increasing computational and statistical complexities. This makes it crucial to overcome the curse of dimensionality in a given dataset, within a reasonable time frame, in a bid to obtain the insights required to keep a competitive edge. To solve HDLSS problems, classical methods such as support vector machines can be utilised to alleviate data piling at the margin. However, when we question geometric domains and their assumptions on input data, we are naturally lead to convex optimisation problems and this gives rise to the development of solutions like distance weighted discrimination (DWD), which can be modelled as a second-order cone programming problem and solved by interior-point methods when sample size and feature dimensions of the data is moderate. In this paper, our focus is on designing an even more scalable and robust algorithm for solving large-scale generalized DWD problems. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.12019,May 2023,141,Editorial:Journal of Trust Management,Stephen Marsh,Unknown,https://www.springeropen.com//journaloftrustmanagement.springeropen.com/articles/10.1186/2196-064X-1-1,20 May 2014,
142,0.000678989392134464,142,Self-Supervised Learning for Point Clouds Data: A Survey,"Changyu Zeng, Wei Wang, Anh Nguyen, Yutao Yue","3D point clouds are a crucial type of data collected by LiDAR sensors and widely used in transportation applications due to its concise descriptions and accurate localization. Deep neural networks (DNNs) have achieved remarkable success in processing large amount of disordered and sparse 3D point clouds, especially in various computer vision tasks, such as pedestrian detection and vehicle recognition. Among all the learning paradigms, Self-Supervised Learning (SSL), an unsupervised training paradigm that mines effective information from the data itself, is considered as an essential solution to solve the time-consuming and labor-intensive data labelling problems via smart pre-training task design. This paper provides a comprehensive survey of recent advances on SSL for point clouds. We first present an innovative taxonomy, categorizing the existing SSL methods into four broad categories based on the pretexts' characteristics. Under each category, we then further categorize the methods into more fine-grained groups and summarize the strength and limitations of the representative methods. We also compare the performance of the notable SSL methods in literature on multiple downstream tasks on benchmark datasets both quantitatively and qualitatively. Finally, we propose a number of future research directions based on the identified limitations of existing SSL research on point clouds. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.11881,May 2023,142,Modeling the spread of computer virus via Caputo fractional derivative and the beta-derivative,"Ebenezer Bonyah, Abdon Atangana and Muhammad Altaf Khan",The concept of information science is inevitable in the human development as science and technology has become the driving force of all economics. The connection of one human being during epidemics is vital an...,https://www.springeropen.com//apjcen.springeropen.com/articles/10.1186/s40540-016-0019-1,3 January 2017,
143,0.000678989392134464,143,LATTE: Label-efficient Incident Phenotyping from Longitudinal Electronic Health Records,"Jun Wen, Jue Hou, Clara-Lea Bonzel, Yihan Zhao, Victor M. Castro, Vivian S. Gainer, Dana Weisenfeld, Tianrun Cai, Yuk-Lam Ho, Vidul A. Panickan, Lauren Costa, Chuan Hong, J. Michael Gaziano, Katherine P. Liao, Junwei Lu, Kelly Cho, Tianxi Cai","Electronic health record (EHR) data are increasingly used to support real-world evidence (RWE) studies. Yet its ability to generate reliable RWE is limited by the lack of readily available precise information on the timing of clinical events such as the onset time of heart failure. We propose a LAbel-efficienT incidenT phEnotyping (LATTE) algorithm to accurately annotate the timing of clinical events from longitudinal EHR data. By leveraging the pre-trained semantic embedding vectors from large-scale EHR data as prior knowledge, LATTE selects predictive EHR features in a concept re-weighting module by mining their relationship to the target event and compresses their information into longitudinal visit embeddings through a visit attention learning network. LATTE employs a recurrent neural network to capture the sequential dependency between the target event and visit embeddings before/after it. To improve label efficiency, LATTE constructs highly informative longitudinal silver-standard labels from large-scale unlabeled patients to perform unsupervised pre-training and semi-supervised joint training. Finally, LATTE enhances cross-site portability via contrastive representation learning. LATTE is evaluated on three analyses: the onset of type-2 diabetes, heart failure, and the onset and relapses of multiple sclerosis. We use various evaluation metrics present in the literature including the $ABC_{gain}$, the proportion of reduction in the area between the observed event indicator and the predicted cumulative incidences in reference to the prediction per incident prevalence. LATTE consistently achieves substantial improvement over benchmark methods such as SAMGEP and RETAIN in all settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.11407,May 2023,143,Who is a scientist? The relationship between counter-stereotypical beliefs about scientists and the STEM major intentions of Black and Latinx male and female students,Ursula Nguyen and Catherine Riegle-Crumb,"Despite the diverse student population in the USA, the labor force in Science, Technology, Engineering, and Mathematics (STEM) does not reflect this reality. While restrictive messages about who belongs in STE...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00288-x,7 April 2021,
144,0.000678989392134464,144,Faster Parallel Exact Density Peaks Clustering,"Yihao Huang, Shangdi Yu, Julian Shun","Clustering multidimensional points is a fundamental data mining task, with applications in many fields, such as astronomy, neuroscience, bioinformatics, and computer vision. The goal of clustering algorithms is to group similar objects together. Density-based clustering is a clustering approach that defines clusters as dense regions of points. It has the advantage of being able to detect clusters of arbitrary shapes, rendering it useful in many applications. In this paper, we propose fast parallel algorithms for Density Peaks Clustering (DPC), a popular version of density-based clustering. Existing exact DPC algorithms suffer from low parallelism both in theory and in practice, which limits their application to large-scale data sets. Our most performant algorithm, which is based on priority search kd-trees, achieves $O(\log n\log\log n)$ span (parallel time complexity) for a data set of $n$ points. Our algorithm is also work-efficient, achieving a work complexity matching the best existing sequential exact DPC algorithm. In addition, we present another DPC algorithm based on a Fenwick tree that makes fewer assumptions for its average-case complexity to hold. We provide optimized implementations of our algorithms and evaluate their performance via extensive experiments. On a 30-core machine with two-way hyperthreading, we find that our best algorithm achieves a 10.8--13169x speedup over the previous best parallel exact DPC algorithm. Compared to the state-of-the-art parallel approximate DPC algorithm, our best algorithm achieves a 1.5--4206x speedup, while being exact. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.11335,May 2023,144,Three-dimensional computer-guided implant placement in oligodontia,"Marieke A. P. Filius, Joep Kraeima, Arjan Vissink, Krista I. Janssen, Gerry M. Raghoebar and Anita Visser","The aim of computer-designed surgical templates is to attain higher precision and accuracy of implant placement, particularly for compromised cases.",https://www.springeropen.com//journalimplantdent.springeropen.com/articles/10.1186/s40729-017-0090-6,8 July 2017,
145,0.000678989392134464,145,ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph,"Salomon Kabongo, Jennifer D'Souza, Sören Auer","The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview of the state-of-the-art (SOTA) across the most common AI tasks and their corresponding datasets via dynamic ORKG frontend views leveraging tables and visualization charts over the machine-actionable data. Our best model achieves performances above 90% F1 on the \textit{leaderboard} extraction task, thus proving Orkg-Leaderboards a practically viable tool for real-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the leaderboard extraction task to an automated digitalization task, which has been, for a long time in the community, a crowdsourced endeavor. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.11068,May 2023,145,Fostering computational thinking through unplugged activities: A systematic literature review and meta-analysis,"Peng Chen, Dong Yang, Ahmed Hosny Saleh Metwally, Jari Lavonen and Xiao Wang","Unplugged activities as a low-cost solution to foster computational thinking (CT) skills seem to be a trend in recent years. However, current evidence of the effectiveness of unplugged activities in promoting ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00434-7,4 July 2023,
146,0.000678989392134464,146,BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval,"Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng","Dense retrieval has shown promise in the first-stage retrieval process when trained on in-domain labeled datasets. However, previous studies have found that dense retrieval is hard to generalize to unseen domains due to its weak modeling of domain-invariant and interpretable feature (i.e., matching signal between two texts, which is the essence of information retrieval). In this paper, we propose a novel method to improve the generalization of dense retrieval via capturing matching signal called BERM. Fully fine-grained expression and query-oriented saliency are two properties of the matching signal. Thus, in BERM, a single passage is segmented into multiple units and two unit-level requirements are proposed for representation as the constraint in training to obtain the effective matching signal. One is semantic unit balance and the other is essential matching unit extractability. Unit-level view and balanced semantics make representation express the text in a fine-grained manner. Essential matching unit extractability makes passage representation sensitive to the given query to extract the pure matching information from the passage containing complex context. Experiments on BEIR show that our method can be effectively combined with different dense retrieval training methods (vanilla, hard negatives mining and knowledge distillation) to improve its generalization ability without any additional inference overhead and target domain data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.11052,May 2023,146,Letter from the editor-in-chief,Hugo Fuks,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192555,June 2008,
147,0.000678989392134464,147,OpenShape: Scaling Up 3D Shape Representation Towards Open-World Understanding,"Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, Hao Su","We introduce OpenShape, a method for learning multi-modal joint representations of text, image, and point clouds. We adopt the commonly used multi-modal contrastive learning framework for representation alignment, but with a specific focus on scaling up 3D representations to enable open-world 3D shape understanding. To achieve this, we scale up training data by ensembling multiple 3D datasets and propose several strategies to automatically filter and enrich noisy text descriptions. We also explore and compare strategies for scaling 3D backbone networks and introduce a novel hard negative mining module for more efficient training. We evaluate OpenShape on zero-shot 3D classification benchmarks and demonstrate its superior capabilities for open-world recognition. Specifically, OpenShape achieves a zero-shot accuracy of 46.8% on the 1,156-category Objaverse-LVIS benchmark, compared to less than 10% for existing methods. OpenShape also achieves an accuracy of 85.3% on ModelNet40, outperforming previous zero-shot baseline methods by 20% and performing on par with some fully-supervised methods. Furthermore, we show that our learned embeddings encode a wide range of visual and semantic concepts (e.g., subcategories, color, shape, style) and facilitate fine-grained text-3D and image-3D interactions. Due to their alignment with CLIP embeddings, our learned shape representations can also be integrated with off-the-shelf CLIP-based models for various applications, such as point cloud captioning and point cloud-conditioned image generation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.10764,May 2023,147,Outcomes and challenges of global high-resolution non-hydrostatic atmospheric simulations using the K computer,"Masaki Satoh, Hirofumi Tomita, Hisashi Yashiro, Yoshiyuki Kajikawa, Yoshiaki Miyamoto, Tsuyoshi Yamaura, Tomoki Miyakawa, Masuo Nakano, Chihiro Kodama, Akira T. Noda, Tomoe Nasuno, Yohei Yamada and Yoshiki Fukutomi",This article reviews the major outcomes of a 5-year (2011–2016) project using the K computer to perform global numerical atmospheric simulations based on the non-hydrostatic icosahedral atmospheric model (NICA...,https://www.springeropen.com//progearthplanetsci.springeropen.com/articles/10.1186/s40645-017-0127-8,28 April 2017,
148,0.000678989392134464,148,A Survey on Time-Series Pre-Trained Models,"Qianli Ma, Zhen Liu, Zhenjing Zheng, Ziyang Huang, Siying Zhu, Zhongzhong Yu, James T. Kwok","Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.10716,May 2023,148,A review of existing and potential computer user interfaces for modern radiology,"Antoine Iannessi, Pierre-Yves Marcy, Olivier Clatz, Anne-Sophie Bertrand and Maki Sugimoto","The digitalization of modern imaging has led radiologists to become very familiar with computers and their user interfaces (UI). New options for display and command offer expanded possibilities, but the mouse ...",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1007/s13244-018-0620-7,16 May 2018,
149,0.000678989392134464,149,Iteratively Learning Representations for Unseen Entities with Inter-Rule Correlations,"Zihan Wang, Kai Zhao, Yongquan He, Zhumin Chen, Pengjie Ren, Maarten de Rijke, Zhaochun Ren","Recent work on knowledge graph completion (KGC) focused on learning embeddings of entities and relations in knowledge graphs. These embedding methods require that all test entities are observed at training time, resulting in a time-consuming retraining process for out-of-knowledge-graph (OOKG) entities. To address this issue, current inductive knowledge embedding methods employ graph neural networks (GNNs) to represent unseen entities by aggregating information of known neighbors. They face three important challenges: (i) data sparsity, (ii) the presence of complex patterns in knowledge graphs (e.g., inter-rule correlations), and (iii) the presence of interactions among rule mining, rule inference, and embedding. In this paper, we propose a virtual neighbor network with inter-rule correlations (VNC) that consists of three stages: (i) rule mining, (ii) rule inference, and (iii) embedding. In the rule mining process, to identify complex patterns in knowledge graphs, both logic rules and inter-rule correlations are extracted from knowledge graphs based on operations over relation embeddings. To reduce data sparsity, virtual neighbors for OOKG entities are predicted and assigned soft labels by optimizing a rule-constrained problem. We also devise an iterative framework to capture the underlying relations between rule learning and embedding learning. In our experiments, results on both link prediction and triple classification tasks show that the proposed VNC framework achieves state-of-the-art performance on four widely-used knowledge graphs. Further analysis reveals that VNC is robust to the proportion of unseen entities and effectively mitigates data sparsity. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.10531,May 2023,149,Effects of digital game-based STEM education on students’ learning achievement: a meta-analysis,"Liang-Hui Wang, Bing Chen, Gwo-Jen Hwang, Jue-Qi Guan and Yun-Qing Wang",Many researchers have explored the impact of digital games on learning effects in different STEM subjects. The purpose of this meta-analysis is to examine the effect of digital game-based STEM education on the...,https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00344-0,17 March 2022,
150,0.000678989392134464,150,Time Series Clustering With Random Convolutional Kernels,"Jorge Marco-Blanco, Rubén Cuevas","Time series data, spanning applications ranging from climatology to finance to healthcare, presents significant challenges in data mining due to its size and complexity. One open issue lies in time series clustering, which is crucial for processing large volumes of unlabeled time series data and unlocking valuable insights. Traditional and modern analysis methods, however, often struggle with these complexities. To address these limitations, we introduce R-Clustering, a novel method that utilizes convolutional architectures with randomly selected parameters. Through extensive evaluations, R-Clustering demonstrates superior performance over existing methods in terms of clustering accuracy, computational efficiency and scalability. Empirical results obtained using the UCR archive demonstrate the effectiveness of our approach across diverse time series datasets. The findings highlight the significance of R-Clustering in various domains and applications, contributing to the advancement of time series data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.10457,May 2023,150,Commercial applications of quantum computing,"Francesco Bova, Avi Goldfarb and Roger G. Melko","Despite the scientific and engineering challenges facing the development of quantum computers, considerable progress is being made toward applying the technology to commercial applications. In this article, we...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00091-1,29 January 2021,
151,0.000678989392134464,151,CLIP-GCD: Simple Language Guided Generalized Category Discovery,"Rabah Ouldnoughi, Chia-Wen Kuo, Zsolt Kira","Generalized Category Discovery (GCD) requires a model to both classify known categories and cluster unknown categories in unlabeled data. Prior methods leveraged self-supervised pre-training combined with supervised fine-tuning on the labeled data, followed by simple clustering methods. In this paper, we posit that such methods are still prone to poor performance on out-of-distribution categories, and do not leverage a key ingredient: Semantic relationships between object categories. We therefore propose to leverage multi-modal (vision and language) models, in two complementary ways. First, we establish a strong baseline by replacing uni-modal features with CLIP, inspired by its zero-shot performance. Second, we propose a novel retrieval-based mechanism that leverages CLIP's aligned vision-language representations by mining text descriptions from a text corpus for the labeled and unlabeled set. We specifically use the alignment between CLIP's visual encoding of the image and textual encoding of the corpus to retrieve top-k relevant pieces of text and incorporate their embeddings to perform joint image+text semi-supervised clustering. We perform rigorous experimentation and ablations (including on where to retrieve from, how much to retrieve, and how to combine information), and validate our results on several datasets including out-of-distribution domains, demonstrating state-of-art results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.10420,May 2023,151,"Fraud, corruption, and collusion in public procurement activities, a systematic literature review on data-driven methods","Marcos S. Lyra, Bruno Damásio, Flávio L. Pinheiro and Fernando Bacao","Fraud, corruption, and collusion are the most common types of crime in public procurement processes; they produce significant monetary losses, inefficiency, and misuse of the public treasury. However, empirica...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-022-00523-6,15 December 2022,
152,0.000678989392134464,152,Bug or not Bug? Analysing the Reasons Behind Metamorphic Relation Violations,"Alejandra Duque-Torres, Dietmar Pfahl, Claus Klammer, Stefan Fischer","Metamorphic Testing (MT) is a testing technique that can effectively alleviate the oracle problem. MT uses Metamorphic Relations (MRs) to determine if a test case passes or fails. MRs specify how the outputs should vary in response to specific input changes when executing the System Under Test (SUT). If a particular MR is violated for at least one test input (and its change), there is a high probability that the SUT has a fault. On the other hand, if a particular MR is not violated, it does not guarantee that the SUT is fault free. However, deciding if the MR is being violated due to a bug or because the MR does not hold/fit for particular conditions generated by specific inputs remains a manual task and unexplored. In this paper, we develop a method for refining MRs to offer hints as to whether a violation results from a bug or arises from the MR not being matched to certain test data under specific circumstances. In our initial proof-of-concept, we derive the relevant information from rules using the Association Rule Mining (ARM) technique. In our initial proof-of-concept, we validate our method on a toy example and discuss the lessons learned from our experiments. Our proof-of-concept demonstrates that our method is applicable and that we can provide suggestions that help strengthen the test suite for regression testing purposes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.09640,May 2023,152,The relationship between students’ use of ICT for social communication and their computer and information literacy,Meral Alkan and Sabine Meinck,This study investigates the relationship between students’ use of information and communication technology (ICT) for social communication and their computer and information literacy (CIL) scores. It also exami...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-016-0029-z,13 September 2016,
153,0.000678989392134464,153,Fuzzy Temporal Protoforms for the Quantitative Description of Processes in Natural Language,"Yago Fontenla-Seco, Alberto Bugarín-Diz, Manuel Lama","In this paper, we propose a series of fuzzy temporal protoforms in the framework of the automatic generation of quantitative and qualitative natural language descriptions of processes. The model includes temporal and causal information from processes and attributes, quantifies attributes in time during the process life-span and recalls causal relations and temporal distances between events, among other features. Through integrating process mining techniques and fuzzy sets within the usual Data-to-Text architecture, our framework is able to extract relevant quantitative temporal as well as structural information from a process and describe it in natural language involving uncertain terms. A real use-case in the cardiology domain is presented, showing the potential of our model for providing natural language explanations addressed to domain experts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.09506,May 2023,153,Logistic regression analysis differentiates high from low computer users by facial skin conditions in a population of Chinese women,"Mary S. Matsui, Jomer Dela Cruz, Jiawen Tang, Bin Wei Deng, Xiaoyuan Xie and Guan Lei","In the past few decades, video display terminals (VDTs) and computer use have been associated with various skin symptoms in several published reports. In addition, internet beauty sites report that extended co...",https://www.springeropen.com//applied-informatics-j.springeropen.com/articles/10.1186/s40535-016-0031-0,7 January 2017,
154,0.000678989392134464,154,"Interplay between Topology and Edge Weights in Real-World Graphs: Concepts, Patterns, and an Algorithm","Fanchen Bu, Shinhwan Kang, Kijung Shin","What are the relations between the edge weights and the topology in real-world graphs? Given only the topology of a graph, how can we assign realistic weights to its edges based on the relations? Several trials have been done for edge-weight prediction where some unknown edge weights are predicted with most edge weights known. There are also existing works on generating both topology and edge weights of weighted graphs. Differently, we are interested in generating edge weights that are realistic in a macroscopic scope, merely from the topology, which is unexplored and challenging. To this end, we explore and exploit the patterns involving edge weights and topology in real-world graphs. Specifically, we divide each graph into layers where each layer consists of the edges with weights at least a threshold. We observe consistent and surprising patterns appearing in multiple layers: the similarity between being adjacent and having high weights, and the nearly-linear growth of the fraction of edges having high weights with the number of common neighbors. We also observe a power-law pattern that connects the layers. Based on the observations, we propose PEAR, an algorithm assigning realistic edge weights to a given topology. The algorithm relies on only two parameters, preserves all the observed patterns, and produces more realistic weights than the baseline methods with more parameters. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.09083,May 2023,154,Discriminated by an algorithm: a systematic review of discrimination and fairness by algorithmic decision-making in the context of HR recruitment and HR development,Alina Köchling and Marius Claus Wehner,Algorithmic decision-making is becoming increasingly common as a new source of advice in HR recruitment and HR development. While firms implement algorithmic decision-making to save costs as well as increase e...,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40685-020-00134-w,20 November 2020,
155,0.000678989392134464,155,New methods for new data? An overview and illustration of quantitative inductive methods for HRM research,Alain LACROUX,"""Data is the new oil"", in short, data would be the essential source of the ongoing fourth industrial revolution, which has led some commentators to assimilate too quickly the quantity of data to a source of wealth in itself, and consider the development of big data as an quasi direct cause of profit. Human resources management is not escaping this trend, and the accumulation of large amounts of data on employees is perceived by some entrepreneurs as a necessary and sufficient condition for the construction of predictive models of complex work behaviors such as absenteeism or job performance. In fact, the analogy is somewhat misleading: unlike oil, there are no major issues here concerning the production of data (whose flows are generated continuously and at low cost by various information systems), but rather their ''refining'', i.e. the operations necessary to transform this data into a useful product, namely into knowledge. This transformation is where the methodological challenges of data valuation lie, both for practitioners and for academic researchers. Considerations on the methods applicable to take advantage of the possibilities offered by these massive data are relatively recent, and often highlight the disruptive aspect of the current ''data deluge'' to point out that this evolution would be the source of a revival of empiricism in a ''fourth paradigm'' based on the intensive and ''agnostic'' exploitation of massive amounts of data in order to bring out new knowledge, following a purely inductive logic. Although we do not adopt this speculative point of view, it is clear that data-driven approaches are scarce in quantitative HRM studies. However, there are well-established methods, particularly in the field of data mining, which are based on inductive approaches. This area of quantitative analysis with an inductive aim is still relatively unexplored in HRM ( apart from typological analyses). The objective of this paper is first to give an overview of data driven methods that can be used for HRM research, before proposing an empirical illustration which consists in an exploratory research combining a latent profile analysis and an exploration by Gaussian graphical models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08889,May 2023,155,Using Noninvasive Wearable Computers to Recognize Human Emotions from Physiological Signals,Christine Lætitia Lisetti and Fatma Nasoz,We discuss the strong relationship between affect and cognition and the importance of emotions in multimodal human computer interaction (HCI) and user modeling. We introduce the overall paradigm for our multim...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704406192,18 September 2004,
156,0.000678989392134464,156,Identification of the Factors Affecting the Reduction of Energy Consumption and Cost in Buildings Using Data Mining Techniques,"Hamed Khosravi, Hadi Sahebi, Rahim khanizad, Imtiaz Ahmed","Optimizing energy consumption and coordination of utility systems have long been a concern of the building industry. Buildings are one of the largest energy consumers in the world, making their energy efficiency crucial for preventing waste and reducing costs. Additionally, buildings generate substantial amounts of raw data, which can be used to understand energy consumption patterns and assist in developing optimization strategies. Using a real-world dataset, this research aims to identify the factors that influence building cost reduction and energy consumption. To achieve this, we utilize three regression models (Lasso Regression, Decision Tree, and Random Forest) to predict primary fuel usage, electrical energy consumption, and cost savings in buildings. An analysis of the factors influencing energy consumption and cost reduction is conducted, and the decision tree algorithm is optimized using metaheuristics. By employing metaheuristic techniques, we fine-tune the decision tree algorithm's parameters and improve its accuracy. Finally, we review the most practical features of potential and nonpotential buildings that can reduce primary fuel usage, electrical energy consumption, and costs △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08886,May 2023,156,Letter from the Editor — in — Chief,Jaelson F. B. Castro,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192378,October 2005,
157,0.000678989392134464,157,Comparing Variation in Tokenizer Outputs Using a Series of Problematic and Challenging Biomedical Sentences,"Christopher Meaney, Therese A Stukel, Peter C Austin, Michael Escobar","Background & Objective: Biomedical text data are increasingly available for research. Tokenization is an initial step in many biomedical text mining pipelines. Tokenization is the process of parsing an input biomedical sentence (represented as a digital character sequence) into a discrete set of word/token symbols, which convey focused semantic/syntactic meaning. The objective of this study is to explore variation in tokenizer outputs when applied across a series of challenging biomedical sentences. Method: Diaz [2015] introduce 24 challenging example biomedical sentences for comparing tokenizer performance. In this study, we descriptively explore variation in outputs of eight tokenizers applied to each example biomedical sentence. The tokenizers compared in this study are the NLTK white space tokenizer, the NLTK Penn Tree Bank tokenizer, Spacy and SciSpacy tokenizers, Stanza/Stanza-Craft tokenizers, the UDPipe tokenizer, and R-tokenizers. Results: For many examples, tokenizers performed similarly effectively; however, for certain examples, there were meaningful variation in returned outputs. The white space tokenizer often performed differently than other tokenizers. We observed performance similarities for tokenizers implementing rule-based systems (e.g. pattern matching and regular expressions) and tokenizers implementing neural architectures for token classification. Oftentimes, the challenging tokens resulting in the greatest variation in outputs, are those words which convey substantive and focused biomedical/clinical meaning (e.g. x-ray, IL-10, TCR/CD3, CD4+ CD8+, and (Ca2+)-regulated). Conclusion: When state-of-the-art, open-source tokenizers from Python and R were applied to a series of challenging biomedical example sentences, we observed subtle variation in the returned outputs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08787,May 2023,157,Letter from the Editor-in-Chief,Jaelson F. B. Castro,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192390,June 2006,
158,0.000678989392134464,158,Causal Data Integration,"Brit Youngmann, Michael Cafarella, Babak Salimi, Anna Zeng","Causal inference is fundamental to empirical scientific discoveries in natural and social sciences; however, in the process of conducting causal inference, data management problems can lead to false discoveries. Two such problems are (i) not having all attributes required for analysis, and (ii) misidentifying which attributes are to be included in the analysis. Analysts often only have access to partial data, and they critically rely on (often unavailable or incomplete) domain knowledge to identify attributes to include for analysis, which is often given in the form of a causal DAG. We argue that data management techniques can surmount both of these challenges. In this work, we introduce the Causal Data Integration (CDI) problem, in which unobserved attributes are mined from external sources and a corresponding causal DAG is automatically built. We identify key challenges and research opportunities in designing a CDI system, and present a system architecture for solving the CDI problem. Our preliminary experimental results demonstrate that solving CDI is achievable and pave the way for future research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08741,May 2023,158,Field-Programmable Gate Arrays in Embedded Systems,"Miriam Leeser, Scott Hauck and Russell Tessier",Unknown,https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/ES/2006/51312,27 September 2006,
159,0.000678989392134464,159,RDF Surfaces: Computer Says No,"Patrick Hochstenbach, Jos De Roo, Ruben Verborgh","Logic can define how agents are provided or denied access to resources, how to interlink resources using mining processes and provide users with choices for possible next steps in a workflow. These decisions are for the most part hidden, internal to machines processing data. In order to exchange this internal logic a portable Web logic is required which the Semantic Web could provide. Combining logic and data provides insights into the reasoning process and creates a new level of trust on the Semantic Web. Current Web logics carries only a fragment of first-order logic (FOL) to keep exchange languages decidable or easily processable. But, this is at a cost: the portability of logic. Machines require implicit agreements to know which fragment of logic is being exchanged and need a strategy for how to cope with the different fragments. These choices could obscure insights into the reasoning process. We created RDF Surfaces in order to express the full expressivity of FOL including saying explicitly `no'. This vision paper provides basic principles and compares existing work. Even though support for FOL is semi-decidable, we argue these problems are surmountable. RDF Surfaces span many use cases, including describing misuse of information, adding explainability and trust to reasoning, and providing scope for reasoning over streams of data and queries. RDF Surfaces provide the direct translation of FOL for the Semantic Web. We hope this vision paper attracts new implementers and opens the discussion to its formal specification. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08476,May 2023,159,AI-perspectives: the Turing option,Frank Kirchner,This paper presents a perspective on AI that starts with going back to early work on this topic originating in theoretical work of Alan Turing. The argument is made that the core idea - that leads to the title...,https://www.springeropen.com//aiperspectives.springeropen.com/articles/10.1186/s42467-020-00006-3,4 September 2020,
160,0.000678989392134464,160,First Principles and Machine Learning Identify Key Pairing Strength Factors of Cuprate Superconductors,"Xinyu He, Ning Chen, Jingpei Chen, Xuezhou Wang, Yang Li","By using band structure calculations of quantum mechanical theory, some important peaks of DoS (Density of States) were obtained and classified based on crystal structure laws of cuprate superconductivity. In particular, the orbital interactions of the in-plane and out-of-plane ions of the copper-oxygen plane were investigated. The position, half-width, and height of DOS peak features were collected for all 35 typical curate systems which have critical temperature maximum data from the works of literature. By training test of 7 common machine learning algorithms, the relationship between the Tc maximum values and these orbital interaction parameters were mined. It was found that the key features of the orbital interaction affecting the Tc maximum were not only the flat band but also a new interaction between core orbitals in a deeper energy band position. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.08038,May 2023,160,Beyond the horizon: immersive developments for animal ecology research,"Ying Zhang, Karsten Klein, Falk Schreiber and Kamran Safi","More diverse data on animal ecology are now available. This “data deluge” presents challenges for both biologists and computer scientists; however, it also creates opportunities to improve analysis and answer ...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-023-00138-3,20 June 2023,
161,0.000678989392134464,161,The use of trade data in the analysis of global phosphate flows,"Matthias Raddant, Martin Bertau, Gerald Steiner","In this paper we present a new method to trace the flows of phosphate from the countries where it is mined to the counties where it is used in agricultural production. We achieve this by combining data on phosphate rock mining with data on fertilizer use and data on international trade of phosphate-related products. We show that by making certain adjustments to data on net exports we can derive the matrix of phosphate flows on the country level to a large degree and thus contribute to the accuracy of material flow analyses, a results that is important for improving environmental accounting, not only for phosphorus but for many other resources. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.07362,May 2023,161,Journal of Umm Al-Qura University for Engineering and Architecture (JUEA),Hamdy M. Youssef,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s43995-022-00010-0,1 December 2022,
162,0.000678989392134464,162,A General-Purpose Multilingual Document Encoder,"Onur Galoğlu, Robert Litschko, Goran Glavaš","Massively multilingual pretrained transformers (MMTs) have tremendously pushed the state of the art on multilingual NLP and cross-lingual transfer of NLP models in particular. While a large body of work leveraged MMTs to mine parallel data and induce bilingual document embeddings, much less effort has been devoted to training general-purpose (massively) multilingual document encoder that can be used for both supervised and unsupervised document-level tasks. In this work, we pretrain a massively multilingual document encoder as a hierarchical transformer model (HMDE) in which a shallow document transformer contextualizes sentence representations produced by a state-of-the-art pretrained multilingual sentence encoder. We leverage Wikipedia as a readily available source of comparable documents for creating training data, and train HMDE by means of a cross-lingual contrastive objective, further exploiting the category hierarchy of Wikipedia for creation of difficult negatives. We evaluate the effectiveness of HMDE in two arguably most common and prominent cross-lingual document-level tasks: (1) cross-lingual transfer for topical document classification and (2) cross-lingual document retrieval. HMDE is significantly more effective than (i) aggregations of segment-based representations and (ii) multilingual Longformer. Crucially, owing to its massively multilingual lower transformer, HMDE successfully generalizes to languages unseen in document-level pretraining. We publicly release our code and models at https://github.com/ogaloglu/pre-training-multilingual-document-encoders . △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.07016,May 2023,162,The computer for the 21st century: present security & privacy challenges,"Leonardo B. Oliveira, Fernando Magno Quintão Pereira, Rafael Misoczki, Diego F. Aranha, Fábio Borges, Michele Nogueira, Michelle Wangham, Min Wu and Jie Liu","Decades went by since Mark Weiser published his influential work on the computer of the 21st century. Over the years, some of the UbiComp features presented in that paper have been gradually adopted by industr...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1186/s13174-018-0095-2,4 December 2018,
163,0.000678989392134464,163,V2Meow: Meowing to the Visual Beat via Music Generation,"Kun Su, Judith Yue Li, Qingqing Huang, Dima Kuzmin, Joonseok Lee, Chris Donahue, Fei Sha, Aren Jansen, Yu Wang, Mauro Verzetti, Timo I. Denk","Generating high quality music that complements the visual content of a video is a challenging task. Most existing visual conditioned music generation systems generate symbolic music data, such as MIDI files, instead of raw audio waveform. Given the limited availability of symbolic music data, such methods can only generate music for a few instruments or for specific types of visual input. In this paper, we propose a novel approach called V2Meow that can generate high-quality music audio that aligns well with the visual semantics of a diverse range of video input types. Specifically, the proposed music generation system is a multi-stage autoregressive model which is trained with a number of O(100K) music audio clips paired with video frames, which are mined from in-the-wild music videos, and no parallel symbolic music data is involved. V2Meow is able to synthesize high-fidelity music audio waveform solely conditioned on pre-trained visual features extracted from an arbitrary silent video clip, and it also allows high-level control over the music style of generation examples via supporting text prompts in addition to the video frames conditioning. Through both qualitative and quantitative evaluations, we demonstrate that our model outperforms several existing music generation systems in terms of both visual-audio correspondence and audio quality. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.06594,May 2023,163,Prospects of quantum computing for molecular sciences,"Hongbin Liu, Guang Hao Low, Damian S. Steiger, Thomas Häner, Markus Reiher and Matthias Troyer","Molecular science is governed by the dynamics of electrons and atomic nuclei, and by their interactions with electromagnetic fields. A faithful physicochemical understanding of these processes is crucial for t...",https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-021-00039-z,7 March 2022,
164,0.000678989392134464,164,Clustering of Time-Varying Graphs Based on Temporal Label Smoothness,"Katsuki Fukumoto, Koki Yamada, Yuichi Tanaka, Hoi-To Wai","We propose a node clustering method for time-varying graphs based on the assumption that the cluster labels are changed smoothly over time. Clustering is one of the fundamental tasks in many science and engineering fields including signal processing, machine learning, and data mining. Although most existing studies focus on the clustering of nodes in static graphs, we often encounter time-varying graphs for time-series data, e.g., social networks, brain functional connectivity, and point clouds. In this paper, we formulate a node clustering of time-varying graphs as an optimization problem based on spectral clustering, with a smoothness constraint of the node labels. We solve the problem with a primal-dual splitting algorithm. Experiments on synthetic and real-world time-varying graphs are performed to validate the effectiveness of the proposed approach. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.06576,May 2023,164,Computer simulation of the entrepreneurial conduction in virtual e-business clusters,Zheng Ye and Le Jiang,"The development of virtual e-business cluster is an effective way to cultivate new industries and realize the transformation and upgrading of traditional industries. Based on computer simulation, in this paper...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-018-1118-1,27 April 2018,
165,0.000678989392134464,165,Long-Tailed Question Answering in an Open World,"Yi Dai, Hao Lang, Yinhe Zheng, Fei Huang, Yongbin Li","Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art. We release the code and data at \url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.06557,May 2023,165,A high-resolution global Vlasov simulation of a small dielectric body with a weak intrinsic magnetic field on the K computer,Takayuki Umeda and Keiichiro Fukazawa,"The interaction between the solar wind and solar system bodies, such as planets, satellites, and asteroids, is one of the fundamental global-scale phenomena in space plasma physics. In the present study, the e...",https://www.springeropen.com//earth-planets-space.springeropen.com/articles/10.1186/s40623-015-0216-0,11 April 2015,
166,0.00673579331412596,166,Do code refactorings influence the merge effort?,"Andre Oliveira, Vania Neves, Alexandre Plastino, Ana Carla Bibiano, Alessandro Garcia, Leonardo Murta","In collaborative software development, multiple contributors frequently change the source code in parallel to implement new features, fix bugs, refactor existing code, and make other changes. These simultaneous changes need to be merged into the same version of the source code. However, the merge operation can fail, and developer intervention is required to resolve the conflicts. Studies in the literature show that 10 to 20 percent of all merge attempts result in conflicts, which require the manual developer's intervention to complete the process. In this paper, we concern about a specific type of change that affects the structure of the source code and has the potential to increase the merge effort: code refactorings. We analyze the relationship between the occurrence of refactorings and the merge effort. To do so, we applied a data mining technique called association rule extraction to find patterns of behavior that allow us to analyze the influence of refactorings on the merge effort. Our experiments extracted association rules from 40,248 merge commits that occurred in 28 popular open-source projects. The results indicate that: (i) the occurrence of refactorings increases the chances of having merge effort; (ii) the more refactorings, the greater the chances of effort; (iii) the more refactorings, the greater the effort; and (iv) parallel refactorings increase even more the chances of having effort, as well as the intensity of it. The results obtained may suggest behavioral changes in the way refactorings are implemented by developer teams. In addition, they can indicate possible ways to improve tools that support code merging and those that recommend refactorings, considering the number of refactorings and merge effort attributes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.06129,May 2023,166,"Racism, sexism and disconnection: contrasting experiences of Black women in STEM before and after transfer from community college","DeeDee Allen, Melissa Dancy, Elizabeth Stearns, Roslyn Mickelson and Martha Bottia","Repeated calls to diversify the population of students earning undergraduate degrees in science, technology, engineering, and mathematics (STEM) fields have noted the greater diversity of community college stu...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00334-2,19 February 2022,
167,0.000678989392134464,167,Efficient pattern-based anomaly detection in a network of multivariate devices,"Len Feremans, Boris Cule, Bart Goethals","Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequency, temporal constraints and minimum description length. We collect and release two public datasets for international broadcasting and X from an Internet company. \textit{BAD} achieves an overall F1-Score of 0.78 on 9 benchmark datasets, significantly outperforming the best baseline by 3\%. Additionally, \textit{BAD} is also an order-of-magnitude faster than state-of-the-art anomaly detection methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.05538,May 2023,167,Retraction Note,"Yunchuan Sun, Hongli Yan, Cheng Lu, Rongfang Bie and Peter Thomas",A retraction article was published for this article. It is available from the following link,http://​www.​comcjournal.​com/​content/​1/​1/​4.,https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-2-3,16 October 2013
168,0.000678989392134464,168,Energy-Efficient Mining for Blockchain-Enabled IoT Applications. An Optimal Multiple-Stopping Time Approach,"Anurag Gupta, Vikram Krishnamurthy","What are the optimal times for an Internet of Things (IoT) device to act as a blockchain miner? The aim is to minimize the energy consumed by low-power IoT devices that log their data into a secure (tamper-proof) distributed ledger. We formulate the energy-efficient blockchain mining for IoT devices as a multiple-stopping time partially observed Markov decision process (POMDP) to maximize the probability of adding a block in the blockchain; we also present a model to optimize the number of stops (mining instants). In general, POMDPs are computationally intractable to solve, but we show mathematically using submodularity that the optimal mining policy has a useful structure: 1) it is monotone in belief space, and 2) it exhibits a threshold structure, which divides the belief space into two connected sets. Exploiting the structural results, we formulate a computationally-efficient linear mining policy for the blockchain-enabled IoT device. We present a policy gradient technique to optimize the parameters of the linear mining policy. Finally, we use synthetic and real Bitcoin datasets to study the performance of our proposed mining policy. We demonstrate the energy efficiency achieved by the optimal linear mining policy in contrast to other heuristic strategies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.05479,May 2023,168,ECR 2022 Book of Abstracts,Unknown,"This article is part of a Supplement:Volume 13
                                        Supplement 4",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-022-01337-x,31 December 2022,
169,0.000678989392134464,169,Measuring Rule-based LTLf Process Specifications: A Probabilistic Data-driven Approach,"Alessio Cecconi, Luca Barbaro, Claudio Di Ciccio, Arik Senderovich","Declarative process specifications define the behavior of processes by means of rules based on Linear Temporal Logic on Finite Traces (LTLf). In a mining context, these specifications are inferred from, and checked on, multi-sets of runs recorded by information systems (namely, event logs). To this end, being able to gauge the degree to which process data comply with a specification is key. However, existing mining and verification techniques analyze the rules in isolation, thereby disregarding their interplay. In this paper, we introduce a framework to devise probabilistic measures for declarative process specifications. Thereupon, we propose a technique that measures the degree of satisfaction of specifications over event logs. To assess our approach, we conduct an evaluation with real-world data, evidencing its applicability in discovery, checking, and drift detection contexts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.05418,May 2023,169,Test-taker perception of and test performance on computer-delivered speaking tests: the mediational role of test-taking motivation,Yujia Zhou and Asako Yoshitomi,Research on the test-taker perception of assessments has been conducted under the assumption that negative test-taker perception may influence test performance by decreasing test-taking motivation. This assump...,https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-019-0086-7,26 June 2019,
170,0.000678989392134464,170,PLM-GNN: A Webpage Classification Method based on Joint Pre-trained Language Model and Graph Neural Network,"Qiwei Lang, Jingbo Zhou, Haoyi Wang, Shiqi Lyu, Rui Zhang","The number of web pages is growing at an exponential rate, accumulating massive amounts of data on the web. It is one of the key processes to classify webpages in web information mining. Some classical methods are based on manually building features of web pages and training classifiers based on machine learning or deep learning. However, building features manually requires specific domain knowledge and usually takes a long time to validate the validity of features. Considering webpages generated by the combination of text and HTML Document Object Model(DOM) trees, we propose a representation and classification method based on a pre-trained language model and graph neural network, named PLM-GNN. It is based on the joint encoding of text and HTML DOM trees in the web pages. It performs well on the KI-04 and SWDE datasets and on practical dataset AHS for the project of scholar's homepage crawling. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.05378,May 2023,170,A comparative study of computer and mobile phone-mediated collaboration: the case of university students in Japan,Gibran Alejandro Garcia Mendoza,"Web-based forums are the major form of asynchronous communication in online courses. They are considered suitable collaborative learning environments to conduct discussions among groups of learners (Lieblein, ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.7238/rusc.v11i1.1898,15 January 2014,
171,0.000678989392134464,171,ALMA High-Level Data Products: Submillimetre counterparts of SDSS quasars in the ALMA footprint,"A. Wong, E. Hatziminaoglou, A. Borkar, G. Popping, I. Pérez-Fournon, F. Poidevin, F. Stoehr, H. Messias","The Atacama Large Millimetre/submillimetre Array (ALMA) is the world's most advanced radio interferometric facility, producing science data with an average rate of about 1 TB per day. After a process of calibration, imaging and quality assurance, the scientific data are stored in the ALMA Science Archive (ASA), along with the corresponding raw data, making the ASA an invaluable resource for original astronomical research. Due to their complexity, each ALMA data set has the potential for scientific results that go well beyond the ideas behind the original proposal that led to each observation. For this reason, the European ALMA Regional Centre initiated the High-Level Data Products initiative to develop science-oriented data products derived from data sets publicly available in the ASA, that go beyond the formal ALMA deliverables. The first instance of this initiative is the creation of a catalogue of submillimetre (submm) detections of Sloan Digital Sky Survey (SDSS) quasars from the SDSS Data Release 14 that lie in the aggregate ALMA footprint observed since ALMA Cycle 0. The ALMA fluxes are extracted in an automatic fashion, using the ALMA Data Mining Toolkit. All extractions above a signal-to-noise cut of 3.5 are considered, they have been visually inspected and the reliable detections are presented in a catalogue of 376 entries, corresponding to 275 unique quasars. Interesting targets found in the process, i.e. lensed or jetted quasars as well as quasars with nearby submm counterparts are highlighted, to facilitate further studies or potential follow up observations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.05173,May 2023,171,Is computer availability at home causally related to reading achievement in grade 4? A longitudinal difference in differences approach to IEA data from 1991 to 2006,Monica Rosén and Jan-Eric Gustafsson,"Research on effects of home computer use on children’s development of cognitive abilities and skills has yielded conflicting results, with some studies showing positive effects, others no effects, and yet othe...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-016-0020-8,24 February 2016,
172,0.000678989392134464,172,Unified Demonstration Retriever for In-Context Learning,"Xiaonan Li, Kai Lv, Hang Yan, Tianyang Lin, Wei Zhu, Yuan Ni, Guotong Xie, Xiaoling Wang, Xipeng Qiu","In-context learning is a new learning paradigm where a language model conditions on a few input-output pairs (demonstrations) and a test input, and directly outputs the prediction. It has been shown highly dependent on the provided demonstrations and thus promotes the research of demonstration retrieval: given a test input, relevant examples are retrieved from the training set to serve as informative demonstrations for in-context learning. While previous works focus on training task-specific retrievers for several tasks separately, these methods are often hard to transfer and scale on various tasks, and separately trained retrievers incur a lot of parameter storage and deployment cost. In this paper, we propose Unified Demonstration Retriever (\textbf{UDR}), a single model to retrieve demonstrations for a wide range of tasks. To train UDR, we cast various tasks' training signals into a unified list-wise ranking formulation by language model's feedback. Then we propose a multi-task list-wise ranking training framework, with an iterative mining strategy to find high-quality candidates, which can help UDR fully incorporate various tasks' signals. Experiments on 30+ tasks across 13 task families and multiple data domains show that UDR significantly outperforms baselines. Further analyses show the effectiveness of each proposed component and UDR's strong ability in various scenarios including different LMs (1.3B - 175B), unseen datasets, varying demonstration quantities, etc. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.04320,May 2023,172,An analysis of Italian university students’ performance through segmented regression models: gender differences in STEM courses,"Andrea Priulla, Nicoletta D’Angelo and Massimo Attanasio","This paper investigates gender differences in university performances in Science, Technology, Engineering and Mathematics (STEM) courses in Italy, proposing a novel application through the segmented regression...",https://www.springeropen.com//genus.springeropen.com/articles/10.1186/s41118-021-00118-6,5 July 2021,
173,0.000678989392134464,173,Influence of Swarm Intelligence in Data Clustering Mechanisms,"Pitawelayalage Dasun Dileepa Pitawela, Gamage Upeksha Ganegoda","Data mining focuses on discovering interesting, non-trivial and meaningful information from large datasets. Data clustering is one of the unsupervised and descriptive data mining task which group data based on similarity features and physically stored together. As a partitioning clustering method, K-means is widely used due to its simplicity and easiness of implementation. But this method has limitations such as local optimal convergence and initial point sensibility. Due to these impediments, nature inspired Swarm based algorithms such as Artificial Bee Colony Algorithm, Ant Colony Optimization, Firefly Algorithm, Bat Algorithm and etc. are used for data clustering to cope with larger datasets with lack and inconsistency of data. In some cases, those algorithms are used with traditional approaches such as K-means as hybrid approaches to produce better results. This paper reviews the performances of these new approaches and compares which is best for certain problematic situation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.04217,May 2023,173,Optimal transmission of messages in computer networks – an optimal control problem involving control-dependent time-delayed arguments,"Kar-hung Wong, Yu-chung Eugene Lee and Heung-wing Joseph Lee","In this paper, we find the optimal transmission of messages in computer networks. This problem has been formulated as a nondelayed optimal control problem in several recent papers on TCP (transmission control ...",https://www.springeropen.com//journalofinequalitiesandapplications.springeropen.com/articles/10.1186/s13660-022-02823-y,7 July 2022,
174,0.000678989392134464,174,Cross-Modal Retrieval for Motion and Text via MildTriple Loss,"Sheng Yan, Haoqiang Wang, Xin Du, Mengyuan Liu, Hong Liu","Cross-modal retrieval has become a prominent research topic in computer vision and natural language processing with advances made in image-text and video-text retrieval technologies. However, cross-modal retrieval between human motion sequences and text has not garnered sufficient attention despite the extensive application value it holds, such as aiding virtual reality applications in better understanding users' actions and language. This task presents several challenges, including joint modeling of the two modalities, demanding the understanding of person-centered information from text, and learning behavior features from 3D human motion sequences. Previous work on motion data modeling mainly relied on autoregressive feature extractors that may forget previous information, while we propose an innovative model that includes simple yet powerful transformer-based motion and text encoders, which can learn representations from the two different modalities and capture long-term dependencies. Furthermore, the overlap of the same atomic actions of different human motions can cause semantic conflicts, leading us to explore a new triplet loss function, MildTriple Loss. it leverages the similarity between samples in intra-modal space to guide soft-hard negative sample mining in the joint embedding space to train the triplet loss and reduce the violation caused by false negative samples. We evaluated our model and method on the latest HumanML3D and KIT Motion-Language datasets, achieving a 62.9\% recall for motion retrieval and a 71.5\% recall for text retrieval (based on R@10) on the HumanML3D dataset. Our code is available at https://github.com/eanson023/rehamot. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.04195,May 2023,174,“Smart” greenhouses and pluridisciplinary spaces: supporting adolescents’ engagement and self-efficacy in computation across disciplines,"David W. Jackson, Yihong Cheng, Qi Meng and Yang Xu","Educational designers are working to embed computation in required classes outside of computer science (CS) courses, to promote equitable access for all students. While many studies embed computation in one di...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00046-1,1 February 2022,
175,0.000678989392134464,175,Mixer: Image to Multi-Modal Retrieval Learning for Industrial Application,"Zida Cheng, Shuai Xiao, Zhonghua Zhai, Xiaoyi Zeng, Weilin Huang","Cross-modal retrieval, where the query is an image and the doc is an item with both image and text description, is ubiquitous in e-commerce platforms and content-sharing social media. However, little research attention has been paid to this important application. This type of retrieval task is challenging due to the facts: 1)~domain gap exists between query and doc. 2)~multi-modality alignment and fusion. 3)~skewed training data and noisy labels collected from user behaviors. 4)~huge number of queries and timely responses while the large-scale candidate docs exist. To this end, we propose a novel scalable and efficient image query to multi-modal retrieval learning paradigm called Mixer, which adaptively integrates multi-modality data, mines skewed and noisy data more efficiently and scalable to high traffic. The Mixer consists of three key ingredients: First, for query and doc image, a shared encoder network followed by separate transformation networks are utilized to account for their domain gap. Second, in the multi-modal doc, images and text are not equally informative. So we design a concept-aware modality fusion module, which extracts high-level concepts from the text by a text-to-image attention mechanism. Lastly, but most importantly, we turn to a new data organization and training paradigm for single-modal to multi-modal retrieval: large-scale classification learning which treats single-modal query and multi-modal doc as equivalent samples of certain classes. Besides, the data organization follows a weakly-supervised manner, which can deal with skewed data and noisy labels inherited in the industrial systems. Learning such a large number of categories for real-world multi-modality data is non-trivial and we design a specific learning strategy for it. The proposed Mixer achieves SOTA performance on public datasets from industrial retrieval systems. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03972,May 2023,175,Empirical research in CSCW — a review of the ACM/CSCW conferences from 1998 to 2004,Jacques Wainer and Claudia Barsottini,This paper reviews all the 169 full papers published in the ACM/CSCW conferences from 1998 to 2004. We classify the papers according to the type of empirical research they report. The classes are evaluation of...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192543,September 2007,
176,0.000678989392134464,176,Automated Spatio-Temporal Graph Contrastive Learning,"Qianru Zhang, Chao Huang, Lianghao Xia, Zheng Wang, Zhonghang Li, Siuming Yiu","Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \model\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region dependencies with respect to POI semantics, mobility flow patterns and geographical positions. To improve the robustness of our GNN encoder against data noise and distribution issues, we design an automated spatio-temporal augmentation scheme with a parameterized contrastive view generator. AutoST can adapt to the spatio-temporal heterogeneous graph with multi-view semantics well preserved. Extensive experiments for three downstream spatio-temporal mining tasks on several real-world datasets demonstrate the significant performance gain achieved by our \model\ over a variety of baselines. The code is publicly available at https://github.com/HKUDS/AutoST. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03920,May 2023,176,Measuring Behaviors and Identifying Indicators of Self-Regulation in Computer-Assisted Language Learning Courses,"Huiyong Li, Brendan Flanagan, Shin’ichi Konomi and Hiroaki Ogata",The aim of this research is to measure self-regulated behavior and identify significant behavioral indicators in computer-assisted language learning courses. The behavioral measures were based on log data from...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-018-0087-7,5 December 2018,
177,0.000678989392134464,177,SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention for Emotion Recognition in Conversation,"Xingwei Liang, You Zou, Ruifeng Xu","Emotion Recognition in Conversation~(ERC) across modalities is of vital importance for a variety of applications, including intelligent healthcare, artificial intelligence for conversation, and opinion mining over chat history. The crux of ERC is to model both cross-modality and cross-time interactions throughout the conversation. Previous methods have made progress in learning the time series information of conversation while lacking the ability to trace down the different emotional states of each speaker in a conversation. In this paper, we propose a recurrent structure called Speaker Information Enhanced Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states of the distinct speaker can be tracked in a sequential way to enhance the learning of the emotion in conversation. Further, to improve the learning of multimodal features in ERC, we utilize a cross-modal attention component to fuse the features between different modalities and model the interaction of the important information from different modalities. Experimental results on two benchmark datasets demonstrate the superiority of the proposed SI-LSTM against the state-of-the-art baseline methods in the ERC task on multimodal data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03506,May 2023,177,scriptLattes: an open-source knowledge extraction system from the Lattes platform,Jesús Pascual Mena-Chalco and Roberto Marcondes Cesar Junior,The Lattes platform is the major scientific information system maintained by the National Council for Scientific and Technological Development (CNPq). This platform allows to manage the curricular information ...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194511,December 2009,
178,0.000678989392134464,178,FM-ViT: Flexible Modal Vision Transformers for Face Anti-Spoofing,"Ajian Liu, Zichang Tan, Zitong Yu, Chenxu Zhao, Jun Wan, Yanyan Liang, Zhen Lei, Du Zhang, Stan Z. Li, Guodong Guo","The availability of handy multi-modal (i.e., RGB-D) sensors has brought about a surge of face anti-spoofing research. However, the current multi-modal face presentation attack detection (PAD) has two defects: (1) The framework based on multi-modal fusion requires providing modalities consistent with the training input, which seriously limits the deployment scenario. (2) The performance of ConvNet-based model on high fidelity datasets is increasingly limited. In this work, we present a pure transformer-based framework, dubbed the Flexible Modal Vision Transformer (FM-ViT), for face anti-spoofing to flexibly target any single-modal (i.e., RGB) attack scenarios with the help of available multi-modal data. Specifically, FM-ViT retains a specific branch for each modality to capture different modal information and introduces the Cross-Modal Transformer Block (CMTB), which consists of two cascaded attentions named Multi-headed Mutual-Attention (MMA) and Fusion-Attention (MFA) to guide each modal branch to mine potential features from informative patch tokens, and to learn modality-agnostic liveness features by enriching the modal information of own CLS token, respectively. Experiments demonstrate that the single model trained based on FM-ViT can not only flexibly evaluate different modal samples, but also outperforms existing single-modal frameworks by a large margin, and approaches the multi-modal frameworks introduced with smaller FLOPs and model parameters. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03277,May 2023,178,Validity of score interpretations on an online English placement writing test,Yun Deok Choi,A much-debated question in the L2 assessment field is if computer familiarity should be considered a potential source of construct-irrelevant variance in computer-based writing (CBW) tests. This study aims to ...,https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-022-00187-0,15 September 2022,
179,0.000678989392134464,179,Forecasting Inter-Destination Tourism Flow via a Hybrid Deep Learning Model,"Hanxi Fang, Song Gao, Feng Zhang","Tourists often go to multiple tourism destinations in one trip. The volume of tourism flow between tourism destinations, also referred to as ITF (Inter-Destination Tourism Flow) in this paper, is commonly used for tourism management on tasks like the classification of destinations' roles and visitation pattern mining. However, the ITF is hard to get due to the limitation of data collection techniques and privacy issues. It is difficult to understand how the volume of ITF is influenced by features of the multi-attraction system. To address these challenges, we utilized multi-source datasets and proposed a graph-based hybrid deep learning model to predict the ITF. The model makes use of both the explicit features of individual tourism attractions and the implicit features of the interactions between multiple attractions. Experiments on ITF data extracted from crowdsourced tourists' travel notes about the city of Beijing verified the usefulness of the proposed model. Besides, we analyze how different features of tourism attractions influence the volume of ITF with explainable AI techniques. Results show that popularity, quality and distance are the main three influential factors. Other features like coordinates will also exert an influence in different ways. The predicted ITF data can be further used for various downstream tasks in tourism management. The research also deepens the understanding of tourists' visiting choice in a tourism system consisting of multiple attractions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03267,May 2023,179,Patterns of students’ computer use and relations to their computer and information literacy: results of a latent class analysis and implications for teaching and learning,Jeppe Bundsgaard and Julia Gerick,Previous studies have shown that there is a complex relationship between students’ computer and information literacy (CIL) and their use of information and communication technologies (ICT) for both recreationa...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-017-0052-8,13 November 2017,
180,0.000678989392134464,180,Out-of-Domain Intent Detection Considering Multi-turn Dialogue Contexts,"Hao Lang, Yinhe Zheng, Binyuan Hui, Fei Huang, Yongbin Li","Out-of-Domain (OOD) intent detection is vital for practical dialogue systems, and it usually requires considering multi-turn dialogue contexts. However, most previous OOD intent detection approaches are limited to single dialogue turns. In this paper, we introduce a context-aware OOD intent detection (Caro) framework to model multi-turn contexts in OOD intent detection tasks. Specifically, we follow the information bottleneck principle to extract robust representations from multi-turn dialogue contexts. Two different views are constructed for each input sample and the superfluous information not related to intent detection is removed using a multi-view information bottleneck loss. Moreover, we also explore utilizing unlabeled data in Caro. A two-stage training process is introduced to mine OOD samples from these unlabeled data, and these OOD samples are used to train the resulting model with a bootstrapping approach. Comprehensive experiments demonstrate that Caro establishes state-of-the-art performances on multi-turn OOD detection tasks by improving the F1-OOD score of over $29\%$ compared to the previous best method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.03237,May 2023,180,Looking at computer-visualized interior wood: A qualitative assessment using focus groups,Enar Nordvik and N. Olof Broman,The objective of this study was to explore and gather human reactions and perceptions on computer visualizations of interior wood. The subjective qualities of such products are important because they infl uenc...,https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/s10086-008-1008-y,1 April 2009,
181,0.000678989392134464,181,evaluating bert and parsbert for analyzing persian advertisement data,"Ali Mehrban, Pegah Ahadian","This paper discusses the impact of the Internet on modern trading and the importance of data generated from these transactions for organizations to improve their marketing efforts. The paper uses the example of Divar, an online marketplace for buying and selling products and services in Iran, and presents a competition to predict the percentage of a car sales ad that would be published on the Divar website. Since the dataset provides a rich source of Persian text data, the authors use the Hazm library, a Python library designed for processing Persian text, and two state-of-the-art language models, mBERT and ParsBERT, to analyze it. The paper's primary objective is to compare the performance of mBERT and ParsBERT on the Divar dataset. The authors provide some background on data mining, Persian language, and the two language models, examine the dataset's composition and statistical features, and provide details on their fine-tuning and training configurations for both approaches. They present the results of their analysis and highlight the strengths and weaknesses of the two language models when applied to Persian text data. The paper offers valuable insights into the challenges and opportunities of working with low-resource languages such as Persian and the potential of advanced language models like BERT for analyzing such data. The paper also explains the data mining process, including steps such as data cleaning and normalization techniques. Finally, the paper discusses the types of machine learning problems, such as supervised, unsupervised, and reinforcement learning, and the pattern evaluation techniques, such as confusion matrix. Overall, the paper provides an informative overview of the use of language models and data mining techniques for analyzing text data in low-resource languages, using the example of the Divar dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.02426,May 2023,181,Computer-delivered or face-to-face: effects of delivery mode on the testing of second language speaking,Yujia Zhou,"The use of computers has increased in speaking assessments; however, there are concerns about how the absence of an interlocutor affects performance on speaking tests.",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-014-0012-y,3 February 2015,
182,0.000678989392134464,182,PopSim: An Individual-level Population Simulator for Equitable Allocation of City Resources,"Khanh Duy Nguyen, Nima Shahbazi, Abolfazl Asudeh","Historical systematic exclusionary tactics based on race have forced people of certain demographic groups to congregate in specific urban areas. Aside from the ethical aspects of such segregation, these policies have implications for the allocation of urban resources including public transportation, healthcare, and education within the cities. The initial step towards addressing these issues involves conducting an audit to assess the status of equitable resource allocation. However, due to privacy and confidentiality concerns, individual-level data containing demographic information cannot be made publicly available. By leveraging publicly available aggregated demographic statistics data, we introduce PopSim, a system for generating semi-synthetic individual-level population data with demographic information. We use PopSim to generate multiple benchmark datasets for the city of Chicago and conduct extensive statistical evaluations to validate those. We further use our datasets for several case studies that showcase the application of our system for auditing equitable allocation of city resources. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.02204,May 2023,182,Gigapixel optical microscopy for meteorite characterization,Ryan C Ogliore and Christine E Jilly,We present an automated microscopy system for the optical characterization of meteorite thin sections. The system employs focus-stacking and high-dynamic range imaging to facilitate high-contrast and unpolishe...,https://www.springeropen.com//planetary-science.springeropen.com/articles/10.1186/2191-2521-2-3,8 October 2013,
183,0.000678989392134464,183,Considerations for Ethical Speech Recognition Datasets,"Orestis Papakyriakopoulos, Alice Xiang","Speech AI Technologies are largely trained on publicly available datasets or by the massive web-crawling of speech. In both cases, data acquisition focuses on minimizing collection effort, without necessarily taking the data subjects' protection or user needs into consideration. This results to models that are not robust when used on users who deviate from the dominant demographics in the training set, discriminating individuals having different dialects, accents, speaking styles, and disfluencies. In this talk, we use automatic speech recognition as a case study and examine the properties that ethical speech datasets should possess towards responsible AI applications. We showcase diversity issues, inclusion practices, and necessary considerations that can improve trained models, while facilitating model explainability and protecting users and data subjects. We argue for the legal & privacy protection of data subjects, targeted data sampling corresponding to user demographics & needs, appropriate meta data that ensure explainability & accountability in cases of model failure, and the sociotechnical \& situated model design. We hope this talk can inspire researchers \& practitioners to design and use more human-centric datasets in speech technologies and other domains, in ways that empower and respect users, while improving machine learning models' robustness and utility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.02081,May 2023,183,Letter from the editor-in-chief,Paulo C. Masiero,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192355,June 2004,
184,0.000678989392134464,184,Information flow simulation community detection of weighted-directed campus friendship network in continuous time,"Ren Chao, Yang Menghui","Educational data mining has become an important research field in studying the social behavior of college students using massive data. However, traditional campus friendship network and their community detection algorithms, which lack time characteristics, have their limitations. This paper proposes a new approach to address these limitations by reconstructing the campus friendship network into weighted directed networks in continuous time, improving the effectiveness of traditional campus friendship network and the accuracy of community detection results. To achieve this, a new weighted directed community detection algorithm for campus friendship network in continuous time is proposed, and it is used to study the community detection of a university student. The results show that the weighted directed friendship network reconstructed in this paper can reveal the real friend relationships better than the initial undirected unauthorized friendship network. Furthermore, the community detection algorithm proposed in this paper obtains better community detection effects. After community detection, students in the same community exhibit similarities in consumption level, eating habits, and behavior regularity. This paper enriches the theoretical research of complex friendship network considering the characteristics of time, and also provides objective scientific guidance for the management of college students. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.01958,May 2023,184,Letter from the editor-in-chief,Hugo Fuks,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192548,March 2008,
185,0.000678989392134464,185,OTIEA:Ontology-enhanced Triple Intrinsic-Correlation for Cross-lingual Entity Alignment,"Zhishuo Zhang, Chengxiang Tan, Xueyan Zhao, Min Yang, Chaoqun Jiang","Cross-lingual and cross-domain knowledge alignment without sufficient external resources is a fundamental and crucial task for fusing irregular data. As the element-wise fusion process aiming to discover equivalent objects from different knowledge graphs (KGs), entity alignment (EA) has been attracting great interest from industry and academic research recent years. Most of existing EA methods usually explore the correlation between entities and relations through neighbor nodes, structural information and external resources. However, the complex intrinsic interactions among triple elements and role information are rarely modeled in these methods, which may lead to the inadequate illustration for triple. In addition, external resources are usually unavailable in some scenarios especially cross-lingual and cross-domain applications, which reflects the little scalability of these methods. To tackle the above insufficiency, a novel universal EA framework (OTIEA) based on ontology pair and role enhancement mechanism via triple-aware attention is proposed in this paper without introducing external resources. Specifically, an ontology-enhanced triple encoder is designed via mining intrinsic correlations and ontology pair information instead of independent elements. In addition, the EA-oriented representations can be obtained in triple-aware entity decoder by fusing role diversity. Finally, a bidirectional iterative alignment strategy is deployed to expand seed entity pairs. The experimental results on three real-world datasets show that our framework achieves a competitive performance compared with baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.01561,May 2023,185,ECR 2021 Book of Abstracts,Unknown,"This article is part of a Supplement:Volume 12
                                        Supplement 2",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-021-01014-5,13 June 2021,
186,0.000678989392134464,186,Uncovering the Spatial and Temporal Variability of Wind Resources in Europe: A Web-Based Data-Mining Tool,"Alban Puech, Jesse Read","We introduce REmap-eu.app, a web-based data-mining visualization tool of the spatial and temporal variability of wind resources. It uses the latest open-access dataset of the daily wind capacity factor in 28 European countries between 1979 and 2019 and proposes several user-configurable visualizations of the temporal and spatial variations of the wind power capacity factor. The platform allows for a deep analysis of the distribution, the crosscountry correlation, and the drivers of low wind power events. It offers an easy-to-use interface that makes it suitable for the needs of researchers and stakeholders. The tool is expected to be useful in identifying areas of high wind potential and possible challenges that may impact the large-scale deployment of wind turbines in Europe. Particular importance is given to the visualization of low wind power events and to the potential of cross-border cooperations in mitigating the variability of wind in the context of increasing reliance on weather-sensitive renewable energy sources. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.01343,May 2023,186,A content analysis of research on technology use for teaching mathematics to students with disabilities: word networks and topic modeling,"Mikyung Shin, Min Wook Ok, Sam Choo, Gahangir Hossain, Diane P. Bryant and Eunyoung Kang",The purpose of this study was to conduct a content analysis of research on technology use for teaching mathematics to students with disabilities. We applied word networks and structural topic modeling of 488 s...,https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00414-x,26 March 2023,
187,0.000678989392134464,187,HTPS: Heterogeneous Transferring Prediction System for Healthcare Datasets,"Jia-Hao Syu, Jerry Chun-Wei Lin, Marcin Fojcik, Rafał Cupek","Medical internet of things leads to revolutionary improvements in medical services, also known as smart healthcare. With the big healthcare data, data mining and machine learning can assist wellness management and intelligent diagnosis, and achieve the P4-medicine. However, healthcare data has high sparsity and heterogeneity. In this paper, we propose a Heterogeneous Transferring Prediction System (HTPS). Feature engineering mechanism transforms the dataset into sparse and dense feature matrices, and autoencoders in the embedding networks not only embed features but also transfer knowledge from heterogeneous datasets. Experimental results show that the proposed HTPS outperforms the benchmark systems on various prediction tasks and datasets, and ablation studies present the effectiveness of each designed mechanism. Experimental results demonstrate the negative impact of heterogeneous data on benchmark systems and the high transferability of the proposed HTPS. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.01252,May 2023,187,Debugging behaviors of early childhood teacher candidates with or without scaffolding,"ChanMin Kim, Lucas Vasconcelos, Brian R. Belland, Duygu Umutlu and Cory Gleasman",It is critical to teach all learners to program and think through programming. But to do so requires that early childhood teacher candidates learn to teach computer science. This in turn requires novel pedagog...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00319-9,10 March 2022,
188,0.000678989392134464,188,Faster OreFSDet : A Lightweight and Effective Few-shot Object Detector for Ore Images,"Yang Zhang, Le Cheng, Yuting Peng, Chengming Xu, Yanwei Fu, Bo Wu, Guodong Sun","For the ore particle size detection, obtaining a sizable amount of high-quality ore labeled data is time-consuming and expensive. General object detection methods often suffer from severe over-fitting with scarce labeled data. Despite their ability to eliminate over-fitting, existing few-shot object detectors encounter drawbacks such as slow detection speed and high memory requirements, making them difficult to implement in a real-world deployment scenario. To this end, we propose a lightweight and effective few-shot detector to achieve competitive performance with general object detection with only a few samples for ore images. First, the proposed support feature mining block characterizes the importance of location information in support features. Next, the relationship guidance block makes full use of support features to guide the generation of accurate candidate proposals. Finally, the dual-scale semantic aggregation module retrieves detailed features at different resolutions to contribute with the prediction process. Experimental results show that our method consistently exceeds the few-shot detectors with an excellent performance gap on all metrics. Moreover, our method achieves the smallest model size of 19MB as well as being competitive at 50 FPS detection speed compared with general object detectors. The source code is available at https://github.com/MVME-HBUT/Faster-OreFSDet. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.01183,May 2023,188,Challenges of a cross-national computer-based test adaptation,"Doreen Holtsch, Silja Rohr-Mentele, Eva Wenger, Franz Eberle and Richard J. Shavelson","In an increasingly globalized world, the call for internationally comparable competence measurements has emerged. After several international studies on pre-college education, the focus has shifted to internat...",https://www.springeropen.com//ervet-journal.springeropen.com/articles/10.1186/s40461-016-0043-y,12 December 2016,
189,0.000678989392134464,189,PMDG: Privacy for Multi-Perspective Process Mining through Data Generalization,"Ryan Hildebrant, Stephan A. Fahrenkrog-Petersen, Matthias Weidlich, Shangping Ren","Anonymization of event logs facilitates process mining while protecting sensitive information of process stakeholders. Existing techniques, however, focus on the privatization of the control-flow. Other process perspectives, such as roles, resources, and objects are neglected or subject to randomization, which breaks the dependencies between the perspectives. Hence, existing techniques are not suited for advanced process mining tasks, e.g., social network mining or predictive monitoring. To address this gap, we propose PMDG, a framework to ensure privacy for multi-perspective process mining through data generalization. It provides group-based privacy guarantees for an event log, while preserving the characteristic dependencies between the control-flow and further process perspectives. Unlike existin privatization techniques that rely on data suppression or noise insertion, PMDG adopts data generalization: a technique where the activities and attribute values referenced in events are generalized into more abstract ones, to obtain equivalence classes that are sufficiently large from a privacy point of view. We demonstrate empirically that PMDG outperforms state-of-the-art anonymization techniques, when mining handovers and predicting outcomes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.00960,May 2023,189,Bringing computational science to the public,"James L. McDonagh, Daniel Barker and Rosanna G. Alderson",The increasing use of computers in science allows for the scientific analyses of large datasets at an increasing pace. We provided examples and interactive demonstrations at Dundee Science Centre as part of th...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/s40064-016-1856-7,2 March 2016,
190,0.000678989392134464,190,SelfDocSeg: A Self-Supervised vision-based Approach towards Document Segmentation,"Subhajit Maity, Sanket Biswas, Siladittya Manna, Ayan Banerjee, Josep Lladós, Saumik Bhattacharya, Umapada Pal","Document layout analysis is a known problem to the documents research community and has been vastly explored yielding a multitude of solutions ranging from text mining, and recognition to graph-based representation, visual feature extraction, etc. However, most of the existing works have ignored the crucial fact regarding the scarcity of labeled data. With growing internet connectivity to personal life, an enormous amount of documents had been available in the public domain and thus making data annotation a tedious task. We address this challenge using self-supervision and unlike, the few existing self-supervised document segmentation approaches which use text mining and textual labels, we use a complete vision-based approach in pre-training without any ground-truth label or its derivative. Instead, we generate pseudo-layouts from the document images to pre-train an image encoder to learn the document object representation and localization in a self-supervised framework before fine-tuning it with an object detection model. We show that our pipeline sets a new benchmark in this context and performs at par with the existing methods and the supervised counterparts, if not outperforms. The code is made publicly available at: https://github.com/MaitySubhajit/SelfDocSeg △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.00795,May 2023,190,Experimenting quantum phenomena on NISQ computers using high level quantum programming,"Duc M. Tran, Duy V. Nguyen, Bin Ho Le and Hung Q. Nguyen","We execute the quantum eraser, the Elitzur–Vaidman bomb, and the Hardy’s paradox experiment using high-level programming language on a generic, gate-based superconducting quantum processor made publicly availa...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00126-1,10 February 2022,
191,0.000678989392134464,191,Leveraging Data Mining Algorithms to Recommend Source Code Changes,"AmirHossein Naghshzan, Saeed Khalilazar, Pierre Poilane, Olga Baysal, Latifa Guerrouj, Foutse Khomh","Context: Recent research has used data mining to develop techniques that can guide developers through source code changes. To the best of our knowledge, very few studies have investigated data mining techniques and--or compared their results with other algorithms or a baseline. Objectives: This paper proposes an automatic method for recommending source code changes using four data mining algorithms. We not only use these algorithms to recommend source code changes, but we also conduct an empirical evaluation. Methods: Our investigation includes seven open-source projects from which we extracted source change history at the file level. We used four widely data mining algorithms \ie{} Apriori, FP-Growth, Eclat, and Relim to compare the algorithms in terms of performance (Precision, Recall and F-measure) and execution time. Results: Our findings provide empirical evidence that while some Frequent Pattern Mining algorithms, such as Apriori may outperform other algorithms in some cases, the results are not consistent throughout all the software projects, which is more likely due to the nature and characteristics of the studied projects, in particular their change history. Conclusion: Apriori seems appropriate for large-scale projects, whereas Eclat appears to be suitable for small-scale projects. Moreover, FP-Growth seems an efficient approach in terms of execution time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.00323,May 2023,191,Challenges on applying genetic improvement in JavaScript using a high-performance computer,"Fábio de Almeida Farzat, Márcio de Oliveira Barros and Guilherme Horta Travassos",Genetic Improvement is an area of Search Based Software Engineering that aims to apply evolutionary computing operators to the software source code to improve it according to one or more quality metrics. This ...,https://www.springeropen.com//jserd.springeropen.com/articles/10.1186/s40411-018-0056-2,6 October 2018,
192,0.000678989392134464,192,Patent Mining by Extracting Functional Analysis Information Modelled As Graph Structure: A Patent Knowledge-base Collaborative Building Approach,Manal E. Helal,"Patents provide a rich source of information about design innovations. Patent mining techniques employ various technologies, such as text mining, machine learning, natural language processing, and ontology-building techniques. An automated graph data modelling method is proposed for extracting functional representations for building a semantic database of patents of mechanical designs. The method has several benefits: The schema-free characteristic of the proposed graph modelling enables the ontology it is based on to evolve and generalise to upper ontologies across technology domains and to specify lower ontologies to more specific domains. Graph modelling benefits from enhanced performance of deep queries across many levels of relationships and interactions and provides efficient storage. Graph modelling also enables visualisation libraries to use the graph data structure immediately, avoiding the need for graph extraction programs from relational databases. Patent/Design comparisons are computed by search queries using counting of overlaps of different levels and weights. This work has produced the PatMine SolidWorks Add-in \c{opyright}, which compares annotated CAD designs with patents and highlights overlapping design concepts. The patent annotation extracts its functional analysis, representing its structure as geometric feature interactions. Additional features such as full-text search and semantic search of the PatMine patents database are available, and graph analytic methods and machine learning algorithms are enabled and can be implemented as plug-ins in future work. Keywords: Patent Mining; Semantic Analysis; Functional Analysis Diagrams; Graph Data Modelling; Visualisation; Similarity Scoring; Big Data Analytics; Machine Learning; Artificial Intelligence; Natural Language Processing △ Less",https://arxiv.orghttps://arxiv.org/abs/2305.00309,May 2023,192,Constant-depth circuits for dynamic simulations of materials on quantum computers,"Lindsay Bassman Oftelie, Roel Van Beeumen, Ed Younis, Ethan Smith, Costin Iancu and Wibe A. de Jong","Dynamic simulation of materials is a promising application for near-term quantum computers. Current algorithms for Hamiltonian simulation, however, produce circuits that grow in depth with increasing simulatio...",https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-022-00043-x,7 March 2022,
193,0.000678989392134464,193,The Emotions of the Crowd: Learning Image Sentiment from Tweets via Cross-modal Distillation,"Alessio Serra, Fabio Carrara, Maurizio Tesconi, Fabrizio Falchi","Trends and opinion mining in social media increasingly focus on novel interactions involving visual media, like images and short videos, in addition to text. In this work, we tackle the problem of visual sentiment analysis of social media images -- specifically, the prediction of image sentiment polarity. While previous work relied on manually labeled training sets, we propose an automated approach for building sentiment polarity classifiers based on a cross-modal distillation paradigm; starting from scraped multimodal (text + images) data, we train a student model on the visual modality based on the outputs of a textual teacher model that analyses the sentiment of the corresponding textual modality. We applied our method to randomly collected images crawled from Twitter over three months and produced, after automatic cleaning, a weakly-labeled dataset of $\sim$1.5 million images. Despite exploiting noisy labeled samples, our training pipeline produces classifiers showing strong generalization capabilities and outperforming the current state of the art on five manually labeled benchmarks for image sentiment polarity prediction. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.14942,April 2023,193,It’s what’s inside that counts: computer-aided tomography for evaluating the rate and extent of wood consumption by shipworms,"François Charles, Jennifer Coston-Guarini, Jean-Marc Guarini and François Lantoine",Experiments were done to investigate in situ colonization of pine wood blocks by marine wood borers at the mouth of a small mountain river in the foothills of the Eastern Pyrenees. Standardized blocks were rec...,https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/s10086-018-1716-x,4 April 2018,
194,0.000678989392134464,194,A Brief Study of Privacy-Preserving Practices (PPP) in Data Mining,"Dhinakaran D, Joe Prathap P. M","Data mining is the way toward mining fascinating patterns or information from an enormous level of the database. Data mining additionally opens another risk to privacy and data security.One of the maximum significant themes in the research fieldis privacy-preserving DM (PPDM). Along these lines, the investigation of ensuring delicate information and securing sensitive mined snippets of data without yielding the utility of the information in a dispersed domain.Extracted information from the analysis can be rules, clusters, meaningful patterns, trends or classification models. Privacy breach occur at some stage in the communication of data and aggregation of data. So far, many effective methods and techniques have been developed for privacy-preserving data mining, but yields into information loss and side effects on data utility and data mining effectiveness downgraded. In the f ocal point of consideration on the viability of Data Mining, Privacy and rightness should be improved and to lessen the expense. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.14607,April 2023,194,Applying multimodal learning analytics to examine the immediate and delayed effects of instructor scaffoldings on small groups’ collaborative programming,"Fan Ouyang, Xinyu Dai and Si Chen","Instructor scaffolding is proved to be an effective means to improve collaborative learning quality, but empirical research indicates discrepancies about the effect of instructor scaffoldings on collaborative ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00361-z,8 July 2022,
195,0.000678989392134464,195,Preserving Data Confidentiality in Association Rule Mining Using Data Share Allocator Algorithm,"D. Dhinakaran, P. M. Joe Prathap","These days, investigations of information are becoming essential for various associations all over the globe. By and large, different associations need to perform information examinations on their joined data sets. Privacy and security have become a relentless concern wherein business experts do not desire to contribute their classified transaction data. Therefore, there is a requirement to build a proficient methodology that can process the broad mixture of data and convert those data into meaningful knowledge for the user without forfeiting the security and privacy of individuals crude information. We devised two unique protocols for frequent mining itemsets in horizontally partitioned datasets while maintaining privacy. In such a scenario, data possessors outwork mining tasks on their multiparty data by preserving privacy. The proposed framework model encompasses two or more data possessors who encrypt their information and dispense their encrypted data to two or more clouds by a data share allocator algorithm. This methodology protects the data possessor raw data from other data possessors and the other clouds. To guarantee data privacy, we plan a proficient enhanced homomorphic encryption conspire. Our approach ensures privacy during communication and accumulation of data and guarantees no information or data adversity and no incidental consequences for data utility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.14605,April 2023,195,Reliability and validity of a computer-based assessment of cognitive and non-cognitive facets of problem-solving competence in the business domain,"Andreas Rausch, Jürgen Seifried, Eveline Wuttke, Kristina Kögler and Steffen Brandt","To measure higher-order outcomes of vocational education and training (VET) we developed a computer-based assessment of domain-specific problem-solving competence. In modeling problem-solving competence, we di...",https://www.springeropen.com//ervet-journal.springeropen.com/articles/10.1186/s40461-016-0035-y,19 July 2016,
196,0.000678989392134464,196,Assessing Text Mining and Technical Analyses on Forecasting Financial Time Series,Ali Lashgari,"Forecasting financial time series (FTS) is an essential field in finance and economics that anticipates market movements in financial markets. This paper investigates the accuracy of text mining and technical analyses in forecasting financial time series. It focuses on the S&P500 stock market index during the pandemic, which tracks the performance of the largest publicly traded companies in the US. The study compares two methods of forecasting the future price of the S&P500: text mining, which uses NLP techniques to extract meaningful insights from financial news, and technical analysis, which uses historical price and volume data to make predictions. The study examines the advantages and limitations of both methods and analyze their performance in predicting the S&P500. The FinBERT model outperforms other models in terms of S&P500 price prediction, as evidenced by its lower RMSE value, and has the potential to revolutionize financial analysis and prediction using financial news data. Keywords: ARIMA, BERT, FinBERT, Forecasting Financial Time Series, GARCH, LSTM, Technical Analysis, Text Mining JEL classifications: G4, C8 △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.14544,April 2023,196,Commercial competence: comparing test results of paper-and-pencil versus computer-based assessments,Julia Sangmeister,"Vocational education and training (VET) aims to enable young adults or trainees to participate in the workplace, and to promote their vocational capacities. In order to examine trainees’ competencies at the en...",https://www.springeropen.com//ervet-journal.springeropen.com/articles/10.1186/s40461-017-0047-2,27 February 2017,
197,0.000678989392134464,197,High-dimensional Clustering onto Hamiltonian Cycle,"Tianyi Huang, Shenghui Cheng, Stan Z. Li, Zhengjun Zhang","Clustering aims to group unlabelled samples based on their similarities. It has become a significant tool for the analysis of high-dimensional data. However, most of the clustering methods merely generate pseudo labels and thus are unable to simultaneously present the similarities between different clusters and outliers. This paper proposes a new framework called High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above problems. First, HCHC combines global structure with local structure in one objective function for deep clustering, improving the labels as relative probabilities, to mine the similarities between different clusters while keeping the local structure in each cluster. Then, the anchors of different clusters are sorted on the optimal Hamiltonian cycle generated by the cluster similarities and mapped on the circumference of a circle. Finally, a sample with a higher probability of a cluster will be mapped closer to the corresponding anchor. In this way, our framework allows us to appreciate three aspects visually and simultaneously - clusters (formed by samples with high probabilities), cluster similarities (represented as circular distances), and outliers (recognized as dots far away from all clusters). The experiments illustrate the superiority of HCHC. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.14531,April 2023,197,Human aspects of internet services: considering the needs of users and providers,Claudio S. Pinhanez,"This paper discusses the need and possible forms of human interfaces to Internet services, challenging the common notion that Internet services are simply computer-computer systems governed by machine protocol...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-010-0017-4,13 January 2011,
198,0.000678989392134464,198,Association Rules Mining with Auto-Encoders,"Théophile Berteloot, Richard Khoury, Audrey Durand","Association rule mining is one of the most studied research fields of data mining, with applications ranging from grocery basket problems to explainable classification systems. Classical association rule mining algorithms have several limitations, especially with regards to their high execution times and number of rules produced. Over the past decade, neural network solutions have been used to solve various optimization problems, such as classification, regression or clustering. However there are still no efficient way association rules using neural networks. In this paper, we present an auto-encoder solution to mine association rule called ARM-AE. We compare our algorithm to FP-Growth and NSGAII on three categorical datasets, and show that our algorithm discovers high support and confidence rule set and has a better execution time than classical methods while preserving the quality of the rule set produced. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.13717,April 2023,198,Volume sculpting based on geometric tools,"Rafael Huff, Roberto Silveira da Rosa, Luciana Nedel and Carla Maria Dal Sasso Freitas","The analysis of volumetric datasets is the main concern in many areas ranging from geophysics to biomedical sciences. The direct visualization of these data plays an important role in this scenario, and in spi...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194498,June 2009,
199,0.000678989392134464,199,Onset of an outline map to get a hold on the wildwood of clustering methods,"Iven Van Mechelen, Christian Hennig, Henk A. L. Kiers","The domain of cluster analysis is a meeting point for a very rich multidisciplinary encounter, with cluster-analytic methods being studied and developed in discrete mathematics, numerical analysis, statistics, data analysis and data science, and computer science (including machine learning, data mining, and knowledge discovery), to name but a few. The other side of the coin, however, is that the domain suffers from a major accessibility problem as well as from the fact that it is rife with division across many pretty isolated islands. As a way out, the present paper offers an outline map for the clustering domain as a whole, which takes the form of an overarching conceptual framework and a common language. With this framework we wish to contribute to structuring the domain, to characterizing methods that have often been developed and studied in quite different contexts, to identifying links between them, and to introducing a frame of reference for optimally setting up cluster analyses in data-analytic practice. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.13406,April 2023,199,Editorial,"Ngoc-Thanh Nguyen, Manh-Hung Nguyen and Dac-Hien Cao",Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-014-0036-3,7 December 2014,
200,0.000678989392134464,200,What Causes Exceptions in Machine Learning Applications? Mining Machine Learning-Related Stack Traces on Stack Overflow,"Amin Ghadesi, Maxime Lamothe, Heng Li","Machine learning (ML), including deep learning, has recently gained tremendous popularity in a wide range of applications. However, like traditional software, ML applications are not immune to the bugs that result from programming errors. Explicit programming errors usually manifest through error messages and stack traces. These stack traces describe the chain of function calls that lead to an anomalous situation, or exception. Indeed, these exceptions may cross the entire software stack (including applications and libraries). Thus, studying the patterns in stack traces can help practitioners and researchers understand the causes of exceptions in ML applications and the challenges faced by ML developers. To that end, we mine Stack Overflow (SO) and study 11,449 stack traces related to seven popular Python ML libraries. First, we observe that ML questions that contain stack traces gain more popularity than questions without stack traces; however, they are less likely to get accepted answers. Second, we observe that recurrent patterns exists in ML stack traces, even across different ML libraries, with a small portion of patterns covering many stack traces. Third, we derive five high-level categories and 25 low-level types from the stack trace patterns: most patterns are related to python basic syntax, model training, parallelization, data transformation, and subprocess invocation. Furthermore, the patterns related to subprocess invocation, external module execution, and remote API call are among the least likely to get accepted answers on SO. Our findings provide insights for researchers, ML library providers, and ML application developers to improve the quality of ML libraries and their applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.12857,April 2023,200,From the Editor,Ngoc-Thanh Nguyen,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-016-0090-0,29 November 2016,
201,0.000678989392134464,201,Beyond the Prior Forgery Knowledge: Mining Critical Clues for General Face Forgery Detection,"Anwei Luo, Chenqi Kong, Jiwu Huang, Yongjian Hu, Xiangui Kang, Alex C. Kot","Face forgery detection is essential in combating malicious digital face attacks. Previous methods mainly rely on prior expert knowledge to capture specific forgery clues, such as noise patterns, blending boundaries, and frequency artifacts. However, these methods tend to get trapped in local optima, resulting in limited robustness and generalization capability. To address these issues, we propose a novel Critical Forgery Mining (CFM) framework, which can be flexibly assembled with various backbones to boost their generalization and robustness performance. Specifically, we first build a fine-grained triplet and suppress specific forgery traces through prior knowledge-agnostic data augmentation. Subsequently, we propose a fine-grained relation learning prototype to mine critical information in forgeries through instance and local similarity-aware losses. Moreover, we design a novel progressive learning controller to guide the model to focus on principal feature components, enabling it to learn critical forgery features in a coarse-to-fine manner. The proposed method achieves state-of-the-art forgery detection performance under various challenging evaluation settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.12489,April 2023,201,From the Editor,Ngoc Thanh Nguyen,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-018-0106-z,20 January 2018,
202,0.000678989392134464,202,Analyzing categorical time series with the R package ctsfeatures,"Ángel López Oriona, José Antonio Vilar Fernández","Time series data are ubiquitous nowadays. Whereas most of the literature on the topic deals with real-valued time series, categorical time series have received much less attention. However, the development of data mining techniques for this kind of data has substantially increased in recent years. The R package ctsfeatures offers users a set of useful tools for analyzing categorical time series. In particular, several functions allowing the extraction of well-known statistical features and the construction of illustrative graphs describing underlying temporal patterns are provided in the package. The output of some functions can be employed to perform traditional machine learning tasks including clustering, classification and outlier detection. The package also includes two datasets of biological sequences introduced in the literature for clustering purposes, as well as three interesting synthetic databases. In this work, the main characteristics of the package are described and its use is illustrated through various examples. Practitioners from a wide variety of fields could benefit from the valuable tools provided by ctsfeatures. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.12332,April 2023,202,Computer-Supported Meta-reflective Learning Model via mathematical word problem learning for training metacognition,"Tama Duangnamol, Thepchai Supnithi, Gun Srijuntongsiri and Mitsuru Ikeda","To become a self-regulated learner, one needs to have a skill required to induce himself to comprehend their own cognition. In this paper, we provided a definition of Seed skill to become a self-regulated lear...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-018-0080-1,25 September 2018,
203,0.000678989392134464,203,USTEP: Structuration des logs en flux gr{â}ce {à} un arbre de recherche {é}volutif,"Arthur Vervaet, Raja Chiky, Mar Callau-Zori","Logs record valuable system information at runtime. They are widely used by data-driven approaches for development and monitoring purposes. Parsing log messages to structure their format is a classic preliminary step for log-mining tasks. As they appear upstream, parsing operations can become a processing time bottleneck for downstream applications. The quality of parsing also has a direct influence on their efficiency. Here, we propose USTEP, an online log parsing method based on an evolving tree structure. Evaluation results on a wide panel of datasets coming from different real-world systems demonstrate USTEP superiority in terms of both effectiveness and robustness when compared to other online methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.12331,April 2023,203,Educational crowdsourcing to support the learning of computer programming,"Dhiya Al-Jumeily, Abir Hussain, Mohammed Alghamdi, Chelsea Dobbins and Jan Lunn",Recent advancements in technology have enabled a shift to occur in teaching and learning. We are living in a connected world where physical boundaries of attending an institution to gain an education no longer...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-015-0011-3,21 July 2015,
204,0.000678989392134464,204,Enriching Source Code with Contextual Data for Code Completion Models: An Empirical Study,"Tim van Dam, Maliheh Izadi, Arie van Deursen","Transformer-based pre-trained models have recently achieved great results in solving many software engineering tasks including automatic code completion which is a staple in a developer's toolkit. While many have striven to improve the code-understanding abilities of such models, the opposite -- making the code easier to understand -- has not been properly investigated. In this study, we aim to answer whether making code easier to understand through using contextual data improves the performance of pre-trained code language models for the task of code completion. We consider type annotations and comments as two common forms of additional contextual information that often help developers understand code better. For the experiments, we study code completion in two granularity levels; token and line completion and take three recent and large-scale language models for source code: UniXcoder, CodeGPT, and InCoder with five evaluation metrics. Finally, we perform the Wilcoxon Signed Rank test to gauge significance and measure the effect size. Contrary to our expectations, all models perform better if type annotations are removed (albeit the effect sizes are small). For comments, we find that the models perform better in the presence of multi-line comments (again with small effect sizes). Based on our observations, we recommend making proper design choices when training, fine-tuning, or simply selecting such models given the intended data and application. Better evaluations and multi-modal techniques can also be further investigated to improve the practicality and accuracy of auto-completions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.12269,April 2023,204,Computer-aided identification of lung cancer inhibitors through homology modeling and virtual screening,Aboubakr Haredi Abdelmonsef,Lung cancer is the most often event cancer around the world and the first leading cause of cancer death in human beings. Rab39a protein is implicated in vesicular trafficking and fusion of phagosomes with lyso...,https://www.springeropen.com//jmhg.springeropen.com/articles/10.1186/s43042-019-0008-3,15 August 2019,
205,0.000678989392134464,205,Towards Top-$K$ Non-Overlapping Sequential Patterns,"Zefeng Chen, Wensheng Gan, Gengsen Huang, Yan Li, Zhenlian Qi","Sequential pattern mining (SPM) has excellent prospects and application spaces and has been widely used in different fields. The non-overlapping SPM, as one of the data mining techniques, has been used to discover patterns that have requirements for gap constraints in some specific mining tasks, such as bio-data mining. And for the non-overlapping sequential patterns with gap constraints, the Nettree structure has been proposed to efficiently compute the support of the patterns. For pattern mining, users usually need to consider the threshold of minimum support (\textit{minsup}). This is especially difficult in the case of large databases. Although some existing algorithms can mine the top-$k$ patterns, they are approximate algorithms with fixed lengths. In this paper, a precise algorithm for mining \underline{T}op-$k$ \underline{N}on-\underline{O}verlapping \underline{S}equential \underline{P}atterns (TNOSP) is proposed. The top-$k$ solution of SPM is an effective way to discover the most frequent non-overlapping sequential patterns without having to set the \textit{minsup}. As a novel pattern mining algorithm, TNOSP can precisely search the top-$k$ patterns of non-overlapping sequences with different gap constraints. We further propose a pruning strategy named \underline{Q}ueue \underline{M}eta \underline{S}et \underline{P}runing (QMSP) to improve TNOSP's performance. TNOSP can reduce redundancy in non-overlapping sequential mining and has better performance in mining precise non-overlapping sequential patterns. The experimental results and comparisons on several datasets have shown that TNOSP outperformed the existing algorithms in terms of precision, efficiency, and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.11947,April 2023,205,University academics’ perceptions regarding the future use of telepresence robots to enhance virtual transnational education: an exploratory investigation in a developing country,Hanaa Ouda Khadri,"There is a growing interest in employing telepresence robots in different educational contexts due to their great potentials to enhance and improve educational experiences for remote learners. However, there i...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00173-8,2 November 2021,
206,0.000678989392134464,206,Diffusion Model for GPS Trajectory Generation,"Yuanshao Zhu, Yongchao Ye, Xiangyu Zhao, James J. Q. Yu","With the deployment of GPS-enabled devices and data acquisition technology, the massively generated GPS trajectory data provide a core support for advancing spatial-temporal data mining research. Nonetheless, GPS trajectories comprise personal geo-location information, rendering inevitable privacy concerns on plain data. One promising solution to this problem is trajectory generation, replacing the original data with the generated privacy-free ones. However, owing to the complex and stochastic behavior of human activities, generating high-quality trajectories is still in its infancy. To achieve the objective, we propose a diffusion-based trajectory generation (Diff-Traj) framework, effectively integrating the generation capability of the diffusion model and learning from the spatial-temporal features of trajectories. Specifically, we gradually convert real trajectories to noise through a forward trajectory noising process. Then, Diff-Traj reconstructs forged trajectories from the noise by a reverse trajectory denoising process. In addition, we design a trajectory UNet (Traj-UNet) structure to extract trajectory features for noise level prediction during the reverse process. Experiments on two real-world datasets show that Diff-Traj can be intuitively applied to generate high-quality trajectories while retaining the original distribution. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.11582,April 2023,206,Participation and performance on paper- and computer-based low-stakes assessments,"Jayson M. Nissen, Manher Jariwala, Eleanor W. Close and Ben Van Dusen","High-stakes assessments, such the Graduate Records Examination, have transitioned from paper to computer administration. Low-stakes research-based assessments (RBAs), such as the Force Concept Inventory, have ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0117-4,7 May 2018,
207,0.000678989392134464,207,Mining Privacy-Preserving Association Rules based on Parallel Processing in Cloud Computing,"Dhinakaran D, Joe Prathap P. M, Selvaraj D, Arul Kumar D, Murugeshwari B","With the onset of the Information Era and the rapid growth of information technology, ample space for processing and extracting data has opened up. However, privacy concerns may stifle expansion throughout this area. The challenge of reliable mining techniques when transactions disperse across sources is addressed in this study. This work looks at the prospect of creating a new set of three algorithms that can obtain maximum privacy, data utility, and time savings while doing so. This paper proposes a unique double encryption and Transaction Splitter approach to alter the database to optimize the data utility and confidentiality tradeoff in the preparation phase. This paper presents a customized apriori approach for the mining process, which does not examine the entire database to estimate the support for each attribute. Existing distributed data solutions have a high encryption complexity and an insufficient specification of many participants' properties. Proposed solutions provide increased privacy protection against a variety of attack models. Furthermore, in terms of communication cycles and processing complexity, it is much simpler and quicker. Proposed work tests on top of a realworld transaction database demonstrate that the aim of the proposed method is realistic. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10836,April 2023,207,ICT Engagement: a new construct and its assessment in PISA 2015,Olga Kunina-Habenicht and Frank Goldhammer,"As a relevant cognitive-motivational aspect of ICT literacy, a new constructICT Engagementis theoretically based on self-determination theory and involves the factors ICT interest, Perceived ICT competence, Per...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00084-z,17 April 2020,
208,0.000678989392134464,208,Graph based Label Enhancement for Multi-instance Multi-label learning,"Houcheng Su, Jintao Huang, Daixian Liu, Rui Yan, Jiao Li, Chi-man Vong","Multi-instance multi-label (MIML) learning is widely applicated in numerous domains, such as the image classification where one image contains multiple instances correlated with multiple logic labels simultaneously. The related labels in existing MIML are all assumed as logical labels with equal significance. However, in practical applications in MIML, significance of each label for multiple instances per bag (such as an image) is significant different. Ignoring labeling significance will greatly lose the semantic information of the object, so that MIML is not applicable in complex scenes with a poor learning performance. To this end, this paper proposed a novel MIML framework based on graph label enhancement, namely GLEMIML, to improve the classification performance of MIML by leveraging label significance. GLEMIML first recognizes the correlations among instances by establishing the graph and then migrates the implicit information mined from the feature space to the label space via nonlinear mapping, thus recovering the label significance. Finally, GLEMIML is trained on the enhanced data through matching and interaction mechanisms. GLEMIML (AvgRank: 1.44) can effectively improve the performance of MIML by mining the label distribution mechanism and show better results than the SOTA method (AvgRank: 2.92) on multiple benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10705,April 2023,208,VQE method: a short survey and recent developments,"Dmitry A. Fedorov, Bo Peng, Niranjan Govind and Yuri Alexeev",The variational quantum eigensolver (VQE) is a method that uses a hybrid quantum-classical computational approach to find eigenvalues of a Hamiltonian. VQE has been proposed as an alternative to fully quantum ...,https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-021-00032-6,6 January 2022,
209,0.000678989392134464,209,Causal Analysis of Customer Churn Using Deep Learning,"David Hason Rudd, Huan Huo, Guandong Xu","Customer churn describes terminating a relationship with a business or reducing customer engagement over a specific period. Two main business marketing strategies play vital roles to increase market share dollar-value: gaining new and preserving existing customers. Customer acquisition cost can be five to six times that for customer retention, hence investing in customers with churn risk is smart. Causal analysis of the churn model can predict whether a customer will churn in the foreseeable future and assist enterprises to identify effects and possible causes for churn and subsequently use that knowledge to apply tailored incentives. This paper proposes a framework using a deep feedforward neural network for classification accompanied by a sequential pattern mining method on high-dimensional sparse data. We also propose a causal Bayesian network to predict cause probabilities that lead to customer churn. Evaluation metrics on test data confirm the XGBoost and our deep learning model outperformed previous techniques. Experimental analysis confirms that some independent causal variables representing the level of super guarantee contribution, account growth, and customer tenure were identified as confounding factors for customer churn with a high degree of belief. This paper provides a real-world customer churn analysis from current status inference to future directions in local superannuation funds. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10604,April 2023,209,The Catchment Feature Model: A Device for Multimodal Fusion and a Bridge between Signal and Sense,Francis Quek,"The catchment feature model addresses two questions in the field of multimodal interaction: how we bridge video and audio processing with the realities of human multimodal communication, and how information fr...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704405101,18 September 2004,
210,0.000678989392134464,210,Data Mining of Telematics Data: Unveiling the Hidden Patterns in Driving Behaviour,"Ian Weng Chan, Spark C. Tseung, Andrei L. Badescu, X. Sheldon Lin","With the advancement in technology, telematics data which capture vehicle movements information are becoming available to more insurers. As these data capture the actual driving behaviour, they are expected to improve our understanding of driving risk and facilitate more accurate auto-insurance ratemaking. In this paper, we analyze an auto-insurance dataset with telematics data collected from a major European insurer. Through a detailed discussion of the telematics data structure and related data quality issues, we elaborate on practical challenges in processing and incorporating telematics information in loss modelling and ratemaking. Then, with an exploratory data analysis, we demonstrate the existence of heterogeneity in individual driving behaviour, even within the groups of policyholders with and without claims, which supports the study of telematics data. Our regression analysis reiterates the importance of telematics data in claims modelling; in particular, we propose a speed transition matrix that describes discretely recorded speed time series and produces statistically significant predictors for claim counts. We conclude that large speed transitions, together with higher maximum speed attained, nighttime driving and increased harsh braking, are associated with increased claim counts. Moreover, we empirically illustrate the learning effects in driving behaviour: we show that both severe harsh events detected at a high threshold and expected claim counts are not directly proportional with driving time or distance, but they increase at a decreasing rate. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10591,April 2023,210,Editorial,Madjid Tavana,"Decision making is a balancing act on a three-legged stool of insight, analysis, and action. If we cannot keep the three legs even, we wobble. And sitting on the stool is not even enough. We have to juggle kno...",https://www.springeropen.com//decisionanalyticsjournal.springeropen.com/articles/10.1186/2193-8636-1-1,19 February 2014,
211,0.000678989392134464,211,A Reference Model for Collaborative Business Intelligence Virtual Assistants,"Olga Cherednichenko, Fahad Muhammad, Jérôme Darmont, Cécile Favre","Collaborative Business Analysis (CBA) is a methodology that involves bringing together different stakeholders, including business users, analysts, and technical specialists, to collaboratively analyze data and gain insights into business operations. The primary objective of CBA is to encourage knowledge sharing and collaboration between the different groups involved in business analysis, as this can lead to a more comprehensive understanding of the data and better decision-making. CBA typically involves a range of activities, including data gathering and analysis, brainstorming, problem-solving, decision-making and knowledge sharing. These activities may take place through various channels, such as in-person meetings, virtual collaboration tools or online forums. This paper deals with virtual collaboration tools as an important part of Business Intelligence (BI) platform. Collaborative Business Intelligence (CBI) tools are becoming more user-friendly, accessible, and flexible, allowing users to customize their experience and adapt to their specific needs. The goal of a virtual assistant is to make data exploration more accessible to a wider range of users and to reduce the time and effort required for data analysis. It describes the unified business intelligence semantic model, coupled with a data warehouse and collaborative unit to employ data mining technology. Moreover, we propose a virtual assistant for CBI and a reference model of virtual tools for CBI, which consists of three components: conversational, data exploration and recommendation agents. We believe that the allocation of these three functional tasks allows you to structure the CBI issue and apply relevant and productive models for human-like dialogue, text-to-command transferring, and recommendations simultaneously. The complex approach based on these three points gives the basis for virtual tool for collaboration. CBI encourages people, processes, and technology to enable everyone sharing and leveraging collective expertise, knowledge and data to gain valuable insights for making better decisions. This allows to respond more quickly and effectively to changes in the market or internal operations and improve the progress. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10556,April 2023,211,Being Extreme in the classroom: Experiences teaching XP,"Alfredo Goldman, Fabio Kon, Paulo J. S. Silva and Joseph W. Yoder",Agile Methods propose a new way of looking at software development that questions many of the beliefs of conventional Software Engineering. Agile methods such as Extreme Programming (XP) have been very effecti...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192356,June 2004,
212,0.000678989392134464,212,Learning Representative Trajectories of Dynamical Systems via Domain-Adaptive Imitation,"Edgardo Solano-Carrillo, Jannis Stoppe","Domain-adaptive trajectory imitation is a skill that some predators learn for survival, by mapping dynamic information from one domain (their speed and steering direction) to a different domain (current position of the moving prey). An intelligent agent with this skill could be exploited for a diversity of tasks, including the recognition of abnormal motion in traffic once it has learned to imitate representative trajectories. Towards this direction, we propose DATI, a deep reinforcement learning agent designed for domain-adaptive trajectory imitation using a cycle-consistent generative adversarial method. Our experiments on a variety of synthetic families of reference trajectories show that DATI outperforms baseline methods for imitation learning and optimal control in this setting, keeping the same per-task hyperparameters. Its generalization to a real-world scenario is shown through the discovery of abnormal motion patterns in maritime traffic, opening the door for the use of deep reinforcement learning methods for spatially-unconstrained trajectory data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10260,April 2023,212,Editorial,Ngoc-Thanh Nguyen,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-016-0057-1,26 January 2016,
213,0.000678989392134464,213,Weakly Supervised Detection of Baby Cry,"Weijun Tan, Qi Yao, Jingfeng Liu","Detection of baby cries is an important part of baby monitoring and health care. Almost all existing methods use supervised SVM, CNN, or their varieties. In this work, we propose to use weakly supervised anomaly detection to detect a baby cry. In this weak supervision, we only need weak annotation if there is a cry in an audio file. We design a data mining technique using the pre-trained VGGish feature extractor and an anomaly detection network on long untrimmed audio files. The obtained datasets are used to train a simple CNN feature network for cry/non-cry classification. This CNN is then used as a feature extractor in an anomaly detection framework to achieve better cry detection performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.10001,April 2023,213,Ontological specification of quality of chronic disease data in EHRs to support decision analytics: a realist review,"Alireza Rahimi, Siaw-Teng Liaw, Pradeep Ray, Jane Taggart and Hairong Yu","This systematic review examined the current state of conceptualization and specification of data quality and the role of ontology based approaches to develop data quality based on ""fitness for purpose"" within ...",https://www.springeropen.com//decisionanalyticsjournal.springeropen.com/articles/10.1186/2193-8636-1-5,19 February 2014,
214,0.000678989392134464,214,Data as voters: instance selection using approval-based multi-winner voting,"Luis Sánchez-Fernández, Jesús A. Fisteus, Rafael López-Zaragoza","We present a novel approach to the instance selection problem in machine learning (or data mining). Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. Each instance in the training set (acting as a voter) approves of the instances (playing the role of candidates) belonging to its local set (except itself), a concept already existing in the literature. We then select the election winners using a representative voting rule, and such winners are the data instances kept in the reduced training set. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.09995,April 2023,214,A review of previous studies on ESL/EFL learners’ interactional feedback exchanges in face-to-face and computer-assisted peer review of writing,"Murad Abdu Saeed, Kamila Ghazali and Musheer Abdulwahid Aljaberi",This paper is a review of previous studies on learners’ interactional feedback exchanges in face-to-face peer review (FFPR) and computer-assisted peer review (CAPR) of English as Second/Foreign Language (ESL/E...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-017-0084-8,19 January 2018,
215,0.000678989392134464,215,Advances on Concept Drift Detection in Regression Tasks using Social Networks Theory,"Jean Paul Barddal, Heitor Murilo Gomes, Fabrício Enembreck","Mining data streams is one of the main studies in machine learning area due to its application in many knowledge areas. One of the major challenges on mining data streams is concept drift, which requires the learner to discard the current concept and adapt to a new one. Ensemble-based drift detection algorithms have been used successfully to the classification task but usually maintain a fixed size ensemble of learners running the risk of needlessly spending processing time and memory. In this paper we present improvements to the Scale-free Network Regressor (SFNR), a dynamic ensemble-based method for regression that employs social networks theory. In order to detect concept drifts SFNR uses the Adaptive Window (ADWIN) algorithm. Results show improvements in accuracy, especially in concept drift situations and better performance compared to other state-of-the-art algorithms in both real and synthetic data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.09788,April 2023,215,Numerical experiments and compartment fires,Nils Johansson,Fires are complex and it is hard to derive relationships from theory in fire science. Full-scale and small-scale experiments have been used with great success in order to increase the understanding of fire che...,https://www.springeropen.com//firesciencereviews.springeropen.com/articles/10.1186/s40038-014-0002-2,19 September 2014,
216,0.00673579331412596,216,NetGPT: Generative Pretrained Transformer for Network Traffic,"Xuying Meng, Chungang Lin, Yequan Wang, Yujun Zhang","All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks. To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unified text inputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrency mining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.09513,April 2023,216,Letter from the guest editors,"Eduardo Santana de Almeida, Silvio Romero de Lemos Meira and Bill Frakes",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192547,March 2008,
217,0.000678989392134464,217,Approximate Nearest Neighbour Phrase Mining for Contextual Speech Recognition,"Maurits Bleeker, Pawel Swietojanski, Stefan Braun, Xiaodan Zhuang","This paper presents an extension to train end-to-end Context-Aware Transformer Transducer ( CATT ) models by using a simple, yet efficient method of mining hard negative phrases from the latent space of the context encoder. During training, given a reference query, we mine a number of similar phrases using approximate nearest neighbour search. These sampled phrases are then used as negative examples in the context list alongside random and ground truth contextual information. By including approximate nearest neighbour phrases (ANN-P) in the context list, we encourage the learned representation to disambiguate between similar, but not identical, biasing phrases. This improves biasing accuracy when there are several similar phrases in the biasing inventory. We carry out experiments in a large-scale data regime obtaining up to 7% relative word error rate reductions for the contextual portion of test data. We also extend and evaluate CATT approach in streaming applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.08862,April 2023,217,Cybersecurity data science: an overview from machine learning perspective,"Iqbal H. Sarker, A. S. M. Kayes, Shahriar Badsha, Hamed Alqahtani, Paul Watters and Alex Ng","In a computing context, cybersecurity is undergoing massive shifts in technology and its operations in recent days, and data science is driving the change. Extractingsecurity incident patternsor insights from c...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00318-5,1 July 2020,
218,0.000678989392134464,218,GlobalMind: Global Multi-head Interactive Self-attention Network for Hyperspectral Change Detection,"Meiqi Hu, Chen Wu, Liangpei Zhang","High spectral resolution imagery of the Earth's surface enables users to monitor changes over time in fine-grained scale, playing an increasingly important role in agriculture, defense, and emergency response. However, most current algorithms are still confined to describing local features and fail to incorporate a global perspective, which limits their ability to capture interactions between global features, thus usually resulting in incomplete change regions. In this paper, we propose a Global Multi-head INteractive self-attention change Detection network (GlobalMind) to explore the implicit correlation between different surface objects and variant land cover transformations, acquiring a comprehensive understanding of the data and accurate change detection result. Firstly, a simple but effective Global Axial Segmentation (GAS) strategy is designed to expand the self-attention computation along the row space or column space of hyperspectral images, allowing the global connection with high efficiency. Secondly, with GAS, the global spatial multi-head interactive self-attention (Global-M) module is crafted to mine the abundant spatial-spectral feature involving potential correlations between the ground objects from the entire rich and complex hyperspectral space. Moreover, to acquire the accurate and complete cross-temporal changes, we devise a global temporal interactive multi-head self-attention (GlobalD) module which incorporates the relevance and variation of bi-temporal spatial-spectral features, deriving the integrate potential same kind of changes in the local and global range with the combination of GAS. We perform extensive experiments on five mostly used hyperspectral datasets, and our method outperforms the state-of-the-art algorithms with high accuracy and efficiency. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.08687,April 2023,218,A Review of the Evolution of Vision-Based Motion Analysis and the Integration of Advanced Computer Vision Methods Towards Developing a Markerless System,"Steffi L. Colyer, Murray Evans, Darren P. Cosker and Aki I. T. Salo","The study of human movement within sports biomechanics and rehabilitation settings has made considerable progress over recent decades. However, developing a motion analysis system that collects accurate kinema...",https://www.springeropen.com//sportsmedicine-open.springeropen.com/articles/10.1186/s40798-018-0139-y,5 June 2018,
219,0.000678989392134464,219,Attentive Q-Matrix Learning for Knowledge Tracing,"Zhongfeng Jia, Wei Su, Jiamin Liu, Wenli Yue","As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where no predefined skill tags are available without sacrificing its performance. With a novel hybrid embedding method based on the q-matrix and Rasch model, QAKT is capable of modeling problems hierarchically and learning the q-matrix efficiently based on students' sequences. Meanwhile, the architecture of QAKT ensures that it is friendly to questions associated with multiple skills and has outstanding interpretability. After conducting experiments on a variety of open datasets, we empirically validated that our model shows similar or even better performance than state-of-the-art KT methods. Results of further experiments suggest that the q-matrix learned by QAKT is highly model-agnostic and more information-sufficient than the one labeled by human experts, which could help with the data mining tasks in existing ITSs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.08168,April 2023,219,Everything in moderation: ICT and reading performance of Dutch 15-year-olds,"Joyce Gubbels, Nicole M. Swart and Margriet A. Groen",Previous research on the relationship between students’ home and school Information and Communication Technology (ICT) resources and academic performance has shown ambiguous results. The availability of ICT re...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-0079-0,30 January 2020,
220,0.000678989392134464,220,MMANet: Margin-aware Distillation and Modality-aware Regularization for Incomplete Multimodal Learning,"Shicai Wei, Yang Luo, Chunbo Luo","Multimodal learning has shown great potentials in numerous scenes and attracts increasing interest recently. However, it often encounters the problem of missing modality data and thus suffers severe performance degradation in practice. To this end, we propose a general framework called MMANet to assist incomplete multimodal learning. It consists of three components: the deployment network used for inference, the teacher network transferring comprehensive multimodal information to the deployment network, and the regularization network guiding the deployment network to balance weak modality combinations. Specifically, we propose a novel margin-aware distillation (MAD) to assist the information transfer by weighing the sample contribution with the classification uncertainty. This encourages the deployment network to focus on the samples near decision boundaries and acquire the refined inter-class margin. Besides, we design a modality-aware regularization (MAR) algorithm to mine the weak modality combinations and guide the regularization network to calculate prediction loss for them. This forces the deployment network to improve its representation ability for the weak modality combinations adaptively. Finally, extensive experiments on multimodal classification and segmentation tasks demonstrate that our MMANet outperforms the state-of-the-art significantly. Code is available at: https://github.com/shicaiwei123/MMANet △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.08028,April 2023,220,Preservice science teachers’ emerging pedagogy of mobile game integration: a tale of two cohorts improvement study,"Phattaraporn Pondee, Patcharin Panjaburee and Niwat Srisawasdi","In the context of the current teacher education program in Thailand, Technology Pedagogical and Content Knowledge (TPACK) framework is formally recognized as essential qualities of knowledge for a highly quali...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-021-00152-0,25 June 2021,
221,0.000678989392134464,221,Automated Self-Admitted Technical Debt Tracking at Commit-Level: A Language-independent Approach,"Mohammad Sadegh Sheikhaei, Yuan Tian","Software and systems traceability is essential for downstream tasks such as data-driven software analysis and intelligent tool development. However, despite the increasing attention to mining and understanding technical debt in software systems, specific tools for supporting the track of technical debts are rarely available. In this work, we propose the first programming language-independent tracking tool for self-admitted technical debt (SATD) -- a sub-optimal solution that is explicitly annotated by developers in software systems. Our approach takes a git repository as input and returns a list of SATDs with their evolution actions (created, deleted, updated) at the commit-level. Our approach also returns a line number indicating the latest starting position of the corresponding SATD in the system. Our SATD tracking approach first identifies an initial set of raw SATDs (which only have created and deleted actions) by detecting and tracking SATDs in commits' hunks, leveraging a state-of-the-art language-independent SATD detection approach. Then it calculates a context-based matching score between pairs of deleted and created raw SATDs in the same commits to identify SATD update actions. The results of our preliminary study on Apache Tomcat and Apache Ant show that our tracking tool can achieve a F1 score of 92.8% and 96.7% respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.07829,April 2023,221,Diogene-CT: tools and methodologies for teaching and learning coding,"Giansalvatore Mecca, Donatello Santoro, Nazzareno Sileno and Enzo Veltri","Computational thinking is the capacity of undertaking a problem-solving process in various disciplines (including STEM, i.e. science, technology, engineering and mathematics) using distinctive techniques that ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00246-1,5 March 2021,
222,0.000678989392134464,222,EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text,"Rafsan Ahmed, Petter Berntsson, Alexander Skafte, Salma Kazemi Rashed, Marcus Klang, Adam Barvesten, Ola Olde, William Lindholm, Antton Lamarca Arrizabalaga, Pierre Nugues, Sonja Aits","Medical research generates a large number of publications with the PubMed database already containing >35 million research articles. Integration of the knowledge scattered across this large body of literature could provide key insights into physiological mechanisms and disease processes leading to novel medical interventions. However, it is a great challenge for researchers to utilize this information in full since the scale and complexity of the data greatly surpasses human processing abilities. This becomes especially problematic in cases of extreme urgency like the COVID-19 pandemic. Automated text mining can help extract and connect information from the large body of medical research articles. The first step in text mining is typically the identification of specific classes of keywords (e.g., all protein or disease names), so called Named Entity Recognition (NER). Here we present an end-to-end pipeline for NER of typical entities found in medical research articles, including diseases, cells, chemicals, genes/proteins, and species. The pipeline can access and process large medical research article collections (PubMed, CORD-19) or raw text and incorporates a series of deep learning models fine-tuned on the HUNER corpora collection. In addition, the pipeline can perform dictionary-based NER related to COVID-19 and other medical topics. Users can also load their own NER models and dictionaries to include additional entities. The output consists of publication-ready ranked lists and graphs of detected entities and files containing the annotated texts. An associated script allows rapid inspection of the results for specific entities of interest. As model use cases, the pipeline was deployed on two collections of autophagy-related abstracts from PubMed and on the CORD19 dataset, a collection of 764 398 research article abstracts related to COVID-19. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.07805,April 2023,222,Social prediction: a new research paradigm based on machine learning,"Yunsong Chen, Xiaogang Wu, Anning Hu, Guangye He and Guodong Ju","Sociology is a science concerned with both the interpretive understanding of social action and the corresponding causal explanation, process, and result. A causal explanation should be the foundation of predic...",https://www.springeropen.com//journalofchinesesociology.springeropen.com/articles/10.1186/s40711-021-00152-z,1 September 2021,
223,0.000678989392134464,223,A Survey of Access Control Misconfiguration Detection Techniques,Bingyu Shen,"Access control mechanisms have been adopted in many real-world systems to control resource sharing for the principals in the system. An error in the access control policy (misconfiguration) can easily cause severe data leakage and system exploitation. Researchers have developed several methodologies to detect the access control misconfigurations through data mining, testing, and verification for various applications. This survey will study the line of works to detect access control misconfigurations and discuss some future research directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.07704,April 2023,223,Quality aware software product line engineering,"Leire Etxeberria, Goiuria Sagardui and Lorea Belategi","Meeting and managing quality requirements such as performance, security… in a reuse context (software product line…) has a problematic that it is not found in single-systems. In this paper, an overview of aspe...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192552,March 2008,
224,0.00514534320972017,224,USNID: A Framework for Unsupervised and Semi-supervised New Intent Discovery,"Hanlei Zhang, Hua Xu, Xin Wang, Fei Long, Kai Gao","New intent discovery is of great value to natural language processing, allowing for a better understanding of user needs and providing friendly services. However, most existing methods struggle to capture the complicated semantics of discrete text representations when limited or no prior knowledge of labeled data is available. To tackle this problem, we propose a novel framework called USNID for unsupervised and semi-supervised new intent discovery, which has three key technologies. First, it takes full use of unsupervised or semi-supervised data to mine shallow semantic similarity relations and provide well-initialized representations for clustering. Second, it designs a centroid-guided clustering mechanism to address the issue of cluster allocation inconsistency and provide high-quality self-supervised targets for representation learning. Third, it captures high-level semantics in unsupervised or semi-supervised data to discover fine-grained intent-wise clusters by optimizing both cluster-level and instance-level objectives. We also propose an effective method for estimating the cluster number in open-world scenarios without knowing the number of new intents beforehand. USNID performs exceptionally well on several intent benchmark datasets, achieving new state-of-the-art results in unsupervised and semi-supervised new intent discovery and demonstrating robust performance with different cluster numbers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.07699,April 2023,224,Student test-taking effort in low-stakes assessments: evidence from the English version of the PISA 2015 science test,Elodie Pools and Christian Monseur,"The idea of using low-stakes assessment results is often mentioned when designing educational system reforms. However, when tests have no consequences for the students, test takers may not make enough effort w...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-021-00104-6,6 May 2021,
225,0.000678989392134464,225,Instance-level Few-shot Learning with Class Hierarchy Mining,"Anh-Khoa Nguyen Vu, Thanh-Toan Do, Nhat-Duy Nguyen, Vinh-Tiep Nguyen, Thanh Duc Ngo, Tam V. Nguyen","Few-shot learning is proposed to tackle the problem of scarce training data in novel classes. However, prior works in instance-level few-shot learning have paid less attention to effectively utilizing the relationship between categories. In this paper, we exploit the hierarchical information to leverage discriminative and relevant features of base classes to effectively classify novel objects. These features are extracted from abundant data of base classes, which could be utilized to reasonably describe classes with scarce data. Specifically, we propose a novel superclass approach that automatically creates a hierarchy considering base and novel classes as fine-grained classes for few-shot instance segmentation (FSIS). Based on the hierarchical information, we design a novel framework called Soft Multiple Superclass (SMS) to extract relevant features or characteristics of classes in the same superclass. A new class assigned to the superclass is easier to classify by leveraging these relevant features. Besides, in order to effectively train the hierarchy-based-detector in FSIS, we apply the label refinement to further describe the associations between fine-grained classes. The extensive experiments demonstrate the effectiveness of our method on FSIS benchmarks. Code is available online. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.07459,April 2023,225,Young people’s tech identity performances: why materiality matters,"Spela Godec, Uma Patel, Louise Archer and Emily Dawson","Identity provides a useful conceptual lens for understanding educational inequalities in science, technology, engineering and mathematics (STEM). In this paper, we examine how paying attention to physical and ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00249-w,6 October 2020,
226,0.000678989392134464,226,Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense,"Jingyuan Wang, Yufan Wu, Mingxuan Li, Xin Lin, Junjie Wu, Chao Li","While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysis-based DNN interpreters and the generation process of adversarial examples, which indicates the Achilles's heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detection-rectification process and features in building multiple sub-detectors and a rectifier upon various types of interpretation information toward target classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to combine sub-detectors into an ensemble detector for adversarial hybrid attacks defense. The non-differentiable property of RF further makes it a precious choice against the counterattack of adversaries. Extensive experiments under various types of state-of-the-art attacks and diverse attack scenarios demonstrate the advantages of X-Ensemble to competitive baseline methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.06919,April 2023,226,Requirement of artificial intelligence technology awareness for thoracic surgeons,"Anshuman Darbari, Krishan Kumar, Shubhankar Darbari and Prashant L. Patil","We have recently witnessed incredible interest in computer-based, internet web-dependent mechanisms and artificial intelligence (AI)-dependent technique emergence in our day-to-day lives. In the recent era of ...",https://www.springeropen.com//cts.springeropen.com/articles/10.1186/s43057-021-00053-4,3 July 2021,
227,0.000678989392134464,227,Full-frame data reduction method: a data mining tool to detect the potential variations in optical photometry,"Zhi-Bin Dai, Hao Zhou, Jin Cao","A Synchronous Photometry Data Extraction (SPDE) program, performing indiscriminate monitors of all stars appearing at the same field of view of astronomical image, is developed by integrating several Astropy affiliated packages to make full use of time series observed by the traditional small/medium aperture ground-based telescope. The complete full-frame stellar photometry data reductions implemented for the two time series of cataclysmic variables: RX J2102.0+3359 and Paloma J0524+4244 produce 363 and 641 optimal light curves, respectively. A cross-identification with the SIMBAD finds 23 known stars, of which 16 red giant-/horizontal-branch stars, 2 W UMa-type eclipsing variables, 2 program stars, a X-ray source and 2 Asteroid Terrestrial-impact Last Alert System variables. Based on the data productions of the SPDE program, a followup Light Curve Analysis (LCA) program identifies 32 potential variable light curves, of which 18 are from the time series of RX J2102.0+3359, and 14 are from that of Paloma J0524+4244. They are preliminarily separated into periodical, transient, and peculiar types. By querying for the 58 VizieR online data catalogs, their physical parameters and multi-band brightness spanning from X-ray to radio are compiled for future analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.06207,April 2023,227,A prototype implementation of a distributed Satisfiability Modulo Theories solver in the ToolBus framework,"David Déharbe, Silvio Ranise and Jorgiano Vidal","An increasing number of verification tools (e.g., software model-checkers) require the use of Satisfiability Modulo Theories (SMT) solvers to implement the back-ends for the automatic analysis of specification...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192553,March 2008,
228,0.000678989392134464,228,NP-Free: A Real-Time Normalization-free and Parameter-tuning-free Representation Approach for Open-ended Time Series,"Ming-Chang Lee, Jia-Chun Lin, Volker Stolz","As more connected devices are implemented in a cyber-physical world and data is expected to be collected and processed in real time, the ability to handle time series data has become increasingly significant. To help analyze time series in data mining applications, many time series representation approaches have been proposed to convert a raw time series into another series for representing the original time series. However, existing approaches are not designed for open-ended time series (which is a sequence of data points being continuously collected at a fixed interval without any length limit) because these approaches need to know the total length of the target time series in advance and pre-process the entire time series using normalization methods. Furthermore, many representation approaches require users to configure and tune some parameters beforehand in order to achieve satisfactory representation results. In this paper, we propose NP-Free, a real-time Normalization-free and Parameter-tuning-free representation approach for open-ended time series. Without needing to use any normalization method or tune any parameter, NP-Free can generate a representation for a raw time series on the fly by converting each data point of the time series into a root-mean-square error (RMSE) value based on Long Short-Term Memory (LSTM) and a Look-Back and Predict-Forward strategy. To demonstrate the capability of NP-Free in representing time series, we conducted several experiments based on real-world open-source time series datasets. We also evaluated the time consumption of NP-Free in generating representations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.06168,April 2023,228,Discrepancies of remote techno-tolerance due to COVID-19 pandemic within Arab middle-east countries,"Muhannad A. Abu-Hashem, Adnan Gutub, Osama Salem, Mohd Khaled Shambour, Qusai Shambour, Mohammad Shehab, Ahmad Izzat and Mufda J. Alrawashdeh",The coronavirus disease (COVID-19) changed the world’s lifestyle switching many techno-services to be provided remotely instead of direct usual physical interactions between people. This study focused on unive...,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s43995-023-00026-0,10 May 2023,
229,0.000678989392134464,229,An Improved Heart Disease Prediction Using Stacked Ensemble Method,"Md. Maidul Islam, Tanzina Nasrin Tania, Sharmin Akter, Kazi Hassan Shakib","Heart disorder has just overtaken cancer as the world's biggest cause of mortality. Several cardiac failures, heart disease mortality, and diagnostic costs can all be reduced with early identification and treatment. Medical data is collected in large quantities by the healthcare industry, but it is not well mined. The discovery of previously unknown patterns and connections in this information can help with an improved decision when it comes to forecasting heart disorder risk. In the proposed study, we constructed an ML-based diagnostic system for heart illness forecasting, using a heart disorder dataset. We used data preprocessing techniques like outlier detection and removal, checking and removing missing entries, feature normalization, cross-validation, nine classification algorithms like RF, MLP, KNN, ETC, XGB, SVC, ADB, DT, and GBM, and eight classifier measuring performance metrics like ramification accuracy, precision, F1 score, specificity, ROC, sensitivity, log-loss, and Matthews' correlation coefficient, as well as eight classification performance evaluations. Our method can easily differentiate between people who have cardiac disease and those are normal. Receiver optimistic curves and also the region under the curves were determined by every classifier. Most of the classifiers, pretreatment strategies, validation methods, and performance assessment metrics for classification models have been discussed in this study. The performance of the proposed scheme has been confirmed, utilizing all of its capabilities. In this work, the impact of clinical decision support systems was evaluated using a stacked ensemble approach that included these nine algorithms △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.06015,April 2023,229,Editorial,F Schweitzer and A Vespignani,Unknown,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds1,17 May 2012,
230,0.000678989392134464,230,DUFormer: A Novel Architecture for Power Line Segmentation of Aerial Images,"Deyu An, Qiang Zhang, Jianshu Chao, Ting Li, Feng Qiao, Yong Deng, Zhenpeng Bian, Jia Xu","Power lines pose a significant safety threat to unmanned aerial vehicles (UAVs) operating at low altitudes. However, detecting power lines in aerial images is challenging due to the small size of the foreground data (i.e., power lines) and the abundance of background information. To address this challenge, we propose DUFormer, a semantic segmentation algorithm designed specifically for power line detection in aerial images. We assume that performing sufficient feature extraction with a convolutional neural network (CNN) that has a strong inductive bias is beneficial for training an efficient Transformer model. To this end, we propose a heavy token encoder responsible for overlapping feature re-mining and tokenization. The encoder comprises a pyramid CNN feature extraction module and a power line feature enhancement module. Following sufficient feature extraction for power lines, the feature fusion is carried out, and then the Transformer block is used for global modeling. The final segmentation result is obtained by fusing local and global features in the decode head. Additionally, we demonstrate the significance of the joint multi-weight loss function in power line segmentation. The experimental results demonstrate that our proposed method achieves the state-of-the-art performance in power line segmentation on the publicly available TTPLA dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.05821,April 2023,230,The relation between teachers’ emphasis on the development of students’ digital information and communication skills and computer self-efficacy: the moderating roles of age and gender,Fazilat Siddiq and Ronny Scherer,"Teachers’ integration of information and communication technology (ICT) has been widely studied, given that digital competence is considered to be a crucial outcome of twenty first century education. In this c...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-016-0032-4,27 September 2016,
231,0.000678989392134464,231,A Predictive Model using Machine Learning Algorithm in Identifying Students Probability on Passing Semestral Course,Anabella C. Doctor,"This study aims to determine a predictive model to learn students probability to pass their courses taken at the earliest stage of the semester. To successfully discover a good predictive model with high acceptability, accurate, and precision rate which delivers a useful outcome for decision making in education systems, in improving the processes of conveying knowledge and uplifting students academic performance, the proponent applies and strictly followed the CRISP-DM (Cross-Industry Standard Process for Data Mining) methodology. This study employs classification for data mining techniques, and decision tree for algorithm. With the utilization of the newly discovered predictive model, the prediction of students probabilities to pass the current courses they take gives 0.7619 accuracy, 0.8333 precision, 0.8823 recall, and 0.8571 f1 score, which shows that the model used in the prediction is reliable, accurate, and recommendable. Considering the indicators and the results, it can be noted that the prediction model used in this study is highly acceptable. The data mining techniques provides effective and efficient innovative tools in analyzing and predicting student performances. The model used in this study will greatly affect the way educators understand and identify the weakness of their students in the class, the way they improved the effectiveness of their learning processes gearing to their students, bring down academic failure rates, and help institution administrators modify their learning system outcomes. Further study for the inclusion of some students demographic information, vast amount of data within the dataset, automated and manual process of predictive criteria indicators where the students can regulate to which criteria, they must improve more for them to pass their courses taken at the end of the semester as early as midterm period are highly needed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.05565,April 2023,231,Inclusion in practice: a systematic review of diversity-focused STEM programming in the United States,"Olivia Palid, Sarah Cashdollar, Sarah Deangelo, Chu Chu and Meg Bates",Colleges across the United States have shown a commitment to advancing diversity in the STEM fields by creating programs aimed at improving outcomes of women and/or racially and ethnically minoritized students...,https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00387-3,6 January 2023,
232,0.000678989392134464,232,Mining the Characteristics of Jupyter Notebooks in Data Science Projects,"Morakot Choetkiertikul, Apirak Hoonlor, Chaiyong Ragkhitwetsagul, Siripen Pongpaichet, Thanwadee Sunetnanta, Tasha Settewong, Vacharavich Jiravatvanich, Urisayar Kaewpichai","Nowadays, numerous industries have exceptional demand for skills in data science, such as data analysis, data mining, and machine learning. The computational notebook (e.g., Jupyter Notebook) is a well-known data science tool adopted in practice. Kaggle and GitHub are two platforms where data science communities are used for knowledge-sharing, skill-practicing, and collaboration. While tutorials and guidelines for novice data science are available on both platforms, there is a low number of Jupyter Notebooks that received high numbers of votes from the community. The high-voted notebook is considered well-documented, easy to understand, and applies the best data science and software engineering practices. In this research, we aim to understand the characteristics of high-voted Jupyter Notebooks on Kaggle and the popular Jupyter Notebooks for data science projects on GitHub. We plan to mine and analyse the Jupyter Notebooks on both platforms. We will perform exploratory analytics, data visualization, and feature importances to understand the overall structure of these notebooks and to identify common patterns and best-practice features separating the low-voted and high-voted notebooks. Upon the completion of this research, the discovered insights can be applied as training guidelines for aspiring data scientists and machine learning practitioners looking to improve their performance from novice ranking Jupyter Notebook on Kaggle to a deployable project on GitHub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.05325,April 2023,232,"Biologically-inspired analysis in the real world: computing, informatics, and ecologies of use",Laura A McNamara,"Biological metaphors abound in computational modeling and simulation, inspiring creative and novel approaches to conceptualizing, representing, simulating and analyzing a wide range of phenomena. Proponents of...",https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-1-17,6 November 2012,
233,0.000678989392134464,233,TodyNet: Temporal Dynamic Graph Neural Network for Multivariate Time Series Classification,"Huaiyuan Liu, Xianzhang Liu, Donghua Yang, Zhiyu Liang, Hongzhi Wang, Yong Cui, Jun Gu","Multivariate time series classification (MTSC) is an important data mining task, which can be effectively solved by popular deep learning technology. Unfortunately, the existing deep learning-based methods neglect the hidden dependencies in different dimensions and also rarely consider the unique dynamic features of time series, which lack sufficient feature extraction capability to obtain satisfactory classification accuracy. To address this problem, we propose a novel temporal dynamic graph neural network (TodyNet) that can extract hidden spatio-temporal dependencies without undefined graph structure. It enables information flow among isolated but implicit interdependent variables and captures the associations between different time slots by dynamic graph mechanism, which further improves the classification performance of the model. Meanwhile, the hierarchical representations of graphs cannot be learned due to the limitation of GNNs. Thus, we also design a temporal graph pooling layer to obtain a global graph-level representation for graph learning with learnable temporal parameters. The dynamic graph, graph information propagation, and temporal convolution are jointly learned in an end-to-end framework. The experiments on 26 UEA benchmark datasets illustrate that the proposed TodyNet outperforms existing deep learning-based methods in the MTSC tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.05078,April 2023,233,Does the material well-being at schools successfully compensate for socioeconomic disadvantages? Analysis of resilient schools in Sweden,Deborah Elin Siebecke and Maria Jarl,A variety of studies point to a deterioration of educational equity in Sweden and increasing school segregation with respect to achievement and socioeconomic composition. Some schools are resilient to socioeco...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-022-00130-y,30 August 2022,
234,0.000678989392134464,234,A Comprehensive Survey on Deep Graph Representation Learning,"Wei Ju, Zheng Fang, Yiyang Gu, Zequn Liu, Qingqing Long, Ziyue Qiao, Yifang Qin, Jianhao Shen, Fang Sun, Zhiping Xiao, Junwei Yang, Jingyang Yuan, Yusheng Zhao, Xiao Luo, Ming Zhang","Graph representation learning aims to effectively encode high-dimensional sparse graph-structured data into low-dimensional dense vectors, which is a fundamental task that has been widely studied in a range of fields, including machine learning and data mining. Classic graph embedding methods follow the basic idea that the embedding vectors of interconnected nodes in the graph can still maintain a relatively close distance, thereby preserving the structural information between the nodes in the graph. However, this is sub-optimal due to: (i) traditional methods have limited model capacity which limits the learning performance; (ii) existing techniques typically rely on unsupervised learning strategies and fail to couple with the latest learning paradigms; (iii) representation learning and downstream tasks are dependent on each other which should be jointly enhanced. With the remarkable success of deep learning, deep graph representation learning has shown great potential and advantages over shallow (traditional) methods, there exist a large number of deep graph representation learning techniques have been proposed in the past decade, especially graph neural networks. In this survey, we conduct a comprehensive survey on current deep graph representation learning algorithms by proposing a new taxonomy of existing state-of-the-art literature. Specifically, we systematically summarize the essential components of graph representation learning and categorize existing approaches by the ways of graph neural network architectures and the most recent advanced learning paradigms. Moreover, this survey also provides the practical and promising applications of deep graph representation learning. Last but not least, we state new perspectives and suggest challenging directions which deserve further investigations in the future. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.05055,April 2023,234,"The relationship between teacher immediacy, perceptions of learning, and computer-mediated graduate course outcomes among primarily Asian international students enrolled in an U.S. university",Jillian L. Wendt and Jennifer Courduff,This study employed a correlational research design to determine if a relationship existed between international students’ perceptions of teacher immediacy and students’ end of course grades in computer-mediat...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-018-0115-0,5 September 2018,
235,0.000678989392134464,235,A Data Set of Generalizable Python Code Change Patterns,"Akalanka Galappaththi, Sarah Nadi","Mining repetitive code changes from version control history is a common way of discovering unknown change patterns. Such change patterns can be used in code recommender systems or automated program repair techniques. While there are such tools and datasets exist for Java, there is little work on finding and recommending such changes in Python. In this paper, we present a data set of manually vetted generalizable Python repetitive code change patterns. We create a coding guideline to identify generalizable change patterns that can be used in automated tooling. We leverage the mined change patterns from recent work that mines repetitive changes in Python projects and use our coding guideline to manually review the patterns. For each change, we also record a description of the change and why it is applied along with other characteristics such as the number of projects it occurs in. This review process allows us to identify and share 72 Python change patterns that can be used to build and advance Python developer support tools. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.04983,April 2023,235,An overview of differential item functioning in multistage computer adaptive testing using three-parameter logistic item response theory,Karim Sadeghi and Zainab Abolfazli Khonbi,"As perfectly summarised by Ida Lawrence, “Testing is growing by leaps and bounds across the world. There is a realization that a nation’s well-being depends crucially on the educational achievement of its popu...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-017-0038-z,12 May 2017,
236,0.000678989392134464,236,DASS Good: Explainable Data Mining of Spatial Cohort Data,"Andrew Wentzel, Carla Floricel, Guadalupe Canahuate, Mohamed A. Naser, Abdallah S. Mohamed, Clifton David Fuller, Lisanne van Dijk, G. Elisabeta Marai","Developing applicable clinical machine learning models is a difficult task when the data includes spatial information, for example, radiation dose distributions across adjacent organs at risk. We describe the co-design of a modeling system, DASS, to support the hybrid human-machine development and validation of predictive models for estimating long-term toxicities related to radiotherapy doses in head and neck cancer patients. Developed in collaboration with domain experts in oncology and data mining, DASS incorporates human-in-the-loop visual steering, spatial data, and explainable AI to augment domain knowledge with automatic data mining. We demonstrate DASS with the development of two practical clinical stratification models and report feedback from domain experts. Finally, we describe the design lessons learned from this collaborative experience. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.04870,April 2023,236,Author multidisciplinarity and disciplinary roles in field of study networks,"Eoghan Cunningham, Barry Smyth and Derek Greene","When studying large research corpora, “distant reading” methods are vital to understand the topics and trends in the corresponding research space. In particular, given the recognised benefits of multidisciplin...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-022-00517-4,18 November 2022,
237,0.000678989392134464,237,FINEX: A Fast Index for Exact & Flexible Density-Based Clustering (Extended Version with Proofs)*,"Konstantin Emil Thiel, Daniel Kocher, Nikolaus Augsten, Thomas Hütter, Willi Mann, Daniel Ulrich Schmitt","Density-based clustering aims to find groups of similar objects (i.e., clusters) in a given dataset. Applications include, e.g., process mining and anomaly detection. It comes with two user parameters (ε, MinPts) that determine the clustering result, but are typically unknown in advance. Thus, users need to interactively test various settings until satisfying clusterings are found. However, existing solutions suffer from the following limitations: (a) Ineffective pruning of expensive neighborhood computations. (b) Approximate clustering, where objects are falsely labeled noise. (c) Restricted parameter tuning that is limited to ε whereas MinPts is constant, which reduces the explorable clusterings. (d) Inflexibility in terms of applicable data types and distance functions. We propose FINEX, a linear-space index that overcomes these limitations. Our index provides exact clusterings and can be queried with either of the two parameters. FINEX avoids neighborhood computations where possible and reduces the complexities of the remaining computations by leveraging fundamental properties of density-based clusters. Hence, our solution is effcient and flexible regarding data types and distance functions. Moreover, FINEX respects the original and straightforward notion of density-based clustering. In our experiments on 12 large real-world datasets from various domains, FINEX frequently outperforms state-of-the-art techniques for exact clustering by orders of magnitude. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.04817,April 2023,237,Joint Audio-Visual Tracking Using Particle Filters,"Dmitry N. Zotkin, Ramani Duraiswami and Larry S. Davis","It is often advantageous to track objects in a scene using multimodal information when such information is available. We use audio as a complementary modality to video data, which, in comparison to vision, can...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206058,28 November 2002,
238,0.000678989392134464,238,Knowledge Relation Rank Enhanced Heterogeneous Learning Interaction Modeling for Neural Graph Forgetting Knowledge Tracing,"Linqing Li, Zhifeng Wang","Recently, knowledge tracing models have been applied in educational data mining such as the Self-attention knowledge tracing model(SAKT), which models the relationship between exercises and Knowledge concepts(Kcs). However, relation modeling in traditional Knowledge tracing models only considers the static question-knowledge relationship and knowledge-knowledge relationship and treats these relationships with equal importance. This kind of relation modeling is difficult to avoid the influence of subjective labeling and considers the relationship between exercises and KCs, or KCs and KCs separately. In this work, a novel knowledge tracing model, named Knowledge Relation Rank Enhanced Heterogeneous Learning Interaction Modeling for Neural Graph Forgetting Knowledge Tracing(NGFKT), is proposed to reduce the impact of the subjective labeling by calibrating the skill relation matrix and the Q-matrix and apply the Graph Convolutional Network(GCN) to model the heterogeneous interactions between students, exercises, and skills. Specifically, the skill relation matrix and Q-matrix are generated by the Knowledge Relation Importance Rank Calibration method(KRIRC). Then the calibrated skill relation matrix, Q-matrix, and the heterogeneous interactions are treated as the input of the GCN to generate the exercise embedding and skill embedding. Next, the exercise embedding, skill embedding, item difficulty, and contingency table are incorporated to generate an exercise relation matrix as the inputs of the Position-Relation-Forgetting attention mechanism. Finally, the Position-Relation-Forgetting attention mechanism is applied to make the predictions. Experiments are conducted on the two public educational datasets and results indicate that the NGFKT model outperforms all baseline models in terms of AUC, ACC, and Performance Stability(PS). △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03945,April 2023,238,"""Our Little Secret"": pinpointing potential predators",Anna Vartapetiance and Lee Gillam,"The word “Paedophilia” has come a long way from its Greek origin of child-companionship to a Mental Disorder, Social Taboo and Criminal Offence. Various laws are in place to help control such behaviour, protec...",https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/s13388-014-0003-7,18 September 2014,
239,0.000678989392134464,239,SGDP: A Stream-Graph Neural Network Based Data Prefetcher,"Yiyuan Yang, Rongshang Li, Qiquan Shi, Xijun Li, Gang Hu, Xing Li, Mingxuan Yuan","Data prefetching is important for storage system optimization and access performance improvement. Traditional prefetchers work well for mining access patterns of sequential logical block address (LBA) but cannot handle complex non-sequential patterns that commonly exist in real-world applications. The state-of-the-art (SOTA) learning-based prefetchers cover more LBA accesses. However, they do not adequately consider the spatial interdependencies between LBA deltas, which leads to limited performance and robustness. This paper proposes a novel Stream-Graph neural network-based Data Prefetcher (SGDP). Specifically, SGDP models LBA delta streams using a weighted directed graph structure to represent interactive relations among LBA deltas and further extracts hybrid features by graph neural networks for data prefetching. We conduct extensive experiments on eight real-world datasets. Empirical results verify that SGDP outperforms the SOTA methods in terms of the hit ratio by 6.21%, the effective prefetching ratio by 7.00%, and speeds up inference time by 3.13X on average. Besides, we generalize SGDP to different variants by different stream constructions, further expanding its application scenarios and demonstrating its robustness. SGDP offers a novel data prefetching solution and has been verified in commercial hybrid storage systems in the experimental phase. Our codes and appendix are available at https://github.com/yyysjz1997/SGDP/. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03864,April 2023,239,Deep Learning applications for COVID-19,"Connor Shorten, Taghi M. Khoshgoftaar and Borko Furht","This survey explores how Deep Learning has battled the COVID-19 pandemic and provides directions for future research on COVID-19. We cover Deep Learning applications in Natural Language Processing, Computer Vi...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00392-9,11 January 2021,
240,0.00991669352293754,240,Gated Mechanism Enhanced Multi-Task Learning for Dialog Routing,"Ziming Huang, Zhuoxuan Jiang, Ke Wang, Juntao Li, Shanshan Feng, Xian-Ling Mao","Currently, human-bot symbiosis dialog systems, e.g., pre- and after-sales in E-commerce, are ubiquitous, and the dialog routing component is essential to improve the overall efficiency, reduce human resource cost, and enhance user experience. Although most existing methods can fulfil this requirement, they can only model single-source dialog data and cannot effectively capture the underlying knowledge of relations among data and subtasks. In this paper, we investigate this important problem by thoroughly mining both the data-to-task and task-to-task knowledge among various kinds of dialog data. To achieve the above targets, we propose a Gated Mechanism enhanced Multi-task Model (G3M), specifically including a novel dialog encoder and two tailored gated mechanism modules. The proposed method can play the role of hierarchical information filtering and is non-invasive to existing dialog systems. Based on two datasets collected from real world applications, extensive experimental results demonstrate the effectiveness of our method, which achieves the state-of-the-art performance by improving 8.7\%/11.8\% on RMSE metric and 2.2\%/4.4\% on F1 metric. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03730,April 2023,240,What do they TEL(L)? A systematic analysis of master programs in technology-enhanced learning,"Mikhail Fominykh, Joshua Weidlich, Marco Kalz and Ingunn Dahler Hybertsen","This article contributes to the debate on the growing number of interdisciplinary study programs in learning and technology, and aims to understand the diversity of programs as well as curricula structure in a...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00305-7,6 January 2022,
241,0.000678989392134464,241,Feature Mining for Encrypted Malicious Traffic Detection with Deep Learning and Other Machine Learning Algorithms,"Zihao Wang, Vrizlynn L. L. Thing","The popularity of encryption mechanisms poses a great challenge to malicious traffic detection. The reason is traditional detection techniques cannot work without the decryption of encrypted traffic. Currently, research on encrypted malicious traffic detection without decryption has focused on feature extraction and the choice of machine learning or deep learning algorithms. In this paper, we first provide an in-depth analysis of traffic features and compare different state-of-the-art traffic feature creation approaches, while proposing a novel concept for encrypted traffic feature which is specifically designed for encrypted malicious traffic analysis. In addition, we propose a framework for encrypted malicious traffic detection. The framework is a two-layer detection framework which consists of both deep learning and traditional machine learning algorithms. Through comparative experiments, it outperforms classical deep learning and traditional machine learning algorithms, such as ResNet and Random Forest. Moreover, to provide sufficient training data for the deep learning model, we also curate a dataset composed entirely of public datasets. The composed dataset is more comprehensive than using any public dataset alone. Lastly, we discuss the future directions of this research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03691,April 2023,241,Extending Technology Acceptance Model to higher-education students’ use of digital academic reading tools on computers,Yupeng Lin and Zhonggen Yu,"Digital academic reading tools on computers bring multiple benefits to higher-education students. Through structural equation modeling methods, this study contributes to the following findings: (1) Perceived e...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00403-8,16 June 2023,
242,0.000678989392134464,242,A lightweight Encryption Method For Privacy-Preserving in Process Mining,"Mohsen Kazemian, Markus Helfert","Novel technological achievements in the fields of business intelligence, business management and data science are based on real-time and complex virtual networks. Sharing data between a large number of organizations that leads to a system with high computational complexity is one of the considerable characteristics of the current business networks. Discovery, conformance and enhancement of the business processes are performed using the generated event logs. In this regard, one of the overlooked challenges is privacy-preserving in the field of process mining in the industry. To preserve the data-privacy with a low computational complexity structure that is a necessity for the current digital business technology, a novel lightweight encryption method based on Haar transform and a private key is proposed in this paper. We compare the proposed method with the well-known homomorphic cryptosystem and Walsh- Hadamard encryption (WHE) in terms of cryptography, computational complexity and structure vulnerability. The analyses show that the proposed method anonymizes the event logs with the lower complexity and more accuracy compared with two aforementioned cryptosystems, significantly. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03579,April 2023,242,Advanced Signal Processing Techniques for Bioinformatics,"Xue-Wen Chen, Sun Kim, Vladimir Pavlović and David P. Casasent",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/51090,1 December 2006,
243,0.000678989392134464,243,Opinion Mining from YouTube Captions Using ChatGPT: A Case Study of Street Interviews Polling the 2023 Turkish Elections,"Tuğrulcan Elmas, İlker Gül","Opinion mining plays a critical role in understanding public sentiment and preferences, particularly in the context of political elections. Traditional polling methods, while useful, can be expensive and less scalable. Social media offers an alternative source of data for opinion mining but presents challenges such as noise, biases, and platform limitations in data collection. In this paper, we propose a novel approach for opinion mining, utilizing YouTube's auto-generated captions from public interviews as a data source, specifically focusing on the 2023 Turkish elections as a case study. We introduce an opinion mining framework using ChatGPT to mass-annotate voting intentions and motivations that represent the stance and frames prior to the election. We report that ChatGPT can predict the preferred candidate with 97\% accuracy and identify the correct voting motivation out of 13 possible choices with 71\% accuracy based on the data collected from 325 interviews. We conclude by discussing the robustness of our approach, accounting for factors such as captions quality, interview length, and channels. This new method will offer a less noisy and cost-effective alternative for opinion mining using social media data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03434,April 2023,243,A performative perspective on UX,Julie R Williamson and Stephen Brewster,"As increasingly more interactions occur in public or social settings, the concept of interaction as a “performance” provides a compelling perspective when evaluating user experience (UX). Building on dramaturg...",https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-1-3,19 June 2012,
244,0.000678989392134464,244,Hierarchical Graph Neural Network with Cross-Attention for Cross-Device User Matching,"Ali Taghibakhshi, Mingyuan Ma, Ashwath Aithal, Onur Yilmaz, Haggai Maron, Matthew West","Cross-device user matching is a critical problem in numerous domains, including advertising, recommender systems, and cybersecurity. It involves identifying and linking different devices belonging to the same person, utilizing sequence logs. Previous data mining techniques have struggled to address the long-range dependencies and higher-order connections between the logs. Recently, researchers have modeled this problem as a graph problem and proposed a two-tier graph contextual embedding (TGCE) neural network architecture, which outperforms previous methods. In this paper, we propose a novel hierarchical graph neural network architecture (HGNN), which has a more computationally efficient second level design than TGCE. Furthermore, we introduce a cross-attention (Cross-Att) mechanism in our model, which improves performance by 5% compared to the state-of-the-art TGCE method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.03215,April 2023,244,Fluency of visualizations: linking spatiotemporal visualizations to improve cybersecurity visual analytics,Zhenyu Cheryl Qian and Yingjie Victor Chen,This paper adopts the metaphor of representational fluency and proposes an auto linking approach to help analysts investigate details of suspicious sections across different cybersecurity visualizations. Analy...,https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/s13388-014-0006-4,15 July 2014,
245,0.000678989392134464,245,LSketch: A Label-Enabled Graph Stream Sketch Toward Time-Sensitive Queries,"Yiling Zeng, Chunyao Song, Yuhan Li, Tingjian Ge","Graph streams represent data interactions in real applications. The mining of graph streams plays an important role in network security, social network analysis, and traffic control, among others. However, the sheer volume and high dynamics cause great challenges for efficient storage and subsequent query analysis on them. Current studies apply sketches to summarize graph streams. We propose LSketch that works for heterogeneous graph streams, which effectively preserves the label information carried by the streams in real scenes, thereby enriching the expressive ability of sketches. In addition, as graph streams continue to evolve over time, edges too old may lose their practical significance. Therefore, we introduce the sliding window model into LSketch to eliminate the expired edges automatically. LSketch uses sub-linear storage space and can support structure based queries and time-sensitive queries with high accuracy. We perform extensive experiments over four real datasets, demonstrating the superiority of the proposed method over state-of-the-art methods, in aspects of query accuracy and time efficiency. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.02897,April 2023,245,Science Tree: a platform for exploring the brazilian academic genealogy,"João M. M. C. Cota, Alberto H. F. Laender and Raquel O. Prates","Identifying and studying the formation of researchers over the years is a challenging task, since the current repositories of theses and dissertations are cataloged in a decentralized manner in different digit...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00118-6,17 November 2021,
246,0.000678989392134464,246,Affect as a proxy for literary mood,"Emily Öhman, Riikka Rossi","We propose to use affect as a proxy for mood in literary texts. In this study, we explore the differences in computationally detecting tone versus detecting mood. Methodologically we utilize affective word embeddings to look at the affective distribution in different text segments. We also present a simple yet efficient and effective method of enhancing emotion lexicons to take both semantic shift and the domain of the text into account producing real-world congruent results closely matching both contemporary and modern qualitative analyses. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.02894,April 2023,246,Correction to: Spatial weight matrix in dimensionality reduction reconstruction for micro-electromechanical system-based photoacoustic microscopy,"Yuanzheng Ma, Chang Lu, Kedi Xiong, Wuyu Zhang and Sihua Yang",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00066-6,21 December 2020,
247,0.000678989392134464,247,Scalable and Accurate Self-supervised Multimodal Representation Learning without Aligned Video and Text Data,"Vladislav Lialin, Stephen Rawls, David Chan, Shalini Ghosh, Anna Rumshisky, Wael Hamza","Scaling up weakly-supervised datasets has shown to be highly effective in the image-text domain and has contributed to most of the recent state-of-the-art computer vision and multimodal neural networks. However, existing large-scale video-text datasets and mining techniques suffer from several limitations, such as the scarcity of aligned data, the lack of diversity in the data, and the difficulty of collecting aligned data. Currently popular video-text data mining approach via automatic speech recognition (ASR) used in HowTo100M provides low-quality captions that often do not refer to the video content. Other mining approaches do not provide proper language descriptions (video tags) and are biased toward short clips (alt text). In this work, we show how recent advances in image captioning allow us to pre-train high-quality video models without any parallel video-text data. We pre-train several video captioning models that are based on an OPT language model and a TimeSformer visual backbone. We fine-tune these networks on several video captioning datasets. First, we demonstrate that image captioning pseudolabels work better for pre-training than the existing HowTo100M ASR captions. Second, we show that pre-training on both images and videos produces a significantly better network (+4 CIDER on MSR-VTT) than pre-training on a single modality. Our methods are complementary to the existing pre-training or data mining approaches and can be used in a variety of settings. Given the efficacy of the pseudolabeling method, we are planning to publicly release the generated captions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.02080,April 2023,247,ECR 2017 – BOOK OF ABSTRACTS,Unknown,"This article is part of a Supplement:Volume 8
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1007/s13244-017-0546-5,16 February 2017,
248,0.000678989392134464,248,Topological Filtering for Visual Data Mining and Analysis of Complex Networks,Faraz Zaidi,"The discovery of small world and scale free properties of many real world networks has revolutionized the way we study, analyze, model and process networks. An important way to analyze these complex networks is to visualize them using graph layout algorithms. Due to their large size and complex connectivity, it is difficult to make deductions from the visual representation of these networks. In this paper, we present a method for interactive analysis of large graphs based on topological filtering, network metrics and visualization. We analyze a number of real world networks and draw interesting conclusions using the proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01896,April 2023,248,A path to automated service creation via semi-automation levels,"Ernani Azevêdo, Carlos Kamienski, Ramide Dantas, Börje Ohlman and Djamel Sadok",The manual creation of new Internet- and IT-based applications is currently a limiting factor in enabling new and innovative services to be quickly available. We advocate that semi-automated service creation t...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/1678-4804-20-2,23 January 2014,
249,0.000678989392134464,249,A proof-theoretic metatheorem for nonlinear semigroups generated by an accretive operator and applications,Nicholas Pischke,"We further develop the theoretical framework of proof mining, a program in mathematical logic that seeks to quantify and extract computational information from prima facie `non-computational' proofs from the mainstream literature. To that end, we establish logical metatheorems that allow for the treatment of proofs involving nonlinear semigroups generated by an accretive operator, structures which in particular arise in the study of the solutions and asymptotic behavior of partial differential equations. In that way, the here established metatheorems facilitate a theoretical basis for the application of methods from the proof mining program to the wide variety of mathematical results established in the context of that theory since the 1960's. We in particular illustrate the applicability of the new systems and their metatheorems introduced here by providing two case studies on two central results due to Reich and Plant, respectively, on the asymptotic behavior of said semigroups and the resolvents of their generators where we derive rates of convergence for the limits involved which are, moreover, polynomial in all data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01723,April 2023,249,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0045-3,22 October 2011,
250,0.000678989392134464,250,Privacy-Preserving Federated Discovery of DNA Motifs with Differential Privacy,"Yao Chen, Wensheng Gan, Gengsen Huang, Yongdong Wu, Philip S. Yu","DNA motif discovery is an important issue in gene research, which aims to identify transcription factor binding sites (i.e., motifs) in DNA sequences to reveal the mechanisms that regulate gene expression. However, the phenomenon of data silos and the problem of privacy leakage have seriously hindered the development of DNA motif discovery. On the one hand, the phenomenon of data silos makes data collection difficult. On the other hand, the collection and use of DNA data become complicated and difficult because DNA is sensitive private information. In this context, how discovering DNA motifs under the premise of ensuring privacy and security and alleviating data silos has become a very important issue. Therefore, this paper proposes a novel method, namely DP-FLMD, to address this problem. Note that this is the first application of federated learning to the field of genetics research. The federated learning technique is used to solve the problem of data silos. It has the advantage of enabling multiple participants to train models together and providing privacy protection services. To address the challenges of federated learning in terms of communication costs, this paper applies a sampling method and a strategy for reducing communication costs to DP-FLMD. In addition, differential privacy, a privacy protection technique with rigorous mathematical proof, is also applied to DP-FLMD. Experiments on the DNA datasets show that DP-FLMD has high mining accuracy and runtime efficiency, and the performance of the algorithm is affected by some parameters. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01689,April 2023,250,"Definition, framework and research issues of smart learning environments - a context-aware ubiquitous learning perspective",Gwo-Jen Hwang,"The rapid progress of mobile, wireless communication and sensing technologies has enabled the development of context-aware ubiquitous learning (u-learning) environments, which are able to detect the real-world...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-014-0004-5,7 November 2014,
251,0.000678989392134464,251,PartMix: Regularization Strategy to Learn Part Discovery for Visible-Infrared Person Re-identification,"Minsu Kim, Seungryong Kim, JungIn Park, Seongheon Park, Kwanghoon Sohn","Modern data augmentation using a mixture-based technique can regularize the models from overfitting to the training data in various computer vision applications, but a proper data augmentation technique tailored for the part-based Visible-Infrared person Re-IDentification (VI-ReID) models remains unexplored. In this paper, we present a novel data augmentation technique, dubbed PartMix, that synthesizes the augmented samples by mixing the part descriptors across the modalities to improve the performance of part-based VI-ReID models. Especially, we synthesize the positive and negative samples within the same and across different identities and regularize the backbone model through contrastive learning. In addition, we also present an entropy-based mining strategy to weaken the adverse impact of unreliable positive and negative samples. When incorporated into existing part-based VI-ReID model, PartMix consistently boosts the performance. We conduct experiments to demonstrate the effectiveness of our PartMix over the existing VI-ReID methods and provide ablation studies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01537,April 2023,251,Editorial,Maria Cristina F. de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-010-0006-2,18 April 2010,
252,0.000678989392134464,252,A Unified Contrastive Transfer Framework with Propagation Structure for Boosting Low-Resource Rumor Detection,"Hongzhan Lin, Jing Ma, Ruichao Yang, Zhiwei Yang, Mingfei Cheng","The truth is significantly hampered by massive rumors that spread along with breaking news or popular topics. Since there is sufficient corpus gathered from the same domain for model training, existing rumor detection algorithms show promising performance on yesterday's news. However, due to a lack of substantial training data and prior expert knowledge, they are poor at spotting rumors concerning unforeseen events, especially those propagated in different languages (i.e., low-resource regimes). In this paper, we propose a unified contrastive transfer framework to detect rumors by adapting the features learned from well-resourced rumor data to that of the low-resourced with only few-shot annotations. More specifically, we first represent rumor circulated on social media as an undirected topology for enhancing the interaction of user opinions, and then train a Multi-scale Graph Convolutional Network via a unified contrastive paradigm to mine effective clues simultaneously from post semantics and propagation structure. Our model explicitly breaks the barriers of the domain and/or language issues, via language alignment and a novel domain-adaptive contrastive learning mechanism. To well-generalize the representation learning using a small set of annotated target events, we reveal that rumor-indicative signal is closely correlated with the uniformity of the distribution of these events. We design a target-wise contrastive training mechanism with three event-level data augmentation strategies, capable of unifying the representations by distinguishing target events. Extensive experiments conducted on four low-resource datasets collected from real-world microblog platforms demonstrate that our framework achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01492,April 2023,252,Dynamically Reconfigurable Architectures,"Neil Bergmann, Marco Platzner and Jürgen Teich",Unknown,https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/2007/28405,15 March 2007,
253,0.000678989392134464,253,Thematic context vector association based on event uncertainty for Twitter,"Vaibhav Khatavkar, Swapnil Mane, Parag Kulkarni","Keyword extraction is a crucial process in text mining. The extraction of keywords with respective contextual events in Twitter data is a big challenge. The challenging issues are mainly because of the informality in the language used. The use of misspelled words, acronyms, and ambiguous terms causes informality. The extraction of keywords with informal language in current systems is pattern based or event based. In this paper, contextual keywords are extracted using thematic events with the help of data association. The thematic context for events is identified using the uncertainty principle in the proposed system. The thematic contexts are weighed with the help of vectors called thematic context vectors which signifies the event as certain or uncertain. The system is tested on the Twitter COVID-19 dataset and proves to be effective. The system extracts event-specific thematic context vectors from the test dataset and ranks them. The extracted thematic context vectors are used for the clustering of contextual thematic vectors which improves the silhouette coefficient by 0.5% than state of art methods namely TF and TF-IDF. The thematic context vector can be used in other applications like Cyberbullying, sarcasm detection, figurative language detection, etc. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.01423,April 2023,253,Vision Systems with the Human in the Loop,"Christian Bauckhage, Marc Hanheide, Sebastian Wrede, Thomas Käster, Michael Pfeiffer and Gerhard Sagerer",The emerging cognitive vision paradigm deals with vision systems that apply machine learning and automatic reasoning in order to learn from what they perceive. Cognitive vision systems can rate the relevance a...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2375,25 August 2005,
254,0.000678989392134464,254,Quantifying Carbon Emissions due to Online Third-Party Tracking,"Michalis Pachilakis, Savino Dambra, Iskander Sanchez-Rola, Leyla Bilge","In the past decade, global warming made several headlines and turned the attention of the whole world to it. Carbon footprint is the main factor that drives greenhouse emissions up and results in the temperature increase of the planet with dire consequences. While the attention of the public is turned to reducing carbon emissions by transportation, food consumption and household activities, we ignore the contribution of CO2eq emissions produced by online activities. In the current information era, we spend a big amount of our days browsing online. This activity consumes electricity which in turn produces CO2eq. While website browsing contributes to the production of greenhouse gas emissions, the impact of the Internet on the environment is further exacerbated by the web-tracking practice. Indeed, most webpages are heavily loaded by tracking content used mostly for advertising, data analytics and usability improvements. This extra content implies big data transmissions which results in higher electricity consumption and thus higher greenhouse gas emissions. In this work, we focus on the overhead caused by web tracking and analyse both its network and carbon footprint. By leveraging the browsing telemetry of 100k users and the results of a crawling experiment of 2.7M websites, we find that web tracking increases data transmissions upwards of 21%, which in turn implies the additional emission of around 11 Mt of greenhouse gases in the atmosphere every year. We find such contribution to be far from negligible, and comparable to many activities of modern life, such as meat production, transportation, and even cryptocurrency mining. Our study also highlights that there exist significant inequalities when considering the footprint of different countries, website categories, and tracking organizations, with a few actors contributing to a much greater extent than the remaining ones. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00927,April 2023,254,The application of AI technologies in STEM education: a systematic review from 2011 to 2021,Weiqi Xu and Fan Ouyang,"The application of artificial intelligence (AI) in STEM education (AI-STEM), as an emerging field, is confronted with a challenge of integrating diverse AI techniques and complex educational elements to meet i...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00377-5,19 September 2022,
255,0.000678989392134464,255,COMIC: An Unsupervised Change Detection Method for Heterogeneous Remote Sensing Images Based on Copula Mixtures and Cycle-Consistent Adversarial Networks,"Chengxi Li, Gang Li, Zhuoyue Wang, Xueqian Wang, Pramod K. Varshney","In this paper, we consider the problem of change detection (CD) with two heterogeneous remote sensing (RS) images. For this problem, an unsupervised change detection method has been proposed recently based on the image translation technique of Cycle-Consistent Adversarial Networks (CycleGANs), where one image is translated from its original modality to the modality of the other image so that the difference map can be obtained by performing arithmetical subtraction. However, the difference map derived from subtraction is susceptible to image translation errors, in which case the changed area and the unchanged area are less distinguishable. To overcome the above shortcoming, we propose a new unsupervised copula mixture and CycleGAN-based CD method (COMIC), which combines the advantages of copula mixtures on statistical modeling and the advantages of CycleGANs on data mining. In COMIC, the pre-event image is first translated from its original modality to the post-event image modality. After that, by constructing a copula mixture, the joint distribution of the features from the heterogeneous images can be learnt according to quantitive analysis of the dependence structure based on the translated image and the original pre-event image, which are of the same modality and contain totally the same objects. Then, we model the CD problem as a binary hypothesis testing problem and derive its test statistics based on the constructed copula mixture. Finally, the difference map can be obtained from the test statistics and the binary change map (BCM) is generated by K-means clustering. We perform experiments on real RS datasets, which demonstrate the superiority of COMIC over the state-of-the-art methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00721,April 2023,255,Digital quantum simulation of quantum gravitational entanglement with IBM quantum computers,Carlos Sabín,"We report the digital quantum simulation of a hamiltonian involved in the generation of quantum entanglement by gravitational means. In particular, we focus on a pair of quantum harmonic oscillators, whose int...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-023-00161-6,8 February 2023,
256,0.000678989392134464,256,Multi-Modal Representation Learning with Text-Driven Soft Masks,"Jaeyoo Park, Bohyung Han","We propose a visual-linguistic representation learning approach within a self-supervised learning framework by introducing a new operation, loss, and data augmentation strategy. First, we generate diverse features for the image-text matching (ITM) task via soft-masking the regions in an image, which are most relevant to a certain word in the corresponding caption, instead of completely removing them. Since our framework relies only on image-caption pairs with no fine-grained annotations, we identify the relevant regions to each word by computing the word-conditional visual attention using multi-modal encoder. Second, we encourage the model to focus more on hard but diverse examples by proposing a focal loss for the image-text contrastive learning (ITC) objective, which alleviates the inherent limitations of overfitting and bias issues. Last, we perform multi-modal data augmentations for self-supervised learning via mining various examples by masking texts and rendering distortions on images. We show that the combination of these three innovations is effective for learning a pretrained model, leading to outstanding performance on multiple vision-language downstream tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00719,April 2023,256,ESICM LIVES 2020,Unknown,"This article is part of a Supplement:Volume 8
                                        Supplement 2",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-020-00354-8,14 December 2020,
257,0.000678989392134464,257,The Archive Query Log: Mining Millions of Search Result Pages of Hundreds of Search Engines from 25 Years of Web Archives,"Jan Heinrich Reimer, Sebastian Schmidt, Maik Fröbe, Lukas Gienapp, Harrisen Scells, Benno Stein, Matthias Hagen, Martin Potthast","The Archive Query Log (AQL) is a previously unused, comprehensive query log collected at the Internet Archive over the last 25 years. Its first version includes 356 million queries, 166 million search result pages, and 1.7 billion search results across 550 search providers. Although many query logs have been studied in the literature, the search providers that own them generally do not publish their logs to protect user privacy and vital business data. Of the few query logs publicly available, none combines size, scope, and diversity. The AQL is the first to do so, enabling research on new retrieval models and (diachronic) search engine analyses. Provided in a privacy-preserving manner, it promotes open research as well as more transparency and accountability in the search industry. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00413,April 2023,257,Editorial,Ngoc Thanh Nguyen,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-013-0011-4,26 November 2013,
258,0.000678989392134464,258,Medical Pathologies Prediction : Systematic Review and Proposed Approach,"Chaimae Taoussi, Imad Hafidi, Abdelmoutalib Metrane","The healthcare sector is an important pillar of every community, numerous research studies have been carried out in this context to optimize medical processes and improve care quality and facilitate patient management. In this article we have analyzed and examined different works concerning the exploitation of the most recent technologies such as big data, artificial intelligence, machine learning, and deep learning for the improvement of health care, which enabled us to propose our general approach concentrating on the collection, preprocessing and clustering of medical data to facilitate access, after analysis, to the patients and health professionals to predict the most frequent pathologies with better precision within a notable timeframe. keywords: Healthcare, big data, artificial intelligence, automatic language processing, data mining, predictive models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00311,April 2023,258,ECR 2019: Book of Abstracts,Unknown,"This article is part of a Supplement:Volume 10
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-019-0713-y,26 February 2019,
259,0.000678989392134464,259,Subject-driven Text-to-Image Generation via Apprenticeship Learning,"Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Ruiz, Xuhui Jia, Ming-Wei Chang, William W. Cohen","Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00186,April 2023,259,Validating distance decay through agent based modeling,"Arvind Verma, Ramyaa Ramyaa and Suresh Marru","The objectives of this research are to display the utility of using agent based model and simulated experiments in understanding criminal behavior. In particular, this research focuses upon the distance decay ...",https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-2-3,19 January 2013,
260,0.000678989392134464,260,A robust deep learning-based damage identification approach for SHM considering missing data,"Fan Deng, Xiaoming Tao, Pengxiang Wei, Shiyin Wei","Data-driven method for Structural Health Monitoring (SHM), that mine the hidden structural performance from the correlations among monitored time series data, has received widely concerns recently. However, missing data significantly impacts the conduction of this method. Missing data is a frequently encountered issue in time series data in SHM and many other real-world applications, that harms to the standardized data mining and downstream tasks, such as condition assessment. Imputation approaches based on spatiotemporal relations among monitoring data are developed to handle this issue, however, no additional information is added during imputation. This paper thus develops a robust method for damage identification that considers the missing data occasions, based on long-short term memory (LSTM) model and dropout mechanism in the autoencoder (AE) framework. Inputs channels are randomly dropped to simulate the missing data in training, and reconstruction errors are used as the loss function and the damage indicator. Quasi-static response (cable tension) of a cable-stayed bridge released in 1st IPC-SHM is employed to verify this proposed method, and results show that the missing data imputation and damage identification can be implemented together in a unified way. △ Less",https://arxiv.orghttps://arxiv.org/abs/2304.00040,April 2023,260,Bifurcation analysis in a delayed computer virus model with the effect of external computers,Zizhen Zhang and Dianjie Bi,A delayed Susceptible-Infected-External (SIE) computer virus propagation model is investigated in the present paper. The linear stability conditions are obtained with characteristic root method. The Hopf bifur...,https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-015-0652-y,14 October 2015,
261,0.000678989392134464,261,No Place to Hide: Dual Deep Interaction Channel Network for Fake News Detection based on Data Augmentation,"Biwei Cao, Lulu Hua, Jiuxin Cao, Jie Gui, Bo Liu, James Tin-Yau Kwok","Online Social Network (OSN) has become a hotbed of fake news due to the low cost of information dissemination. Although the existing methods have made many attempts in news content and propagation structure, the detection of fake news is still facing two challenges: one is how to mine the unique key features and evolution patterns, and the other is how to tackle the problem of small samples to build the high-performance model. Different from popular methods which take full advantage of the propagation topology structure, in this paper, we propose a novel framework for fake news detection from perspectives of semantic, emotion and data enhancement, which excavates the emotional evolution patterns of news participants during the propagation process, and a dual deep interaction channel network of semantic and emotion is designed to obtain a more comprehensive and fine-grained news representation with the consideration of comments. Meanwhile, the framework introduces a data enhancement module to obtain more labeled data with high quality based on confidence which further improves the performance of the classification model. Experiments show that the proposed approach outperforms the state-of-the-art methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.18049,March 2023,261,Highlighting photonics: looking into the next decade,Zhigang Chen and Mordechai Segev,"Let there belight–to change the world we want to be! Over the past several decades, and ever since the birth of the first laser, mankind has witnessed the development of the science of light, as light-based tech...",https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-021-00002-y,8 June 2021,
262,0.000678989392134464,262,CoSMo: a Framework for Implementing Conditioned Process Simulation Models,"Rafael S. Oyamada, Gabriel M. Tavares, Paolo Ceravolo","Process simulation is an analysis tool in process mining that allows users to measure the impact of changes, prevent losses, and update the process without risks or costs. In the literature, several process simulation techniques are available and they are usually built upon process models discovered from a given event log or learned via deep learning. Each group of approaches has its own strengths and limitations. The former is usually restricted to the control-flow but it is more interpretable, whereas the latter is not interpretable by nature but has a greater generalization capability on large event logs. Despite the great performance achieved by deep learning approaches, they are still not suitable to be applied to real scenarios and generate value for users. This issue is mainly due to fact their stochasticity is hard to control. To address this problem, we propose the CoSMo framework for implementing process simulation models fully based on deep learning. This framework enables simulating event logs that satisfy a constraint by conditioning the learning phase of a deep neural network. Throughout experiments, the simulation is validated from both control-flow and data-flow perspectives, demonstrating the proposed framework's capability of simulating cases while satisfying imposed conditions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17879,March 2023,262,Letter from the editor-in-chief,Hugo Fuks,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194252,December 2007,
263,0.000678989392134464,263,FairGen: Towards Fair Graph Generation,"Lecheng Zheng, Dawei Zhou, Hanghang Tong, Jiejun Xu, Yada Zhu, Jingrui He","There have been tremendous efforts over the past decades dedicated to the generation of realistic graphs in a variety of domains, ranging from social networks to computer networks, from gene regulatory networks to online transaction networks. Despite the remarkable success, the vast majority of these works are unsupervised in nature and are typically trained to minimize the expected graph reconstruction loss, which would result in the representation disparity issue in the generated graphs, i.e., the protected groups (often minorities) contribute less to the objective and thus suffer from systematically higher errors. In this paper, we aim to tailor graph generation to downstream mining tasks by leveraging label information and user-preferred parity constraint. In particular, we start from the investigation of representation disparity in the context of graph generative models. To mitigate the disparity, we propose a fairness-aware graph generative model named FairGen. Our model jointly trains a label-informed graph generation module and a fair representation learning module by progressively learning the behaviors of the protected and unprotected groups, from the `easy' concepts to the `hard' ones. In addition, we propose a generic context sampling strategy for graph generative models, which is proven to be capable of fairly capturing the contextual information of each group with a high probability. Experimental results on seven real-world data sets, including web-based graphs, demonstrate that FairGen (1) obtains performance on par with state-of-the-art graph generative models across six network properties, (2) mitigates the representation disparity issues in the generated graphs, and (3) substantially boosts the model performance by up to 17% in downstream tasks via data augmentation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17743,March 2023,263,The influences of social agents in completing a STEM degree: an examination of female graduates of selective science high schools,"Linlin Luo, Heidrun Stoeger and Rena F. Subotnik","Women are underrepresented in science, technology, engineering, and mathematics (STEM) professions. Even the most promising female students’ interest in STEM subjects often decreases during secondary school. U...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00324-w,20 January 2022,
264,0.000678989392134464,264,Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text,"Hasin Rehana, Nur Bengisu Çam, Mert Basmaci, Yongqun He, Arzucan Özgür, Junguk Hur","Detecting protein-protein interactions (PPIs) is crucial for understanding genetic mechanisms, disease pathogenesis, and drug design. However, with the fast-paced growth of biomedical literature, there is a growing need for automated and accurate extraction of PPIs to facilitate scientific knowledge discovery. Pre-trained language models, such as generative pre-trained transformer (GPT) and bidirectional encoder representations from transformers (BERT), have shown promising results in natural language processing (NLP) tasks. We evaluated the PPI identification performance of various GPT and BERT models using a manually curated benchmark corpus of 164 PPIs in 77 sentences from learning language in logic (LLL). BERT-based models achieved the best overall performance, with PubMedBERT achieving the highest precision (85.17%) and F1-score (86.47%) and BioM-ALBERT achieving the highest recall (93.83%). Despite not being explicitly trained for biomedical texts, GPT-4 achieved comparable performance to the best BERT models with 83.34% precision, 76.57% recall, and 79.18% F1-score. These findings suggest that GPT models can effectively detect PPIs from text data and have the potential for use in biomedical literature mining tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17728,March 2023,264,The contribution of technology in business growth: the case of Greek ladies,Angelina Michaelidis Vassilakopoulou,"The overall image of the use of digital technology by Greek women is not that encouraging. In information communication technology companies in the Greek market, there are only few ladies in high management ma...",https://www.springeropen.com//innovation-entrepreneurship.springeropen.com/articles/10.1186/2192-5372-2-3,21 May 2013,
265,0.000678989392134464,265,Explainable Intrusion Detection Systems Using Competitive Learning Techniques,"Jesse Ables, Thomas Kirby, Sudip Mittal, Ioana Banicescu, Shahram Rahimi, William Anderson, Maria Seale","The current state of the art systems in Artificial Intelligence (AI) enabled intrusion detection use a variety of black box methods. These black box methods are generally trained using Error Based Learning (EBL) techniques with a focus on creating accurate models. These models have high performative costs and are not easily explainable. A white box Competitive Learning (CL) based eXplainable Intrusion Detection System (X-IDS) offers a potential solution to these problem. CL models utilize an entirely different learning paradigm than EBL approaches. This different learning process makes the CL family of algorithms innately explainable and less resource intensive. In this paper, we create an X-IDS architecture that is based on DARPA's recommendation for explainable systems. In our architecture we leverage CL algorithms like, Self Organizing Maps (SOM), Growing Self Organizing Maps (GSOM), and Growing Hierarchical Self Organizing Map (GHSOM). The resulting models can be data-mined to create statistical and visual explanations. Our architecture is tested using NSL-KDD and CIC-IDS-2017 benchmark datasets, and produces accuracies that are 1% - 3% less than EBL models. However, CL models are much more explainable than EBL models. Additionally, we use a pruning process that is able to significantly reduce the size of these CL based models. By pruning our models, we are able to increase prediction speeds. Lastly, we analyze the statistical and visual explanations generated by our architecture, and we give a strategy that users could use to help navigate the set of explanations. These explanations will help users build trust with an Intrusion Detection System (IDS), and allow users to discover ways to increase the IDS's potency. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17387,March 2023,265,Human-artificial intelligence approaches for secure analysis in CAPTCHA codes,Nghia Dinh and Lidia Ogiela,CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart) has long been used to keep automated bots from misusing web services by leveraging human-artificial intelligence (HAI) interact...,https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-022-00134-9,12 December 2022,
266,0.000678989392134464,266,GAT-COBO: Cost-Sensitive Graph Neural Network for Telecom Fraud Detection,"Xinxin Hu, Haotian Chen, Junjie Zhang, Hongchang Chen, Shuxin Liu, Xing Li, Yahui Wang, Xiangyang Xue","Along with the rapid evolution of mobile communication technologies, such as 5G, there has been a drastically increase in telecom fraud, which significantly dissipates individual fortune and social wealth. In recent years, graph mining techniques are gradually becoming a mainstream solution for detecting telecom fraud. However, the graph imbalance problem, caused by the Pareto principle, brings severe challenges to graph data mining. This is a new and challenging problem, but little previous work has been noticed. In this paper, we propose a Graph ATtention network with COst-sensitive BOosting (GAT-COBO) for the graph imbalance problem. First, we design a GAT-based base classifier to learn the embeddings of all nodes in the graph. Then, we feed the embeddings into a well-designed cost-sensitive learner for imbalanced learning. Next, we update the weights according to the misclassification cost to make the model focus more on the minority class. Finally, we sum the node embeddings obtained by multiple cost-sensitive learners to obtain a comprehensive node representation, which is used for the downstream anomaly detection task. Extensive experiments on two real-world telecom fraud detection datasets demonstrate that our proposed method is effective for the graph imbalance problem, outperforming the state-of-the-art GNNs and GNN-based fraud detectors. In addition, our model is also helpful for solving the widespread over-smoothing problem in GNNs. The GAT-COBO code and datasets are available at https://github.com/xxhu94/GAT-COBO. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17334,March 2023,266,Computer-based technology and student engagement: a critical review of the literature,"Laura A. Schindler, Gary J. Burkholder, Osama A. Morad and Craig Marsh","Computer-based technology has infiltrated many aspects of life and industry, yet there is little understanding of how it can be used to promote student engagement, a concept receiving strong attention in highe...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-017-0063-0,2 October 2017,
267,0.000678989392134464,267,Investigating and Mitigating the Side Effects of Noisy Views in Multi-view Clustering in Practical Scenarios,"Jie Xu, Gang Niu, Xiaolong Wang, Yazhou Ren, Lei Feng, Xiaoshuang Shi, Heng Tao Shen, Xiaofeng Zhu","Multi-view clustering (MvC) aims at exploring the category structure among multi-view data without label supervision. Multiple views provide more information than single views and thus existing MvC methods can achieve satisfactory performance. However, their performance might seriously degenerate when the views are noisy in practical scenarios. In this paper, we first formally investigate the drawback of noisy views and then propose a theoretically grounded deep MvC method (namely MvCAN) to address this issue. Specifically, we propose a novel MvC objective that enables un-shared parameters and inconsistent clustering predictions across multiple views to reduce the side effects of noisy views. Furthermore, a non-parametric iterative process is designed to generate a robust learning target for mining multiple views' useful information. Theoretical analysis reveals that MvCAN works by achieving the multi-view consistency, complementarity, and noise robustness. Finally, experiments on public datasets demonstrate that MvCAN outperforms state-of-the-art methods and is robust against the existence of noisy views. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.17245,March 2023,267,Gait Recognition Using Image Self-Similarity,"Chiraz BenAbdelkader, Ross G Cutler and Larry S Davis","Gait is one of the few biometrics that can be measured at a distance, and is hence useful for passive surveillance as well as biometric applications. Gait recognition research is still at its infancy, however,...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704309236,21 April 2004,
268,0.000678989392134464,268,ALUM: Adversarial Data Uncertainty Modeling from Latent Model Uncertainty Compensation,"Wei Wei, Jiahuan Zhou, Hongze Li, Ying Wu","It is critical that the models pay attention not only to accuracy but also to the certainty of prediction. Uncertain predictions of deep models caused by noisy data raise significant concerns in trustworthy AI areas. To explore and handle uncertainty due to intrinsic data noise, we propose a novel method called ALUM to simultaneously handle the model uncertainty and data uncertainty in a unified scheme. Rather than solely modeling data uncertainty in the ultimate layer of a deep model based on randomly selected training data, we propose to explore mined adversarial triplets to facilitate data uncertainty modeling and non-parametric uncertainty estimations to compensate for the insufficiently trained latent model layers. Thus, the critical data uncertainty and model uncertainty caused by noisy data can be readily quantified for improving model robustness. Our proposed ALUM is model-agnostic which can be easily implemented into any existing deep model with little extra computation overhead. Extensive experiments on various noisy learning tasks validate the superior robustness and generalization ability of our method. The code is released at https://github.com/wwzjer/ALUM. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.16866,March 2023,268,ESICM LIVES 2022: part 2,Unknown,"This article is part of a Supplement:Volume 10
                                        Supplement 2",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-022-00469-0,19 October 2022,
269,0.000678989392134464,269,TraVaG: Differentially Private Trace Variant Generation Using GANs,"Majid Rafiei, Frederik Wangelik, Mahsa Pourbafrani, Wil M. P. van der Aalst","Process mining is rapidly growing in the industry. Consequently, privacy concerns regarding sensitive and private information included in event data, used by process mining algorithms, are becoming increasingly relevant. State-of-the-art research mainly focuses on providing privacy guarantees, e.g., differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques for releasing trace variants still do not fulfill all the requirements of industry-scale usage. Moreover, providing privacy guarantees when there exists a high rate of infrequent trace variants is still a challenge. In this paper, we introduce TraVaG as a new approach for releasing differentially private trace variants based on \text{Generative Adversarial Networks} (GANs) that provides industry-scale benefits and enhances the level of privacy guarantees when there exists a high ratio of infrequent variants. Moreover, TraVaG overcomes shortcomings of conventional privacy preservation techniques such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data show that our approach outperforms state-of-the-art techniques in terms of privacy guarantees, plain data utility preservation, and result utility preservation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.16704,March 2023,269,Early school entrance and middle-run academic performance in Mexico: evidence for 15-year-old students from the PISA test,Ernesto Aguayo-Téllez and Flor E. Martínez-Rodríguez,Using an unanticipated policy reform in Mexico that shifted 4 months the cutoff date for elementary school eligibility in 2006 and information on academic performance of 15-year-old students from the Programme...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00089-8,27 September 2020,
270,0.000678989392134464,270,Oracle Counterpoint: Relationships between On-chain and Off-chain Market Data,"Zhimeng Yang, Ariah Klages-Mundt, Lewis Gudgeon","We investigate the theoretical and empirical relationships between activity in on-chain markets and pricing in off-chain cryptocurrency markets (e.g., ETH/USD prices). The motivation is to develop methods for proxying off-chain market data using data and computation that is in principle verifiable on-chain and could provide an alternative approach to blockchain price oracles. We explore relationships in PoW mining, PoS validation, block space markets, network decentralization, usage and monetary velocity, and on-chain liquidity pools and AMMs. We select key features from these markets, which we analyze through graphical models, mutual information, and ensemble machine learning models to explore the degree to which off-chain pricing information can be recovered entirely on-chain. We find that a large amount of pricing information is contained in on-chain data, but that it is generally hard to recover precise prices except on short time scales of retraining the model. We discuss how even a noisy trustless data source such as this can be helpful toward minimizing trust requirements of oracle designs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.16331,March 2023,270,Algorithmic criminology,Richard Berk,Computational criminology has been seen primarily as computer-intensive simulations of criminal wrongdoing. But there is a growing menu of computer-intensive applications in criminology that one might call “co...,https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-2-5,23 January 2013,
271,0.000678989392134464,271,TraffNet: Learning Causality of Traffic Generation for Road Network Digital Twins,"Ming Xu, Yunyi Ma, Ruimin Li, Geqi Qi, Xiangfu Meng, Haibo Jin","Road network digital twins (RNDTs) play a critical role in the development of next-generation intelligent transportation systems, enabling more precise traffic planning and control. To support just-in-time (JIT) decision making, RNDTs require a model that dynamically learns the traffic patterns from online sensor data and generates high-fidelity simulation results. Although current traffic prediction techniques based on graph neural networks have achieved state-of-the-art performance, these techniques only predict future traffic by mining correlations in historical traffic data, disregarding the causes of traffic generation, such as Origin-Destination (OD) demands and route selection. Therefore, their performance is unreliable for JIT decision making. To fill this gap, we introduce a novel deep learning framework called TraffNet that learns the causality of traffic volumes from vehicle trajectory data. First, we use a heterogeneous graph to represent the road network, allowing the model to incorporate causal features of traffic volumes. Next, inspired by the traffic domain knowledge, we propose a traffic causality learning method to learn an embedding vector that encodes OD demands and path-level dependencies for each road segment. Then, we model temporal dependencies to match the underlying process of traffic generation. Finally, the experiments verify the utility of TraffNet. The code of TraffNet is available at https://github.com/mayunyi-1999/TraffNet_code.git. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15954,March 2023,271,A contextualised Learning Interaction Memory,"Sandra A. Siebra, Ana Carolina Salgado and Patrícia A. Tedesco",The interaction is the key element used in Collaborative Learning Environments to understand the process of knowledge building and the role played by each student in it. Interaction analysis can provide suppor...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192545,September 2007,
272,0.000678989392134464,272,Make the Most Out of Your Net: Alternating Between Canonical and Hard Datasets for Improved Image Demosaicing,"Yuval Becker, Raz Z. Nossek, Tomer Peleg","Image demosaicing is an important step in the image processing pipeline for digital cameras, and it is one of the many tasks within the field of image restoration. A well-known characteristic of natural images is that most patches are smooth, while high-content patches like textures or repetitive patterns are much rarer, which results in a long-tailed distribution. This distribution can create an inductive bias when training machine learning algorithms for image restoration tasks and for image demosaicing in particular. There have been many different approaches to address this challenge, such as utilizing specific losses or designing special network architectures. What makes our work is unique in that it tackles the problem from a training protocol perspective. Our proposed training regime consists of two key steps. The first step is a data-mining stage where sub-categories are created and then refined through an elimination process to only retain the most helpful sub-categories. The second step is a cyclic training process where the neural network is trained on both the mined sub-categories and the original dataset. We have conducted various experiments to demonstrate the effectiveness of our training method for the image demosaicing task. Our results show that this method outperforms standard training across a range of architecture sizes and types, including CNNs and Transformers. Moreover, we are able to achieve state-of-the-art results with a significantly smaller neural network, compared to previous state-of-the-art methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15792,March 2023,272,Authentication Based on Multilayer Clustering in Ad Hoc Networks,"Keun-Ho Lee, Sang-Bum Han, Heyi-Sook Suh, Sang Keun Lee and Chong-Sun Hwang","In this paper, we describe a secure cluster-routing protocol based on a multilayer scheme in ad hoc networks. This work provides scalable, threshold authentication scheme in ad hoc networks. We present detaile...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN.2005.731,12 December 2005,
273,0.000678989392134464,273,Imbalance Knowledge-Driven Multi-modal Network for Land-Cover Semantic Segmentation Using Images and LiDAR Point Clouds,"Yameng Wang, Yi Wan, Yongjun Zhang, Bin Zhang, Zhi Gao","Despite the good results that have been achieved in unimodal segmentation, the inherent limitations of individual data increase the difficulty of achieving breakthroughs in performance. For that reason, multi-modal learning is increasingly being explored within the field of remote sensing. The present multi-modal methods usually map high-dimensional features to low-dimensional spaces as a preprocess before feature extraction to address the nonnegligible domain gap, which inevitably leads to information loss. To address this issue, in this paper we present our novel Imbalance Knowledge-Driven Multi-modal Network (IKD-Net) to extract features from raw multi-modal heterogeneous data directly. IKD-Net is capable of mining imbalance information across modalities while utilizing a strong modal to drive the feature map refinement of the weaker ones in the global and categorical perspectives by way of two sophisticated plug-and-play modules: the Global Knowledge-Guided (GKG) and Class Knowledge-Guided (CKG) gated modules. The whole network then is optimized using a holistic loss function. While we were developing IKD-Net, we also established a new dataset called the National Agriculture Imagery Program and 3D Elevation Program Combined dataset in California (N3C-California), which provides a particular benchmark for multi-modal joint segmentation tasks. In our experiments, IKD-Net outperformed the benchmarks and state-of-the-art methods both in the N3C-California and the small-scale ISPRS Vaihingen dataset. IKD-Net has been ranked first on the real-time leaderboard for the GRSS DFC 2018 challenge evaluation until this paper's submission. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15777,March 2023,273,Exploring the behavioral patterns of Co-regulation in mobile computer-supported collaborative learning,Lanqin Zheng and Junhui Yu,This study examined the behavioral patterns of co-regulation in a mobile computer-supported collaborative learning context. Participants in this study included 101 undergraduate students majoring in law or Chi...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-016-0024-4,19 January 2016,
274,0.000678989392134464,274,Predicting Thermoelectric Power Factor of Bismuth Telluride During Laser Powder Bed Fusion Additive Manufacturing,"Ankita Agarwal, Tanvi Banerjee, Joy Gockel, Saniya LeBlanc, Joe Walker, John Middendorf","An additive manufacturing (AM) process, like laser powder bed fusion, allows for the fabrication of objects by spreading and melting powder in layers until a freeform part shape is created. In order to improve the properties of the material involved in the AM process, it is important to predict the material characterization property as a function of the processing conditions. In thermoelectric materials, the power factor is a measure of how efficiently the material can convert heat to electricity. While earlier works have predicted the material characterization properties of different thermoelectric materials using various techniques, implementation of machine learning models to predict the power factor of bismuth telluride (Bi2Te3) during the AM process has not been explored. This is important as Bi2Te3 is a standard material for low temperature applications. Thus, we used data about manufacturing processing parameters involved and in-situ sensor monitoring data collected during AM of Bi2Te3, to train different machine learning models in order to predict its thermoelectric power factor. We implemented supervised machine learning techniques using 80% training and 20% test data and further used the permutation feature importance method to identify important processing parameters and in-situ sensor features which were best at predicting power factor of the material. Ensemble-based methods like random forest, AdaBoost classifier, and bagging classifier performed the best in predicting power factor with the highest accuracy of 90% achieved by the bagging classifier model. Additionally, we found the top 15 processing parameters and in-situ sensor features to characterize the material manufacturing property like power factor. These features could further be optimized to maximize power factor of the thermoelectric material and improve the quality of the products built using this material. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15663,March 2023,274,Teaching computing for complex problems in civil engineering and geosciences using big data and machine learning: synergizing four different computing paradigms and four different management domains,"Zoran Babović, Branislav Bajat, Dusan Barac, Vesna Bengin, Vladan Đokić, Filip Đorđević, Dražen Drašković, Nenad Filipović, Stephan French, Borko Furht, Marija Ilić, Ayhan Irfanoglu, Aleksandar Kartelj, Milan Kilibarda, Gerhard Klimeck, Nenad Korolija…","This article describes a teaching strategy that synergizes computing and management, aimed at the running of complex projects in industry and academia, in the areas of civil engineering, physics, geosciences, ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00730-7,31 May 2023,
275,0.000678989392134464,275,RNAprofiling 2.0: Enhanced cluster analysis of structural ensembles,"Forrest Hurley, Christine Heitsch","Understanding the base pairing of an RNA sequence provides insight into its molecular structure.By mining suboptimal sampling data, RNAprofiling 1.0 identifies the dominant helices in low-energy secondary structures as features, organizes them into profiles which partition the Boltzmann sample, and highlights key similarities/differences among the most informative, i.e. selected, profiles in a graphical format. Version 2.0 enhances every step of this approach. First, the featured substructures are expanded from helices to stems. Second, profile selection includes low-frequency pairings similar to featured ones. In conjunction, these updates extend the utility of the method to sequences up to length 600, as evaluated over a sizable dataset. Third, relationships are visualized in a decision tree which highlights the most important structural differences. Finally, this cluster analysis is made accessible to experimental researchers in a portable format as an interactive webpage, permitting a much greater understanding of trade-offs among different possible base pairing combinations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15552,March 2023,275,Impact of impulsive detoxication on the spread of computer virus,"Xianxiu Zhang, Chuandong Li and Tingwen Huang","We discuss the dynamical properties of a SIRS computer virus propagation model with impulsive detoxication and saturation effect. By the Internet, new antivirus software can be released immediately and take ef...",https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-016-0944-x,25 August 2016,
276,0.000678989392134464,276,Unimodal Training-Multimodal Prediction: Cross-modal Federated Learning with Hierarchical Aggregation,"Rongyu Zhang, Xiaowei Chi, Guiliang Liu, Wenyi Zhang, Yuan Du, Fangxin Wang","Multimodal learning has seen great success mining data features from multiple modalities with remarkable model performance improvement. Meanwhile, federated learning (FL) addresses the data sharing problem, enabling privacy-preserved collaborative training to provide sufficient precious data. Great potential, therefore, arises with the confluence of them, known as multimodal federated learning. However, limitation lies in the predominant approaches as they often assume that each local dataset records samples from all modalities. In this paper, we aim to bridge this gap by proposing an Unimodal Training - Multimodal Prediction (UTMP) framework under the context of multimodal federated learning. We design HA-Fedformer, a novel transformer-based model that empowers unimodal training with only a unimodal dataset at the client and multimodal testing by aggregating multiple clients' knowledge for better accuracy. The key advantages are twofold. Firstly, to alleviate the impact of data non-IID, we develop an uncertainty-aware aggregation method for the local encoders with layer-wise Markov Chain Monte Carlo sampling. Secondly, to overcome the challenge of unaligned language sequence, we implement a cross-modal decoder aggregation to capture the hidden signal correlation between decoders trained by data from different modalities. Our experiments on popular sentiment analysis benchmarks, CMU-MOSI and CMU-MOSEI, demonstrate that HA-Fedformer significantly outperforms state-of-the-art multimodal models under the UTMP federated learning frameworks, with 15%-20% improvement on most attributes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15486,March 2023,276,Bibliometric review of visual computing in the construction industry,"Heng-Wei Wang, Zhen-Zhong Hu and Jia-Rui Lin","In the construction area, visuals such as drawings, photos, videos, and 3D models, play a significant role in the design, build and maintenance of a facility, bringing efficiency to generate, transfer, and sto...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00050-0,8 June 2020,
277,0.000678989392134464,277,ByteTrackV2: 2D and 3D Multi-Object Tracking by Associating Every Detection Box,"Yifu Zhang, Xinggang Wang, Xiaoqing Ye, Wei Zhang, Jincheng Lu, Xiao Tan, Errui Ding, Peize Sun, Jingdong Wang","Multi-object tracking (MOT) aims at estimating bounding boxes and identities of objects across video frames. Detection boxes serve as the basis of both 2D and 3D MOT. The inevitable changing of detection scores leads to object missing after tracking. We propose a hierarchical data association strategy to mine the true objects in low-score detection boxes, which alleviates the problems of object missing and fragmented trajectories. The simple and generic data association strategy shows effectiveness under both 2D and 3D settings. In 3D scenarios, it is much easier for the tracker to predict object velocities in the world coordinate. We propose a complementary motion prediction strategy that incorporates the detected velocities with a Kalman filter to address the problem of abrupt motion and short-term disappearing. ByteTrackV2 leads the nuScenes 3D MOT leaderboard in both camera (56.4% AMOTA) and LiDAR (70.1% AMOTA) modalities. Furthermore, it is nonparametric and can be integrated with various detectors, making it appealing in real applications. The source code is released at https://github.com/ifzhang/ByteTrack-V2. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15334,March 2023,277,Computer-aided detection of breast lesions in DCE-MRI using region growing based on fuzzy C-means clustering and vesselness filter,"Shahriar B. Shokouhi, Aida Fooladivanda and Nasrin Ahmadinejad",A computer-aided detection (CAD) system is introduced in this paper for detection of breast lesions in dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI). The proposed CAD system firstly compensate...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-017-0476-x,25 May 2017,
278,0.000678989392134464,278,Multi-Granularity Archaeological Dating of Chinese Bronze Dings Based on a Knowledge-Guided Relation Graph,"Rixin Zhou, Jiafu Wei, Qian Zhang, Ruihua Qi, Xi Yang, Chuntao Li","The archaeological dating of bronze dings has played a critical role in the study of ancient Chinese history. Current archaeology depends on trained experts to carry out bronze dating, which is time-consuming and labor-intensive. For such dating, in this study, we propose a learning-based approach to integrate advanced deep learning techniques and archaeological knowledge. To achieve this, we first collect a large-scale image dataset of bronze dings, which contains richer attribute information than other existing fine-grained datasets. Second, we introduce a multihead classifier and a knowledge-guided relation graph to mine the relationship between attributes and the ding era. Third, we conduct comparison experiments with various existing methods, the results of which show that our dating method achieves a state-of-the-art performance. We hope that our data and applied networks will enrich fine-grained classification research relevant to other interdisciplinary areas of expertise. The dataset and source code used are included in our supplementary materials, and will be open after submission owing to the anonymity policy. Source codes and data are available at: https://github.com/zhourixin/bronze-Ding. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15266,March 2023,278,A road map for computational surgery: challenges and opportunities,Barbara Lee Bass and Marc Garbey,This paper introduces the fundamental concepts of computational surgery by Garbey et al. and proposes a road map for progress in this new multidisciplinary field of applied investigation. Recognizing this intr...,https://www.springeropen.com//computationalsurgery.springeropen.com/articles/10.1186/2194-3990-1-2,10 January 2014,
279,0.000678989392134464,279,An End-to-End Framework For Universal Lesion Detection With Missing Annotations,"Xiaoyu Bai, Yong Xia","Fully annotated large-scale medical image datasets are highly valuable. However, because labeling medical images is tedious and requires specialized knowledge, the large-scale datasets available often have missing annotation issues. For instance, DeepLesion, a large-scale CT image dataset with labels for various kinds of lesions, is reported to have a missing annotation rate of 50\%. Directly training a lesion detector on it would suffer from false negative supervision caused by unannotated lesions. To address this issue, previous works have used sophisticated multi-stage strategies to switch between lesion mining and detector training. In this work, we present a novel end-to-end framework for mining unlabeled lesions while simultaneously training the detector. Our framework follows the teacher-student paradigm. In each iteration, the teacher model infers the input data and creates a set of predictions. High-confidence predictions are combined with partially-labeled ground truth for training the student model. On the DeepLesion dataset, using the original partially labeled training set, our model can outperform all other more complicated methods and surpass the previous best method by 2.3\% on average sensitivity and 2.7\% on average precision, achieving state-of-the-art universal lesion detection results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.15024,March 2023,279,Adaptive software systems,Dilma M. da Silva and Fabio Kon,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192349,February 2004,
280,0.000678989392134464,280,Lexicon-Enhanced Self-Supervised Training for Multilingual Dense Retrieval,"Houxing Ren, Linjun Shou, Jian Pei, Ning Wu, Ming Gong, Daxin Jiang","Recent multilingual pre-trained models have shown better performance in various multilingual tasks. However, these models perform poorly on multilingual retrieval tasks due to lacking multilingual training data. In this paper, we propose to mine and generate self-supervised training data based on a large-scale unlabeled corpus. We carefully design a mining method which combines the sparse and dense models to mine the relevance of unlabeled queries and passages. And we introduce a query generator to generate more queries in target languages for unlabeled passages. Through extensive experiments on Mr. TYDI dataset and an industrial dataset from a commercial search engine, we demonstrate that our method performs better than baselines based on various pre-trained multilingual models. Our method even achieves on-par performance with the supervised method on the latter dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14979,March 2023,280,Comparison of ablation defect on MR imaging with computer simulation estimated treatment zone following irreversible electroporation of patient prostate,"Govindarajan Srimathveeravalli, Francois Cornelis, Joseph Mashni, Haruyuki Takaki, Jeremy C. Durack, Stephen B. Solomon and Jonathan A. Coleman",To determine whether patient specific numerical simulations of irreversible electroporation (IRE) of the prostate correlates with the treatment effect seen on follow-up MR imaging. Computer models were created...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/s40064-016-1879-0,29 February 2016,
281,0.000678989392134464,281,A Lot of Talk and a Badge: An Empirical Analysis of Personal Achievements in GitHub,"Fabio Calefato, Luigi Quaranta, Filippo Lanubile","GitHub has introduced gamification via personal achievements, whereby badges are unlocked and displayed on developers' personal profile pages in recognition of their development activities. In this paper, we present a mixed-methods empirical investigation to study the diffusion of personal badges in GitHub in addition to the effects of and the reactions to their introduction. First, we conducted an observational study by mining longitudinal data for over 6,000 developers and performed correlation as well as regression analysis. Then, we analyzed 33 answers to a survey and 312 GITHUB community discussions about personal badges to gauge how the community reacted to the introduction of the new feature. We found that most of the sampled developers own at least a badge, but we also observed an increasing number of users who choose to keep their profile private and opt out from displaying badges. Besides, badges are in general poorly correlated with developers' qualities and dispositions such as timeliness and desire to collaborate. We also found that, with the exception of the Starstruck badge and the number of followers, their introduction to GitHub had no effects. Finally, the reaction of the community has been in general mixed, as developers find them appealing in principle but without a clear purpose and hardly reflecting their abilities in the current form. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14702,March 2023,281,Turn off the graphics: designing non-visual interfaces for mobile phone games,"Luis Valente, Clarisse Sieckenius de Souza and Bruno Feijó","Mobile phones are a widespread platform for ICT applications because they are highly pervasive in contemporary society. Hence, we can think of mobile gaming as a serious candidate to being a prominent form of ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192576,March 2009,
282,0.000678989392134464,282,Targeted Mining of Top-k High Utility Itemsets,"Shan Huang, Wensheng Gan, Jinbao Miao, Xuming Han, Philippe Fournier-Viger","Finding high-importance patterns in data is an emerging data mining task known as High-utility itemset mining (HUIM). Given a minimum utility threshold, a HUIM algorithm extracts all the high-utility itemsets (HUIs) whose utility values are not less than the threshold. This can reveal a wealth of useful information, but the precise needs of users are not well taken into account. In particular, users often want to focus on patterns that have some specific items rather than find all patterns. To overcome that difficulty, targeted mining has emerged, focusing on user preferences, but only preliminary work has been conducted. For example, the targeted high-utility itemset querying algorithm (TargetUM) was proposed, which uses a lexicographic tree to query itemsets containing a target pattern. However, selecting the minimum utility threshold is difficult when the user is not familiar with the processed database. As a solution, this paper formulates the task of targeted mining of the top-k high-utility itemsets and proposes an efficient algorithm called TMKU based on the TargetUM algorithm to discover the top-k target high-utility itemsets (top-k THUIs). At the same time, several pruning strategies are used to reduce memory consumption and execution time. Extensive experiments show that the proposed TMKU algorithm has good performance on real and synthetic datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14510,March 2023,282,Smart computational light microscopes (SCLMs) of smart computational imaging laboratory (SCILab),"Yao Fan, Jiaji Li, Linpeng Lu, Jiasong Sun, Yan Hu, Jialin Zhang, Zhuoshi Li, Qian Shen, Bowen Wang, Runnan Zhang, Qian Chen and Chao Zuo","Computational microscopy, as a subfield of computational imaging, combines optical manipulation and image algorithmic reconstruction to recover multi-dimensional microscopic images or information of micro-obje...",https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-021-00040-2,3 September 2021,
283,0.000678989392134464,283,"Informed Machine Learning, Centrality, CNN, Relevant Document Detection, Repatriation of Indigenous Human Remains","Md Abul Bashar, Richi Nayak, Gareth Knapman, Paul Turnbull, Cressida Fforde","Among the pressing issues facing Australian and other First Nations peoples is the repatriation of the bodily remains of their ancestors, which are currently held in Western scientific institutions. The success of securing the return of these remains to their communities for reburial depends largely on locating information within scientific and other literature published between 1790 and 1970 documenting their theft, donation, sale, or exchange between institutions. This article reports on collaborative research by data scientists and social science researchers in the Research, Reconcile, Renew Network (RRR) to develop and apply text mining techniques to identify this vital information. We describe our work to date on developing a machine learning-based solution to automate the process of finding and semantically analysing relevant texts. Classification models, particularly deep learning-based models, are known to have low accuracy when trained with small amounts of labelled (i.e. relevant/non-relevant) documents. To improve the accuracy of our detection model, we explore the use of an Informed Neural Network (INN) model that describes documentary content using expert-informed contextual knowledge. Only a few labelled documents are used to provide specificity to the model, using conceptually related keywords identified by RRR experts in provenance research. The results confirm the value of using an INN network model for identifying relevant documents related to the investigation of the global commercial trade in Indigenous human remains. Empirical analysis suggests that this INN model can be generalized for use by other researchers in the social sciences and humanities who want to extract relevant information from large textual corpora. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14475,March 2023,283,Skills-approximate occupations: using networks to guide jobs retraining,Keith Waters and Shade T. Shutters,"An issue often confronting economic development agencies is how to minimize unemployment due to disruptions like technological change, trade wars, recessions, or other economic shocks. Decision makers are left...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-022-00487-7,28 June 2022,
284,0.000678989392134464,284,Optimizing Sepsis Care through Heuristics Methods in Process Mining: A Trajectory Analysis,"Alireza Bakhshi, Erfan Hassannayebi, Amir Hossein Sadeghi","Process mining can help acquire insightful knowledge and heighten the system's performance. In this study, we surveyed the trajectories of 1050 sepsis patients in a regional hospital in the Netherlands from the registration to the discharge phase. Based on this real-world case study, the event log comprises events and activities related to the emergency ward, admission to hospital wards, and discharge enriched with data from lab experiments and triage checklists. At first, we aim to discover this process through Heuristics Miner (HM) and Inductive Miner (IM) methods. Then, we analyze a systematic process model based on organizational information and knowledge. Besides, we address conformance checking given medical guidelines for these patients and monitor the related flows on the systematic process model. The results show that HM and IM are inadequate in identifying the relevant process. However, using a systematic process model based on expert knowledge and organizational information resulted in an average fitness of 97.8%, a simplicity of 77.7%, and a generalization of 80.2%. The analyses demonstrate that process mining can shed light on the patient flow in the hospital and inspect the day-to-day clinical performance versus medical guidelines. Also, the process models obtained by the HM and IM methods cannot provide a concrete comprehension of the process structure for stakeholders compared to the systematic process model. The implications of our findings include the potential for process mining to improve the quality of healthcare services, optimize resource allocation, and reduce costs. Our study also highlights the importance of considering expert knowledge and organizational information in developing effective process models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14328,March 2023,284,AI literacy in K-12: a systematic literature review,"Lorena Casal-Otero, Alejandro Catala, Carmen Fernández-Morante, Maria Taboada, Beatriz Cebreiro and Senén Barro","The successful irruption of AI-based technology in our daily lives has led to a growing educational, social, and political interest in training citizens in AI. Education systems now need to train students at t...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00418-7,19 April 2023,
285,0.000678989392134464,285,"PENTACET data -- 23 Million Contextual Code Comments and 500,000 SATD comments","Murali Sridharan, Leevi Rantala, Mika Mäntylä","Most Self-Admitted Technical Debt (SATD) research utilizes explicit SATD features such as 'TODO' and 'FIXME' for SATD detection. A closer look reveals several SATD research uses simple SATD ('Easy to Find') code comments without the contextual data (preceding and succeeding source code context). This work addresses this gap through PENTACET (or 5C dataset) data. PENTACET is a large Curated Contextual Code Comments per Contributor and the most extensive SATD data. We mine 9,096 Open Source Software Java projects with a total of 435 million LOC. The outcome is a dataset with 23 million code comments, preceding and succeeding source code context for each comment, and more than 500,000 comments labeled as SATD, including both 'Easy to Find' and 'Hard to Find' SATD. We believe PENTACET data will further SATD research using Artificial Intelligence techniques. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.14029,March 2023,285,Correction to: Recent developments in photoacoustic imaging and sensing for nondestructive testing and evaluation,Sung-Liang Chen and Chao Tian,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00077-x,29 April 2021,
286,0.000678989392134464,286,Few Shot Medical Image Segmentation with Cross Attention Transformer,"Yi Lin, Yufan Chen, Kwang-Ting Cheng, Hao Chen","Medical image segmentation has made significant progress in recent years. Deep learning-based methods are recognized as data-hungry techniques, requiring large amounts of data with manual annotations. However, manual annotation is expensive in the field of medical image analysis, which requires domain-specific expertise. To address this challenge, few-shot learning has the potential to learn new classes from only a few examples. In this work, we propose a novel framework for few-shot medical image segmentation, termed CAT-Net, based on cross masked attention Transformer. Our proposed network mines the correlations between the support image and query image, limiting them to focus only on useful foreground information and boosting the representation capacity of both the support prototype and query features. We further design an iterative refinement framework that refines the query image segmentation iteratively and promotes the support feature in turn. We validated the proposed method on three public datasets: Abd-CT, Abd-MRI, and Card-MRI. Experimental results demonstrate the superior performance of our method compared to state-of-the-art methods and the effectiveness of each component. we will release the source codes of our method upon acceptance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13867,March 2023,286,Correction to: Classes of tree-based networks,"Mareike Fischer, Lina Herbst, Michelle Galla, Yangjing Long and Kristina Wicke",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00069-x,26 January 2021,
287,0.000678989392134464,287,Two-level Graph Network for Few-Shot Class-Incremental Learning,"Hao Chen, Linyan Li, Fan Lyu, Fuyuan Hu, Zhenping Xia, Fenglei Xu","Few-shot class-incremental learning (FSCIL) aims to design machine learning algorithms that can continually learn new concepts from a few data points, without forgetting knowledge of old classes. The difficulty lies in that limited data from new classes not only lead to significant overfitting issues but also exacerbates the notorious catastrophic forgetting problems. However, existing FSCIL methods ignore the semantic relationships between sample-level and class-level. % Using the advantage that graph neural network (GNN) can mine rich information among few samples, In this paper, we designed a two-level graph network for FSCIL named Sample-level and Class-level Graph Neural Network (SCGN). Specifically, a pseudo incremental learning paradigm is designed in SCGN, which synthesizes virtual few-shot tasks as new tasks to optimize SCGN model parameters in advance. Sample-level graph network uses the relationship of a few samples to aggregate similar samples and obtains refined class-level features. Class-level graph network aims to mitigate the semantic conflict between prototype features of new classes and old classes. SCGN builds two-level graph networks to guarantee the latent semantic of each few-shot class can be effectively represented in FSCIL. Experiments on three popular benchmark datasets show that our method significantly outperforms the baselines and sets new state-of-the-art results with remarkable advantages. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13862,March 2023,287,The use of learning dashboards to support complex in-class pedagogical scenarios in medical training: how do they influence students’ cognitive engagement?,Bas de Leng and Friedrich Pawelka,This study aims to contribute to empirical and interdisciplinary knowledge on how visual learning analytics tools can support students’ cognitive engagement in complex in-class scenarios. Taking a holistic app...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-020-00135-7,11 June 2020,
288,0.000678989392134464,288,Human Behavior in the Time of COVID-19: Learning from Big Data,"Hanjia Lyu, Arsal Imtiaz, Yufei Zhao, Jiebo Luo","Since the World Health Organization (WHO) characterized COVID-19 as a pandemic in March 2020, there have been over 600 million confirmed cases of COVID-19 and more than six million deaths as of October 2022. The relationship between the COVID-19 pandemic and human behavior is complicated. On one hand, human behavior is found to shape the spread of the disease. On the other hand, the pandemic has impacted and even changed human behavior in almost every aspect. To provide a holistic understanding of the complex interplay between human behavior and the COVID-19 pandemic, researchers have been employing big data techniques such as natural language processing, computer vision, audio signal processing, frequent pattern mining, and machine learning. In this study, we present an overview of the existing studies on using big data techniques to study human behavior in the time of the COVID-19 pandemic. In particular, we categorize these studies into three groups - using big data to measure, model, and leverage human behavior, respectively. The related tasks, data, and methods are summarized accordingly. To provide more insights into how to fight the COVID-19 pandemic and future global catastrophes, we further discuss challenges and potential opportunities. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13452,March 2023,288,Undergraduates’ awareness of White and male privilege in STEM,"Melissa Dancy, Katherine Rainey, Elizabeth Stearns, Roslyn Mickelson and Stephanie Moller","It is well-documented that experiences in STEM courses for women and students of color are different from the experiences of White men. As part of a larger interview study, 183 college seniors from diverse gen...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00250-3,12 October 2020,
289,0.000678989392134464,289,Machine learning-enabled tomographic imaging of chemical short-range atomic ordering,"Yue Li, Timoteo Colnaghi, Yilun Gong, Huaide Zhang, Yuan Yu, Ye Wei, Bin Gan, Min Song, Andreas Marek, Markus Rampp, Siyuan Zhang, Zongrui Pei, Matthias Wuttig, Jörg Neugebauer, Zhangwei Wang, Baptiste Gault","In solids, chemical short-range order (CSRO) refers to the self-organisation of atoms of certain species occupying specific crystal sites. CSRO is increasingly being envisaged as a lever to tailor the mechanical and functional properties of materials. Yet quantitative relationships between properties and the morphology, number density, and atomic configurations of CSRO domains remain elusive. Herein, we showcase how machine learning-enhanced atom probe tomography (APT) can mine the near-atomically resolved APT data and jointly exploit the technique's high elemental sensitivity to provide a 3D quantitative analysis of CSRO in a CoCrNi medium-entropy alloy. We reveal multiple CSRO configurations, with their formation supported by state-of-the-art Monte-Carlo simulations. Quantitative analysis of these CSROs allows us to establish relationships between processing parameters and physical properties. The unambiguous characterization of CSRO will help refine strategies for designing advanced materials by manipulating atomic-scale architectures. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13433,March 2023,289,Communication and leadership differences in virtual design teams: Why some teams do better than others,Jerry Fjermestad and Rosalie J. Ocker,Organizations in today’s global economy face continual pressures to remain responsive to changes in the competitive marketplace. One way that firms have adapted to these pressures is to use web-based communica...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192544,September 2007,
290,0.000678989392134464,290,Extended High Utility Pattern Mining: An Answer Set Programming Based Framework and Applications,"Francesco Cauteruccio, Giorgio Terracina","Detecting sets of relevant patterns from a given dataset is an important challenge in data mining. The relevance of a pattern, also called utility in the literature, is a subjective measure and can be actually assessed from very different points of view. Rule-based languages like Answer Set Programming (ASP) seem well suited for specifying user-provided criteria to assess pattern utility in a form of constraints; moreover, declarativity of ASP allows for a very easy switch between several criteria in order to analyze the dataset from different points of view. In this paper, we make steps toward extending the notion of High Utility Pattern Mining (HUPM); in particular we introduce a new framework that allows for new classes of utility criteria not considered in the previous literature. We also show how recent extensions of ASP with external functions can support a fast and effective encoding and testing of the new framework. To demonstrate the potential of the proposed framework, we exploit it as a building block for the definition of an innovative method for predicting ICU admission for COVID-19 patients. Finally, an extensive experimental activity demonstrates both from a quantitative and a qualitative point of view the effectiveness of the proposed approach. Under consideration in Theory and Practice of Logic Programming (TPLP) △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13191,March 2023,290,Wireless Network Security,"Yang Xiao, Yi-Bing Lin and Ding-Zhu Du",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN/2006/48374,28 January 2007,
291,0.000678989392134464,291,PointGame: Geometrically and Adaptively Masked Auto-Encoder on Point Clouds,"Yun Liu, Xuefeng Yan, Zhilei Chen, Zhiqi Li, Zeyong Wei, Mingqiang Wei","Self-supervised learning is attracting large attention in point cloud understanding. However, exploring discriminative and transferable features still remains challenging due to their nature of irregularity and sparsity. We propose a geometrically and adaptively masked auto-encoder for self-supervised learning on point clouds, termed \textit{PointGame}. PointGame contains two core components: GATE and EAT. GATE stands for the geometrical and adaptive token embedding module; it not only absorbs the conventional wisdom of geometric descriptors that captures the surface shape effectively, but also exploits adaptive saliency to focus on the salient part of a point cloud. EAT stands for the external attention-based Transformer encoder with linear computational complexity, which increases the efficiency of the whole pipeline. Unlike cutting-edge unsupervised learning models, PointGame leverages geometric descriptors to perceive surface shapes and adaptively mines discriminative features from training data. PointGame showcases clear advantages over its competitors on various downstream tasks under both global and local fine-tuning strategies. The code and pre-trained models will be publicly available. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.13100,March 2023,291,Content-Based Object Movie Retrieval and Relevance Feedbacks,"Cheng-Chieh Chiang, Li-Wei Chan, Yi-Ping Hung and Greg C. Lee",Object movie refers to a set of images captured from different perspectives around a 3D object. Object movie provides a good representation of a physical object because it can provide 3D interactive viewing ef...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/89691,1 December 2007,
292,0.000678989392134464,292,TSI-GAN: Unsupervised Time Series Anomaly Detection using Convolutional Cycle-Consistent Generative Adversarial Networks,"Shyam Sundar Saravanan, Tie Luo, Mao Van Ngo","Anomaly detection is widely used in network intrusion detection, autonomous driving, medical diagnosis, credit card frauds, etc. However, several key challenges remain open, such as lack of ground truth labels, presence of complex temporal patterns, and generalizing over different datasets. This paper proposes TSI-GAN, an unsupervised anomaly detection model for time-series that can learn complex temporal patterns automatically and generalize well, i.e., no need for choosing dataset-specific parameters, making statistical assumptions about underlying data, or changing model architectures. To achieve these goals, we convert each input time-series into a sequence of 2D images using two encoding techniques with the intent of capturing temporal patterns and various types of deviance. Moreover, we design a reconstructive GAN that uses convolutional layers in an encoder-decoder network and employs cycle-consistency loss during training to ensure that inverse mappings are accurate as well. In addition, we also instrument a Hodrick-Prescott filter in post-processing to mitigate false positives. We evaluate TSI-GAN using 250 well-curated and harder-than-usual datasets and compare with 8 state-of-the-art baseline methods. The results demonstrate the superiority of TSI-GAN to all the baselines, offering an overall performance improvement of 13% and 31% over the second-best performer MERLIN and the third-best performer LSTM-AE, respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.12952,March 2023,292,The engagement of students when learning to use a personal audio classifier to control robot cars in a computational thinking board game,Ting-Chia Hsu and Mu-Sheng Chen,"This research explored the creative thinking, learning achievement, and engagement of students when they integrated the application of the personal audio classifier (PAC) into the competition of a computationa...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00202-1,16 July 2022,
293,0.000678989392134464,293,AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection,"Puning Yang, Jian Liang, Jie Cao, Ran He","Out-of-distribution (OOD) detection is a crucial aspect of deploying machine learning models in open-world applications. Empirical evidence suggests that training with auxiliary outliers substantially improves OOD detection. However, such outliers typically exhibit a distribution gap compared to the test OOD data and do not cover all possible test OOD scenarios. Additionally, incorporating these outliers introduces additional training burdens. In this paper, we introduce a novel paradigm called test-time OOD detection, which utilizes unlabeled online data directly at test time to improve OOD detection performance. While this paradigm is efficient, it also presents challenges such as catastrophic forgetting. To address these challenges, we propose adaptive outlier optimization (AUTO), which consists of an in-out-aware filter, an ID memory bank, and a semantically-consistent objective. AUTO adaptively mines pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize networks in real time during inference. Extensive results on CIFAR-10, CIFAR-100, and ImageNet benchmarks demonstrate that AUTO significantly enhances OOD detection performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.12267,March 2023,293,A literature review and integrated framework for the determinants of crowdfunding success,"Lingfei Deng, Qiang Ye, DaPeng Xu, Wenjun Sun and Guangxin Jiang","Crowdfunding is an innovative strategy for financing a new business venture from the general public instead of seeking funds in traditional ways, such as issuing bonds or bank lending. This study aims to ident...",https://www.springeropen.com//jfin-swufe.springeropen.com/articles/10.1186/s40854-022-00345-6,1 May 2022,
294,0.00673579331412596,294,"AlphaMat: A Material Informatics Hub Connecting Data, Features, Models and Applications","Zhilong Wang, Junfei Cai, An Chen, Yanqiang Han, Kehao Tao, Simin Ye, Shiwei Wang, Imran Ali, Jinjin Li","The development of modern civil industry, energy and information technology is inseparable from the rapid explorations of new materials, which are hampered by months to years of painstaking attempts, resulting in only a small fraction of materials being determined in a vast chemical space. Artificial intelligence (AI)-based methods are promising to address this gap, but face many challenges such as data scarcity and inaccurate material descriptor coding. Here, we develop an AI platform, AlphaMat, that connects materials and applications. AlphaMat is not limited by the data scale (from 101 to 106) and can design structural and component descriptors that are effective for docking with various AI models. With prediction time of milliseconds and high accuracy, AlphaMat exhibits strong powers to model at least 12 common attributes (formation energy, band gap, ionic conductivity, magnetism, phonon property, bulk modulus, dielectric constant, adsorption energy, etc.), resulting in an unexplored material database with over 117,000 entries. We further demonstrate the ability of AlphaMat to mine and design materials, which successfully discover thousands of new materials in photonics, batteries, catalysts, and capacitors from the largest inorganic compound databases that cover all elements in periodic table. This work proposes the first material informatics hub that does not require users to have strong programming knowledge to build AI models to design materials. Users can either directly retrieve our database or easily build AI models through AlphaMat to discover and design the required materials. AlphaMat can shorten the cycle of database construction and material discovery by at least decades, and its effective use will facilitate the applications of AI technology in material science and lead scientific and technological progress to a new height. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.11651,March 2023,294,The Non-hydrostatic Icosahedral Atmospheric Model: description and development,"Masaki Satoh, Hirofumi Tomita, Hisashi Yashiro, Hiroaki Miura, Chihiro Kodama, Tatsuya Seiki, Akira T Noda, Yohei Yamada, Daisuke Goto, Masahiro Sawada, Takemasa Miyoshi, Yosuke Niwa, Masayuki Hara, Tomoki Ohno, Shin-ichi Iga, Takashi Arakawa…","This article reviews the development of a global non-hydrostatic model, focusing on the pioneering research of the Non-hydrostatic Icosahedral Atmospheric Model (NICAM). Very high resolution global atmospheric...",https://www.springeropen.com//progearthplanetsci.springeropen.com/articles/10.1186/s40645-014-0018-1,14 October 2014,
295,0.000678989392134464,295,Heterogeneity of AI-Induced Societal Harms and the Failure of Omnibus AI Laws,Sangchul Park,"AI-induced societal harms mirror existing problems in domains where AI replaces or complements traditional methodologies. However, trustworthy AI discourses postulate the homogeneity of AI, aim to derive common causes regarding the harms they generate, and demand uniform human interventions. Such AI monism has spurred legislation for omnibus AI laws requiring any high-risk AI systems to comply with a full, uniform package of rules on fairness, transparency, accountability, human oversight, accuracy, robustness, and security, as demonstrated by the EU AI Regulation and the U.S. draft Algorithmic Accountability Act. However, it is irrational to require high-risk or critical AIs to comply with all the safety, fairness, accountability, and privacy regulations when it is possible to separate AIs entailing safety risks, biases, infringements, and privacy problems. Legislators should gradually adapt existing regulations by categorizing AI systems according to the types of societal harms they induce. Accordingly, this paper proposes the following categorizations, subject to ongoing empirical reassessments. First, regarding intelligent agents, safety regulations must be adapted to address incremental accident risks arising from autonomous behavior. Second, regarding discriminative models, law must focus on the mitigation of allocative harms and the disclosure of marginal effects of immutable features. Third, for generative models, law should optimize developer liability for data mining and content generation, balancing potential social harms arising from infringing content and the negative impact of excessive filtering and identify cases where its non-human identity should be disclosed. Lastly, for cognitive models, data protection law should be adapted to effectively address privacy, surveillance, and security problems and facilitate governance built on public-private partnerships. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.11196,March 2023,295,Research on denoising processing of computer video electromagnetic leakage reduction image based on fuzzy degree,Chunwei Miao,"On the basis of analyzing, receiving, and parsing the computer video electromagnetic leakage emission signal, an image of the screen display content can be obtained. Due to the interference noise existing in t...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-018-0405-4,11 January 2019,
296,0.000678989392134464,296,Learning Multi-Stage Multi-Grained Semantic Embeddings for E-Commerce Search,"Binbin Wang, Mingming Li, Zhixiong Zeng, Jingwei Zhuo, Songlin Wang, Sulong Xu, Bo Long, Weipeng Yan","Retrieving relevant items that match users' queries from billion-scale corpus forms the core of industrial e-commerce search systems, in which embedding-based retrieval (EBR) methods are prevailing. These methods adopt a two-tower framework to learn embedding vectors for query and item separately and thus leverage efficient approximate nearest neighbor (ANN) search to retrieve relevant items. However, existing EBR methods usually ignore inconsistent user behaviors in industrial multi-stage search systems, resulting in insufficient retrieval efficiency with a low commercial return. To tackle this challenge, we propose to improve EBR methods by learning Multi-level Multi-Grained Semantic Embeddings(MMSE). We propose the multi-stage information mining to exploit the ordered, clicked, unclicked and random sampled items in practical user behavior data, and then capture query-item similarity via a post-fusion strategy. We then propose multi-grained learning objectives that integrate the retrieval loss with global comparison ability and the ranking loss with local comparison ability to generate semantic embeddings. Both experiments on a real-world billion-scale dataset and online A/B tests verify the effectiveness of MMSE in achieving significant performance improvements on metrics such as offline recall and online conversion rate (CVR). △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.11009,March 2023,296,Integration of computational thinking in K-12 mathematics education: a systematic review on CT-based mathematics instruction and student learning,"Huiyan Ye, Biyao Liang, Oi-Lam Ng and Ching Sing Chai","There has been substantial research undertaken on the integration of computational thinking (CT) in K-12 mathematics education in recent years, particularly since 2018 when relevant systematic reviews were con...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00396-w,18 January 2023,
297,0.000678989392134464,297,An Evaluation of GPU Filters for Accelerating the 2D Convex Hull,"Roberto Carrasco, Héctor Ferrada, Cristóbal A. Navarro, Nancy Hitschfeld","The Convex Hull algorithm is one of the most important algorithms in computational geometry, with many applications such as in computer graphics, robotics, and data mining. Despite the advances in the new algorithms in this area, it is often needed to improve the performance to solve more significant problems quickly or in real-time processing. This work presents an experimental evaluation of GPU filters to reduce the cost of computing the 2D convex hull. The technique first performs a preprocessing of the input set, filtering all points within an eight-vertex polygon in logarithmic time, to obtain a reduced set of candidate points. We use parallel computation and the use of the Manhattan distance as a metric to find the vertices of the polygon and perform the point filtering. For the filtering stage we study different approaches; from custom CUDA kernels to libraries such as Thrust and CUB. Three types of point distributions are tested: a normal distribution (favorable case), circumference (the worst case), and a case where points are shifted randomly from the circumference (intermediate case). Experimental evaluation shows that the GPU filtering algorithm can be up to 23x faster than a sequential CPU implementation, and the whole convex hull computation can be up to 30x faster than the fastest implementation provided by the CGAL library. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.10581,March 2023,297,Ontology Issues and Applications Guest Editors’ Introduction,"Fred Freitas, Heiner Stuckenschmidt and Natalya F. Noy",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192372,June 2005,
298,0.000678989392134464,298,Discovering Predictable Latent Factors for Time Series Forecasting,"Jingyi Hou, Zhen Dong, Jiayu Zhou, Zhijie Liu","Modern time series forecasting methods, such as Transformer and its variants, have shown strong ability in sequential data modeling. To achieve high performance, they usually rely on redundant or unexplainable structures to model complex relations between variables and tune the parameters with large-scale data. Many real-world data mining tasks, however, lack sufficient variables for relation reasoning, and therefore these methods may not properly handle such forecasting problems. With insufficient data, time series appear to be affected by many exogenous variables, and thus, the modeling becomes unstable and unpredictable. To tackle this critical issue, in this paper, we develop a novel algorithmic framework for inferring the intrinsic latent factors implied by the observable time series. The inferred factors are used to form multiple independent and predictable signal components that enable not only sparse relation reasoning for long-term efficiency but also reconstructing the future temporal data for accurate prediction. To achieve this, we introduce three characteristics, i.e., predictability, sufficiency, and identifiability, and model these characteristics via the powerful deep latent dynamics models to infer the predictable signal components. Empirical results on multiple real datasets show the efficiency of our method for different kinds of time series forecasting. The statistical analysis validates the predictability of the learned latent factors. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.10426,March 2023,298,Open source and accessibility: advantages and limitations,"Michael Heron, Vicki L Hanson and Ian Ricketts",In this paper we discuss the open source process as it relates to accessibility software. Open source is a development model that has shown considerable benefits in a number of application areas. However the n...,https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/2194-0827-1-2,7 May 2013,
299,0.000678989392134464,299,ExplainFix: Explainable Spatially Fixed Deep Networks,"Alex Gaudio, Christos Faloutsos, Asim Smailagic, Pedro Costa, Aurelio Campilho","Is there an initialization for deep networks that requires no learning? ExplainFix adopts two design principles: the ""fixed filters"" principle that all spatial filter weights of convolutional neural networks can be fixed at initialization and never learned, and the ""nimbleness"" principle that only few network parameters suffice. We contribute (a) visual model-based explanations, (b) speed and accuracy gains, and (c) novel tools for deep convolutional neural networks. ExplainFix gives key insights that spatially fixed networks should have a steered initialization, that spatial convolution layers tend to prioritize low frequencies, and that most network parameters are not necessary in spatially fixed models. ExplainFix models have up to 100x fewer spatial filter kernels than fully learned models and matching or improved accuracy. Our extensive empirical analysis confirms that ExplainFix guarantees nimbler models (train up to 17\% faster with channel pruning), matching or improved predictive performance (spanning 13 distinct baseline models, four architectures and two medical image datasets), improved robustness to larger learning rate, and robustness to varying model size. We are first to demonstrate that all spatial filters in state-of-the-art convolutional deep networks can be fixed at initialization, not learned. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.10408,March 2023,299,Distributed Bayesian Multiple-Target Tracking in Crowded Environments Using Multiple Collaborative Cameras,"Wei Qu, Dan Schonfeld and Magdi Mohamed","Multiple-target tracking has received tremendous attention due to its wide practical applicability in video processing and analysis applications. Most existing techniques, however, suffer from the well-known""mul...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/38373,1 December 2006,
300,0.000678989392134464,300,She Elicits Requirements and He Tests: Software Engineering Gender Bias in Large Language Models,"Christoph Treude, Hideaki Hata","Implicit gender bias in software development is a well-documented issue, such as the association of technical roles with men. To address this bias, it is important to understand it in more detail. This study uses data mining techniques to investigate the extent to which 56 tasks related to software development, such as assigning GitHub issues and testing, are affected by implicit gender bias embedded in large language models. We systematically translated each task from English into a genderless language and back, and investigated the pronouns associated with each task. Based on translating each task 100 times in different permutations, we identify a significant disparity in the gendered pronoun associations with different tasks. Specifically, requirements elicitation was associated with the pronoun ""he"" in only 6% of cases, while testing was associated with ""he"" in 100% of cases. Additionally, tasks related to helping others had a 91% association with ""he"" while the same association for tasks related to asking coworkers was only 52%. These findings reveal a clear pattern of gender bias related to software development tasks and have important implications for addressing this issue both in the training of large language models and in broader society. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.10131,March 2023,300,Promoting computational thinking through project-based learning,"Namsoo Shin, Jonathan Bowers, Joseph Krajcik and Daniel Damelin","This paper introduces project-based learning (PBL) features for developing technological, curricular, and pedagogical supports to engage students in computational thinking (CT) through modeling. CT is recogniz...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-021-00033-y,2 August 2021,
301,0.000678989392134464,301,Stochastic Submodular Maximization via Polynomial Estimators,"Gözde Özcan, Stratis Ioannidis","In this paper, we study stochastic submodular maximization problems with general matroid constraints, that naturally arise in online learning, team formation, facility location, influence maximization, active learning and sensing objective functions. In other words, we focus on maximizing submodular functions that are defined as expectations over a class of submodular functions with an unknown distribution. We show that for monotone functions of this form, the stochastic continuous greedy algorithm attains an approximation ratio (in expectation) arbitrarily close to $(1-1/e) \approx 63\%$ using a polynomial estimation of the gradient. We argue that using this polynomial estimator instead of the prior art that uses sampling eliminates a source of randomness and experimentally reduces execution time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09960,March 2023,301,eLight: enlightening and exploring light,Aydogan Ozcan and Cheng-Wei Qiu,Unknown,https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-020-00001-5,8 June 2021,
302,0.000678989392134464,302,Towards Understanding the Open Source Interest in Gender-Related GitHub Projects,"Rita Garcia, Christoph Treude, Wendy La","The open-source community uses the GitHub platform to exchange and share software applications and services of interest. This paper aims to identify the open-source community's interest in gender-related projects on GitHub. Our findings create research opportunities and identify resources by the open-source community that promote diversity, equity, and inclusion. We use data mining to identify GitHub projects that focus on gender-related topics. We apply quantitative and qualitative methodologies to examine the projects' attributes and to classify them within a gender social structure and a gender bias taxonomy. We aim to understand the open-source community's efforts and interests in gender topics through active projects. In this paper, we report on a preponderance of projects focusing on specific gender topics and identify those with a narrow focus. We examine projects focusing on gender bias and how they address this non-inclusive behaviour. Results show a propensity of GitHub projects focusing on recognising and detecting an individual's gender and a dearth of projects concentrating on the cultural expectations placed on women and men. In the gender bias domain, the projects mainly focus on occupational biases. These findings raise opportunities to address the limited focus of GitHub on gender-related topics through developing projects that mitigate exclusive behaviours. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09727,March 2023,302,Towards European standards for quantum technologies,"Oskar van Deventer, Nicolas Spethmann, Marius Loeffler, Michele Amoretti, Rob van den Brink, Natalia Bruno, Paolo Comi, Noel Farrugia, Marco Gramegna, Andreas Jenet, Ben Kassenberg, Wojciech Kozlowski, Thomas Länger, Tobias Lindstrom, Vicente Martin, Niels Neumann…","The Second Quantum Revolution facilitates the engineering of new classes of sensors, communication technologies, and computers with unprecedented capabilities. Supply chains for quantum technologies are emergi...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00150-1,29 November 2022,
303,0.000678989392134464,303,Tribe or Not? Critical Inspection of Group Differences Using TribalGram,"Yongsu Ahn, Muheng Yan, Yu-Ru Lin, Wen-Ting Chung, Rebecca Hwa","With the rise of AI and data mining techniques, group profiling and group-level analysis have been increasingly used in many domains including policy making and direct marketing. In some cases, the statistics extracted from data may provide insights to a group's shared characteristics; in others, the group-level analysis can lead to problems including stereotyping and systematic oppression. How can analytic tools facilitate a more conscientious process in group analysis? In this work, we identify a set of accountable group analytics design guidelines to explicate the needs for group differentiation and preventing overgeneralization of a group. Following the design guidelines, we develop TribalGram, a visual analytic suite that leverages interpretable machine learning algorithms and visualization to offer inference assessment, model explanation, data corroboration, and sense-making. Through the interviews with domain experts, we showcase how our design and tools can bring a richer understanding of ""groups"" mined from the data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09664,March 2023,303,Research on music creation and audition based on wireless environment,Lingzi Wang and Mingxia Pan,"Traditional computer music creation is carried out through computer simulation of fixed conversion of syllables. However, there are phenomena of syllables and pitch distortions in music creation that changes r...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-020-01763-9,17 July 2020,
304,0.000678989392134464,304,Recommending the optimal policy by learning to act from temporal data,"Stefano Branchi, Andrei Buliga, Chiara Di Francescomarino, Chiara Ghidini, Francesca Meneghello, Massimiliano Ronzani","Prescriptive Process Monitoring is a prominent problem in Process Mining, which consists in identifying a set of actions to be recommended with the goal of optimising a target measure of interest or Key Performance Indicator (KPI). One challenge that makes this problem difficult is the need to provide Prescriptive Process Monitoring techniques only based on temporally annotated (process) execution data, stored in, so-called execution logs, due to the lack of well crafted and human validated explicit models. In this paper we aim at proposing an AI based approach that learns, by means of Reinforcement Learning (RL), an optimal policy (almost) only from the observation of past executions and recommends the best activities to carry on for optimizing a KPI of interest. This is achieved first by learning a Markov Decision Process for the specific KPIs from data, and then by using RL training to learn the optimal policy. The approach is validated on real and synthetic datasets and compared with off-policy Deep RL approaches. The ability of our approach to compare with, and often overcome, Deep RL approaches provides a contribution towards the exploitation of white box RL techniques in scenarios where only temporal execution data are available. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09209,March 2023,304,Correction to: Recent developments in photoacoustic imaging and sensing for nondestructive testing and evaluation,Sung-Liang Chen and Chao Tian,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00077-x,29 April 2021,
305,0.000678989392134464,305,Wiki-based Communities of Interest: Demographics and Outliers,"Hiba Arnaout, Simon Razniewski, Jeff Z. Pan","In this paper, we release data about demographic information and outliers of communities of interest. Identified from Wiki-based sources, mainly Wikidata, the data covers 7.5k communities, such as members of the White House Coronavirus Task Force, and 345k subjects, e.g., Deborah Birx. We describe the statistical inference methodology adopted to mine such data. We release subject-centric and group-centric datasets in JSON format, as well as a browsing interface. Finally, we forsee three areas this research can have an impact on: in social sciences research, it provides a resource for demographic analyses; in web-scale collaborative encyclopedias, it serves as an edit recommender to fill knowledge gaps; and in web search, it offers lists of salient statements about queried subjects for higher user engagement. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09189,March 2023,305,ECR 2018 - BOOK OF ABSTRACTS,Unknown,"This article is part of a Supplement:Volume 9
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1007/s13244-018-0603-8,15 February 2018,
306,0.000678989392134464,306,Optimal Intervention on Weighted Networks via Edge Centrality,"Dongyue Li, Tina Eliassi-Rad, Hongyang R. Zhang","Suppose there is a spreading process such as an infectious disease propagating on a graph. How would we reduce the number of affected nodes in the spreading process? This question appears in recent studies about implementing mobility interventions on mobility networks (Chang et al. (2021)). A practical algorithm to reduce infections on unweighted graphs is to remove edges with the highest edge centrality score (Tong et al. (2012)), which is the product of two adjacent nodes' eigenscores. However, mobility networks have weighted edges; Thus, an intervention measure would involve edge-weight reduction besides edge removal. Motivated by this example, we revisit the problem of minimizing top eigenvalue(s) on weighted graphs by decreasing edge weights up to a fixed budget. We observe that the edge centrality score of Tong et al. (2012) is equal to the gradient of the largest eigenvalue of $WW^{\top}$, where $W$ denotes the weight matrix of the graph. We then present generalized edge centrality scores as the gradient of the sum of the largest $r$ eigenvalues of $WW^{\top}$. With this generalization, we design an iterative algorithm to find the optimal edge-weight reduction to shrink the largest $r$ eigenvalues of $WW^{\top}$ under a given edge-weight reduction budget. We also extend our algorithm and its guarantee to time-varying graphs, whose weights evolve over time. We perform a detailed empirical study to validate our approach. Our algorithm significantly reduces the number of infections compared with existing methods on eleven weighted networks. Further, we illustrate several properties of our algorithm, including the benefit of choosing the rank $r$, fast convergence to global optimum, and an almost linear runtime per iteration. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09086,March 2023,306,"A critical evaluation, challenges, and future perspectives of using artificial intelligence and emerging technologies in smart classrooms",Eleni Dimitriadou and Andreas Lanitis,"The term ""Smart Classroom"" has evolved over time and nowadays reflects the technological advancements incorporated in educational spaces. The rapid advances in technology, and the need to create more efficient...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00231-3,6 February 2023,
307,0.000678989392134464,307,Physical and Economic Viability of Cryptocurrency Mining for Provision of Frequency Regulation: A Real-World Texas Case Study,"Rayan El Helou, Ali Menati, Le Xie","Demand flexibility plays a pivotal role in modern power systems with high penetration of variable energy resources. In recent years, one of the fastest-growing flexible energy demands has been proof-of-work-based cryptocurrency mining facilities. Due to their competitive ramping capabilities and demonstrated flexibility, such fast-responding loads are capable of participating in frequency regulation services for the grid while simultaneously increasing their own operational revenue. In this paper, we investigate the physical and economic viability of employing cryptocurrency mining facilities to provide frequency regulation in large power systems. We quantify mining facilities' operational profit, and propose a decision-making framework to explore their optimal participation strategy and account for the most influential factors. We employ real-world ERCOT ancillary services data in our case study to investigate the conditions under which provision of frequency regulation in the Texas grid is profitable. We also perform transient level simulations using a synthetic Texas grid to demonstrate the competitiveness of mining facilities at frequency regulation provision. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.09081,March 2023,307,Hopf bifurcation of a computer virus spreading model in the network with limited anti-virus ability,Tao Zhao and Dianjie Bi,A delayed computer virus spreading model in the network with limited anti-virus ability is proposed in the present paper. Local stability and the existence of a Hopf bifurcation are proved by taking the time d...,https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-017-1243-x,27 June 2017,
308,0.000678989392134464,308,PTMTorrent: A Dataset for Mining Open-source Pre-trained Model Packages,"Wenxin Jiang, Nicholas Synovic, Purvish Jajal, Taylor R. Schorlemmer, Arav Tewari, Bhavesh Pareek, George K. Thiruvathukal, James C. Davis","Due to the cost of developing and training deep learning models from scratch, machine learning engineers have begun to reuse pre-trained models (PTMs) and fine-tune them for downstream tasks. PTM registries known as ""model hubs"" support engineers in distributing and reusing deep learning models. PTM packages include pre-trained weights, documentation, model architectures, datasets, and metadata. Mining the information in PTM packages will enable the discovery of engineering phenomena and tools to support software engineers. However, accessing this information is difficult - there are many PTM registries, and both the registries and the individual packages may have rate limiting for accessing the data. We present an open-source dataset, PTMTorrent, to facilitate the evaluation and understanding of PTM packages. This paper describes the creation, structure, usage, and limitations of the dataset. The dataset includes a snapshot of 5 model hubs and a total of 15,913 PTM packages. These packages are represented in a uniform data schema for cross-hub mining. We describe prior uses of this data and suggest research opportunities for mining using our dataset. The PTMTorrent dataset (v1) is available at: https://app.globus.org/file-manager?origin_id=55e17a6e-9d8f-11ed-a2a2-8383522b48d9&origin_path=%2F~%2F. Our dataset generation tools are available on GitHub: https://doi.org/10.5281/zenodo.7570357. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.08934,March 2023,308,Correction to: Classes of tree-based networks,"Mareike Fischer, Lina Herbst, Michelle Galla, Yangjing Long and Kristina Wicke",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00069-x,26 January 2021,
309,0.000678989392134464,309,"Cognitive Semantic Communication Systems Driven by Knowledge Graph: Principle, Implementation, and Performance Evaluation","Fuhui Zhou, Yihao Li, Ming Xu, Lu Yuan, Qihui Wu, Rose Qingyang Hu, Naofal Al-Dhahir","Semantic communication is envisioned as a promising technique to break through the Shannon limit. However, semantic inference and semantic error correction have not been well studied. Moreover, error correction methods of existing semantic communication frameworks are inexplicable and inflexible, which limits the achievable performance. In this paper, to tackle this issue, a knowledge graph is exploited to develop semantic communication systems. Two cognitive semantic communication frameworks are proposed for the single-user and multiple-user communication scenarios. Moreover, a simple, general, and interpretable semantic alignment algorithm for semantic information detection is proposed. Furthermore, an effective semantic correction algorithm is proposed by mining the inference rule from the knowledge graph. Additionally, the pre-trained model is fine-tuned to recover semantic information. For the multi-user cognitive semantic communication system, a message recovery algorithm is proposed to distinguish messages of different users by matching the knowledge level between the source and the destination. Extensive simulation results conducted on a public dataset demonstrate that our proposed single-user and multi-user cognitive semantic communication systems are superior to benchmark communication systems in terms of the data compression rate and communication reliability. Finally, we present realistic single-user and multi-user cognitive semantic communication systems results by building a software-defined radio prototype system. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.08546,March 2023,309,Delay-induced Hopf bifurcation of an SVEIR computer virus model with nonlinear incidence rate,"Tao Zhao, Zizhen Zhang and Ranjit Kumar Upadhyay","We are concerned with the Hopf bifurcation of an SVEIR computer virus model with time delay and nonlinear incident rate. First of all, by analyzing the associated characteristic equation we obtain sufficient c...",https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-018-1698-4,27 July 2018,
310,0.000678989392134464,310,Mining Mini-Halos with MeerKAT I. Calibration and Imaging,"K. S. Trehaeven, V. Parekh, N. Oozeer, B. Hugo, O. Smirnov, G. Bernardi, K. Knowles, C. Tasse, K. M. B. Asad, S. Giacintucci","Radio mini-halos are clouds of diffuse, low surface brightness synchrotron emission that surround the Brightest Cluster Galaxy (BCG) in massive cool-core galaxy clusters. In this paper, we use third generation calibration (3GC), also called direction-dependent (DD) calibration, and point source subtraction on MeerKAT extragalactic continuum data. We calibrate and image archival MeerKAT L-band observations of a sample of five galaxy clusters (ACO 1413, ACO 1795, ACO 3444, MACS J1115.8+0129, MACS J2140.2-2339). We use the CARACal pipeline for direction-independent (DI) calibration, DDFacet and killMS for 3GC, followed by visibility-plane point source subtraction to image the underlying mini-halo without bias from any embedded sources. Our 3GC process shows a drastic improvement in artefact removal, to the extent that the local noise around severely affected sources was halved and ultimately resulted in a 7\% improvement in global image noise. Thereafter, using these spectrally deconvolved Stokes I continuum images, we directly measure for four mini-halos the flux density, radio power, size and in-band integrated spectra. Further to that, we show the in-band spectral index maps of the mini-halo (with point sources). We present a new mini-halo detection hosted by MACS J2140.2-2339, having flux density $S_{\rm 1.28\,GHz} = 2.61 \pm 0.31$ mJy, average diameter 296 kpc and $α^{\rm 1.5\,GHz}_{\rm 1\,GHz} = 1.21 \pm 0.36$. We also found a $\sim$100 kpc southern extension to the ACO 3444 mini-halo which was not detected in previous VLA L-band observations. Our description of MeerKAT wide-field, wide-band data reduction will be instructive for conducting further mini-halo science. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.08427,March 2023,310,Integration of computational thinking in K-12 mathematics education: a systematic review on CT-based mathematics instruction and student learning,"Huiyan Ye, Biyao Liang, Oi-Lam Ng and Ching Sing Chai","There has been substantial research undertaken on the integration of computational thinking (CT) in K-12 mathematics education in recent years, particularly since 2018 when relevant systematic reviews were con...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00396-w,18 January 2023,
311,0.000678989392134464,311,A hybrid quantum-classical classifier based on branching multi-scale entanglement renormalization ansatz,"Yan-Yan Hou, Jian Li, Xiu-Bo Chen, Chong-Qiang Ye","Label propagation is an essential semi-supervised learning method based on graphs, which has a broad spectrum of applications in pattern recognition and data mining. This paper proposes a quantum semi-supervised classifier based on label propagation. Considering the difficulty of graph construction, we develop a variational quantum label propagation (VQLP) method. In this method, a locally parameterized quantum circuit is created to reduce the parameters required in the optimization. Furthermore, we design a quantum semi-supervised binary classifier based on hybrid Bell and $Z$ bases measurement, which has shallower circuit depth and is more suitable for implementation on near-term quantum devices. We demonstrate the performance of the quantum semi-supervised classifier on the Iris data set, and the simulation results show that the quantum semi-supervised classifier has higher classification accuracy than the swap test classifier. This work opens a new path to quantum machine learning based on graphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.07906,March 2023,311,Systematic review of digital twin technology and applications,"Jun-Feng Yao, Yong Yang, Xue-Cheng Wang and Xiao-Peng Zhang","As one of the most important applications of digitalization, intelligence, and service, the digital twin (DT) breaks through the constraints of time, space, cost, and security on physical entities, expands and...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-023-00137-4,30 May 2023,
312,0.000678989392134464,312,InferFix: End-to-End Program Repair with LLMs,"Matthew Jin, Syed Shahriar, Michele Tufano, Xin Shi, Shuai Lu, Neel Sundaresan, Alexey Svyatkovskiy","Software development life cycle is profoundly influenced by bugs: their introduction, identification, and eventual resolution account for a significant portion of software cost. This has motivated software engineering researchers and practitioners to propose different approaches for automating the identification and repair of software defects. Large language models have been adapted to the program repair task through few-shot demonstration learning and instruction prompting, treating this as an infilling task. However, these models have only focused on learning general bug-fixing patterns for uncategorized bugs mined from public repositories. In this paper, we propose InferFix: a transformer-based program repair framework paired with a state-of-the-art static analyzer to fix critical security and performance bugs. InferFix combines a Retriever -- transformer encoder model pretrained via contrastive learning objective, which aims at searching for semantically equivalent bugs and corresponding fixes; and a Generator -- a large language model (Codex Cushman) finetuned on supervised bug-fix data with prompts augmented via bug type annotations and semantically similar fixes retrieved from an external non-parametric memory. To train and evaluate our approach, we curated InferredBugs, a novel, metadata-rich dataset of bugs extracted by executing the Infer static analyzer on the change histories of thousands of Java and C# repositories. Our evaluation demonstrates that InferFix outperforms strong LLM baselines, with a top-1 accuracy of 65.6% for generating fixes in C# and 76.8% in Java. We discuss the deployment of InferFix alongside Infer at Microsoft which offers an end-to-end solution for detection, classification, and localization of bugs, as well as fixing and validation of candidate patches, integrated in the continuous integration pipeline to automate the software development workflow. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.07263,March 2023,312,The dynamics of faculty hiring networks,"Eun Lee, Aaron Clauset and Daniel B. Larremore","Faculty hiring networks—who hires whose graduates as faculty—exhibit steep hierarchies, which can reinforce both social and epistemic inequalities in academia. Understanding the mechanisms driving these patter...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00303-9,15 September 2021,
313,0.000678989392134464,313,VMCDL: Vulnerability Mining Based on Cascaded Deep Learning Under Source Control Flow,Wen Zhou,"With the rapid development of the computer industry and computer software, the risk of software vulnerabilities being exploited has greatly increased. However, there are still many shortcomings in the existing mining techniques for leakage source research, such as high false alarm rate, coarse-grained detection, and dependence on expert experience. In this paper, we mainly use the c/c++ source code data of the SARD dataset, process the source code of CWE476, CWE469, CWE516 and CWE570 vulnerability types, test the Joern vulnerability scanning function of the cutting-edge tool, and propose a new cascading deep learning model VMCDL based on source code control flow to effectively detect vulnerabilities. First, this paper uses joern to locate and extract sensitive functions and statements to form a sensitive statement library of vulnerable code. Then, the CFG flow vulnerability code snippets are generated by bidirectional breadth-first traversal, and then vectorized by Doc2vec. Finally, the cascade deep learning model based on source code control flow is used for classification to obtain the classification results. In the experimental evaluation, we give the test results of Joern on specific vulnerabilities, and give the confusion matrix and label data of the binary classification results of the model algorithm on single vulnerability type source code, and compare and verify the five indicators of FPR, FNR, ACC, P and F1, respectively reaching 10.30%, 5.20%, 92.50%,85.10% and 85.40%,which shows that it can effectively reduce the false alarm rate of static analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.07128,March 2023,313,"A computational analysis of accessibility, readability, and explainability of figures in open access publications","Han Zhuang, Tzu-Yang Huang and Daniel E. Acuna","Figures are an essential part of scientific communication. Yet little is understood about how accessible (e.g., color-blind safe), readable (e.g., good contrast), and explainable (e.g., contain captions and le...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-023-00380-y,2 March 2023,
314,0.000678989392134464,314,TranSG: Transformer-Based Skeleton Graph Prototype Contrastive Learning with Structure-Trajectory Prompted Reconstruction for Person Re-Identification,"Haocong Rao, Chunyan Miao","Person re-identification (re-ID) via 3D skeleton data is an emerging topic with prominent advantages. Existing methods usually design skeleton descriptors with raw body joints or perform skeleton sequence representation learning. However, they typically cannot concurrently model different body-component relations, and rarely explore useful semantics from fine-grained representations of body joints. In this paper, we propose a generic Transformer-based Skeleton Graph prototype contrastive learning (TranSG) approach with structure-trajectory prompted reconstruction to fully capture skeletal relations and valuable spatial-temporal semantics from skeleton graphs for person re-ID. Specifically, we first devise the Skeleton Graph Transformer (SGT) to simultaneously learn body and motion relations within skeleton graphs, so as to aggregate key correlative node features into graph representations. Then, we propose the Graph Prototype Contrastive learning (GPC) to mine the most typical graph features (graph prototypes) of each identity, and contrast the inherent similarity between graph representations and different prototypes from both skeleton and sequence levels to learn discriminative graph representations. Last, a graph Structure-Trajectory Prompted Reconstruction (STPR) mechanism is proposed to exploit the spatial and temporal contexts of graph nodes to prompt skeleton graph reconstruction, which facilitates capturing more valuable patterns and graph semantics for person re-ID. Empirical evaluations demonstrate that TranSG significantly outperforms existing state-of-the-art methods. We further show its generality under different graph modeling, RGB-estimated skeletons, and unsupervised scenarios. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06819,March 2023,314,Perceived abilities or academic interests? Longitudinal high school science and mathematics effects on postsecondary STEM outcomes by gender and race,Teng Zhao and Lara Perez-Felkner,"Previous literature has examined the relationship between high school students’ postsecondary STEM major choices and their prior interest and perceived ability in mathematics. Yet, we have limited understandin...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00356-w,22 June 2022,
315,0.000678989392134464,315,SecretBench: A Dataset of Software Secrets,"Setu Kumar Basak, Lorenzo Neil, Bradley Reaves, Laurie Williams","According to GitGuardian's monitoring of public GitHub repositories, the exposure of secrets (API keys and other credentials) increased two-fold in 2021 compared to 2020, totaling more than six million secrets. However, no benchmark dataset is publicly available for researchers and tool developers to evaluate secret detection tools that produce many false positive warnings. The goal of our paper is to aid researchers and tool developers in evaluating and improving secret detection tools by curating a benchmark dataset of secrets through a systematic collection of secrets from open-source repositories. We present a labeled dataset of source codes containing 97,479 secrets (of which 15,084 are true secrets) of various secret types extracted from 818 public GitHub repositories. The dataset covers 49 programming languages and 311 file types. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06729,March 2023,315,Determining Vision Graphs for Distributed Camera Networks Using Feature Digests,"Zhaolin Cheng, Dhanya Devarajan and Richard J. Radke","We propose a decentralized method for obtaining the vision graph for a distributed, ad-hoc camera network, in which each edge of the graph represents two cameras that image a sufficiently large part of the sam...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/57034,1 December 2006,
316,0.000678989392134464,316,"PyPoll: A python library automating mining of networks, discussions and polarization on Twitter","Dimitrios Panteleimon Giakatos, Pavlos Sermpezis, Athena Vakali","Today online social networks have a high impact in our society as more and more people use them for communicating with each other, express their opinions, participating in public discussions, etc. In particular, Twitter is one of the most popular social network platforms people mainly use for political discussions. This attracted the interest of many research studies that analyzed social phenomena on Twitter, by collecting data, analysing communication patterns, and exploring the structure of user networks. While previous works share many common methodologies for data collection and analysis, these are mainly re-implemented every time by researchers in a custom way. In this paper, we introduce PyPoll an open-source Python library that operationalizes common analysis tasks for Twitter discussions. With PyPoll users can perform Twitter graph mining, calculate the polarization index and generate interactive visualizations without needing third-party tools. We believe that PyPoll can help researchers automate their tasks by giving them methods that are easy to use. Also, we demonstrate the use of the library by presenting two use cases; the PyPoll visualization app, an online application for graph visualizing and sharing, and the Political Lighthouse, a Web portal for displaying the polarization in various political topics on Twitter. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06478,March 2023,316,"Erratum to: A multi-agent cooperative reinforcement learning model using a hierarchy of consultants, tutors and workers","Bilal H. Abed-alguni, Stephan K. Chalup, Frans A. Henskens and David J. Paul",Theoriginal articlewas published inVietnam Journal of Computer Science20152:s40595-015-0045-x,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-015-0047-8,28 July 2015,
317,0.000678989392134464,317,Generalizing Greenwald-Khanna Streaming Quantile Summaries for Weighted Inputs,"Sepehr Assadi, Nirmit Joshi, Milind Prabhu, Vihan Shah","Estimating quantiles, like the median or percentiles, is a fundamental task in data mining and data science. A (streaming) quantile summary is a data structure that can process a set S of n elements in a streaming fashion and at the end, for any phi in (0,1], return a phi-quantile of S up to an eps error, i.e., return a phi'-quantile with phi'=phi +- eps. We are particularly interested in comparison-based summaries that only compare elements of the universe under a total ordering and are otherwise completely oblivious of the universe. The best known deterministic quantile summary is the 20-year old Greenwald-Khanna (GK) summary that uses O((1/eps) log(eps n)) space [SIGMOD'01]. This bound was recently proved to be optimal for all deterministic comparison-based summaries by Cormode and Vesleý [PODS'20]. In this paper, we study weighted quantiles, a generalization of the quantiles problem, where each element arrives with a positive integer weight which denotes the number of copies of that element being inserted. The only known method of handling weighted inputs via GK summaries is the naive approach of breaking each weighted element into multiple unweighted items and feeding them one by one to the summary, which results in a prohibitively large update time (proportional to the maximum weight of input elements). We give the first non-trivial extension of GK summaries for weighted inputs and show that it takes O((1/eps) log(eps n)) space and O(log(1/eps)+ log log(eps n)) update time per element to process a stream of length n (under some quite mild assumptions on the range of weights and eps). En route to this, we also simplify the original GK summaries for unweighted quantiles. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06288,March 2023,317,Mobile Robot Visual Navigation Using Multiple Features,"Nick Pears, Bojian Liang and Zezhi Chen",We propose a method to segment the ground plane from a mobile robot's visual field of view and then measure the height of nonground plane features above the mobile robot's ground plane. Thus a mobile robot can...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2250,25 August 2005,
318,0.000678989392134464,318,HYperbolic Self-Paced Learning for Self-Supervised Skeleton-based Action Representations,"Luca Franco, Paolo Mandica, Bharti Munjal, Fabio Galasso","Self-paced learning has been beneficial for tasks where some initial knowledge is available, such as weakly supervised learning and domain adaptation, to select and order the training sample sequence, from easy to complex. However its applicability remains unexplored in unsupervised learning, whereby the knowledge of the task matures during training. We propose a novel HYperbolic Self-Paced model (HYSP) for learning skeleton-based action representations. HYSP adopts self-supervision: it uses data augmentations to generate two views of the same sample, and it learns by matching one (named online) to the other (the target). We propose to use hyperbolic uncertainty to determine the algorithmic learning pace, under the assumption that less uncertain samples should be more strongly driving the training, with a larger weight and pace. Hyperbolic uncertainty is a by-product of the adopted hyperbolic neural networks, it matures during training and it comes with no extra cost, compared to the established Euclidean SSL framework counterparts. When tested on three established skeleton-based action recognition datasets, HYSP outperforms the state-of-the-art on PKU-MMD I, as well as on 2 out of 3 downstream tasks on NTU-60 and NTU-120. Additionally, HYSP only uses positive pairs and bypasses therefore the complex and computationally-demanding mining procedures required for the negatives in contrastive techniques. Code is available at https://github.com/paolomandica/HYSP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06242,March 2023,318,The life cycle of a customized communication device for a child with cerebral palsy: contributions toward the PD4CAT method,"Luciana Correia Lima de Faria Borges, Lucia Vilela Leite Filgueiras, Cristiano Maciel and Vinicius Carvalho Pereira","In this paper, we report the results of an action research conducted to design a communication device to help a non-verbal child develop language skills. Participatory design (PD) is frequently regarded as a c...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/1678-4804-20-10,24 April 2014,
319,0.000678989392134464,319,Self-supervised Training Sample Difficulty Balancing for Local Descriptor Learning,"Jiahan Zhang, Dayong Tian","In the case of an imbalance between positive and negative samples, hard negative mining strategies have been shown to help models learn more subtle differences between positive and negative samples, thus improving recognition performance. However, if too strict mining strategies are promoted in the dataset, there may be a risk of introducing false negative samples. Meanwhile, the implementation of the mining strategy disrupts the difficulty distribution of samples in the real dataset, which may cause the model to over-fit these difficult samples. Therefore, in this paper, we investigate how to trade off the difficulty of the mined samples in order to obtain and exploit high-quality negative samples, and try to solve the problem in terms of both the loss function and the training strategy. The proposed balance loss provides an effective discriminant for the quality of negative samples by combining a self-supervised approach to the loss function, and uses a dynamic gradient modulation strategy to achieve finer gradient adjustment for samples of different difficulties. The proposed annealing training strategy then constrains the difficulty of the samples drawn from negative sample mining to provide data sources with different difficulty distributions for the loss function, and uses samples of decreasing difficulty to train the model. Extensive experiments show that our new descriptors outperform previous state-of-the-art descriptors for patch validation, matching, and retrieval tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06124,March 2023,319,Empowering computing students with proficiency in robotics via situated learning,"Weitian Wang, Constantine Coutras and Michelle Zhu","With the increasing employment of robots in multiple areas such as smart manufacturing and intelligent transportation, both undergraduate and graduate students from computing related majors (e.g., computer sci...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00167-6,18 October 2021,
320,0.000678989392134464,320,Toward NeuroDM: Where Computational Neuroscience Meets Data Mining,"Xin Li, Bin Liu, Shuo Wang","At the intersection of computational neuroscience (CN) and data mining (DM), we advocate a holistic view toward their rich connections. On the one hand, fundamental concepts in neuroscience such as saliency, memory, and emotion can find novel applications in data mining. On the other hand, multimodal imaging has opened the door for data mining to facilitate the extraction of important cognitive and behavioral information from multimodal neural data. By NeuroDM, we advocate for more collaboration between CN and DM to expedite the advances in two well-established fields. The analogy between the over-parameterization of biological and artificial neural networks might suggest a unifying perspective of advancing both fields. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.06047,March 2023,320,Secondary school mathematics and entrance into the STEM professions: a longitudinal study,Ortal Nitzan-Tamar and Zehavit Kohen,"STEM (Science, Technology, Engineering, and Mathematics) fields are in high demand for qualified personnel worldwide, yet drop-out rates of a career path in STEM occur at various points in lifespan. Based on a...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00381-9,8 October 2022,
321,0.000678989392134464,321,German BERT Model for Legal Named Entity Recognition,"Harshil Darji, Jelena Mitrović, Michael Granitzer","The use of BERT, one of the most popular language models, has led to improvements in many Natural Language Processing (NLP) tasks. One such task is Named Entity Recognition (NER) i.e. automatic identification of named entities such as location, person, organization, etc. from a given text. It is also an important base step for many NLP tasks such as information extraction and argumentation mining. Even though there is much research done on NER using BERT and other popular language models, the same is not explored in detail when it comes to Legal NLP or Legal Tech. Legal NLP applies various NLP techniques such as sentence similarity or NER specifically on legal data. There are only a handful of models for NER tasks using BERT language models, however, none of these are aimed at legal documents in German. In this paper, we fine-tune a popular BERT language model trained on German data (German BERT) on a Legal Entity Recognition (LER) dataset. To make sure our model is not overfitting, we performed a stratified 10-fold cross-validation. The results we achieve by fine-tuning German BERT on the LER dataset outperform the BiLSTM-CRF+ model used by the authors of the same LER dataset. Finally, we make the model openly available via HuggingFace. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.05388,March 2023,321,Towards real-time communication betweenin vivoneurophysiological data sources and simulator-based brain biomimetic models,"Giljae Lee, Andréa Matsunaga, Salvador Dura-Bernal, Wenjie Zhang, William W Lytton, Joseph T Francis and José A B Fortes",Development of more sophisticated implantable brain-machine interface (BMI) will require both interpretation of the neurophysiological data being measured and subsequent determination of signals to be delivere...,https://www.springeropen.com//computationalsurgery.springeropen.com/articles/10.1186/s40244-014-0012-3,11 November 2014,
322,0.000678989392134464,322,Making a Computational Attorney,"Dell Zhang, Frank Schilder, Jack G. Conrad, Masoud Makrehchi, David von Rickenbach, Isabelle Moulinier","This ""blue sky idea"" paper outlines the opportunities and challenges in data mining and machine learning involving making a computational attorney -- an intelligent software agent capable of helping human lawyers with a wide range of complex high-level legal tasks such as drafting legal briefs for the prosecution or defense in court. In particular, we discuss what a ChatGPT-like Large Legal Language Model (L$^3$M) can and cannot do today, which will inspire researchers with promising short-term and long-term research objectives. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.05383,March 2023,322,Empowering computing students with proficiency in robotics via situated learning,"Weitian Wang, Constantine Coutras and Michelle Zhu","With the increasing employment of robots in multiple areas such as smart manufacturing and intelligent transportation, both undergraduate and graduate students from computing related majors (e.g., computer sci...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00167-6,18 October 2021,
323,0.000678989392134464,323,Spatio-Temporal Trajectory Similarity Measures: A Comprehensive Survey and Quantitative Study,"Danlei Hu, Lu Chen, Hanxi Fang, Ziquan Fang, Tianyi Li, Yunjun Gao","Spatio-temporal trajectory analytics is at the core of smart mobility solutions, which offers unprecedented information for diversified applications such as urban planning, infrastructure development, and vehicular networks. Trajectory similarity measure, which aims to evaluate the distance between two trajectories, is a fundamental functionality of trajectory analytics. In this paper, we propose a comprehensive survey that investigates all the most common and representative spatio-temporal trajectory measures. First, we provide an overview of spatio-temporal trajectory measures in terms of three hierarchical perspectives: Non-learning vs. Learning, Free Space vs. Road Network, and Standalone vs. Distributed. Next, we present an evaluation benchmark by designing five real-world transformation scenarios. Based on this benchmark, extensive experiments are conducted to study the effectiveness, robustness,nefficiency, and scalability of each measure, which offers guidelines for trajectory measure selection among multiple techniques and applications such as trajectory data mining, deep learning, and distributed processing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.05012,March 2023,323,A delayed e-epidemic SLBS model for computer virus,"Zizhen Zhang, Sangeeta Kumari and Ranjit Kumar Upadhyay",Unknown,https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-019-2341-8,27 September 2019,
324,0.000678989392134464,324,Contribution of clinical course to outcome after traumatic brain injury: mining patient trajectories from European intensive care unit data,"Shubhayu Bhattacharyay, Pier Francesco Caruso, Cecilia Åkerlund, Lindsay Wilson, Robert D Stevens, David K Menon, Ewout W Steyerberg, David W Nelson, Ari Ercole, the CENTER-TBI investigators/participants","Existing methods to characterise the evolving condition of traumatic brain injury (TBI) patients in the intensive care unit (ICU) do not capture the context necessary for individualising treatment. We aimed to develop a modelling strategy which integrates all data stored in medical records to produce an interpretable disease course for each TBI patient's ICU stay. From a prospective, European cohort (n=1,550, 65 centres, 19 countries) of TBI patients, we extracted all 1,166 variables collected before or during ICU stay as well as 6-month functional outcome on the Glasgow Outcome Scale-Extended (GOSE). We trained recurrent neural network models to map a token-embedded time series representation of all variables (including missing data) to an ordinal GOSE prognosis every 2 hours. With repeated cross-validation, we evaluated calibration and the explanation of ordinal variance in GOSE with Somers' Dxy. Furthermore, we applied TimeSHAP to calculate the contribution of variables and prior timepoints towards transitions in patient trajectories. Our modelling strategy achieved calibration at 8 hours, and the full range of variables explained up to 52% (95% CI: 50-54%) of the variance in ordinal functional outcome. Up to 91% (90-91%) of this explanation was derived from pre-ICU and admission information. Information collected in the ICU increased explanation (by up to 5% [4-6%]), though not enough to counter poorer performance in longer-stay (>5.75 days) patients. Static variables with the highest contributions were physician prognoses and certain demographic and CT features. Among dynamic variables, markers of intracranial hypertension and neurological function contributed the most. Whilst static information currently accounts for the majority of functional outcome explanation, our data-driven analysis highlights investigative avenues to improve dynamic characterisation of longer-stay patients. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.04630,March 2023,324,Cross-country analysis of spambots,"Vaibhav Garg, Thomas Koster and Linda Jean Camp",Spam is a vector for cybercrime and commonly legally prohibited. Why do certain national jurisdictions produce a higher percentage of spam than others despite its prohibition? Why do some countries have a high...,https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1186/1687-417X-2013-3,30 October 2013,
325,0.000678989392134464,325,Does Synthetic Data Generation of LLMs Help Clinical Text Mining?,"Ruixiang Tang, Xiaotian Han, Xiaoqian Jiang, Xia Hu","Recent advancements in large language models (LLMs) have led to the development of highly potent models like OpenAI's ChatGPT. These models have exhibited exceptional performance in a variety of tasks, such as question answering, essay composition, and code generation. However, their effectiveness in the healthcare sector remains uncertain. In this study, we seek to investigate the potential of ChatGPT to aid in clinical text mining by examining its ability to extract structured information from unstructured healthcare texts, with a focus on biological named entity recognition and relation extraction. However, our preliminary results indicate that employing ChatGPT directly for these tasks resulted in poor performance and raised privacy concerns associated with uploading patients' information to the ChatGPT API. To overcome these limitations, we propose a new training paradigm that involves generating a vast quantity of high-quality synthetic data with labels utilizing ChatGPT and fine-tuning a local model for the downstream task. Our method has resulted in significant improvements in the performance of downstream tasks, improving the F1-score from 23.37% to 63.99% for the named entity recognition task and from 75.86% to 83.59% for the relation extraction task. Furthermore, generating data using ChatGPT can significantly reduce the time and effort required for data collection and labeling, as well as mitigate data privacy concerns. In summary, the proposed framework presents a promising solution to enhance the applicability of LLM models to clinical text mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.04360,March 2023,325,Editorial,"Magdy Bayoumi, Shuvra S. Bhattacharyya and Rudy Lauwereins",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703002907,20 May 2003,
326,0.000678989392134464,326,CroCoSum: A Benchmark Dataset for Cross-Lingual Code-Switched Summarization,"Ruochen Zhang, Carsten Eickhoff","Cross-lingual summarization (CLS) has attracted increasing interest in recent years due to the availability of large-scale web-mined datasets and the advancements of multilingual language models. However, given the rareness of naturally occurring CLS resources, the majority of datasets are forced to rely on translation which can contain overly literal artifacts. This restricts our ability to observe naturally occurring CLS pairs that capture organic diction, including instances of code-switching. This alteration between languages in mid-message is a common phenomenon in multilingual settings yet has been largely overlooked in cross-lingual contexts due to data scarcity. To address this gap, we introduce CroCoSum, a dataset of cross-lingual code-switched summarization of technology news. It consists of over 24,000 English source articles and 18,000 human-curated Chinese news summaries, with more than 92% of the summaries containing code-switched phrases. For reference, we evaluate the performance of existing approaches including pipeline, end-to-end, and zero-shot methods. We show that leveraging existing resources as a pretraining step does not improve performance on CroCoSum, indicating the limited generalizability of existing resources. Finally, we discuss the challenges of evaluating cross-lingual summarizers on code-switched generation through qualitative error analyses. Our collection and code can be accessed at https://github.com/RosenZhang/CroCoSum. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.04092,March 2023,326,Dynamic Chest Image Analysis: Model-Based Perfusion Analysis in Dynamic Pulmonary Imaging,"Jianming Liang, Timo Järvi, Aaro Kiuru, Martti Kormano and Erkki Svedström","The ""Dynamic Chest Image Analysis"" project aims to develop model-based computer analysis and visualization methods for showing focal and general abnormalities of lung ventilation and perfusion based on a seque...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703212117,14 April 2003,
327,0.000678989392134464,327,An End-to-End Approach for Online Decision Mining and Decision Drift Analysis in Process-Aware Information Systems: Extended Version,"Beate Scheibel, Stefanie Rinderle-Ma","Decision mining enables the discovery of decision rules from event logs or streams, and constitutes an important part of in-depth analysis and optimisation of business processes. So far, decision mining has been merely applied in an ex-post way resulting in a snapshot of decision rules for the given chunk of log data. Online decision mining, by contrast, enables continuous monitoring of decision rule evolution and decision drift. Hence this paper presents an end-to-end approach for the discovery as well as monitoring of decision points and the corresponding decision rules during runtime, bridging the gap between online control flow discovery and decision mining. The approach provides automatic decision support for process-aware information systems with efficient decision drift discovery and monitoring. For monitoring, not only the performance, in terms of accuracy, of decision rules is taken into account, but also the occurrence of data elements and changes in branching frequency. The paper provides two algorithms, which are evaluated on four synthetic and one real-life data set, showing feasibility and applicability of the approach. Overall, the approach fosters the understanding of decisions in business processes and hence contributes to an improved human-process interaction. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.03961,March 2023,327,Creating a Virtual Science Lab (VSL): the adoption of virtual labs in Saudi schools,"Khulood Aljuhani, Marwa Sonbul, Mashail Althabiti and Maram Meccawy",Laboratory activities are playing a substantial role in supporting scientific learning fields by enabling students to obtain practical skills through experiments and by giving them the chance to have a more pr...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-018-0067-9,14 September 2018,
328,0.000678989392134464,328,Fast and Multi-aspect Mining of Complex Time-stamped Event Streams,"Kota Nakamura, Yasuko Matsubara, Koki Kawabata, Yuhei Umeda, Yuichiro Wada, Yasushi Sakurai","Given a huge, online stream of time-evolving events with multiple attributes, such as online shopping logs: (item, price, brand, time), and local mobility activities: (pick-up and drop-off locations, time), how can we summarize large, dynamic high-order tensor streams? How can we see any hidden patterns, rules, and anomalies? Our answer is to focus on two types of patterns, i.e., ''regimes'' and ''components'', for which we present CubeScope, an efficient and effective method over high-order tensor streams. Specifically, it identifies any sudden discontinuity and recognizes distinct dynamical patterns, ''regimes'' (e.g., weekday/weekend/holiday patterns). In each regime, it also performs multi-way summarization for all attributes (e.g., item, price, brand, and time) and discovers hidden ''components'' representing latent groups (e.g., item/brand groups) and their relationship. Thanks to its concise but effective summarization, CubeScope can also detect the sudden appearance of anomalies and identify the types of anomalies that occur in practice. Our proposed method has the following properties: (a) Effective: it captures dynamical multi-aspect patterns, i.e., regimes and components, and statistically summarizes all the events; (b) General: it is practical for successful application to data compression, pattern discovery, and anomaly detection on various types of tensor streams; (c) Scalable: our algorithm does not depend on the length of the data stream and its dimensionality. Extensive experiments on real datasets demonstrate that CubeScope finds meaningful patterns and anomalies correctly, and consistently outperforms the state-of-the-art methods as regards accuracy and execution speed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.03789,March 2023,328,Influence of algorithmic abstraction and mathematical knowledge on rates of dropout from Computing degree courses,"Raphael Magalhães Hoed, Marcelo Ladeira and Leticia Lopes Leite","This paper presents a study of rates of dropout from Brazilian degree courses, based on data provided by the National Institute for Educational Studies and Research “Anísio Teixeira” (INEP) and a case study ca...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-018-0074-2,9 August 2018,
329,0.000678989392134464,329,Computing Effective Resistances on Large Graphs Based on Approximate Inverse of Cholesky Factor,"Zhiqiang Liu, Wenjian Yu","Effective resistance, which originates from the field of circuits analysis, is an important graph distance in spectral graph theory. It has found numerous applications in various areas, such as graph data mining, spectral graph sparsification, circuits simulation, etc. However, computing effective resistances accurately can be intractable and we still lack efficient methods for estimating effective resistances on large graphs. In this work, we propose an efficient algorithm to compute effective resistances on general weighted graphs, based on a sparse approximate inverse technique. Compared with a recent competitor, the proposed algorithm shows several hundreds of speedups and also one to two orders of magnitude improvement in the accuracy of results. Incorporating the proposed algorithm with the graph sparsification based power grid (PG) reduction framework, we develop a fast PG reduction method, which achieves an average 6.4X speedup in the reduction time without loss of reduction accuracy. In the applications of power grid transient analysis and DC incremental analysis, the proposed method enables 1.7X and 2.5X speedup of overall time compared to using the PG reduction based on accurate effective resistances, without increase in the error of solution. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.03617,March 2023,329,"Erratum to: A multi-agent cooperative reinforcement learning model using a hierarchy of consultants, tutors and workers","Bilal H. Abed-alguni, Stephan K. Chalup, Frans A. Henskens and David J. Paul",Theoriginal articlewas published inVietnam Journal of Computer Science20152:s40595-015-0045-x,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-015-0047-8,28 July 2015,
330,0.000678989392134464,330,Implementation of a noisy hyperlink removal system: A semantic and relatedness approach,"Kazem Taghandiki, Elnaz Rezaei Ehsan","As the volume of data on the web grows, the web structure graph, which is a graph representation of the web, continues to evolve. The structure of this graph has gradually shifted from content-based to non-content-based. Furthermore, spam data, such as noisy hyperlinks, in the web structure graph adversely affect the speed and efficiency of information retrieval and link mining algorithms. Previous works in this area have focused on removing noisy hyperlinks using structural and string approaches. However, these approaches may incorrectly remove useful links or be unable to detect noisy hyperlinks in certain circumstances. In this paper, a data collection of hyperlinks is initially constructed using an interactive crawler. The semantic and relatedness structure of the hyperlinks is then studied through semantic web approaches and tools such as the DBpedia ontology. Finally, the removal process of noisy hyperlinks is carried out using a reasoner on the DBpedia ontology. Our experiments demonstrate the accuracy and ability of semantic web technologies to remove noisy hyperlinks △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.03321,March 2023,330,Correction: Improved wild horse optimization with levy fight algorithm for effective task scheduling in cloud computing,"G. Saravanan, S. Neelakandan, P. Ezhumalai and Sudhanshu Maurya",Theoriginal articlewas published inJournal of Cloud Computing202312:24,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00432-8,12 April 2023,
331,0.000678989392134464,331,Mining archival data from wide-field astronomical surveys in search of near-Earth objects,"Teymoor Saifollahi, Gijs Verdoes Kleijn, Rees Williams, Marco Micheli, Toni Santana-Ros, Ewout Helmich, Detlef Koschny, Luca Conversi","Increasing our knowledge of the orbits and compositions of Near-Earth Objects (NEOs) is important for a better understanding of the evolution of the Solar System and of life. The detection of serendipitous NEO appearances among the millions of archived exposures from large astronomical imaging surveys can provide a contribution which is complementary to NEO surveys. Using the AstroWISE information system, this work aims to assess the detectability rate, the achieved recovery rate and the quality of astrometry when data mining the ESO archive for the OmegaCAM wide-field imager at the VST. We developed an automatic pipeline that searches for the NEO appearances inside the AstroWISE environment. Throughout the recovery process, the pipeline uses several public web-tools to identify possible images that overlap with the position of NEOs, and acquires information on the NEOs predicted position and other properties (e.g., magnitude, rate and direction of motion) at the time of observations. We have recovered 196 appearances of NEOs from a set of 968 appearances predicted to be recoverable. It includes appearances for three NEOs which were on the impact risk list at that point. These appearances were well before their discovery. The subsequent risk assessment using the extracted astrometry removes these NEOs from the risk list. We estimate a detectability rate of 0.05 per NEO at an SNR>3 for NEOs in the OmegaCAM archive. Our automatic recovery rates are 40% and 20% for NEOs on the risk list and the full list, respectively. The achieved astrometric and photometric accuracy is on average 0.12 arcsec and 0.1 mag. These results show the high potential of the archival imaging data of the ground-based wide-field surveys as useful instruments for the search, (p)recovery and characterization of NEOs. Highly automated approaches, as possible using AstroWISE, make this undertaking feasible. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.03164,March 2023,331,STEAM in education: a bibliometric analysis of performance and co-words in Web of Science,"José-Antonio Marín-Marín, Antonio-José Moreno-Guerrero, Pablo Dúo-Terrón and Jesús López-Belmonte","Emerging methodologies that apply and integrate science, technology, engineering, art, and math (STEAM) in education have appeared in recent years as a pedagogical alternative providing more holistic and attra...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00296-x,25 June 2021,
332,0.000678989392134464,332,Continual Causal Inference with Incremental Observational Data,"Zhixuan Chu, Ruopeng Li, Stephen Rathbun, Sheng Li","The era of big data has witnessed an increasing availability of observational data from mobile and social networking, online advertising, web mining, healthcare, education, public policy, marketing campaigns, and so on, which facilitates the development of causal effect estimation. Although significant advances have been made to overcome the challenges in the academic area, such as missing counterfactual outcomes and selection bias, they only focus on source-specific and stationary observational data, which is unrealistic in most industrial applications. In this paper, we investigate a new industrial problem of causal effect estimation from incrementally available observational data and present three new evaluation criteria accordingly, including extensibility, adaptability, and accessibility. We propose a Continual Causal Effect Representation Learning method for estimating causal effects with observational data, which are incrementally available from non-stationary data distributions. Instead of having access to all seen observational data, our method only stores a limited subset of feature representations learned from previous data. Combining selective and balanced representation learning, feature representation distillation, and feature transformation, our method achieves the continual causal effect estimation for new data without compromising the estimation capability for original data. Extensive experiments demonstrate the significance of continual causal effect estimation and the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.01775,March 2023,332,The distorted mirror of Wikipedia: a quantitative analysis of Wikipedia coverage of academics,Anna Samoilenko and Taha Yasseri,"Activity of modern scholarship creates online footprints galore. Along with traditional metrics of research quality, such as citation counts, online images of researchers and institutions increasingly matter i...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds20,22 January 2014,
333,0.000678989392134464,333,Uses and Gratifications of Alternative Social Media: Why do people use Mastodon?,"Kijung Lee, Mian Wang","The primary purpose of this investigation is to answer the research questions; 1) What are users' motivations for joining Mastodon?; 2) What are users' gratifications for using Mastodon?; and 3) What are the primary reasons that the users continue to use Mastodon? We analyzed the collected data from the perspective of the Uses and Gratifications Theory. A questionnaire was designed to measure the opinions of Mastodon users from 15 different Mastodon instances. We examined 47 items through exploratory factor analysis using principal components extraction with Varimax with Kaiser Normalization. The results extracted 7 factors of gratification sought (expectation) and 7 factors of gratification obtained. We discovered that the primary reason that the users join and use Mastodon is the ease of controlling and sheltering users' information from data mining. The findings of the gratification sought structure are similar to findings of the gratification obtained structure, and the comparison between the two groups of data suggests that users are satisfied with the ongoing use of Mastodon. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.01285,March 2023,333,"Teachers’ participation in professional development concerning the implementation of new technologies in class: a latent class analysis of teachers and the relationship with the use of computers, ICT self-efficacy and emphasis on teaching ICT skills",Kerstin Drossel and Birgit Eickelmann,"The increasing availability of new technologies in an ever more digitalized world has gained momentum in practically all spheres of life, making technology-related skills a key competence not only in professio...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-017-0053-7,27 November 2017,
334,0.000678989392134464,334,WEARDA: recording wearable sensor data for human activity monitoring,"Richard van Dijk, Daniela Gawehns, Matthijs van Leeuwen","We present WEARDA, the open source WEARable sensor Data Acquisition software package. WEARDA facilitates the acquisition of human activity data with smartwatches and is primarily aimed at researchers who require transparency, full control, and access to raw sensor data. It provides functionality to simultaneously record raw data from four sensors -- tri-axis accelerometer, tri-axis gyroscope, barometer, and GPS -- which should enable researchers to, for example, estimate energy expenditure and mine movement trajectories. A Samsung smartwatch running the Tizen OS was chosen because of 1) the required functionalities of the smartwatch software API, 2) the availability of software development tools and accessible documentation, 3) having the required sensors, and 4) the requirements on case design for acceptance by the target user group. WEARDA addresses five practical challenges concerning preparation, measurement, logistics, privacy preservation, and reproducibility to ensure efficient and errorless data collection. The software package was initially created for the project ``Dementia back at the heart of the community'', and has been successfully used in that context. △ Less",https://arxiv.orghttps://arxiv.org/abs/2303.00064,March 2023,334,Implementing patient-reported outcomes in routine clinical care for diverse and underrepresented patients in the United States,"Colby J. Hyland, Ruby Guo, Ravi Dhawan, Manraj N. Kaur, Paul A. Bain, Maria O. Edelen and Andrea L. Pusic","Patient-reported outcomes (PROs) are used increasingly in routine clinical care and inform policies, reimbursements, and quality improvement. Less is known regarding PRO implementation in routine clinical care...",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-022-00428-z,7 March 2022,
335,0.000678989392134464,335,Identifying roadway departure crash patterns on rural two-lane highways under different lighting conditions: association knowledge using data mining approach,"Ahmed Hossain, Xiaoduan Sun, Shahrin Islam, Shah Alam, Md Mahmud Hossain","More than half of all fatalities on U.S. highways occur due to roadway departure (RwD) each year. Previous research has explored various risk factors that contribute to RwD crashes, however, a comprehensive investigation considering the effect of lighting conditions has been insufficiently addressed. Using the Louisiana Department of Transportation and Development crash database, fatal and injury RwD crashes occurring on rural two-lane (R2L) highways between 2008-2017 were analyzed based on daylight and dark (with/without streetlight). This research employed a safe system approach to explore meaningful complex interactions among multidimensional crash risk factors. To accomplish this, an unsupervised data mining algorithm association rules mining (ARM) was utilized. Based on the generated rules, the findings reveal several interesting crash patterns in the daylight, dark-with-streetlight, and dark-no-streetlight, emphasizing the importance of investigating RwD crash patterns depending on the lighting conditions. In daylight, fatal RwD crashes are associated with cloudy weather conditions, distracted drivers, standing water on the roadway, no seat belt use, and construction zones. In dark lighting conditions (with/without streetlight), the majority of the RwD crashes are associated with alcohol/drug involvement, young drivers (15-24 years), driver condition (e.g., inattentive, distracted, illness/fatigued/asleep) and colliding with animal (s). The findings reveal how certain driver behavior patterns are connected to RwD crashes, such as a strong association between alcohol/drug intoxication and no seat belt usage in the dark-no-streetlight condition. Based on the identified crash patterns and behavioral characteristics under different lighting conditions, the findings could aid researchers and safety specialists in developing the most effective RwD crash mitigation strategies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.14754,February 2023,335,Letter from the guest editors,"Fernando Santos Osório, Denis Fernando Wolf and Pablo Javier Alsina",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194501,September 2009,
336,0.000678989392134464,336,Self-training through Classifier Disagreement for Cross-Domain Opinion Target Extraction,"Kai Sun, Richong Zhang, Samuel Mensah, Nikolaos Aletras, Yongyi Mao, Xudong Liu","Opinion target extraction (OTE) or aspect extraction (AE) is a fundamental task in opinion mining that aims to extract the targets (or aspects) on which opinions have been expressed. Recent work focus on cross-domain OTE, which is typically encountered in real-world scenarios, where the testing and training distributions differ. Most methods use domain adversarial neural networks that aim to reduce the domain gap between the labelled source and unlabelled target domains to improve target domain performance. However, this approach only aligns feature distributions and does not account for class-wise feature alignment, leading to suboptimal results. Semi-supervised learning (SSL) has been explored as a solution, but is limited by the quality of pseudo-labels generated by the model. Inspired by the theoretical foundations in domain adaptation [2], we propose a new SSL approach that opts for selecting target samples whose model output from a domain-specific teacher and student network disagree on the unlabelled target data, in an effort to boost the target domain performance. Extensive experiments on benchmark cross-domain OTE datasets show that this approach is effective and performs consistently well in settings with large domain shifts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.14719,February 2023,336,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0034-6,18 May 2011,
337,0.000678989392134464,337,Identification of pattern mining algorithm for rugby league players positional groups separation based on movement patterns,"Victor Elijah Adeyemo, Anna Palczewska, Ben Jones, Dan Weaving","The application of pattern mining algorithms to extract movement patterns from sports big data can improve training specificity by facilitating a more granular evaluation of movement. As there are various pattern mining algorithms, this study aimed to validate which algorithm discovers the best set of movement patterns for player movement profiling in professional rugby league and the similarity in extracted movement patterns between the algorithms. Three pattern mining algorithms (l-length Closed Contiguous [LCCspm], Longest Common Subsequence [LCS] and AprioriClose) were used to profile elite rugby football league hookers (n = 22 players) and wingers (n = 28 players) match-games movements across 319 matches. Machine learning classification algorithms were used to identify which algorithm gives the best set of movement patterns to separate playing positions with Jaccard similarity score identifying the extent of similarity between algorithms' movement patterns. LCCspm and LCS movement patterns shared a 0.19 Jaccard similarity score. AprioriClose movement patterns shared no significant similarity with LCCspm and LCS patterns. The closed contiguous movement patterns profiled by LCCspm best-separated players into playing positions. Multi-layered Perceptron algorithm achieved the highest accuracy of 91.02% and precision, recall and F1 scores of 0.91 respectively. Therefore, we recommend the extraction of closed contiguous (consecutive) over non-consecutive movement patterns for separating groups of players. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.14058,February 2023,337,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-013-0100-3,8 February 2013,
338,0.000678989392134464,338,Multi-Feature Integration for Perception-Dependent Examination-Bias Estimation,"Xiaoshu Chen, Xiangsheng Li, Kunliang Wei, Bin Hu, Lei Jiang, Zeqian Huang, Zhanhui Kang","Eliminating examination bias accurately is pivotal to apply click-through data to train an unbiased ranking model. However, most examination-bias estimators are limited to the hypothesis of Position-Based Model (PBM), which supposes that the calculation of examination bias only depends on the rank of the document. Recently, although some works introduce information such as clicks in the same query list and contextual information when calculating the examination bias, they still do not model the impact of document representation on search engine result pages (SERPs) that seriously affects one's perception of document relevance to a query when examining. Therefore, we propose a Multi-Feature Integration Model (MFIM) where the examination bias depends on the representation of document except the rank of it. Furthermore, we mine a key factor slipoff counts that can indirectly reflects the influence of all perception-bias factors. Real world experiments on Baidu-ULTR dataset demonstrate the superior effectiveness and robustness of the new approach. The source code is available at \href{https://github.com/lixsh6/Tencent_wsdm_cup2023/tree/main/pytorch_unbias}{https://github.com/lixsh6/Tencent\_wsdm\_cup2023} △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.13756,February 2023,338,Frontiers of business intelligence and analytics 3.0: a taxonomy-based literature review and research agenda,Mathias Eggert and Jens Alberts,"Researching the field of business intelligence and analytics (BI & A) has a long tradition within information systems research. Thereby, in each decade the rapid development of technologies opened new room for...",https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40685-020-00108-y,24 April 2020,
339,0.000678989392134464,339,Strategize Before Teaching: A Conversational Tutoring System with Pedagogy Self-Distillation,"Lingzhi Wang, Mrinmaya Sachan, Xingshan Zeng, Kam-Fai Wong","Conversational tutoring systems (CTSs) aim to help students master educational material with natural language interaction in the form of a dialog. CTSs have become a key pillar in educational data mining research. A key challenge in CTSs is to engage the student in the conversation while exposing them to a diverse set of teaching strategies, akin to a human teacher, thereby, helping them learn in the process. Different from previous work that generates responses given the strategies as input, we propose to jointly predict teaching strategies and generate tutor responses accordingly, which fits a more realistic application scenario. We benchmark several competitive models on three dialog tutoring datasets and propose a unified framework that combines teaching response generation and pedagogical strategy prediction, where a self-distillation mechanism is adopted to guide the teaching strategy learning and facilitate tutor response generation. Our experiments and analyses shed light on how teaching strategies affect dialog tutoring. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.13496,February 2023,339,Understanding the development trends of big data technologies: an analysis of patents and the cited scholarly works,Tahereh Saheb and Tayebeh Saheb,Big data innovation is a key instrument for economic and social development and for the creation of new business opportunities. This study analyzes the patenting activities of global jurisdictions in the field...,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00287-9,22 February 2020,
340,0.000678989392134464,340,Self-similarity Driven Scale-invariant Learning for Weakly Supervised Person Search,"Benzhi Wang, Yang Yang, Jinlin Wu, Guo-jun Qi, Zhen Lei","Weakly supervised person search aims to jointly detect and match persons with only bounding box annotations. Existing approaches typically focus on improving the features by exploring relations of persons. However, scale variation problem is a more severe obstacle and under-studied that a person often owns images with different scales (resolutions). On the one hand, small-scale images contain less information of a person, thus affecting the accuracy of the generated pseudo labels. On the other hand, the similarity of cross-scale images is often smaller than that of images with the same scale for a person, which will increase the difficulty of matching. In this paper, we address this problem by proposing a novel one-step framework, named Self-similarity driven Scale-invariant Learning (SSL). Scale invariance can be explored based on the self-similarity prior that it shows the same statistical properties of an image at different scales. To this end, we introduce a Multi-scale Exemplar Branch to guide the network in concentrating on the foreground and learning scale-invariant features by hard exemplars mining. To enhance the discriminative power of the features in an unsupervised manner, we introduce a dynamic multi-label prediction which progressively seeks true labels for training. It is adaptable to different types of unlabeled data and serves as a compensation for clustering based strategy. Experiments on PRW and CUHK-SYSU databases demonstrate the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.12986,February 2023,340,Efficient Reduction of Access Latency through Object Correlations in Virtual Environments,Shao-Shin Hung and Damon Shing-Min Liu,"Object correlations are common semantic patterns in virtual environments. They can be exploited to improve the effectiveness of storage caching, prefetching, data layout, and disk scheduling. However, we have ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/10289,1 December 2007,
341,0.000678989392134464,341,Deep Graph Stream SVDD: Anomaly Detection in Cyber-Physical Systems,"Ehtesamul Azim, Dongjie Wang, Yanjie Fu","Our work focuses on anomaly detection in cyber-physical systems. Prior literature has three limitations: (1) Failing to capture long-delayed patterns in system anomalies; (2) Ignoring dynamic changes in sensor connections; (3) The curse of high-dimensional data samples. These limit the detection performance and usefulness of existing works. To address them, we propose a new approach called deep graph stream support vector data description (SVDD) for anomaly detection. Specifically, we first use a transformer to preserve both short and long temporal patterns of monitoring data in temporal embeddings. Then we cluster these embeddings according to sensor type and utilize them to estimate the change in connectivity between various sensors to construct a new weighted graph. The temporal embeddings are mapped to the new graph as node attributes to form weighted attributed graph. We input the graph into a variational graph auto-encoder model to learn final spatio-temporal representation. Finally, we learn a hypersphere that encompasses normal embeddings and predict the system status by calculating the distances between the hypersphere and data samples. Extensive experiments validate the superiority of our model, which improves F1-score by 35.87%, AUC by 19.32%, while being 32 times faster than the best baseline at training and inference. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.12918,February 2023,341,Effectiveness of digital educational game and game design in STEM learning: a meta-analytic review,"Yang Gui, Zhihui Cai, Yajiao Yang, Lingyuan Kong, Xitao Fan and Robert H. Tai","Digital educational games exhibit substantial promise in advancing STEM education. Nevertheless, the empirical evidence on both the efficacy of digital game-based learning and its designs in STEM education is ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00424-9,19 May 2023,
342,0.000678989392134464,342,Hybrid machine-learned homogenization: Bayesian data mining and convolutional neural networks,"Julian Lißner, Felix Fritzen","Beyond the generally deployed features for microstructure property prediction this study aims to improve the machine learned prediction by developing novel feature descriptors. Therefore, Bayesian infused data mining is conducted to acquire samples containing characteristics inexplicable to the current feature set, and suitable feature descriptors to describe these characteristics are proposed. The iterative development of feature descriptors resulted in 37 novel features, being able to reduce the prediction error by roughly one third. To further improve the predictive model, convolutional neural networks (Conv Nets) are deployed to generate auxiliary features in a supervised machine learning manner. The Conv Nets were able to outperform the feature based approach. A key ingredient for that is a newly proposed data augmentation scheme and the development of so-called deep inception modules. A combination of the feature based approach and the convolutional neural network leads to a hybrid neural network: A parallel deployment of the both neural network archetypes in a single model achieved a relative rooted mean squared error below 1%, more than halving the error compared to prior models operating on the same data. The hybrid neural network was found powerful enough to be extended to predict variable material parameters, from a low to high phase contrast, while allowing for arbitrary microstructure geometry at the same time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.12545,February 2023,342,A bibliometric review of cryptocurrencies: how have they grown?,"Francisco Javier García-Corral, José Antonio Cordero-García, Jaime de Pablo-Valenciano and Juan Uribe-Toril","With the development of new technologies, some concepts become relevant in the economic area, as is the case with cryptocurrencies, in general, or Bitcoin and Ethereum, in particular. Due to the impact of thes...",https://www.springeropen.com//jfin-swufe.springeropen.com/articles/10.1186/s40854-021-00306-5,1 January 2022,
343,0.000678989392134464,343,Better Predict the Dynamic of Geometry of In-Pit Stockpiles Using Geospatial Data and Polygon Models,"Mehala. Balamurali, Konstantin M. Seiler","Modelling stockpile is a key factor of a project economic and operation in mining, because not all the mined ores are not able to mill for many reasons. Further, the financial value of the ore in the stockpile needs to be reflected on the balance sheet. Therefore, automatically tracking the frontiers of the stockpile facilitates the mine scheduling engineers to calculate the tonnage of the ore remaining in the stockpile. This paper suggests how the dynamic of stockpile shape changes caused by dumping and reclaiming operations can be inferred using polygon models. The presented work also demonstrates how the geometry of stockpiles can be inferred in the absence of reclaimed bucket information, in which case the reclaim polygons are established using the diggers GPS positional data at the time of truck loading. This work further compares two polygon models for creating 2D shapes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.12392,February 2023,343,Technosocial predictive analytics for security informatics,"Antonio Sanfilippo, Nigel Gilbert and Mark Greaves",Unknown,https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-1-8,22 August 2012,
344,0.000678989392134464,344,Extensions to Generalized Annotated Logic and an Equivalent Neural Architecture,"Paulo Shakarian, Gerardo I. Simari","While deep neural networks have led to major advances in image recognition, language translation, data mining, and game playing, there are well-known limits to the paradigm such as lack of explainability, difficulty of incorporating prior knowledge, and modularity. Neuro symbolic hybrid systems have recently emerged as a straightforward way to extend deep neural networks by incorporating ideas from symbolic reasoning such as computational logic. In this paper, we propose a list desirable criteria for neuro symbolic systems and examine how some of the existing approaches address these criteria. We then propose an extension to generalized annotated logic that allows for the creation of an equivalent neural architecture comprising an alternate neuro symbolic hybrid. However, unlike previous approaches that rely on continuous optimization for the training process, our framework is designed as a binarized neural network that uses discrete optimization. We provide proofs of correctness and discuss several of the challenges that must be overcome to realize this framework in an implemented system. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.12195,February 2023,344,Cloud-Based Code Execution Framework for scientific problem solving environments,"Thomas Ludescher, Thomas Feilhauer and Peter Brezany","In this paper we present a novel Code Execution Framework that can execute code of different problem solving environments (PSE), such as MATLAB, R and Octave, in parallel. In many e-Science domains different s...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-2-11,10 May 2013,
345,0.000678989392134464,345,MFBE: Leveraging Multi-Field Information of FAQs for Efficient Dense Retrieval,"Debopriyo Banerjee, Mausam Jain, Ashish Kulkarni","In the domain of question-answering in NLP, the retrieval of Frequently Asked Questions (FAQ) is an important sub-area which is well researched and has been worked upon for many languages. Here, in response to a user query, a retrieval system typically returns the relevant FAQs from a knowledge-base. The efficacy of such a system depends on its ability to establish semantic match between the query and the FAQs in real-time. The task becomes challenging due to the inherent lexical gap between queries and FAQs, lack of sufficient context in FAQ titles, scarcity of labeled data and high retrieval latency. In this work, we propose a bi-encoder-based query-FAQ matching model that leverages multiple combinations of FAQ fields (like, question, answer, and category) both during model training and inference. Our proposed Multi-Field Bi-Encoder (MFBE) model benefits from the additional context resulting from multiple FAQ fields and performs well even with minimal labeled data. We empirically support this claim through experiments on proprietary as well as open-source public datasets in both unsupervised and supervised settings. Our model achieves around 27% and 20% better top-1 accuracy for the FAQ retrieval task on internal and open datasets, respectively over the best performing baseline. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11953,February 2023,345,Cultivating a research agenda for data science,Chris A Mattmann,"I describe a research agenda for data science based on a decade of research and operational work in data-intensive systems at NASA, the University of Southern California, and in the context of open source work...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/2196-1115-1-6,6 August 2014,
346,0.000678989392134464,346,Counterfactual Situation Testing: Uncovering Discrimination under Fairness given the Difference,"Jose M. Alvarez, Salvatore Ruggieri","We present counterfactual situation testing (CST), a causal data mining framework for detecting discrimination in classifiers. CST aims to answer in an actionable and meaningful way the intuitive question ""what would have been the model outcome had the individual, or complainant, been of a different protected status?"" It extends the legally-grounded situation testing of Thanh et al. (2011) by operationalizing the notion of fairness given the difference using counterfactual reasoning. For any complainant, we find and compare similar protected and non-protected instances in the dataset used by the classifier to construct a control and test group, where a difference between the decision outcomes of the two groups implies potential individual discrimination. Unlike situation testing, which builds both groups around the complainant, we build the test group on the complainant's counterfactual generated using causal knowledge. The counterfactual is intended to reflect how the protected attribute when changed affects the seemingly neutral attributes used by the classifier, which is taken for granted in many frameworks for discrimination. Under CST, we compare similar individuals within each group but dissimilar individuals across both groups due to the possible difference between the complainant and its counterfactual. Evaluating our framework on two classification scenarios, we show that it uncovers a greater number of cases than situation testing, even when the classifier satisfies the counterfactual fairness condition of Kusner et al. (2017). △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11944,February 2023,346,Correction: Violence detection approach based on cloud data and Neutrosophic cognitive maps,"Mohammad Saif Wajid, Hugo Terashima-Marin, Peyman Najafrad Paul Rad and Mohd Anas Wajid",Theoriginal articlewas published inJournal of Cloud Computing202211:85,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-022-00381-8,27 December 2022,
347,0.000678989392134464,347,Factors Influencing Autonomously Generated 3D Geophysical Spatial Models,"M. Balamurali, A. Melkumyan, J. Zigman","Understanding the contribution of geophysical variables is vital for identifying the ore indicator regions. Both magnetometry and gamma-rays are used to identify the geophysical signatures of the rocks. Density is another key variable for tonnage estimation in mining and needs to be re-estimated in areas of change when a boundary update has been conducted. Modelling these geophysical variables in 3D will enable investigate the properties of the rocks and improve our understanding of the ore. Gaussian Process (GP) was previously used to generate 3D spatial models for grade estimation using geochemical assays. This study investigates the influence of the following two factors on the GP-based autonomously generated 3D geophysical models: the resolution of the input data and the number of nearest samples used in the training process. A case study was conducted on a typical Hammersley Ranges iron ore deposit using geophysical logs, including density, collected from the exploration holes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11572,February 2023,347,Preservice teachers perceptions about the use of blended learning in a science education methods course,Özkan Yılmaz and Kathy L. Malone,The purpose of this quantitative study was to determine the effectiveness of blended learning within the context of a science education methods course for early childhood elementary preservice teachers in Turk...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00126-7,18 June 2020,
348,0.00673579331412596,348,One-Pot Multi-Frame Denoising,"Lujia Jin, Shi Zhao, Lei Zhu, Qian Chen, Yanye Lu","The performance of learning-based denoising largely depends on clean supervision. However, it is difficult to obtain clean images in many scenes. On the contrary, the capture of multiple noisy frames for the same field of view is available and often natural in real life. Therefore, it is necessary to avoid the restriction of clean labels and make full use of noisy data for model training. So we propose an unsupervised learning strategy named one-pot denoising (OPD) for multi-frame images. OPD is the first proposed unsupervised multi-frame denoising (MFD) method. Different from the traditional supervision schemes including both supervised Noise2Clean (N2C) and unsupervised Noise2Noise (N2N), OPD executes mutual supervision among all of the multiple frames, which gives learning more diversity of supervision and allows models to mine deeper into the correlation among frames. N2N has also been proved to be actually a simplified case of the proposed OPD. From the perspectives of data allocation and loss function, two specific implementations, random coupling (RC) and alienation loss (AL), are respectively provided to accomplish OPD during model training. In practice, our experiments demonstrate that OPD behaves as the SOTA unsupervised denoising method and is comparable to supervised N2C methods for synthetic Gaussian and Poisson noise, and real-world optical coherence tomography (OCT) speckle noise. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11544,February 2023,348,"Research progress in optical neural networks: theory, applications and developments","Jia Liu, Qiuhao Wu, Xiubao Sui, Qian Chen, Guohua Gu, Liping Wang and Shengcai Li","With the advent of the era of big data, artificial intelligence has attracted continuous attention from all walks of life, and has been widely used in medical image analysis, molecular and material science, la...",https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-021-00026-0,19 April 2021,
349,0.000678989392134464,349,"Advancements in Federated Learning: Models, Methods, and Privacy","Huiming Chen, Huandong Wang, Qingyue Long, Depeng Jin, Yong Li","Federated learning (FL) is a promising technique for addressing the rising privacy and security issues. Its main ingredient is to cooperatively learn the model among the distributed clients without uploading any sensitive data. In this paper, we conducted a thorough review of the related works, following the development context and deeply mining the key technologies behind FL from both theoretical and practical perspectives. Specifically, we first classify the existing works in FL architecture based on the network topology of FL systems with detailed analysis and summarization. Next, we abstract the current application problems, summarize the general techniques and frame the application problems into the general paradigm of FL base models. Moreover, we provide our proposed solutions for model training via FL. We have summarized and analyzed the existing FedOpt algorithms, and deeply revealed the algorithmic development principles of many first-order algorithms in depth, proposing a more generalized algorithm design framework. Based on these frameworks, we have instantiated FedOpt algorithms. As privacy and security is the fundamental requirement in FL, we provide the existing attack scenarios and the defense methods. To the best of our knowledge, we are among the first tier to review the theoretical methodology and propose our strategies since there are very few works surveying the theoretical approaches. Our survey targets motivating the development of high-performance, privacy-preserving, and secure methods to integrate FL into real-world applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11466,February 2023,349,Spatio-Temporal Graphical-Model-Based Multiple Facial Feature Tracking,Congyong Su and Li Huang,"It is challenging to track multiple facial features simultaneously when rich expressions are presented on a face. We propose a two-step solution. In the first step, several independent condensation-style parti...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2091,15 August 2005,
350,0.000678989392134464,350,Mining compact high utility sequential patterns,"Tai Dinh, Philippe Fournier-Viger, Huynh Van Hong","High utility sequential pattern mining (HUSPM) aims to mine all patterns that yield a high utility (profit) in a sequence dataset. HUSPM is useful for several applications such as market basket analysis, marketing, and website clickstream analysis. In these applications, users may also consider high utility patterns frequently appearing in the dataset to obtain more fruitful information. However, this task is high computation since algorithms may generate a combinatorial explosive number of candidates that may be redundant or of low importance. To reduce complexity and obtain a compact set of frequent high utility sequential patterns (FHUSPs), this paper proposes an algorithm named CHUSP for mining closed frequent high utility sequential patterns (CHUSPs). Such patterns keep a concise representation while preserving the same expressive power of the complete set of FHUSPs. The proposed algorithm relies on a CHUS data structure to maintain information during mining. It uses three pruning strategies to eliminate early low-utility and non-frequent patterns, thereby reducing the search space. An extensive experimental evaluation was performed on six real-life datasets to evaluate the performance of CHUSP in terms of execution time, memory usage, and the number of generated patterns. Experimental results show that CHUSP can efficiently discover the compact set of CHUSPs under different user-defined thresholds. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11426,February 2023,350,Learning log-based automatic group formation: system design and classroom implementation study,"Changhao Liang, Rwitajit Majumdar and Hiroaki Ogata","Collaborative learning in the form of group work is becoming increasingly significant in education since interpersonal skills count in modern society. However, teachers often get overwhelmed by the logistics i...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-021-00156-w,12 May 2021,
351,0.000678989392134464,351,Explainable Contextual Anomaly Detection using Quantile Regression Forests,"Zhong Li, Matthijs van Leeuwen","Traditional anomaly detection methods aim to identify objects that deviate from most other objects by treating all features equally. In contrast, contextual anomaly detection methods aim to detect objects that deviate from other objects within a context of similar objects by dividing the features into contextual features and behavioral features. In this paper, we develop connections between dependency-based traditional anomaly detection methods and contextual anomaly detection methods. Based on resulting insights, we propose a novel approach to inherently interpretable contextual anomaly detection that uses Quantile Regression Forests to model dependencies between features. Extensive experiments on various synthetic and real-world datasets demonstrate that our method outperforms state-of-the-art anomaly detection methods in identifying contextual anomalies in terms of accuracy and interpretability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.11239,February 2023,351,Appearance-based odometry and mapping with feature descriptors for underwater robots,"Silvia Silva da Costa Botelho, Paulo Lilles Jorge Drews Junior, Mônica da Silva Figueiredo, Celina Haffele Da Rocha and Gabriel Leivas Oliveira","The use of Autonomous Underwater Vehicles (AUVs) for underwater tasks is a promising robotic field. These robots can carry visual inspection cameras. Besides serving the activities of inspection and mapping, t...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194505,September 2009,
352,0.000678989392134464,352,Discovering Process Models that Support Desired Behavior and Avoid Undesired Behavior,"Ali Norouzifar, Wil van der Aalst","Process discovery is one of the primary process mining tasks and starting point for process improvements using event data. Existing process discovery techniques aim to find process models that best describe the observed behavior. The focus can be on recall (i.e., replay fitness) or precision. Here, we take a different perspective. We aim to discover a process model that allows for the good behavior observed, and does not allow for the bad behavior. In order to do this, we assume that we have a desirable event log ($L^+$) and an undesirable event log ($L^-$). For example, the desirable event log consists of the cases that were handled within two weeks, and the undesirable event log consists of the cases that took longer. Our discovery approach explores the tradeoff between supporting the cases in the desirable event log and avoiding the cases in the undesirable event log. The proposed framework uses a new inductive mining approach that has been implemented and tested on several real-life event logs. Experimental results show that our approach outperforms other approaches that use only the desirable event log ($L^+$). This supports the intuitive understanding that problematic cases can and should be used to improve processes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10984,February 2023,352,Scientific networks and success in science,Frank Schweitzer,No abstract,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-014-0035-8,20 December 2014,
353,0.000678989392134464,353,V4142 Sgr: a Double Periodic Variable with an accretor surrounded by the accretion-disk's atmosphere,"J. A. Rosales, R. E. Mennickent, G. Djurašević, I. Araya, M. Curé, D. R. G. Schleicher, J. Petrović","Context: A detailed study of the close interacting binary V4142\,Sgr based on photometric and spectroscopic analysis is presented.This system belongs to the enigmatic class of Algol-like variables showing a long photometric cycle of unknown nature. Aims: Performing photometric data-mining and spectroscopic observations covering the orbital cycle, we obtain the orbital parameters and the stellar properties of the binary system, along with the physical properties of the accretion disk located around the hot star. Insights on the evolutive path of the system are obtained. Methods: The light curve was modeled through an inverse modeling method using a theoretical light curve of the binary system, considering the light curve contribution of both stars and the accretion disk of the hot star to obtain the fundamental parameters. To constrain the main stellar parameters the mass ratio was fixed, as well as the donor temperature using the obtained values from our spectroscopic analysis including deblending methods to isolate the spectral lines of the stellar components. The system parameters were compared with a grid of binary star evolutive models in order to get insights on the evolutionary history of the system. Results: The orbital period and the long cycle were re-calculated and found to be of $30.633 \pm 0.002 ~\mathrm{days}$ and $1201 \pm 14 ~\mathrm{days}$. The spectral analysis reveals H$α$ double emission with a persistent $V \leq R$ asymmetry which is considered evidence of a possible wind emergin from the hotspot region... △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10962,February 2023,353,A fast and accurate approach for computing the dimensions of boxes from single perspective images,"Leandro A. F. Fernandes, Manuel M. Oliveira, Roberto da Silva and Gustavo J. Crespo",This paper describes an accurate method for computing the dimensions of boxes directly from perspective projection images acquired by conventional cameras. The approach is based on projective geometry and comp...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192392,June 2006,
354,0.000678989392134464,354,Machine Learning Techniques for Predicting the Short-Term Outcome of Resective Surgery in Lesional-Drug Resistance Epilepsy,"Zahra Jourahmad, Jafar Mehvari Habibabadi, Houshang Moein, Reza Basiratnia, Ali Rahmani Geranqayeh, Saeed Shiry Ghidary, Seyed-Ali Sadegh-Zadeh","In this study, we developed and tested machine learning models to predict epilepsy surgical outcome using noninvasive clinical and demographic data from patients. Methods: Seven dif-ferent categorization algorithms were used to analyze the data. The techniques are also evaluated using the Leave-One-Out method. For precise evaluation of the results, the parameters accuracy, precision, recall and, F1-score are calculated. Results: Our findings revealed that a machine learning-based presurgical model of patients' clinical features may accurately predict the outcome of epilepsy surgery in patients with drug-resistant lesional epilepsy. The support vector machine (SVM) with the linear kernel yielded 76.1% in terms of accuracy could predict results in 96.7% of temporal lobe epilepsy (TLE) patients and 79.5% of extratemporal lobe epilepsy (ETLE) cases using ten clinical features. Significance: To predict the outcome of epilepsy surgery, this study recommends the use of a machine learning strategy based on supervised classification and se-lection of feature subsets data mining. Progress in the development of machine learning-based prediction models offers optimism for personalised medicine access. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10901,February 2023,354,Signal Processing Technologies for Ambient Intelligence in Home-Care Applications,"Francesco G. B. De Natale, Aggelos K. Katsaggelos, Oscar Mayora and Ying Wu",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/91730,1 December 2007,
355,0.000678989392134464,355,On Inductive Biases for Machine Learning in Data Constrained Settings,Grégoire Mialon,"Learning with limited data is one of the biggest problems of machine learning. Current approaches to this issue consist in learning general representations from huge amounts of data before fine-tuning the model on a small dataset of interest. While such technique, coined transfer learning, is very effective in domains such as computer vision or natural langage processing, it does not yet solve common problems of deep learning such as model interpretability or the overall need for data. This thesis explores a different answer to the problem of learning expressive models in data constrained settings: instead of relying on big datasets to learn neural networks, we will replace some modules by known functions reflecting the structure of the data. Very often, these functions will be drawn from the rich literature of kernel methods. Indeed, many kernels can reflect the underlying structure of the data, thus sparing learning parameters to some extent. Our approach falls under the hood of ""inductive biases"", which can be defined as hypothesis on the data at hand restricting the space of models to explore during learning. We demonstrate the effectiveness of this approach in the context of sequences, such as sentences in natural language or protein sequences, and graphs, such as molecules. We also highlight the relationship between our work and recent advances in deep learning. Additionally, we study convex machine learning models. Here, rather than proposing new models, we wonder which proportion of the samples in a dataset is really needed to learn a ""good"" model. More precisely, we study the problem of safe sample screening, i.e, executing simple tests to discard uninformative samples from a dataset even before fitting a machine learning model, without affecting the optimal model. Such techniques can be used to prune datasets or mine for rare samples. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10692,February 2023,355,Information Mining from Multimedia Databases,"Ling Guan, Horace HS Ip, Paul H Lewis, Hau San Wong and Paisarn Muneesawang",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/49073,1 December 2006,
356,0.000678989392134464,356,FedST: Secure Federated Shapelet Transformation for Time Series Classification,"Zhiyu Liang, Hongzhi Wang","This paper explores how to customize time series classification (TSC) methods with the help of external data in a privacy-preserving federated learning (FL) scenario. To the best of our knowledge, we are the first to study on this essential topic. Achieving this goal requires us to seamlessly integrate the techniques from multiple fields including Data Mining, Machine Learning, and Security. In this paper, we systematically investigate existing TSC solutions for the centralized scenario and propose FedST, a novel FL-enabled TSC framework based on a shapelet transformation method. We recognize the federated shapelet search step as the kernel of FedST. Thus, we design a basic protocol for the FedST kernel that we prove to be secure and accurate. However, we identify that the basic protocol suffers from efficiency bottlenecks and the centralized acceleration techniques lose their efficacy due to the security issues. To speed up the federated protocol with security guarantee, we propose several optimizations tailored for the FL setting. Our theoretical analysis shows that the proposed methods are secure and more efficient. We conduct extensive experiments using both synthetic and real-world datasets. Empirical results show that our FedST solution is effective in terms of TSC accuracy, and the proposed optimizations can achieve three orders of magnitude of speedup. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10631,February 2023,356,"Correction: The penumbra of open source: projects outside of centralized platforms are longer maintained, more academic and more collaborative","Milo Z. Trujillo, Laurent Hébert-Dufresne and James Bagrow",Theoriginal articlewas published inEPJ Data Science202211:31,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00348-4,4 July 2022,
357,0.000678989392134464,357,HierCat: Hierarchical Query Categorization from Weakly Supervised Data at Facebook Marketplace,"Yunzhong He, Cong Zhang, Ruoyan Kong, Chaitanya Kulkarni, Qing Liu, Ashish Gandhe, Amit Nithianandan, Arul Prakash","Query categorization at customer-to-customer e-commerce platforms like Facebook Marketplace is challenging due to the vagueness of search intent, noise in real-world data, and imbalanced training data across languages. Its deployment also needs to consider challenges in scalability and downstream integration in order to translate modeling advances into better search result relevance. In this paper we present HierCat, the query categorization system at Facebook Marketplace. HierCat addresses these challenges by leveraging multi-task pre-training of dual-encoder architectures with a hierarchical inference step to effectively learn from weakly supervised training data mined from searcher engagement. We show that HierCat not only outperforms popular methods in offline experiments, but also leads to 1.4% improvement in NDCG and 4.3% increase in searcher engagement at Facebook Marketplace Search in online A/B testing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.10527,February 2023,357,Quantum technologies in the telecommunications industry,"Vicente Martin, Juan Pedro Brito, Carmen Escribano, Marco Menchetti, Catherine White, Andrew Lord, Felix Wissel, Matthias Gunkel, Paulette Gavignet, Naveena Genay, Olivier Le Moult, Carlos Abellán, Antonio Manzalini, Antonio Pastor-Perales, Victor López and Diego López","Quantum based technologies have been fundamental in our world. After producing the laser and the transistor, the devices that have shaped our modern information society, the possibilities enabled by the abilit...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00108-9,22 July 2021,
358,0.000678989392134464,358,A Text Mining Analysis of Data Protection Politics: The Case of Plenary Sessions of the European Parliament,Jukka Ruohonen,"Data protection laws and policies have been studied extensively in recent years, but little is known about the parliamentary politics of data protection. This imitation applies even to the European Union (EU) that has taken the global lead in data protection and privacy regulation. For patching this notable gap in existing research, this paper explores the data protection questions raised by the Members of the European Parliament (MEPs) in the Parliament's plenary sessions and the answers given to these by the European Commission. Over a thousand of such questions and answers are covered in a period from 1995 to early 2023. Given computational analysis based on text mining, the results indicate that (a) data protection has been actively debated in the Parliament during the past twenty years. No noticeable longitudinal trends are present; the debates have been relatively constant. As could be expected, (b) the specific data protection laws in the EU have frequently been referenced in these debates, which (c) do not seem to align along conventional political dimensions such as the left-right axis. Furthermore, (d) numerous distinct data protection topics have been debated by the parliamentarians, indicating that data protection politics in the EU go well-beyond the recently enacted regulations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.09939,February 2023,358,RETRACTED ARTICLE: Concordance-based Kendall’s correlation for computationally-light vs. computationally-heavy centrality metrics: lower bound for correlation,Natarajan Meghanathan,Unknown,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-017-0097-1,28 June 2017,
359,0.000678989392134464,359,PriSTI: A Conditional Diffusion Framework for Spatiotemporal Imputation,"Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, Yanjie Fu","Spatiotemporal data mining plays an important role in air quality monitoring, crowd flow modeling, and climate forecasting. However, the originally collected spatiotemporal data in real-world scenarios is usually incomplete due to sensor failures or transmission loss. Spatiotemporal imputation aims to fill the missing values according to the observed values and the underlying spatiotemporal dependence of them. The previous dominant models impute missing values autoregressively and suffer from the problem of error accumulation. As emerging powerful generative models, the diffusion probabilistic models can be adopted to impute missing values conditioned by observations and avoid inferring missing values from inaccurate historical imputation. However, the construction and utilization of conditional information are inevitable challenges when applying diffusion models to spatiotemporal imputation. To address above issues, we propose a conditional diffusion framework for spatiotemporal imputation with enhanced prior modeling, named PriSTI. Our proposed framework provides a conditional feature extraction module first to extract the coarse yet effective spatiotemporal dependencies from conditional information as the global context prior. Then, a noise estimation module transforms random noise to realistic values, with the spatiotemporal attention weights calculated by the conditional feature, as well as the consideration of geographic relationships. PriSTI outperforms existing imputation methods in various missing patterns of different real-world spatiotemporal data, and effectively handles scenarios such as high missing rates and sensor failure. The implementation code is available at https://github.com/LMZZML/PriSTI. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.09746,February 2023,359,Sleep disorder diagnosis: the design and implications of online tools,Jacqueline Blake and Don Kerr,"Sleep disorders are a significant and growing problem, both for the economy of the nation and for the physical and psychological well-being of individual sufferers. The purpose of this research was to investig...",https://www.springeropen.com//decisionanalyticsjournal.springeropen.com/articles/10.1186/2193-8636-1-7,7 March 2014,
360,0.000678989392134464,360,Fairly Adaptive Negative Sampling for Recommendations,"Xiao Chen, Wenqi Fan, Jingfan Chen, Haochen Liu, Zitao Liu, Zhaoxiang Zhang, Qing Li","Pairwise learning strategies are prevalent for optimizing recommendation models on implicit feedback data, which usually learns user preference by discriminating between positive (i.e., clicked by a user) and negative items (i.e., obtained by negative sampling). However, the size of different item groups (specified by item attribute) is usually unevenly distributed. We empirically find that the commonly used uniform negative sampling strategy for pairwise algorithms (e.g., BPR) can inherit such data bias and oversample the majority item group as negative instances, severely countering group fairness on the item side. In this paper, we propose a Fairly adaptive Negative sampling approach (FairNeg), which improves item group fairness via adaptively adjusting the group-level negative sampling distribution in the training process. In particular, it first perceives the model's unfairness status at each step and then adjusts the group-wise sampling distribution with an adaptive momentum update strategy for better facilitating fairness optimization. Moreover, a negative sampling distribution Mixup mechanism is proposed, which gracefully incorporates existing importance-aware sampling techniques intended for mining informative negative samples, thus allowing for achieving multiple optimization purposes. Extensive experiments on four public datasets show our proposed method's superiority in group fairness enhancement and fairness-utility tradeoff. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.08266,February 2023,360,A Bayesian-network-based quantum procedure for failure risk analysis,"Gines Carrascal, Guillermo Botella, Alberto del Barrio and David Kremer","Studying the propagation of failure probabilities in interconnected systems such as electrical distribution networks is traditionally performed by means of Monte Carlo simulations. In this paper, we propose a ...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-023-00171-4,8 May 2023,
361,0.000678989392134464,361,SynGraphy: Succinct Summarisation of Large Networks via Small Synthetic Representative Graphs,"Jérôme Kunegis, Pawan Kumar, Jun Sun, Anna Samoilenko, Giuseppe Pirró","We describe SynGraphy, a method for visually summarising the structure of large network datasets that works by drawing smaller graphs generated to have similar structural properties to the input graphs. Visualising complex networks is crucial to understand and make sense of networked data and the relationships it represents. Due to the large size of many networks, visualisation is extremely difficult; the simple method of drawing large networks like those of Facebook or Twitter leads to graphics that convey little or no information. While modern graph layout algorithms can scale computationally to large networks, their output tends to a common ""hairball"" look, which makes it difficult to even distinguish different graphs from each other. Graph sampling and graph coarsening techniques partially address these limitations but they are only able to preserve a subset of the properties of the original graphs. In this paper we take the problem of visualising large graphs from a novel perspective: we leave the original graph's nodes and edges behind, and instead summarise its properties such as the clustering coefficient and bipartivity by generating a completely new graph whose structural properties match that of the original graph. To verify the utility of this approach as compared to other graph visualisation algorithms, we perform an experimental evaluation in which we repeatedly asked experimental subjects (professionals in graph mining and related areas) to determine which of two given graphs has a given structural property and then assess which visualisation algorithm helped in identifying the correct answer. Our summarisation approach SynGraphy compares favourably to other techniques on a variety of networks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.07755,February 2023,361,What is interaction science? Revisiting the aims and scope of JoIS,Gisela Susanne Bahr and Christian Stary,Interaction Science has undergone rapid development since JoIS’ (the Journal of Interaction Science) founding in 2013. The advent of novel techniques and tools required reviewing the understanding of Interacti...,https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/s40166-016-0015-5,12 December 2016,
362,0.000678989392134464,362,FedABC: Targeting Fair Competition in Personalized Federated Learning,"Dui Wang, Li Shen, Yong Luo, Han Hu, Kehua Su, Yonggang Wen, Dacheng Tao","Federated learning aims to collaboratively train models without accessing their client's local private data. The data may be Non-IID for different clients and thus resulting in poor performance. Recently, personalized federated learning (PFL) has achieved great success in handling Non-IID data by enforcing regularization in local optimization or improving the model aggregation scheme on the server. However, most of the PFL approaches do not take into account the unfair competition issue caused by the imbalanced data distribution and lack of positive samples for some classes in each client. To address this issue, we propose a novel and generic PFL framework termed Federated Averaging via Binary Classification, dubbed FedABC. In particular, we adopt the ``one-vs-all'' training strategy in each client to alleviate the unfair competition between classes by constructing a personalized binary classification problem for each class. This may aggravate the class imbalance challenge and thus a novel personalized binary classification loss that incorporates both the under-sampling and hard sample mining strategies is designed. Extensive experiments are conducted on two popular datasets under different settings, and the results demonstrate that our FedABC can significantly outperform the existing counterparts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.07450,February 2023,362,A mapping study of the Brazilian SBSE community,"Wesley KG Assunção, Márcio de O Barros, Thelma E Colanzi, Arilo C Dias-Neto, Matheus HE Paixão, Jerffeson T de Souza and Silvia R Vergilio","Research communities evolve over time, changing their interests for specific problems or research areas. Mapping the evolution of a research community, including the most frequently addressed problems, the str...",https://www.springeropen.com//jserd.springeropen.com/articles/10.1186/2195-1721-2-3,6 May 2014,
363,0.000678989392134464,363,A dataset for Audio-Visual Sound Event Detection in Movies,"Rajat Hebbar, Digbalay Bose, Krishna Somandepalli, Veena Vijai, Shrikanth Narayanan","Audio event detection is a widely studied audio processing task, with applications ranging from self-driving cars to healthcare. In-the-wild datasets such as Audioset have propelled research in this field. However, many efforts typically involve manual annotation and verification, which is expensive to perform at scale. Movies depict various real-life and fictional scenarios which makes them a rich resource for mining a wide-range of audio events. In this work, we present a dataset of audio events called Subtitle-Aligned Movie Sounds (SAM-S). We use publicly-available closed-caption transcripts to automatically mine over 110K audio events from 430 movies. We identify three dimensions to categorize audio events: sound, source, quality, and present the steps involved to produce a final taxonomy of 245 sounds. We discuss the choices involved in generating the taxonomy, and also highlight the human-centered nature of sounds in our dataset. We establish a baseline performance for audio-only sound classification of 34.76% mean average precision and show that incorporating visual information can further improve the performance by about 5%. Data and code are made available for research at https://github.com/usc-sail/mica-subtitle-aligned-movie-sounds △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.07315,February 2023,363,Storytelling in early childhood education: Time to go digital,Maila D. H. Rahiem,"Digital storytelling blends the ancient art of storytelling with a range of contemporary tools to weave stories together with the author's narrative voice, including digital images, graphics, music and sound. ...",https://www.springeropen.com//ijccep.springeropen.com/articles/10.1186/s40723-021-00081-x,6 April 2021,
364,0.000678989392134464,364,A Data Mining Approach for Detecting Collusion in Unproctored Online Exams,"Janine Langerbein, Till Massing, Jens Klenke, Natalie Reckmann, Michael Striewe, Michael Goedicke, Christoph Hanck","Due to the precautionary measures during the COVID-19 pandemic many universities offered unproctored take-home exams. We propose methods to detect potential collusion between students and apply our approach on event log data from take-home exams during the pandemic. We find groups of students with suspiciously similar exams. In addition, we compare our findings to a proctored control group. By this, we establish a rule of thumb for evaluating which cases are ""outstandingly similar"", i.e., suspicious cases. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.07014,February 2023,364,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0076-4,28 April 2012,
365,0.000678989392134464,365,Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents,"T. Y. S. S. Santosh, Philipp Bock, Matthias Grabmair","Segmentation and Rhetorical Role Labeling of legal judgements play a crucial role in retrieval and adjacent tasks, including case summarization, semantic search, argument mining etc. Previous approaches have formulated this task either as independent classification or sequence labeling of sentences. In this work, we reformulate the task at span level as identifying spans of multiple consecutive sentences that share the same rhetorical role label to be assigned via classification. We employ semi-Markov Conditional Random Fields (CRF) to jointly learn span segmentation and span label assignment. We further explore three data augmentation strategies to mitigate the data scarcity in the specialized domain of law where individual documents tend to be very long and annotation cost is high. Our experiments demonstrate improvement of span-level prediction metrics with a semi-Markov CRF model over a CRF baseline. This benefit is contingent on the presence of multi sentence spans in the document. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.06448,February 2023,365,Editorial,Maria Cristina F. de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-010-0014-2,1 July 2010,
366,0.000678989392134464,366,Deep Transfer Tensor Factorization for Multi-View Learning,"Penghao Jiang, Ke Xin, Chunxi Li","This paper studies the data sparsity problem in multi-view learning. To solve data sparsity problem in multiview ratings, we propose a generic architecture of deep transfer tensor factorization (DTTF) by integrating deep learning and cross-domain tensor factorization, where the side information is embedded to provide effective compensation for the tensor sparsity. Then we exhibit instantiation of our architecture by combining stacked denoising autoencoder (SDAE) and CANDECOMP/ PARAFAC (CP) tensor factorization in both source and target domains, where the side information of both users and items is tightly coupled with the sparse multi-view ratings and the latent factors are learned based on the joint optimization. We tightly couple the multi-view ratings and the side information to improve cross-domain tensor factorization based recommendations. Experimental results on real-world datasets demonstrate that our DTTF schemes outperform state-of-the-art methods on multi-view rating predictions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.06133,February 2023,366,Muta-Pro: Towards the definition of a mutation testing process,"A. M. R. Vincenzi, A. S. Simão, M. E. Delamaro and J. C. Maldonado",Mutation Testing originated from a classical method for digital circuit testing and today is used at program and specification levels. It can be used either to generate or to assess the quality of test sets. I...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192394,June 2006,
367,0.000678989392134464,367,Unsupervised Deep One-Class Classification with Adaptive Threshold based on Training Dynamics,"Minkyung Kim, Junsik Kim, Jongmin Yu, Jun Kyun Choi","One-class classification has been a prevailing method in building deep anomaly detection models under the assumption that a dataset consisting of normal samples is available. In practice, however, abnormal samples are often mixed in a training dataset, and they detrimentally affect the training of deep models, which limits their applicability. For robust normality learning of deep practical models, we propose an unsupervised deep one-class classification that learns normality from pseudo-labeled normal samples, i.e., outlier detection in single cluster scenarios. To this end, we propose a pseudo-labeling method by an adaptive threshold selected by ranking-based training dynamics. The experiments on 10 anomaly detection benchmarks show that our method effectively improves performance on anomaly detection by sizable margins. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.06048,February 2023,367,Scientific networks and success in science,Frank Schweitzer,No abstract,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-014-0035-8,20 December 2014,
368,0.000678989392134464,368,Time-to-event modeling of subreddits transitions to r/SuicideWatch,"Xueying Liu, Shiaofen Fang, George Mohler, Joan Carlson, Yunyu Xiao","Recent data mining research has focused on the analysis of social media text, content and networks to identify suicide ideation online. However, there has been limited research on the temporal dynamics of users and suicide ideation. In this work, we use time-to-event modeling to identify which subreddits have a higher association with users transitioning to posting on r/suicidewatch. For this purpose we use a Cox proportional hazards model that takes as input text and subreddit network features and outputs a probability distribution for the time until a Reddit user posts on r/suicidewatch. In our analysis we find a number of statistically significant features that predict earlier transitions to r/suicidewatch. While some patterns match existing intuition, for example r/depression is positively associated with posting sooner on r/suicidewatch, others were more surprising (for example, the average time between a high risk post on r/Wishlist and a post on r/suicidewatch is 10.2 days). We then discuss these results as well as directions for future research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.06030,February 2023,368,"Exploring the roles of students’ home resources and attitudes towards science in science achievement: a comparison of South Korea, Turkey, and the United States in TIMSS 2015","Rachel Louise Geesa, Burcu Izci, Hyuksoon S. Song and Shiyi Chen","Educational opportunities in the home, at school, and within the community differ across countries and affect students’ achievement and future success in science fields. Bandura’s Social (Social foundations of...",https://www.springeropen.com//apse-journal.springeropen.com/articles/10.1186/s41029-019-0038-7,16 December 2019,
369,0.000678989392134464,369,Machine Learning Based Approach to Recommend MITRE ATT&CK Framework for Software Requirements and Design Specifications,"Nicholas Lasky, Benjamin Hallis, Mounika Vanamala, Rushit Dave, Jim Seliya","Engineering more secure software has become a critical challenge in the cyber world. It is very important to develop methodologies, techniques, and tools for developing secure software. To develop secure software, software developers need to think like an attacker through mining software repositories. These aim to analyze and understand the data repositories related to software development. The main goal is to use these software repositories to support the decision-making process of software development. There are different vulnerability databases like Common Weakness Enumeration (CWE), Common Vulnerabilities and Exposures database (CVE), and CAPEC. We utilized a database called MITRE. MITRE ATT&CK tactics and techniques have been used in various ways and methods, but tools for utilizing these tactics and techniques in the early stages of the software development life cycle (SDLC) are lacking. In this paper, we use machine learning algorithms to map requirements to the MITRE ATT&CK database and determine the accuracy of each mapping depending on the data split. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.05530,February 2023,369,Editorial,"Biao Chen, Wendi B Heinzelman, Mingyan Liu and Andrew T Campbell",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN.2005.459,8 September 2005,
370,0.000678989392134464,370,"ACE, a generic constraint solver",Christophe Lecoutre,"Constraint Programming (CP) is a useful technology for modeling and solving combinatorial constrained problems. On the one hand, on can use a library like PyCSP3 for easily modeling problems arising in various application fields (e.g., scheduling, planning, data-mining, cryptography, bio-informatics, organic chemistry, etc.). Problem instances can then be directly generated from specific models and data. On the other hand, for solving instances (notably, represented in XCSP3 format), one can use a constraint solver like ACE, which is presented in this paper. ACE is an open-source constraint solver, developed in Java, which focuses on integer variables (including 0/1-Boolean variables), state-of-the-art table constraints, popular global constraints, search heuristics and (mono-criterion) optimization. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.05405,February 2023,370,Expanding the field: using digital to diversify learning in outdoor science,"Bethan C. Stagg, Justin Dillon and Janine Maddison","This is an empirical study of teacher experiences with school learners (7–18 years) engaging in cross-curricular environmental science during the COVID-19 pandemic. The study focuses on #FieldworkLive, a progr...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00047-0,10 March 2022,
371,0.000678989392134464,371,Unsupervised ore/waste classification on open-cut mine faces using close-range hyperspectral data,"Lloyd Windrim, Arman Melkumyan, Richard J. Murphy, Anna Chlingaryan, Raymond Leung","The remote mapping of minerals and discrimination of ore and waste on surfaces are important tasks for geological applications such as those in mining. Such tasks have become possible using ground-based, close-range hyperspectral sensors which can remotely measure the reflectance properties of the environment with high spatial and spectral resolution. However, autonomous mapping of mineral spectra measured on an open-cut mine face remains a challenging problem due to the subtleness of differences in spectral absorption features between mineral and rock classes as well as variability in the illumination of the scene. An additional layer of difficulty arises when there is no annotated data available to train a supervised learning algorithm. A pipeline for unsupervised mapping of spectra on a mine face is proposed which draws from several recent advances in the hyperspectral machine learning literature. The proposed pipeline brings together unsupervised and self-supervised algorithms in a unified system to map minerals on a mine face without the need for human-annotated training data. The pipeline is evaluated with a hyperspectral image dataset of an open-cut mine face comprising mineral ore martite and non-mineralised shale. The combined system is shown to produce a superior map to its constituent algorithms, and the consistency of its mapping capability is demonstrated using data acquired at two different times of day. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.04936,February 2023,371,Teaching during COVID-19: reflections of early-career science teachers,Jeanna R. Wieselmann and Elizabeth A. Crotty,"The unique circumstances of the COVID-19 pandemic required that instruction be shifted online through asynchronous, synchronous, or hybrid models of instruction. This created a need for many K-12 teachers to d...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00057-y,15 April 2022,
372,0.000678989392134464,372,Lightweight Transformers for Clinical Natural Language Processing,"Omid Rohanian, Mohammadmahdi Nouriborji, Hannah Jauncey, Samaneh Kouchaki, ISARIC Clinical Characterisation Group, Lei Clifton, Laura Merson, David A. Clifton","Specialised pre-trained language models are becoming more frequent in NLP since they can potentially outperform models trained on generic texts. BioBERT and BioClinicalBERT are two examples of such models that have shown promise in medical NLP tasks. Many of these models are overparametrised and resource-intensive, but thanks to techniques like Knowledge Distillation (KD), it is possible to create smaller versions that perform almost as well as their larger counterparts. In this work, we specifically focus on development of compact language models for processing clinical texts (i.e. progress notes, discharge summaries etc). We developed a number of efficient lightweight clinical transformers using knowledge distillation and continual learning, with the number of parameters ranging from 15 million to 65 million. These models performed comparably to larger models such as BioBERT and ClinicalBioBERT and significantly outperformed other compact models trained on general or biomedical data. Our extensive evaluation was done across several standard datasets and covered a wide range of clinical text-mining tasks, including Natural Language Inference, Relation Extraction, Named Entity Recognition, and Sequence Classification. To our knowledge, this is the first comprehensive study specifically focused on creating efficient and compact transformers for clinical NLP tasks. The models and code used in this study can be found on our Huggingface profile at https://huggingface.co/nlpie and Github page at https://github.com/nlpie-research/Lightweight-Clinical-Transformers, respectively, promoting reproducibility of our results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.04725,February 2023,372,Teaching controversial socio-scientific issues in online exhibits of science museums: Covid-19 on the scene,"Carolina Sotério, Adriele Ribeiro dos Santos Lamim and Salete Linhares Queiroz","The Covid-19 pandemic has sparked an unprecedented public debate over socio-scientific controversies, particularly regarding vaccination and social distancing measures. Despite the potential of such subjects f...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00069-8,2 January 2023,
373,0.000678989392134464,373,Generating High-Precision Feedback for Programming Syntax Errors using Large Language Models,"Tung Phung, José Cambronero, Sumit Gulwani, Tobias Kohn, Rupak Majumdar, Adish Singla, Gustavo Soares","Large language models (LLMs), such as Codex, hold great promise in enhancing programming education by automatically generating feedback for students. We investigate using LLMs to generate feedback for fixing syntax errors in Python programs, a key scenario in introductory programming. More concretely, given a student's buggy program, our goal is to generate feedback comprising a fixed program along with a natural language explanation describing the errors/fixes, inspired by how a human tutor would give feedback. While using LLMs is promising, the critical challenge is to ensure high precision in the generated feedback, which is imperative before deploying such technology in classrooms. The main research question we study is: Can we develop LLMs-based feedback generation techniques with a tunable precision parameter, giving educators quality control over the feedback that students receive? To this end, we introduce PyFiXV, our technique to generate high-precision feedback powered by Codex. The key idea behind PyFiXV is to use a novel run-time validation mechanism to decide whether the generated feedback is suitable for sharing with the student; notably, this validation mechanism also provides a precision knob to educators. We perform an extensive evaluation using two real-world datasets of Python programs with syntax errors and show the efficacy of PyFiXV in generating high-precision feedback. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.04662,February 2023,373,The cross-cultural validation of the technology-enhanced social constructivist learning environment questionnaire in the Iraqi Kurdistan Region,Saif Husam Mohammed and László Kinyó,This study’s primary aim is to validate a research instrument in Iraqi Kurdistan middle and secondary schools to explore learners’ perspectives concerning social constructivist learning environments and e-lear...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00199-7,21 June 2022,
374,0.000678989392134464,374,Principled and Efficient Motif Finding for Structure Learning of Lifted Graphical Models,"Jonathan Feldstein, Dominic Phillips, Efthymia Tsamoura","Structure learning is a core problem in AI central to the fields of neuro-symbolic AI and statistical relational learning. It consists in automatically learning a logical theory from data. The basis for structure learning is mining repeating patterns in the data, known as structural motifs. Finding these patterns reduces the exponential search space and therefore guides the learning of formulas. Despite the importance of motif learning, it is still not well understood. We present the first principled approach for mining structural motifs in lifted graphical models, languages that blend first-order logic with probabilistic models, which uses a stochastic process to measure the similarity of entities in the data. Our first contribution is an algorithm, which depends on two intuitive hyperparameters: one controlling the uncertainty in the entity similarity measure, and one controlling the softness of the resulting rules. Our second contribution is a preprocessing step where we perform hierarchical clustering on the data to reduce the search space to the most relevant data. Our third contribution is to introduce an O(n ln n) (in the size of the entities in the data) algorithm for clustering structurally-related data. We evaluate our approach using standard benchmarks and show that we outperform state-of-the-art structure learning approaches by up to 6% in terms of accuracy and up to 80% in terms of runtime. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.04599,February 2023,374,"Correction: The penumbra of open source: projects outside of centralized platforms are longer maintained, more academic and more collaborative","Milo Z. Trujillo, Laurent Hébert-Dufresne and James Bagrow",Theoriginal articlewas published inEPJ Data Science202211:31,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-022-00348-4,4 July 2022,
375,0.00514534320972017,375,Sentiment analysis and opinion mining on educational data: A survey,"Thanveer Shaik, Xiaohui Tao, Christopher Dann, Haoran Xie, Yan Li, Linda Galligan","Sentiment analysis AKA opinion mining is one of the most widely used NLP applications to identify human intentions from their reviews. In the education sector, opinion mining is used to listen to student opinions and enhance their learning-teaching practices pedagogically. With advancements in sentiment annotation techniques and AI methodologies, student comments can be labelled with their sentiment orientation without much human intervention. In this review article, (1) we consider the role of emotional analysis in education from four levels: document level, sentence level, entity level, and aspect level, (2) sentiment annotation techniques including lexicon-based and corpus-based approaches for unsupervised annotations are explored, (3) the role of AI in sentiment analysis with methodologies like machine learning, deep learning, and transformers are discussed, (4) the impact of sentiment analysis on educational procedures to enhance pedagogy, decision-making, and evaluation are presented. Educational institutions have been widely invested to build sentiment analysis tools and process their student feedback to draw their opinions and insights. Applications built on sentiment analysis of student feedback are reviewed in this study. Challenges in sentiment analysis like multi-polarity, polysemous, negation words, and opinion spam detection are explored and their trends in the research space are discussed. The future directions of sentiment analysis in education are discussed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.04359,February 2023,375,Instructional improv to analyze inquiry-based science teaching: Zed’s dead and the missing flower,"Maggie Dahn, Christine Lee, Noel Enyedy and Joshua Danish",In inquiry-based science lessons teachers face the challenge of adhering to curricular goals while simultaneously following students’ intuitive understandings. Improvisation (improv) provides a useful frame fo...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00156-9,25 June 2021,
376,0.000678989392134464,376,Classification of Methods to Reduce Clinical Alarm Signals for Remote Patient Monitoring: A Critical Review,"Teena Arora, Venki Balasubramanian, Andrew Stranieri, Shenhan Mai, Rajkumar Buyya, Sardar Islam","Remote Patient Monitoring (RPM) is an emerging technology paradigm that helps reduce clinician workload by automated monitoring and raising intelligent alarm signals. High sensitivity and intelligent data-processing algorithms used in RPM devices result in frequent false-positive alarms, resulting in alarm fatigue. This study aims to critically review the existing literature to identify the causes of these false-positive alarms and categorize the various interventions used in the literature to eliminate these causes. That act as a catalog and helps in false alarm reduction algorithm design. A step-by-step approach to building an effective alarm signal generator for clinical use has been proposed in this work. Second, the possible causes of false-positive alarms amongst RPM applications were analyzed from the literature. Third, a critical review has been done of the various interventions used in the literature depending on causes and classification based on four major approaches: clinical knowledge, physiological data, medical sensor devices, and clinical environments. A practical clinical alarm strategy could be developed by following our pentagon approach. The first phase of this approach emphasizes identifying the various causes for the high number of false-positive alarms. Future research will focus on developing a false alarm reduction method using data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.03885,February 2023,376,Robust Face Image Matching under Illumination Variations,"Chyuan-Huei Thomas Yang, Shang-Hong Lai and Long-Wen Chang","Face image matching is an essential step for face recognition and face verification. It is difficult to achieve robust face matching under various image acquisition conditions. In this paper, a novel face imag...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704410014,2 December 2004,
377,0.000678989392134464,377,Estimation of Gaussian Bi-Clusters with General Block-Diagonal Covariance Matrix and Applications,"Anastasiia Livochka, Ryan Browne, Sanjeena Subedi","Bi-clustering is a technique that allows for the simultaneous clustering of observations and features in a dataset. This technique is often used in bioinformatics, text mining, and time series analysis. An important advantage of biclustering algorithm is the ability to uncover multiple ``views'' (i.e., through rows and column groupings) in the data. Several Gaussian mixture model based biclustering approach currently exist in the literature. However, they impose severe restrictions on the structure of the covariance matrix. Here, we propose a Gaussian mixture model-based bi-clustering approach that provides a more flexible block-diagonal covariance structure. We show that the clustering accuracy of the proposed model is comparable to other known techniques but our approach provides a more flexible covariance structure and has substantially lower computational time. We demonstrate the application of the proposed model in bioinformatics and topic modelling. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.03849,February 2023,377,Exploring high scientific productivity in international co-authorship of a small developing country based on collaboration patterns,"Irena Mitrović, Marko Mišić and Jelica Protić","The number of published scientific paper grows rapidly each year, totaling more than 2.9 million annually. New methodologies and systems have been developed to analyze scientific production and performance ind...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00744-1,15 May 2023,
378,0.00673579331412596,378,A Privacy-Preserving Hybrid Federated Learning Framework for Financial Crime Detection,"Haobo Zhang, Junyuan Hong, Fan Dong, Steve Drew, Liangjie Xue, Jiayu Zhou","The recent decade witnessed a surge of increase in financial crimes across the public and private sectors, with an average cost of scams of $102m to financial institutions in 2022. Developing a mechanism for battling financial crimes is an impending task that requires in-depth collaboration from multiple institutions, and yet such collaboration imposed significant technical challenges due to the privacy and security requirements of distributed financial data. For example, consider the modern payment network systems, which can generate millions of transactions per day across a large number of global institutions. Training a detection model of fraudulent transactions requires not only secured transactions but also the private account activities of those involved in each transaction from corresponding bank systems. The distributed nature of both samples and features prevents most existing learning systems from being directly adopted to handle the data mining task. In this paper, we collectively address these challenges by proposing a hybrid federated learning system that offers secure and privacy-aware learning and inference for financial crime detection. We conduct extensive empirical studies to evaluate the proposed framework's detection performance and privacy-protection capability, evaluating its robustness against common malicious attacks of collaborative learning. We release our source code at https://github.com/illidanlab/HyFL . △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.03654,February 2023,378,Design patterns as aspects: A quantitative assessment,"Cláudio Sant’Anna, Alessandro Garcia, Uirá Kulesza, Carlos Lucena and Arndt von Staa","Design patterns offer flexible solutions to common problems in software development. Recent studies have shown that several design patterns involve crosscutting concerns. Unfortunately, object-oriented (OO) ab...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192358,June 2004,
379,0.000678989392134464,379,Look around and learn: self-improving object detection by exploration,"Gianluca Scarpellini, Stefano Rosa, Pietro Morerio, Lorenzo Natale, Alessio Del Bue","Object detectors often experience a drop in performance when new environmental conditions are insufficiently represented in the training data. This paper studies how to automatically fine-tune a pre-existing object detector while exploring and acquiring images in a new environment without relying on human intervention, i.e., in an utterly self-supervised fashion. In our setting, an agent initially learns to explore the environment using a pre-trained off-the-shelf detector to locate objects and associate pseudo-labels. By assuming that pseudo-labels for the same object must be consistent across different views, we learn an exploration policy mining hard samples and we devise a novel mechanism for producing refined predictions from the consensus among observations. Our approach outperforms the current state-of-the-art, and it closes the performance gap against a fully supervised setting without relying on ground-truth annotations. We also compare various exploration policies for the agent to gather more informative observations. Code and dataset will be made available upon paper acceptance △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.03566,February 2023,379,Young people’s technological images of the future: implications for science and technology education,Tapio Rasa and Antti Laherto,"Modern technology has had and continues to have various impacts on societies and human life in general. While technology in some ways defines the ‘digital age’ of today, discourses of ‘technological progress’ ...",https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-022-00190-x,3 April 2022,
380,0.000678989392134464,380,L'explicabilité au service de l'extraction de connaissances : application à des données médicales,"Robin Cugny, Emmanuel Doumard, Elodie Escriva, Haomiao Wang","The use of machine learning has increased dramatically in the last decade. The lack of transparency is now a limiting factor, which the field of explainability wants to address. Furthermore, one of the challenges of data mining is to present the statistical relationships of a dataset when they can be highly non-linear. One of the strengths of supervised learning is its ability to find complex statistical relationships that explainability allows to represent in an intelligible way. This paper shows that explanations can be used to extract knowledge from data and shows how feature selection, data subgroup analysis and selection of highly informative instances benefit from explanations. We then present a complete data processing pipeline using these methods on medical data. -- -- L'utilisation de l'apprentissage automatique a connu un bond cette dernière décennie. Le manque de transparence est aujourd'hui un frein, que le domaine de l'explicabilité veut résoudre. Par ailleurs, un des défis de l'exploration de données est de présenter les relations statistiques d'un jeu de données alors que celles-ci peuvent être hautement non-linéaires. Une des forces de l'apprentissage supervisé est sa capacité à trouver des relations statistiques complexes que l'explicabilité permet de représenter de manière intelligible. Ce papier montre que les explications permettent de faire de l'extraction de connaissance sur des données et comment la sélection de variables, l'analyse de sous-groupes de données et la sélection d'instances avec un fort pouvoir informatif bénéficient des explications. Nous présentons alors un pipeline complet de traitement des données utilisant ces méthodes pour l'exploration de données médicales. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02653,February 2023,380,Ontology matching for dynamic publication in semantic portals,"Fernanda Aparecida Lachtim, Ana Maria de Carvalho Moura and Maria Cláudia Cavalcanti","Semantic portals are characterized for storing and structuring content according to specific domain ontologies. This content is represented through ontological languages, which enable not only adding semantic ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192575,March 2009,
381,0.00514534320972017,381,Personalized Interpretable Classification,"Zengyou He, Yifan Tang, Lianyu Hu, Mudi Jiang, Yan Liu","How to interpret a data mining model has received much attention recently, because people may distrust a black-box predictive model if they do not understand how the model works. Hence, it will be trustworthy if a model can provide transparent illustrations on how to make the decision. Although many rule-based interpretable classification algorithms have been proposed, all these existing solutions cannot directly construct an interpretable model to provide personalized prediction for each individual test sample. In this paper, we make a first step towards formally introducing personalized interpretable classification as a new data mining problem to the literature. In addition to the problem formulation on this new issue, we present a greedy algorithm called PIC (Personalized Interpretable Classifier) to identify a personalized rule for each individual test sample. To demonstrate the necessity, feasibility and advantages of such a personalized interpretable classification method, we conduct a series of empirical studies on real data sets. The experimental results show that: (1) The new problem formulation enables us to find interesting rules for test samples that may be missed by existing non-personalized classifiers. (2) Our algorithm can achieve the same-level predictive accuracy as those state-of-the-art (SOTA) interpretable classifiers. (3) On a real data set for predicting breast cancer metastasis, such a personalized interpretable classifier can outperform SOTA methods in terms of both accuracy and interpretability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02528,February 2023,381,A Domain-Independent Window Approach to Multiclass Object Detection Using Genetic Programming,"Mengjie Zhang, Victor B. Ciesielski and Peter Andreae",This paper describes a domain-independent approach to the use of genetic programming for object detection problems in which the locations of small objects of multiple classes in large images must be found. The...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703303063,21 July 2003,
382,0.000678989392134464,382,Timed Partial Order Inference Algorithm,"Kandai Watanabe, Bardh Hoxha, Danil Prokhorov, Georgios Fainekos, Morteza Lahijanian, Sriram Sankaranarayana, Tomoya Yamaguchi","In this work, we propose the model of timed partial orders (TPOs) for specifying workflow schedules, especially for modeling manufacturing processes. TPOs integrate partial orders over events in a workflow, specifying ``happens-before'' relations, with timing constraints specified using guards and resets on clocks -- an idea borrowed from timed-automata specifications. TPOs naturally allow us to capture event ordering, along with a restricted but useful class of timing relationships. Next, we consider the problem of mining TPO schedules from workflow logs, which include events along with their time stamps. We demonstrate a relationship between formulating TPOs and the graph-coloring problem, and present an algorithm for learning TPOs with correctness guarantees. We demonstrate our approach on synthetic datasets, including two datasets inspired by real-life applications of aircraft turnaround and gameplay videos of the Overcooked computer game. Our TPO mining algorithm can infer TPOs involving hundreds of events from thousands of data-points within a few seconds. We show that the resulting TPOs provide useful insights into the dependencies and timing constraints for workflows. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02501,February 2023,382,Introduction to a special issue on biologically inspired analysis of social systems: a security informatics perspective,"Victor Asal, Kristin Glass and Richard Colbaugh",Unknown,https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-1-22,6 November 2012,
383,0.000678989392134464,383,Precursor recommendation for inorganic synthesis by machine learning materials similarity from scientific literature,"Tanjin He, Haoyan Huo, Christopher J. Bartel, Zheren Wang, Kevin Cruse, Gerbrand Ceder","Synthesis prediction is a key accelerator for the rapid design of advanced materials. However, determining synthesis variables such as the choice of precursor materials is challenging for inorganic materials because the sequence of reactions during heating is not well understood. In this work, we use a knowledge base of 29,900 solid-state synthesis recipes, text-mined from the scientific literature, to automatically learn which precursors to recommend for the synthesis of a novel target material. The data-driven approach learns chemical similarity of materials and refers the synthesis of a new target to precedent synthesis procedures of similar materials, mimicking human synthesis design. When proposing five precursor sets for each of 2,654 unseen test target materials, the recommendation strategy achieves a success rate of at least 82%. Our approach captures decades of heuristic synthesis data in a mathematical form, making it accessible for use in recommendation engines and autonomous laboratories. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02303,February 2023,383,Equity in mathematics education in Hong Kong: evidence from TIMSS 2011 to 2019,Xue-Lan Qiu and Frederick K. S. Leung,This study investigated the status of equity in mathematics education in Hong Kong based on data from TIMSS 2011 to 2019 and the changes in education equity across the three cycles. The effects of various stud...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-022-00121-z,8 June 2022,
384,0.000678989392134464,384,Structural Explanations for Graph Neural Networks using HSIC,"Ayato Toyokuni, Makoto Yamada","Graph neural networks (GNNs) are a type of neural model that tackle graphical tasks in an end-to-end manner. Recently, GNNs have been receiving increased attention in machine learning and data mining communities because of the higher performance they achieve in various tasks, including graph classification, link prediction, and recommendation. However, the complicated dynamics of GNNs make it difficult to understand which parts of the graph features contribute more strongly to the predictions. To handle the interpretability issues, recently, various GNN explanation methods have been proposed. In this study, a flexible model agnostic explanation method is proposed to detect significant structures in graphs using the Hilbert-Schmidt independence criterion (HSIC), which captures the nonlinear dependency between two variables through kernels. More specifically, we extend the GraphLIME method for node explanation with a group lasso and a fused lasso-based node explanation method. The group and fused regularization with GraphLIME enables the interpretation of GNNs in substructure units. Then, we show that the proposed approach can be used for the explanation of sequential graph classification tasks. Through experiments, it is demonstrated that our method can identify crucial structures in a target graph in various settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02139,February 2023,384,Search and Retrieval of 3D Content and Associated Knowledge Extraction and Propagation,"Petros Daras, Ming Ouhyoung and Tsuhan Chen",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/39792,1 December 2007,
385,0.000678989392134464,385,Inferencing the earth moving equipment-environment interaction in open pit mining,M. Balamurali,"In mining, grade control generally focuses on blast hole sampling and the estimation of ore control block models with little or no attention given to how the materials are being excavated from the ground. In the process of loading trucks, the underlying variability of the individual bucket load will determine the variability of truck payload. Hence, accurate material movement demands a good knowledge of the excavation process and the buckets interaction with the environment. However, equipment frequently goes into off nominal states due to unexpected delays, disturbances or faults. The large amount of such disturbances causes information loss that reduces the statistical power and biases estimates, leading to increased uncertainty in the production. A reliable method that inferences the missing knowledge about the interaction between the machine and the environment from the available data sources, is vital to accurately model the material movement. In this study, a twostep method was implemented that performed unsupervised clustering and then predicted the missing information. The first method is DBSCAN based spatial clustering which divides the diggers and buckets positional data into connected loading segments. Clear patterns of segmented bucket dig positions were observed. The second model utilized Gaussian process regression which was trained with the clustered data and the model was then used to infer the mean locations of the test clusters. Bucket dig locations were then simulated at the inferred mean locations for different durations and compared against the known bucket dig locations. This method was tested at an open pit mine in the Pilbara of Western Australia. The results demonstrate the advantage of the proposed method in inferencing the missing information of bucket environment interactions and therefore enables miners to continuously track the material movement. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02130,February 2023,385,Research on fault diagnosis system of hall for workshop of meta-synthetic engineering based on information fusion,"Guang Yang, Shuofeng Yu and Shouwen Wen","By analyzing multi-sensor information fusion system and hall for workshop of meta-synthetic engineering (HWME) essentially, a universal information fusion system of HWME based on multi-sensor is put forward. A...",https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1186/s13635-017-0070-7,29 December 2017,
386,0.000678989392134464,386,Augmenting Rule-based DNS Censorship Detection at Scale with Machine Learning,"Jacob Brown, Xi Jiang, Van Tran, Arjun Nitin Bhagoji, Nguyen Phong Hoang, Nick Feamster, Prateek Mittal, Vinod Yegneswaran","The proliferation of global censorship has led to the development of a plethora of measurement platforms to monitor and expose it. Censorship of the domain name system (DNS) is a key mechanism used across different countries. It is currently detected by applying heuristics to samples of DNS queries and responses (probes) for specific destinations. These heuristics, however, are both platform-specific and have been found to be brittle when censors change their blocking behavior, necessitating a more reliable automated process for detecting censorship. In this paper, we explore how machine learning (ML) models can (1) help streamline the detection process, (2) improve the potential of using large-scale datasets for censorship detection, and (3) discover new censorship instances and blocking signatures missed by existing heuristic methods. Our study shows that supervised models, trained using expert-derived labels on instances of known anomalies and possible censorship, can learn the detection heuristics employed by different measurement platforms. More crucially, we find that unsupervised models, trained solely on uncensored instances, can identify new instances and variations of censorship missed by existing heuristics. Moreover, both methods demonstrate the capability to uncover a substantial number of new DNS blocking signatures, i.e., injected fake IP addresses overlooked by existing heuristics. These results are underpinned by an important methodological finding: comparing the outputs of models trained using the same probes but with labels arising from independent processes allows us to more reliably detect cases of censorship in the absence of ground-truth labels of censorship. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.02031,February 2023,386,Kinetics of the hydroxymethylation of phenol I: Rate equation and method of analysis,"Mitsuo Higuchi, Sho-Ichi Nohno and Shin-ichiro Tohmura",Seven reactions take place consecutively and competitively during the hydroxymethylation of phenol in aqueous alkaline media. This hydroxymethylation is the first step in the formation of phenolic resins and h...,https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/BF00521963,1 June 1998,
387,0.00673579331412596,387,"Sequential pattern mining in educational data: The application context, potential, strengths, and limitations","Yingbin Zhang, Luc Paquette","Increasingly, researchers have suggested the benefits of temporal analysis to improve our understanding of the learning process. Sequential pattern mining (SPM), as a pattern recognition technique, has the potential to reveal the temporal aspects of learning and can be a valuable tool in educational data science. However, its potential is not well understood and exploited. This chapter addresses this gap by reviewing work that utilizes sequential pattern mining in educational contexts. We identify that SPM is suitable for mining learning behaviors, analyzing and enriching educational theories, evaluating the efficacy of instructional interventions, generating features for prediction models, and building educational recommender systems. SPM can contribute to these purposes by discovering similarities and differences in learners' activities and revealing the temporal change in learning behaviors. As a sequential analysis method, SPM can reveal unique insights about learning processes and be powerful for self-regulated learning research. It is more flexible in capturing the relative arrangement of learning events than the other sequential analysis methods. Future research may improve its utility in educational data science by developing tools for counting pattern occurrences as well as identifying and removing unreliable patterns. Future work needs to establish a systematic guideline for data preprocessing, parameter setting, and interpreting sequential patterns. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01932,February 2023,387,Quality of Service in Mobile Ad Hoc Networks,"Wei Li, Mohsen Guizani and Demetrios Kazakos",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN/2006/92039,9 May 2006,
388,0.000678989392134464,388,"Customer Profiling, Segmentation, and Sales Prediction using AI in Direct Marketing","Mahmoud SalahEldin Kasem, Mohamed Hamada, Islam Taj-Eddin","In an increasingly customer-centric business environment, effective communication between marketing and senior management is crucial for success. With the rise of globalization and increased competition, utilizing new data mining techniques to identify potential customers is essential for direct marketing efforts. This paper proposes a data mining preprocessing method for developing a customer profiling system to improve sales performance, including customer equity estimation and customer action prediction. The RFM-analysis methodology is used to evaluate client capital and a boosting tree for prediction. The study highlights the importance of customer segmentation methods and algorithms to increase the accuracy of the prediction. The main result of this study is the creation of a customer profile and forecast for the sale of goods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01786,February 2023,388,Editorial,"Hervé Bourlard, Ioannis Pitas, KennethKin-Man Lam and Yue Wang",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002720,21 April 2004,
389,0.000678989392134464,389,Group Fairness in Non-monotone Submodular Maximization,"Jing Yuan, Shaojie Tang","Maximizing a submodular function has a wide range of applications in machine learning and data mining. One such application is data summarization whose goal is to select a small set of representative and diverse data items from a large dataset. However, data items might have sensitive attributes such as race or gender, in this setting, it is important to design \emph{fairness-aware} algorithms to mitigate potential algorithmic bias that may cause over- or under- representation of particular groups. Motivated by that, we propose and study the classic non-monotone submodular maximization problem subject to novel group fairness constraints. Our goal is to select a set of items that maximizes a non-monotone submodular function, while ensuring that the number of selected items from each group is proportionate to its size, to the extent specified by the decision maker. We develop the first constant-factor approximation algorithms for this problem. We also extend the basic model to incorporate an additional global size constraint on the total number of selected items. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01546,February 2023,389,PROMIS® Health Organization (PHO) 2021 Conference Abstracts,Unknown,"This article is part of a Supplement:Volume 5
                                        Supplement 1",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-021-00349-3,1 October 2021,
390,0.000678989392134464,390,ANTM: An Aligned Neural Topic Model for Exploring Evolving Topics,"Hamed Rahimi, Hubert Naacke, Camelia Constantin, Bernd Amann","This paper presents an algorithmic family of dynamic topic models called Aligned Neural Topic Models (ANTM), which combine novel data mining algorithms to provide a modular framework for discovering evolving topics. ANTM maintains the temporal continuity of evolving topics by extracting time-aware features from documents using advanced pre-trained Large Language Models (LLMs) and employing an overlapping sliding window algorithm for sequential document clustering. This overlapping sliding window algorithm identifies a different number of topics within each time frame and aligns semantically similar document clusters across time periods. This process captures emerging and fading trends across different periods and allows for a more interpretable representation of evolving topics. Experiments on four distinct datasets show that ANTM outperforms probabilistic dynamic topic models in terms of topic coherence and diversity metrics. Moreover, it improves the scalability and flexibility of dynamic topic models by being accessible and adaptable to different types of algorithms. Additionally, a Python package is developed for researchers and scientists who wish to study the trends and evolving patterns of topics in large-scale textual data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01501,February 2023,390,Modelling community structure and temporal spreading on complex networks,Vesa Kuikka,We present methods for analysing hierarchical and overlapping community structure and spreading phenomena on complex networks. Different models can be developed for describing static connectivity or dynamical ...,https://www.springeropen.com//computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-021-00094-z,18 March 2021,
391,0.000678989392134464,391,Unsupervised learning of representative local atomic arrangements in molecular dynamics data,"Fabrice Roncoroni, Ana Sanz-Matias, Siddharth Sundararaman, David Prendergast","Molecular dynamics (MD) simulations present a data-mining challenge, given that they can generate a considerable amount of data but often rely on limited or biased human interpretation to examine their information content. By not asking the right questions of MD data we may miss critical information hidden within it. We combine dimensionality reduction (UMAP) and unsupervised hierarchical clustering (HDBSCAN) to quantitatively characterize the coordination environment of chemical species within MD data. By focusing on local coordination, we significantly reduce the amount of data to be analyzed by extracting all distinct molecular formulas within a given coordination sphere. We then efficiently combine UMAP and HDBSCAN with alignment or shape-matching algorithms to classify these formulas into distinct structural isomer families. The outcome is a quantitative mapping of the multiple coordination environments present in the MD data. The method was employed to reveal details of cation coordination in electrolytes based on molecular liquids. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01465,February 2023,391,Designing a tangible tabletop installation and enacting a socioenactive experience with TangiTime,Yusseli Lizeth Méndez Mendoza and M. Cecília C. Baranauskas,"Contemporary computational technology (tangible and ubiquitous) are still challenging the mainstream systems design methods, demanding new ways of considering the interaction design and its evaluation. In this...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00112-y,28 May 2021,
392,0.000678989392134464,392,Generalized Uncertainty of Deep Neural Networks: Taxonomy and Applications,Chengyu Dong,"Deep neural networks have seen enormous success in various real-world applications. Beyond their predictions as point estimates, increasing attention has been focused on quantifying the uncertainty of their predictions. In this review, we show that the uncertainty of deep neural networks is not only important in a sense of interpretability and transparency, but also crucial in further advancing their performance, particularly in learning systems seeking robustness and efficiency. We will generalize the definition of the uncertainty of deep neural networks to any number or vector that is associated with an input or an input-label pair, and catalog existing methods on ``mining'' such uncertainty from a deep model. We will include those methods from the classic field of uncertainty quantification as well as those methods that are specific to deep neural networks. We then show a wide spectrum of applications of such generalized uncertainty in realistic learning tasks including robust learning such as noisy learning, adversarially robust learning; data-efficient learning such as semi-supervised and weakly-supervised learning; and model-efficient learning such as model compression and knowledge distillation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01440,February 2023,392,A standards-based framework to foster geospatial data and process interoperability,"Gilberto Zonta Pastorello, Rodrigo Dias Arruda Senra and Claudia Bauzer Medeiros","The quest for interoperability is one of the main driving forces behind international organizations such as OGC and W3C. In parallel, a trend in systems design and development is to break down GIS functionalit...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192574,March 2009,
393,0.000678989392134464,393,Neural Insights for Digital Marketing Content Design,"Fanjie Kong, Yuan Li, Houssam Nassif, Tanner Fiez, Ricardo Henao, Shreya Chakrabarti","In digital marketing, experimenting with new website content is one of the key levers to improve customer engagement. However, creating successful marketing content is a manual and time-consuming process that lacks clear guiding principles. This paper seeks to close the loop between content creation and online experimentation by offering marketers AI-driven actionable insights based on historical data to improve their creative process. We present a neural-network-based system that scores and extracts insights from a marketing content design, namely, a multimodal neural network predicts the attractiveness of marketing contents, and a post-hoc attribution method generates actionable insights for marketers to improve their content in specific marketing locations. Our insights not only point out the advantages and drawbacks of a given current content, but also provide design recommendations based on historical data. We show that our scoring model and insights work well both quantitatively and qualitatively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01416,February 2023,393,On Securing Real-Time Speech Transmission over the Internet: An Experimental Study,"Alessandro Aldini, Marco Roccetti and Roberto Gorrieri",We analyze and compare several soft real-time applications designed for the secure transmission of packetized audio over the Internet. The main metrics we consider for the purposes of our analysis are (i) the ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703304019,8 September 2003,
394,0.000678989392134464,394,De Novo Molecular Generation via Connection-aware Motif Mining,"Zijie Geng, Shufang Xie, Yingce Xia, Lijun Wu, Tao Qin, Jie Wang, Yongdong Zhang, Feng Wu, Tie-Yan Liu","De novo molecular generation is an essential task for science discovery. Recently, fragment-based deep generative models have attracted much research attention due to their flexibility in generating novel molecules based on existing molecule fragments. However, the motif vocabulary, i.e., the collection of frequent fragments, is usually built upon heuristic rules, which brings difficulties to capturing common substructures from large amounts of molecules. In this work, we propose a new method, MiCaM, to generate molecules based on mined connection-aware motifs. Specifically, it leverages a data-driven algorithm to automatically discover motifs from a molecule library by iteratively merging subgraphs based on their frequency. The obtained motif vocabulary consists of not only molecular motifs (i.e., the frequent fragments), but also their connection information, indicating how the motifs are connected with each other. Based on the mined connection-aware motifs, MiCaM builds a connection-aware generator, which simultaneously picks up motifs and determines how they are connected. We test our method on distribution-learning benchmarks (i.e., generating novel molecules to resemble the distribution of a given training set) and goal-directed benchmarks (i.e., generating molecules with target properties), and achieve significant improvements over previous fragment-based baselines. Furthermore, we demonstrate that our method can effectively mine domain-specific motifs for different tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01129,February 2023,394,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0085-3,22 August 2012,
395,0.000678989392134464,395,Sentiment Overflow in the Testing Stack: Analysing Software Testing Posts on Stack Overflow,"Mark Swillus, Andy Zaidman","Software testing is an integral part of modern software engineering practice. Past research has not only underlined its significance, but also revealed its multi-faceted nature. The practice of software testing and its adoption is influenced by many factors that go beyond tools or technology. This paper sets out to investigate the context of software testing from the practitioners' point of view by mining and analyzing sentimental posts on the widely used question and answer website Stack Overflow. By qualitatively analyzing sentimental expressions of practitioners, which we extract from the Stack Overflow dataset using sentiment analysis tools, we discern factors that help us to better understand the lived experience of software engineers with regards to software testing. Grounded in the data that we have analyzed, we argue that sentiments like insecurity, despair and aspiration, have an impact on practitioners' attitude towards testing. We suggest that they are connected to concrete factors like the level of complexity of projects in which software testing is practiced. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.01037,February 2023,395,Editorial,Maria Cristina F. de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0030-x,9 February 2011,
396,0.000678989392134464,396,Exploring Invariant Representation for Visible-Infrared Person Re-Identification,"Lei Tan, Yukang Zhang, Shengmei Shen, Yan Wang, Pingyang Dai, Xianming Lin, Yongjian Wu, Rongrong Ji","Cross-spectral person re-identification, which aims to associate identities to pedestrians across different spectra, faces a main challenge of the modality discrepancy. In this paper, we address the problem from both image-level and feature-level in an end-to-end hybrid learning framework named robust feature mining network (RFM). In particular, we observe that the reflective intensity of the same surface in photos shot in different wavelengths could be transformed using a linear model. Besides, we show the variable linear factor across the different surfaces is the main culprit which initiates the modality discrepancy. We integrate such a reflection observation into an image-level data augmentation by proposing the linear transformation generator (LTG). Moreover, at the feature level, we introduce a cross-center loss to explore a more compact intra-class distribution and modality-aware spatial attention to take advantage of textured regions more efficiently. Experiment results on two standard cross-spectral person re-identification datasets, i.e., RegDB and SYSU-MM01, have demonstrated state-of-the-art performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2302.00884,February 2023,396,Human behavior in image-based Road Health Inspection Systems despite the emerging AutoML,Thitirat Siriborvornratanakul,The emergence of automated machine learning or AutoML has raised an interesting trend of no-code and low-code machine learning where most tasks in the machine learning pipeline can possibly be automated withou...,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00646-8,20 July 2022,
397,0.000678989392134464,397,Time Series Forecasting via Semi-Asymmetric Convolutional Architecture with Global Atrous Sliding Window,Yuanpeng He,"The proposed method in this paper is designed to address the problem of time series forecasting. Although some exquisitely designed models achieve excellent prediction performances, how to extract more useful information and make accurate predictions is still an open issue. Most of modern models only focus on a short range of information, which are fatal for problems such as time series forecasting which needs to capture long-term information characteristics. As a result, the main concern of this work is to further mine relationship between local and global information contained in time series to produce more precise predictions. In this paper, to satisfactorily realize the purpose, we make three main contributions that are experimentally verified to have performance advantages. Firstly, original time series is transformed into difference sequence which serves as input to the proposed model. And secondly, we introduce the global atrous sliding window into the forecasting model which references the concept of fuzzy time series to associate relevant global information with temporal data within a time period and utilizes central-bidirectional atrous algorithm to capture underlying-related features to ensure validity and consistency of captured data. Thirdly, a variation of widely-used asymmetric convolution which is called semi-asymmetric convolution is devised to more flexibly extract relationships in adjacent elements and corresponding associated global features with adjustable ranges of convolution on vertical and horizontal directions. The proposed model in this paper achieves state-of-the-art on most of time series datasets provided compared with competitive modern models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.13691,January 2023,397,The potential for computational IT tools in disaster relief and shelter design,"Noorullah Kuchai, Paul Shepherd, Juliana Calabria-Holley, Alexander Copping, Aude Matard and David Coley","The expanding use of IT has brought an increase in productivity to the world of business, industry and commerce. However, this is not mirrored by an equivalent growth in the use of IT by aid agencies in post-d...",https://www.springeropen.com//jhumanitarianaction.springeropen.com/articles/10.1186/s41018-020-00069-1,3 April 2020,
398,0.000678989392134464,398,Privacy Preserving Ultra-Short-term Wind Power Prediction Based on Secure Multi Party Computation,"Hang Fan, Xiaoyu Fan, Tianyi Hao, Wei Wei, Kun Chen, Guosai Wang, Xiaofeng Jia, Yidong Li, Wei Xu","Mining the spatial and temporal correlation of wind farm output data is beneficial for enhancing the precision of ultra-short-term wind power prediction. However, if the wind farms are owned by separate entities, they may be reluctant to share their data directly due to privacy concerns as well as business management regulation policies. Although cryptographic approaches have been designed to protect privacy in the process of data sharing, it is still a challenging problem to encrypt the original data while extracting the nonlinear relationship among multiple wind farms in the machine learning process. This paper presents pwXGBoost, a technique based on the machine learning tree model and secure multi-party computation (SMPC) that can successfully extract complicated relationships while preserving data privacy. A maximum mean discrepancy (MMD) based scheme is proposed to effectively choose adjacent candidate wind farms to participate in the collaborative model training, therefore improving the accuracy and reducing the burden of data acquisition. The proposed method was evaluated on real world data collected from a cluster of wind farms in Inner Mongolia, China, demonstrating that it is capable of achieving considerable efficiency and performance improvements while preserving privacy △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.13513,January 2023,398,Exploring science-and-technology-led innovation: a cross-country study,Viju Raghupathi and Wullianallur Raghupathi,"Countries can enhance endogenous innovation using multifaceted incentives for science and technology indicators. We explore country-level innovation using OECD data for research and development (R&D), patents,...",https://www.springeropen.com//innovation-entrepreneurship.springeropen.com/articles/10.1186/s13731-018-0097-0,24 January 2019,
399,0.000678989392134464,399,Affinity Uncertainty-based Hard Negative Mining in Graph Contrastive Learning,"Chaoxi Niu, Guansong Pang, Ling Chen","Hard negative mining has shown effective in enhancing self-supervised contrastive learning (CL) on diverse data types, including graph contrastive learning (GCL). Existing hardness-aware CL methods typically treat negative instances that are most similar to the anchor instance as hard negatives, which helps improve the CL performance, especially on image data. However, this approach often fails to identify the hard negatives but leads to many false negatives on graph data. This is mainly due to that the learned graph representations are not sufficiently discriminative due to over-smooth representations and/or non-i.i.d. issues in graph data. To tackle this problem, this paper proposes a novel approach that builds a discriminative model on collective affinity information (i.e, two sets of pairwise affinities between the negative instances and the anchor instance) to mine hard negatives in GCL. In particular, the proposed approach evaluates how confident/uncertain the discriminative model is about the affinity of each negative instance to an anchor instance to determine its hardness weight relative to the anchor instance. This uncertainty information is then incorporated into existing GCL loss functions via a weighting term to enhance their performance. The enhanced GCL is theoretically grounded that the resulting GCL loss is equivalent to a triplet loss with an adaptive margin being exponentially proportional to the learned uncertainty of each negative instance. Extensive experiments on 10 graph datasets show that our approach i) consistently enhances different state-of-the-art GCL methods in both graph and node classification tasks, and ii) significantly improves their robustness against adversarial attacks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.13340,January 2023,399,"Welcome Home, Systems Chemists!","Günter von Kiedrowski, Sijbren Otto and Piet Herdewijn",It is our utmost pleasure to launch the Journal of Systems Chemistry. What systems chemistry exactly is will be known in a few years from now when one is able to sketch the scope and vision of the field also b...,https://www.springeropen.com//jsystchem.springeropen.com/articles/10.1186/1759-2208-1-1,18 August 2010,
400,0.000678989392134464,400,A Human Word Association based model for topic detection in social networks,"Mehrdad Ranjbar Khadivi, Shahin Akbarpour, Mohammad-Reza Feizi-Derakhshi, Babak Anari","With the widespread use of social networks, detecting the topics discussed in these networks has become a significant challenge. The current works are mainly based on frequent pattern mining or semantic relations, and the language structure is not considered. The meaning of language structural methods is to discover the relationship between words and how humans understand them. Therefore, this paper uses the Concept of the Imitation of the Mental Ability of Word Association to propose a topic detection framework in social networks. This framework is based on the Human Word Association method. The performance of this method is evaluated on the FA-CUP dataset. It is a benchmark dataset in the field of topic detection. The results show that the proposed method is a good improvement compared to other methods, based on the Topic-recall and the keyword F1 measure. Also, most of the previous works in the field of topic detection are limited to the English language, and the Persian language, especially microblogs written in this language, is considered a low-resource language. Therefore, a data set of Telegram posts in the Farsi language has been collected. Applying the proposed method to this dataset also shows that this method works better than other topic detection methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.13066,January 2023,400,An ontological knowledge and multiple abstraction level decision support system in healthcare,"Luca Piovesan, Gianpaolo Molino and Paolo Terenziani","The rationalization of the healthcare processes and organizations is a task of fundamental importance to grant both the quality and the standardization of healthcare services, and the minimization of costs. Cl...",https://www.springeropen.com//decisionanalyticsjournal.springeropen.com/articles/10.1186/2193-8636-1-8,2 May 2014,
401,0.000678989392134464,401,Harnessing the Power of Decision Trees to Detect IoT Malware,Marwan Omar,"Due to its simple installation and connectivity, the Internet of Things (IoT) is susceptible to malware attacks. Being able to operate autonomously. As IoT devices have become more prevalent, they have become the most tempting targets for malware. Weak, guessable, or hard-coded passwords, and a lack of security measures contribute to these vulnerabilities along with insecure network connections and outdated update procedures. To understand IoT malware, current methods and analysis ,using static methods, are ineffective. The field of deep learning has made great strides in recent years due to their tremendous data mining, learning, and expression capabilities, cybersecurity has enjoyed tremendous growth in recent years. As a result, malware analysts will not have to spend as much time analyzing malware. In this paper, we propose a novel detection and analysis method that harnesses the power and simplicity of decision trees. The experiments are conducted using a real word dataset, MaleVis which is a publicly available dataset. Based on the results, we show that our proposed approach outperforms existing state-of-the-art solutions in that it achieves 97.23% precision and 95.89% recall in terms of detection and classification. A specificity of 96.58%, F1-score of 96.40%, an accuracy of 96.43. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.12039,January 2023,401,Classification of imbalanced cloud image data using deep neural networks: performance improvement through a data science competition,Daisuke Matsuoka,"Image data classification using machine learning is an effective method for detecting atmospheric phenomena. However, extreme weather events with a small number of cases cause a decrease in classification pred...",https://www.springeropen.com//progearthplanetsci.springeropen.com/articles/10.1186/s40645-021-00459-y,15 December 2021,
402,0.000678989392134464,402,ExplainableFold: Understanding AlphaFold Prediction with Explainable AI,"Juntao Tan, Yongfeng Zhang","This paper presents ExplainableFold, an explainable AI framework for protein structure prediction. Despite the success of AI-based methods such as AlphaFold in this field, the underlying reasons for their predictions remain unclear due to the black-box nature of deep learning models. To address this, we propose a counterfactual learning framework inspired by biological principles to generate counterfactual explanations for protein structure prediction, enabling a dry-lab experimentation approach. Our experimental results demonstrate the ability of ExplainableFold to generate high-quality explanations for AlphaFold's predictions, providing near-experimental understanding of the effects of amino acids on 3D protein structure. This framework has the potential to facilitate a deeper understanding of protein structures. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.11765,January 2023,402,Special issue on animal and insect behaviour understanding in image sequences,"Concetto Spampinato, Giovanni Maria Farinella, Bas Boom, Vasileios Mezaris, Margrit Betke and Robert B Fisher",Unknown,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/1687-5281-2015-1,30 January 2015,
403,0.000678989392134464,403,Bayesian Self-Supervised Contrastive Learning,"Bin Liu, Bang Wang","Recent years have witnessed many successful applications of contrastive learning in diverse domains, yet its self-supervised version still remains many exciting challenges. As the negative samples are drawn from unlabeled datasets, a randomly selected sample may be actually a false negative to an anchor, leading to incorrect encoder training. This paper proposes a new self-supervised contrastive loss called the BCL loss that still uses random samples from the unlabeled data while correcting the resulting bias with importance weights. The key idea is to design the desired sampling distribution for sampling hard true negative samples under the Bayesian framework. The prominent advantage lies in that the desired sampling distribution is a parametric structure, with a location parameter for debiasing false negative and concentration parameter for mining hard negative, respectively. Experiments validate the effectiveness and superiority of the BCL loss. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.11673,January 2023,403,NGSS-based teacher professional development to implement engineering practices in STEM instruction,"Kimberly B. Christian, Angela M. Kelly and Mónica F. Bugallo","With widespread adoption of the Next Generation Science Standards (NGSS) in the USA (US), research is needed on how secondary science, technology, engineering, and mathematics (STEM) teachers conceptualize the...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00284-1,23 March 2021,
404,0.000678989392134464,404,Theme-driven Keyphrase Extraction to Analyze Social Media Discourse,"William Romano, Omar Sharif, Madhusudan Basak, Joseph Gatto, Sarah Preum","Social media platforms are vital resources for sharing self-reported health experiences, offering rich data on various health topics. Despite advancements in Natural Language Processing (NLP) enabling large-scale social media data analysis, a gap remains in applying keyphrase extraction to health-related content. Keyphrase extraction is used to identify salient concepts in social media discourse without being constrained by predefined entity classes. This paper introduces a theme-driven keyphrase extraction framework tailored for social media, a pioneering approach designed to capture clinically relevant keyphrases from user-generated health texts. Themes are defined as broad categories determined by the objectives of the extraction task. We formulate this novel task of theme-driven keyphrase extraction and demonstrate its potential for efficiently mining social media text for the use case of treatment for opioid use disorder. This paper leverages qualitative and quantitative analysis to demonstrate the feasibility of extracting actionable insights from social media data and efficiently extracting keyphrases using minimally supervised NLP models. Our contributions include the development of a novel data collection and curation framework for theme-driven keyphrase extraction and the creation of MOUD-Keyphrase, the first dataset of its kind comprising human-annotated keyphrases from a Reddit community. We also identify the scope of minimally supervised NLP models to extract keyphrases from social media data efficiently. Lastly, we found that a large language model (ChatGPT) outperforms unsupervised keyphrase extraction models, and we evaluate its efficacy in this task. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.11508,January 2023,404,"A critical review of intrusion detection systems in the internet of things: techniques, deployment strategy, validation strategy, attacks, public datasets and challenges",Ansam Khraisat and Ammar Alazab,"The Internet of Things (IoT) has been rapidly evolving towards making a greater impact on everyday life to large industrial systems. Unfortunately, this has attracted the attention of cybercriminals who made I...",https://www.springeropen.com//cybersecurity.springeropen.com/articles/10.1186/s42400-021-00077-7,8 March 2021,
405,0.000678989392134464,405,A Systematic Review of Green AI,"Roberto Verdecchia, June Sallou, Luís Cruz","With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115%, with savings over 50% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.11047,January 2023,405,A knowledge representation approach using fuzzy cognitive maps for better navigation support in an adaptive learning system,Konstantina Chrysafiadi and Maria Virvou,In this paper a knowledge representation approach of an adaptive and/or personalized tutoring system is presented. The domain knowledge should be represented in a more realistic way in order to allow the adapt...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-2-81,5 March 2013,
406,0.000678989392134464,406,Towards Knowledge-Centric Process Mining,"Asjad Khan, Arsal Huda, Aditya Ghose, Hoa Khanh Dam","Process analytic approaches play a critical role in supporting the practice of business process management and continuous process improvement by leveraging process-related data to identify performance bottlenecks, extracting insights about reducing costs and optimizing the utilization of available resources. Process analytic techniques often have to contend with real-world settings where available logs are noisy or incomplete. In this paper we present an approach that permits process analytics techniques to deliver value in the face of noisy/incomplete event logs. Our approach leverages knowledge graphs to mitigate the effects of noise in event logs while supporting process analysts in understanding variability associated with event logs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10927,January 2023,406,"AcMus: an open, integrated platform for room acoustics research","Marcelo Queiroz, Fernando Iazzetta, Fabio Kon, Márcio Henrique A. Gomes, Fábio L. Figueiredo, Bruno Masiero, Leo K. Ueda, Luciana Dias, Mário Henrique C. Torres and Leandro F. Thomaz","This article describes the design, implementation, and experiences with AcMus, an open and integrated software platform for room acoustics research, which comprises tools for measurement, analysis, and simulat...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192566,September 2008,
407,0.000678989392134464,407,Generating large-scale network analyses of scientific landscapes in seconds using Dimensions on Google BigQuery,"Michele Pasin, Richard Abdill","The growth of large, programatically accessible bibliometrics databases presents new opportunities for complex analyses of publication metadata. In addition to providing a wealth of information about authors and institutions, databases such as those provided by Dimensions also provide conceptual information and links to entities such as grants, funders and patents. However, data is not the only challenge in evaluating patterns in scholarly work: These large datasets can be challenging to integrate, particularly for those unfamiliar with the complex schemas necessary for accommodating such heterogeneous information, and those most comfortable with data mining may not be as experienced in data visualisation. Here, we present an open-source Python library that streamlines the process accessing and diagramming subsets of the Dimensions on Google BigQuery database and demonstrate its use on the freely available Dimensions COVID-19 dataset. We are optimistic that this tool will expand access to this valuable information by streamlining what would otherwise be multiple complex technical tasks, enabling more researchers to examine patterns in research focus and collaboration over time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10736,January 2023,407,Editorial,Yuke Wang and Yu Hen Hu,Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702001841,1 September 2002,
408,0.000678989392134464,408,Cross-lingual Argument Mining in the Medical Domain,"Anar Yeginbergenova, Rodrigo Agerri","Nowadays the medical domain is receiving more and more attention in applications involving Artificial Intelligence. Clinicians have to deal with an enormous amount of unstructured textual data to make a conclusion about patients' health in their everyday life. Argument mining helps to provide a structure to such data by detecting argumentative components in the text and classifying the relations between them. However, as it is the case for many tasks in Natural Language Processing in general and in medical text processing in particular, the large majority of the work on computational argumentation has been done only for English. This is also the case with the only dataset available for argumentation in the medical domain, namely, the annotated medical data of abstracts of Randomized Controlled Trials (RCT) from the MEDLINE database. In order to mitigate the lack of annotated data for other languages, we empirically investigate several strategies to perform argument mining and classification in medical texts for a language for which no annotated data is available. This project shows that automatically translating and project annotations from English to a target language (Spanish) is an effective way to generate annotated data without manual intervention. Furthermore, our experiments demonstrate that the translation and projection approach outperforms zero-shot cross-lingual approaches using a large masked multilingual language model. Finally, we show how the automatically generated data in Spanish can also be used to improve results in the original English evaluation setting. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10527,January 2023,408,Improving the Quality of Color Colonoscopy Videos,"Rozenn Dahyot, Fernando Vilariño and Gerard Lacey","Colonoscopy is currently one of the best methods to detect colorectal cancer. Nowadays, one of the widely used colonoscopes has a monochrome chipset recording successively at 60 Hz...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/139429,22 January 2008,
409,0.000678989392134464,409,What are the Machine Learning best practices reported by practitioners on Stack Exchange?,"Anamaria Mojica-Hanke, Andrea Bayona, Mario Linares-Vásquez, Steffen Herbold, Fabio A. González","Machine Learning (ML) is being used in multiple disciplines due to its powerful capability to infer relationships within data. In particular, Software Engineering (SE) is one of those disciplines in which ML has been used for multiple tasks, like software categorization, bugs prediction, and testing. In addition to the multiple ML applications, some studies have been conducted to detect and understand possible pitfalls and issues when using ML. However, to the best of our knowledge, only a few studies have focused on presenting ML best practices or guidelines for the application of ML in different domains. In addition, the practices and literature presented in previous literature (i) are domain-specific (e.g., concrete practices in biomechanics), (ii) describe few practices, or (iii) the practices lack rigorous validation and are presented in gray literature. In this paper, we present a study listing 127 ML best practices systematically mining 242 posts of 14 different Stack Exchange (STE) websites and validated by four independent ML experts. The list of practices is presented in a set of categories related to different stages of the implementation process of an ML-enabled system; for each practice, we include explanations and examples. In all the practices, the provided examples focus on SE tasks. We expect this list of practices could help practitioners to understand better the practices and use ML in a more informed way, in particular newcomers to this new area that sits at the intersection of software engineering and machine learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10516,January 2023,409,"The past, present, and future of experimental software engineering",Victor Robert Basili,"This paper gives a 40 year overview of the evolution of experimental software engineering, from the past to the future, from a personal perspective. My hypothesis is that my work followed the evolution of the ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194492,October 2006,
410,0.0130975937317491,410,Spectral Cross-Domain Neural Network with Soft-adaptive Threshold Spectral Enhancement,"Che Liu, Sibo Cheng, Weiping Ding, Rossella Arcucci","Electrocardiography (ECG) signals can be considered as multi-variable time-series. The state-of-the-art ECG data classification approaches, based on either feature engineering or deep learning techniques, treat separately spectral and time domains in machine learning systems. No spectral-time domain communication mechanism inside the classifier model can be found in current approaches, leading to difficulties in identifying complex ECG forms. In this paper, we proposed a novel deep learning model named Spectral Cross-domain neural network (SCDNN) with a new block called Soft-adaptive threshold spectral enhancement (SATSE), to simultaneously reveal the key information embedded in spectral and time domains inside the neural network. More precisely, the domain-cross information is captured by a general Convolutional neural network (CNN) backbone, and different information sources are merged by a self-adaptive mechanism to mine the connection between time and spectral domains. In SATSE, the knowledge from time and spectral domains is extracted via the Fast Fourier Transformation (FFT) with soft trainable thresholds in modified Sigmoid functions. The proposed SCDNN is tested with several classification tasks implemented on the public ECG databases \textit{PTB-XL} and \textit{MIT-BIH}. SCDNN outperforms the state-of-the-art approaches with a low computational cost regarding a variety of metrics in all classification tasks on both databases, by finding appropriate domains from the infinite spectral mapping. The convergence of the trainable thresholds in the spectral domain is also numerically investigated in this paper. The robust performance of SCDNN provides a new perspective to exploit knowledge across deep learning models from time and spectral domains. The repository can be found: https://github.com/DL-WG/SCDNN-TS △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10171,January 2023,410,Studying technology-based strategies for enhancing motivation in mathematics,"Jon R Star, Jason A Chen, Megan W Taylor, Kelley Durkin, Chris Dede and Theodore Chao","During the middle school years, students frequently show significant declines in motivation toward school in general and mathematics in particular. One way in which researchers have sought to spark students’ i...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/2196-7822-1-7,1 October 2014,
411,0.000678989392134464,411,Computational Solar Energy -- Ensemble Learning Methods for Prediction of Solar Power Generation based on Meteorological Parameters in Eastern India,"Debojyoti Chakraborty, Jayeeta Mondal, Hrishav Bakul Barua, Ankur Bhattacharjee","The challenges in applications of solar energy lies in its intermittency and dependency on meteorological parameters such as; solar radiation, ambient temperature, rainfall, wind-speed etc., and many other physical parameters like dust accumulation etc. Hence, it is important to estimate the amount of solar photovoltaic (PV) power generation for a specific geographical location. Machine learning (ML) models have gained importance and are widely used for prediction of solar power plant performance. In this paper, the impact of weather parameters on solar PV power generation is estimated by several Ensemble ML (EML) models like Bagging, Boosting, Stacking, and Voting for the first time. The performance of chosen ML algorithms is validated by field dataset of a 10kWp solar PV power plant in Eastern India region. Furthermore, a complete test-bed framework has been designed for data mining as well as to select appropriate learning models. It also supports feature selection and reduction for dataset to reduce space and time complexity of the learning models. The results demonstrate greater prediction accuracy of around 96% for Stacking and Voting EML models. The proposed work is a generalized one and can be very useful for predicting the performance of large-scale solar PV power plants also. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10159,January 2023,411,Localization and mapping in urban environments using mobile Robots,Denis F. Wolf and Gaurav S. Sukhatme,Mapping is a basic capability for mobile robots. Most applications demand some level of knowledge about the environment to be accomplished. Most mapping approaches in the literature are designed to perform in ...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194257,December 2007,
412,0.00514534320972017,412,"Remote patient monitoring using artificial intelligence: Current state, applications, and challenges","Thanveer Shaik, Xiaohui Tao, Niall Higgins, Lin Li, Raj Gururajan, Xujuan Zhou, U. Rajendra Acharya","The adoption of artificial intelligence (AI) in healthcare is growing rapidly. Remote patient monitoring (RPM) is one of the common healthcare applications that assist doctors to monitor patients with chronic or acute illness at remote locations, elderly people in-home care, and even hospitalized patients. The reliability of manual patient monitoring systems depends on staff time management which is dependent on their workload. Conventional patient monitoring involves invasive approaches which require skin contact to monitor health status. This study aims to do a comprehensive review of RPM systems including adopted advanced technologies, AI impact on RPM, challenges and trends in AI-enabled RPM. This review explores the benefits and challenges of patient-centric RPM architectures enabled with Internet of Things wearable devices and sensors using the cloud, fog, edge, and blockchain technologies. The role of AI in RPM ranges from physical activity classification to chronic disease monitoring and vital signs monitoring in emergency settings. This review results show that AI-enabled RPM architectures have transformed healthcare monitoring applications because of their ability to detect early deterioration in patients' health, personalize individual patient health parameter monitoring using federated learning, and learn human behavior patterns using techniques such as reinforcement learning. This review discusses the challenges and trends to adopt AI to RPM systems and implementation issues. The future directions of AI in RPM applications are analyzed based on the challenges and trends △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.10009,January 2023,412,Correction to: Flow of online misinformation during the peak of the COVID-19 pandemic in Italy,"Guido Caldarelli, Rocco De Nicola, Marinella Petrocchi, Manuel Pratelli and Fabio Saracco",Theoriginal articlewas published inEPJ Data Science202110:34,https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00296-5,29 July 2021,
413,0.000678989392134464,413,Learning Effective Strategies for Moving Target Defense with Switching Costs,"Vignesh Viswanathan, Megha Bose, Praveen Paruchuri","Moving Target Defense (MTD) has emerged as a key technique in various security applications as it takes away the attacker's ability to perform reconnaissance for exploiting a system's vulnerabilities. However, most of the existing research in the field assumes unrealistic access to information about the attacker's motivations and/or actions when developing MTD strategies. Many of the existing approaches also assume complete knowledge regarding the vulnerabilities of a system and how each of these vulnerabilities can be exploited by an attacker. In this work, we aim to create algorithms that generate effective Moving Target Defense strategies that do not rely on prior knowledge about the attackers. Our work assumes that the only way the defender receives information about its own reward is via interaction with the attacker in a repeated game setting. Depending on the amount of information that can be obtained from the interactions, we devise two different algorithms using multi-armed bandit formulation to identify efficient strategies. We then evaluate our algorithms using data mined from the National Vulnerability Database to showcase that they match the performance of the state-of-the-art techniques, despite using a lot less amount of information. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.09892,January 2023,413,In the right order of brush strokes: a sketch of a software philosophy retrospective,Evgeny Pyshkin,This paper follows a discourse on software recognized as a product of art and human creativity progressing probably for as long as software exists. A retrospective view on computer science and software philoso...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-3-186,11 April 2014,
414,0.000678989392134464,414,Modeling Non-deterministic Human Behaviors in Discrete Food Choices,"Andrew Starnes, Anton Dereventsov, E. Susanne Blazek, Folasade Phillips",We establish a non-deterministic model that predicts a user's food preferences from their demographic information. Our simulator is based on NHANES dataset and domain expert knowledge in the form of established behavioral studies. Our model can be used to generate an arbitrary amount of synthetic datapoints that are similar in distribution to the original dataset and align with behavioral science expectations. Such a simulator can be used in a variety of machine learning tasks and especially in applications requiring human behavior prediction. △ Less,https://arxiv.orghttps://arxiv.org/abs/2301.09454,January 2023,414,Correction to: Network cartography of university students’ knowledge landscapes about the history of science: landmarks and thematic communities,Henri Lommi and Ismo T. Koponen,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-020-00284-0,24 July 2020,
415,0.000678989392134464,415,A Survey on Actionable Knowledge,Sayed Erfan Arefin,"Actionable Knowledge Discovery (AKD) is a crucial aspect of data mining that is gaining popularity and being applied in a wide range of domains. This is because AKD can extract valuable insights and information, also known as knowledge, from large datasets. The goal of this paper is to examine different research studies that focus on various domains and have different objectives. The paper will review and discuss the methods used in these studies in detail. AKD is a process of identifying and extracting actionable insights from data, which can be used to make informed decisions and improve business outcomes. It is a powerful tool for uncovering patterns and trends in data that can be used for various applications such as customer relationship management, marketing, and fraud detection. The research studies reviewed in this paper will explore different techniques and approaches for AKD in different domains, such as healthcare, finance, and telecommunications. The paper will provide a thorough analysis of the current state of AKD in the field and will review the main methods used by various research studies. Additionally, the paper will evaluate the advantages and disadvantages of each method and will discuss any novel or new solutions presented in the field. Overall, this paper aims to provide a comprehensive overview of the methods and techniques used in AKD and the impact they have on different domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.09317,January 2023,415,Trust-based Decision-making for the Adaptation of Public Displays in Changing Social Contexts,"Michael Wißner, Stephan Hammer, Ekatarina Kurdyukova and Elisabeth André","Public displays may adapt intelligently to the social context, tailoring information on the screen, for example, to the profiles of spectators, their gender or based on their mutual proximity. However, such ad...",https://www.springeropen.com//journaloftrustmanagement.springeropen.com/articles/10.1186/2196-064X-1-6,20 May 2014,
416,0.000678989392134464,416,Debiasing the Cloze Task in Sequential Recommendation with Bidirectional Transformers,"Khalil Damak, Sami Khenissi, Olfa Nasraoui","Bidirectional Transformer architectures are state-of-the-art sequential recommendation models that use a bi-directional representation capacity based on the Cloze task, a.k.a. Masked Language Modeling. The latter aims to predict randomly masked items within the sequence. Because they assume that the true interacted item is the most relevant one, an exposure bias results, where non-interacted items with low exposure propensities are assumed to be irrelevant. The most common approach to mitigating exposure bias in recommendation has been Inverse Propensity Scoring (IPS), which consists of down-weighting the interacted predictions in the loss function in proportion to their propensities of exposure, yielding a theoretically unbiased learning. In this work, we argue and prove that IPS does not extend to sequential recommendation because it fails to account for the temporal nature of the problem. We then propose a novel propensity scoring mechanism, which can theoretically debias the Cloze task in sequential recommendation. Finally we empirically demonstrate the debiasing capabilities of our proposed approach and its robustness to the severity of exposure bias. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.09210,January 2023,416,Evaluating Edge Detection through Boundary Detection,"Song Wang, Feng Ge and Tiecheng Liu","Edge detection has been widely used in computer vision and image processing. However, the performance evaluation of the edge-detection results is still a challenging problem. A major dilemma in edge-detection ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/76278,1 December 2006,
417,0.000678989392134464,417,In-situ Water quality monitoring in Oil and Gas operations,"Satish Kumar, Rui Kou, Henry Hill, Jake Lempges, Eric Qian, Vikram Jayaram","From agriculture to mining, to energy, surface water quality monitoring is an essential task. As oil and gas operators work to reduce the consumption of freshwater, it is increasingly important to actively manage fresh and non-fresh water resources over the long term. For large-scale monitoring, manual sampling at many sites has become too time-consuming and unsustainable, given the sheer number of dispersed ponds, small lakes, playas, and wetlands over a large area. Therefore, satellite-based environmental monitoring presents great potential. Many existing satellite-based monitoring studies utilize index-based methods to monitor large water bodies such as rivers and oceans. However, these existing methods fail when monitoring small ponds-the reflectance signal received from small water bodies is too weak to detect. To address this challenge, we propose a new Water Quality Enhanced Index (WQEI) Model, which is designed to enable users to determine contamination levels in water bodies with weak reflectance patterns. Our results show that 1) WQEI is a good indicator of water turbidity validated with 1200 water samples measured in the laboratory, and 2) by applying our method to commonly available satellite data (e.g. LandSat8), one can achieve high accuracy water quality monitoring efficiently in large regions. This provides a tool for operators to optimize the quality of water stored within surface storage ponds and increasing the readiness and availability of non-fresh water. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.08800,January 2023,417,The Brazilian digital television system access device architecture,"Eduardo Rodrigues de Carvalho, Gil Garcia de Barros, Laisa Caroline de Paula Costa, Regis Rossi Alves Faria, Rogério Pernas Nunes, Roseli de Deus Lopes and Marcelo Knörich Zuffo","In early 2003, the Brazilian government accelerated the decision process on analog to digital transition of terrestrial TV broadcast infrastructure, naming this initiative The Brazilian Digital Television Syst...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192404,March 2007,
418,0.000678989392134464,418,STORM-GAN: Spatio-Temporal Meta-GAN for Cross-City Estimation of Human Mobility Responses to COVID-19,"Han Bao, Xun Zhou, Yiqun Xie, Yanhua Li, Xiaowei Jia","Human mobility estimation is crucial during the COVID-19 pandemic due to its significant guidance for policymakers to make non-pharmaceutical interventions. While deep learning approaches outperform conventional estimation techniques on tasks with abundant training data, the continuously evolving pandemic poses a significant challenge to solving this problem due to data nonstationarity, limited observations, and complex social contexts. Prior works on mobility estimation either focus on a single city or lack the ability to model the spatio-temporal dependencies across cities and time periods. To address these issues, we make the first attempt to tackle the cross-city human mobility estimation problem through a deep meta-generative framework. We propose a Spatio-Temporal Meta-Generative Adversarial Network (STORM-GAN) model that estimates dynamic human mobility responses under a set of social and policy conditions related to COVID-19. Facilitated by a novel spatio-temporal task-based graph (STTG) embedding, STORM-GAN is capable of learning shared knowledge from a spatio-temporal distribution of estimation tasks and quickly adapting to new cities and time periods with limited training samples. The STTG embedding component is designed to capture the similarities among cities to mitigate cross-task heterogeneity. Experimental results on real-world data show that the proposed approach can greatly improve estimation performance and out-perform baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.08648,January 2023,418,Comparing the score interpretation across modes in PISA: an investigation of how item facets affect difficulty,"Scott Harrison, Ulf Kroehne, Frank Goldhammer, Oliver Lüdtke and Alexander Robitzsch","Mode effects, the variations in item and scale properties attributed to the mode of test administration (paper vs. computer), have stimulated research around test equivalence and trend estimation in PISA. The ...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-023-00157-9,11 March 2023,
419,0.000678989392134464,419,"Hypercore Decomposition for Non-Fragile Hyperedges: Concepts, Algorithms, Observations, and Applications","Fanchen Bu, Geon Lee, Kijung Shin","Hypergraphs are a powerful abstraction for modeling high-order relations, which are ubiquitous in many fields. A hypergraph consists of nodes and hyperedges (i.e., subsets of nodes); and there have been a number of attempts to extend the notion of $k$-cores, which proved useful with numerous applications for pairwise graphs, to hypergraphs. However, the previous extensions are based on an unrealistic assumption that hyperedges are fragile, i.e., a high-order relation becomes obsolete as soon as a single member leaves it. In this work, we propose a new substructure model, called ($k$, $t$)-hypercore, based on the assumption that high-order relations remain as long as at least $t$ fraction of the members remain. Specifically, it is defined as the maximal subhypergraph where (1) every node is contained in at least $k$ hyperedges in it and (2) at least $t$ fraction of the nodes remain in every hyperedge. We first prove that, given $t$ (or $k$), finding the ($k$, $t$)-hypercore for every possible $k$ (or $t$) can be computed in time linear w.r.t the sum of the sizes of hyperedges. Then, we demonstrate that real-world hypergraphs from the same domain share similar ($k$, $t$)-hypercore structures, which capture different perspectives depending on $t$. Lastly, we show the successful applications of our model in identifying influential nodes, dense substructures, and vulnerability in hypergraphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.08440,January 2023,419,Ecolingua: a Formal ontology for data in ecology,Virgínia Brilhante,"Ecolinguais an ontology for ecological quantitative data, which has been designed through reuse of a conceptualisation of quantities and their physical dimensions provided by theEngMathfamily o...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192376,June 2005,
420,0.000678989392134464,420,Causal conditional hidden Markov model for multimodal traffic prediction,"Yu Zhao, Pan Deng, Junting Liu, Xiaofeng Jia, Mulan Wang","Multimodal traffic flow can reflect the health of the transportation system, and its prediction is crucial to urban traffic management. Recent works overemphasize spatio-temporal correlations of traffic flow, ignoring the physical concepts that lead to the generation of observations and their causal relationship. Spatio-temporal correlations are considered unstable under the influence of different conditions, and spurious correlations may exist in observations. In this paper, we analyze the physical concepts affecting the generation of multimode traffic flow from the perspective of the observation generation principle and propose a Causal Conditional Hidden Markov Model (CCHMM) to predict multimodal traffic flow. In the latent variables inference stage, a posterior network disentangles the causal representations of the concepts of interest from conditional information and observations, and a causal propagation module mines their causal relationship. In the data generation stage, a prior network samples the causal latent variables from the prior distribution and feeds them into the generator to generate multimodal traffic flow. We use a mutually supervised training method for the prior and posterior to enhance the identifiability of the model. Experiments on real-world datasets show that CCHMM can effectively disentangle causal representations of concepts of interest and identify causality, and accurately predict multimodal traffic flow. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.08249,January 2023,420,A Transcription System from MusicXML Format to Braille Music Notation,"D Goto, T Gotoh, R Minamikawa-Tachino and N Tamura","The Internet enables us to freely access music as recorded sound and even music scores. For the visually impaired, music scores must be transcribed from computer-based musical formats to Braille music notation...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/42498,1 December 2006,
421,0.000678989392134464,421,Boosting Synthetic Data Generation with Effective Nonlinear Causal Discovery,"Martina Cinquini, Fosca Giannotti, Riccardo Guidotti","Synthetic data generation has been widely adopted in software testing, data privacy, imbalanced learning, and artificial intelligence explanation. In all such contexts, it is crucial to generate plausible data samples. A common assumption of approaches widely used for data generation is the independence of the features. However, typically, the variables of a dataset depend on one another, and these dependencies are not considered in data generation leading to the creation of implausible records. The main problem is that dependencies among variables are typically unknown. In this paper, we design a synthetic dataset generator for tabular data that can discover nonlinear causalities among the variables and use them at generation time. State-of-the-art methods for nonlinear causal discovery are typically inefficient. We boost them by restricting the causal discovery among the features appearing in the frequent patterns efficiently retrieved by a pattern mining algorithm. We design a framework for generating synthetic datasets with known causalities to validate our proposal. Broad experimentation on many synthetic and real datasets with known causalities shows the effectiveness of the proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.07427,January 2023,421,Dependable Computing,Rogério de Lemos and Eliane Martins,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192361,October 2004,
422,0.000678989392134464,422,Cross-institution text mining to uncover clinical associations: a case study relating social factors and code status in intensive care medicine,"Madhumita Sushil, Atul J. Butte, Ewoud Schuit, Maarten van Smeden, Artuur M. Leeuwenberg","Objective: Text mining of clinical notes embedded in electronic medical records is increasingly used to extract patient characteristics otherwise not or only partly available, to assess their association with relevant health outcomes. As manual data labeling needed to develop text mining models is resource intensive, we investigated whether off-the-shelf text mining models developed at external institutions, together with limited within-institution labeled data, could be used to reliably extract study variables to conduct association studies. Materials and Methods: We developed multiple text mining models on different combinations of within-institution and external-institution data to extract social factors from discharge reports of intensive care patients. Subsequently, we assessed the associations between social factors and having a do-not-resuscitate/intubate code. Results: Important differences were found between associations based on manually labeled data compared to text-mined social factors in three out of five cases. Adopting external-institution text mining models using manually labeled within-institution data resulted in models with higher F1-scores, but not in meaningfully different associations. Discussion: While text mining facilitated scaling analyses to larger samples leading to discovering a larger number of associations, the estimates may be unreliable. Confirmation is needed with better text mining models, ideally on a larger manually labeled dataset. Conclusion: The currently used text mining models were not sufficiently accurate to be used reliably in an association study. Model adaptation using within-institution data did not improve the estimates. Further research is needed to set conditions for reliable use of text mining in medical research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.06570,January 2023,422,The Bose-Einstein Condensate and Cold Atom Laboratory,"Kai Frye, Sven Abend, Wolfgang Bartosch, Ahmad Bawamia, Dennis Becker, Holger Blume, Claus Braxmaier, Sheng-Wey Chiow, Maxim A. Efremov, Wolfgang Ertmer, Peter Fierlinger, Tobias Franz, Naceur Gaaloul, Jens Grosse, Christoph Grzeschik, Ortwin Hellmig…",Microgravity eases several constraints limiting experiments with ultracold and condensed atoms on ground. It enables extended times of flight without suspension and eliminates the gravitational sag for trapped...,https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-020-00090-8,4 January 2021,
423,0.000678989392134464,423,An Efficient Approach for Discovering Graph Entity Dependencies (GEDs),"Dehua Liu, Selasi Kwashie, Yidi Zhang, Guangtong Zhou, Michael Bewong, Xiaoying Wu, Xi Guo, Keqing He, Zaiwen Feng","Graph entity dependencies (GEDs) are novel graph constraints, unifying keys and functional dependencies, for property graphs. They have been found useful in many real-world data quality and data management tasks, including fact checking on social media networks and entity resolution. In this paper, we study the discovery problem of GEDs -- finding a minimal cover of valid GEDs in a given graph data. We formalise the problem, and propose an effective and efficient approach to overcome major bottlenecks in GED discovery. In particular, we leverage existing graph partitioning algorithms to enable fast GED-scope discovery, and employ effective pruning strategies over the prohibitively large space of candidate dependencies. Furthermore, we define an interestingness measure for GEDs based on the minimum description length principle, to score and rank the mined cover set of GEDs. Finally, we demonstrate the scalability and effectiveness of our GED discovery approach through extensive experiments on real-world benchmark graph data sets; and present the usefulness of the discovered rules in different downstream data quality management applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.06264,January 2023,423,Occupancy distribution estimation for smart light delivery with perturbation-modulated light sensing,"Quan Wang, Xinchi Zhang and Kim L Boyer","The advent of modern light-emitting diode (LED) techniques enables us to develop novel lighting systems with numerous previously unavailable features. Specifically, by using the fixtures for both illumination ...",https://www.springeropen.com//journalofsolidstatelighting.springeropen.com/articles/10.1186/s40539-014-0017-2,14 October 2014,
424,0.000678989392134464,424,A data science and machine learning approach to continuous analysis of Shakespeare's plays,"Charles Swisher, Lior Shamir","The availability of quantitative methods that can analyze text has provided new ways of examining literature in a manner that was not available in the pre-information era. Here we apply comprehensive machine learning analysis to the work of William Shakespeare. The analysis shows clear change in style of writing over time, with the most significant changes in the sentence length, frequency of adjectives and adverbs, and the sentiments expressed in the text. Applying machine learning to make a stylometric prediction of the year of the play shows a Pearson correlation of 0.71 between the actual and predicted year, indicating that Shakespeare's writing style as reflected by the quantitative measurements changed over time. Additionally, it shows that the stylometrics of some of the plays is more similar to plays written either before or after the year they were written. For instance, Romeo and Juliet is dated 1596, but is more similar in stylometrics to plays written by Shakespeare after 1600. The source code for the analysis is available for free download. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.06024,January 2023,424,The impact of annotation on concrete and abstract visual representations in science education: testing the expertise reversal effect,"Robert Zheng, Holly Cordner and Jeffrey Spears",This study investigates the effects of annotation on abstract and concrete visual representations in science education. Two studies were conducted: Study 1 investigated the interaction between annotation and v...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00194-y,8 May 2022,
425,0.000678989392134464,425,Interpretable and Scalable Graphical Models for Complex Spatio-temporal Processes,Yu Wang,"This thesis focuses on data that has complex spatio-temporal structure and on probabilistic graphical models that learn the structure in an interpretable and scalable manner. We target two research areas of interest: Gaussian graphical models for tensor-variate data and summarization of complex time-varying texts using topic models. This work advances the state-of-the-art in several directions. First, it introduces a new class of tensor-variate Gaussian graphical models via the Sylvester tensor equation. Second, it develops an optimization technique based on a fast-converging proximal alternating linearized minimization method, which scales tensor-variate Gaussian graphical model estimations to modern big-data settings. Third, it connects Kronecker-structured (inverse) covariance models with spatio-temporal partial differential equations (PDEs) and introduces a new framework for ensemble Kalman filtering that is capable of tracking chaotic physical systems. Fourth, it proposes a modular and interpretable framework for unsupervised and weakly-supervised probabilistic topic modeling of time-varying data that combines generative statistical models with computational geometric methods. Throughout, practical applications of the methodology are considered using real datasets. This includes brain-connectivity analysis using EEG data, space weather forecasting using solar imaging data, longitudinal analysis of public opinions using Twitter data, and mining of mental health related issues using TalkLife data. We show in each case that the graphical modeling framework introduced here leads to improved interpretability, accuracy, and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.06021,January 2023,425,Editorial,"Kung Yao, Deborah Estrin and Yu Hen Hu",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703002944,30 March 2003,
426,0.000678989392134464,426,Discovery of 2D materials using Transformer Network based Generative Design,"Rongzhi Dong, Yuqi Song, Edirisuriya M. D. Siriwardane, Jianjun Hu","Two-dimensional (2D) materials have wide applications in superconductors, quantum, and topological materials. However, their rational design is not well established, and currently less than 6,000 experimentally synthesized 2D materials have been reported. Recently, deep learning, data-mining, and density functional theory (DFT)-based high-throughput calculations are widely performed to discover potential new materials for diverse applications. Here we propose a generative material design pipeline, namely material transformer generator(MTG), for large-scale discovery of hypothetical 2D materials. We train two 2D materials composition generators using self-learning neural language models based on Transformers with and without transfer learning. The models are then used to generate a large number of candidate 2D compositions, which are fed to known 2D materials templates for crystal structure prediction. Next, we performed DFT computations to study their thermodynamic stability based on energy-above-hull and formation energy. We report four new DFT-verified stable 2D materials with zero e-above-hull energies, including NiCl$_4$, IrSBr, CuBr$_3$, and CoBrCl. Our work thus demonstrates the potential of our MTG generative materials design pipeline in the discovery of novel 2D materials and other functional materials. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05824,January 2023,426,Soundscape Design Through Evolutionary Engines,"José Fornari, Adolfo Maia and Jônatas Manzolli",Two implementations of an Evolutionary Sound Synthesis method using the Interaural Time Difference (ITD) and psychoacoustic descriptors are presented here as a way to develop criteria for fitness evaluation. W...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192564,September 2008,
427,0.000678989392134464,427,Composite model of seismic monitoring data analysis during mining operations on the example of the Kukisvumchorrskoye deposit of JSC Apatit,Ilia Revin,"Geomechanical monitoring of a rock massif is an actively developing branch of geomechanics. It is almost impossible to single out a methodology and approaches for data collection and analysis in developing seismic monitoring systems. In the process of mining in rock massif, changes in the state of structural inhomogeneities are most clearly manifested. Existing natural structural inhomogeneities are revealed, there are movements in discontinuous disturbances, and new technogenic disturbances are formed, which are accompanied by a change in the natural stress state of various blocks of the massif. An important task is to develop a mining forecasting model that can take into account the structural heterogeneity of the rock massif and select the necessary forecast horizon depending on monitoring data The developed method of evaluating the results of monitoring geomechanical processes in the rock massif allowed us to forecast of zones of possible rock bursts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05701,January 2023,427,Synobins: An Intermediate Level towards Annotation and Semantic Retrieval,Daniela Stan Raicu and Ishwar K. Sethi,"To reason about the meaning of an image, useful information should be provided with that image; however, images often contain little to no textual information about the objects they are depicting, which is the...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/63124,1 December 2006,
428,0.000678989392134464,428,Structural phase transitions in perovskite BaCeO3 revisited by data mining-cum-first-principles theoretical approaches,"Farha Naaz, Manendra S. Chauhan, Kedar Yadav, Surender Singh, Ashok Kumar, Dasari L. V. K. Prasad","Several experiments conducted over decades have revealed that the perovskite-structured BaCeO3 goes through a series of temperature-induced structural phase transitions. However, it has been frequently observed that the number of phases and the sequence in which they appear as a function of temperature differ between experiments. Insofar as neutron diffraction and Raman spectroscopy experiments are concern, four structures are well characterized with three transitions: Pnma to Imma [563 K] to R-3c [673 K] to Pm-3m [1173 K]. In contrast, thermoanalytical methods showed multiple singularities corresponding to at-least three more structural transitions at around 830 K, 900 K, and 1030 K. In account of these conflicting experimental findings, we computed free energy phase diagram for BaCeO3 employing crystal structure data mining in conjunction with first principles electronic structure and phonon lattice dynamics. A total of 34 polymorphs have been predicted, the most stable of which follows the Glazer classification of the perovskite tilt system. It has been predicted that the Cmcm and P4/mbm phases surpass Pnma at 666 K and 1210 K, respectively. At any temperature, two alternate tetragonal phases (P42/nmc and I4/mcm) are also found to be 20 to 30 meV less favored than the Pnma. While the calculated stability order of the predicted polymorphs is in acceptable agreement with the results of neutron diffraction, the transitions observed in thermoanalytical studies could be ascribed to the development of four novel phases (Cmcm, P4/mbm, P42/nmc, and I4/mcm) at intermediate temperatures. However, we analyze that the R-3c phase predominantly stabilized over a broad temperature field, masking all subsequent phases up until the cubic Pm-3m. Consequently, the novel phases predicted to occur in thermoanalytical studies are only fleetingly metastable. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05691,January 2023,428,A Learning State-Space Model for Image Retrieval,"Cheng-Chieh Chiang, Yi-Ping Hung and Greg C. Lee","This paper proposes an approach based on a state-space model for learning the user concepts in image retrieval. We first design a scheme of region-based image representation based on concept units, which are i...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/83526,1 December 2007,
429,0.000678989392134464,429,UserSimCRS: A User Simulation Toolkit for Evaluating Conversational Recommender Systems,"Jafar Afzali, Aleksander Mark Drzewiecki, Krisztian Balog, Shuo Zhang","We present an extensible user simulation toolkit to facilitate automatic evaluation of conversational recommender systems. It builds on an established agenda-based approach and extends it with several novel elements, including user satisfaction prediction, persona and context modeling, and conditional natural language generation. We showcase the toolkit with a pre-existing movie recommender system and demonstrate its ability to simulate dialogues that mimic real conversations, while requiring only a handful of manually annotated dialogues as training data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05544,January 2023,429,Automatic Hierarchical Color Image Classification,"Jing Huang, S. Ravi Kumar and Ramin Zabih","Organizing images into semantic categories can be extremely useful for content-based image retrieval and image annotation. Grouping images into semantic classes is a difficult problem, however. Image classific...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703211161,25 February 2003,
430,0.000678989392134464,430,Understanding Concept Identification as Consistent Data Clustering Across Multiple Feature Spaces,"Felix Lanfermannn, Sebastian Schmitt, Patricia Wollstadt","Identifying meaningful concepts in large data sets can provide valuable insights into engineering design problems. Concept identification aims at identifying non-overlapping groups of design instances that are similar in a joint space of all features, but which are also similar when considering only subsets of features. These subsets usually comprise features that characterize a design with respect to one specific context, for example, constructive design parameters, performance values, or operation modes. It is desirable to evaluate the quality of design concepts by considering several of these feature subsets in isolation. In particular, meaningful concepts should not only identify dense, well separated groups of data instances, but also provide non-overlapping groups of data that persist when considering pre-defined feature subsets separately. In this work, we propose to view concept identification as a special form of clustering algorithm with a broad range of potential applications beyond engineering design. To illustrate the differences between concept identification and classical clustering algorithms, we apply a recently proposed concept identification algorithm to two synthetic data sets and show the differences in identified solutions. In addition, we introduce the mutual information measure as a metric to evaluate whether solutions return consistent clusters across relevant subsets. To support the novel understanding of concept identification, we consider a simulated data set from a decision-making problem in the energy management domain and show that the identified clusters are more interpretable with respect to relevant feature subsets than clusters found by common clustering algorithms and are thus more suitable to support a decision maker. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05525,January 2023,430,Distance Measures for Image Segmentation Evaluation,"Xiaoyi Jiang, Cyril Marti, Christophe Irniger and Horst Bunke","The task considered in this paper is performance evaluation of region segmentation algorithms in the ground-truth-based paradigm. Given a machine segmentation and a ground-truth segmentation, performance measu...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/35909,1 December 2006,
431,0.000678989392134464,431,Scalable Batch Acquisition for Deep Bayesian Active Learning,"Aleksandr Rubashevskii, Daria Kotova, Maxim Panov","In deep active learning, it is especially important to choose multiple examples to markup at each step to work efficiently, especially on large datasets. At the same time, existing solutions to this problem in the Bayesian setup, such as BatchBALD, have significant limitations in selecting a large number of examples, associated with the exponential complexity of computing mutual information for joint random variables. We, therefore, present the Large BatchBALD algorithm, which gives a well-grounded approximation to the BatchBALD method that aims to achieve comparable quality while being more computationally efficient. We provide a complexity analysis of the algorithm, showing a reduction in computation time, especially for large batches. Furthermore, we present an extensive set of experimental results on image and text data, both on toy datasets and larger ones such as CIFAR-100. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05490,January 2023,431,NLOOK: a computational attention model for robot vision,Milton Roberto Heinen and Paulo Martins Engel,"The computational models of visual attention, originally proposed as cognitive models of human attention, nowadays are being used as front-ends to some robotic vision systems, like automatic object recognition...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194502,September 2009,
432,0.000678989392134464,432,A Nearly-Linear Time Algorithm for Minimizing Risk of Conflict in Social Networks,"Liwang Zhu, Zhongzhi Zhang","Concomitant with the tremendous prevalence of online social media platforms, the interactions among individuals are unprecedentedly enhanced. People are free to interact with acquaintances, express and exchange their own opinions through commenting, liking, retweeting on online social media, leading to resistance, controversy and other important phenomena over controversial social issues, which have been the subject of many recent works. In this paper, we study the problem of minimizing risk of conflict in social networks by modifying the initial opinions of a small number of nodes. We show that the objective function of the combinatorial optimization problem is monotone and supermodular. We then propose a naïve greedy algorithm with a $(1-1/e)$ approximation ratio that solves the problem in cubic time. To overcome the computation challenge for large networks, we further integrate several effective approximation strategies to provide a nearly linear time algorithm with a $(1-1/e-ε)$ approximation ratio for any error parameter $ε>0$. Extensive experiments on various real-world datasets demonstrate both the efficiency and effectiveness of our algorithms. In particular, the fast one scales to large networks with more than two million nodes, and achieves up to $20\times$ speed-up over the state-of-the-art algorithm. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05466,January 2023,432,Performance evaluation of a discovery and scheduling protocol for multihop ad hoc mobile grids,"Antônio Tadeu Azevedo Gomes, Artur Ziviani, Luciana dos Santos Lima and Markus Endler","Despite the many research efforts addressing the integration of mobile nodes into grids, only a few of them have considered the establishment of mobile grids over wireless ad hoc networks (hereafter,mobile ad hoc...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194510,December 2009,
433,0.000678989392134464,433,Evolve Path Tracer: Early Detection of Malicious Addresses in Cryptocurrency,"Ling Cheng, Feida Zhu, Yong Wang, Ruicheng Liang, Huiwen Liu","With the ever-increasing boom of Cryptocurrency, detecting fraudulent behaviors and associated malicious addresses draws significant research effort. However, most existing studies still rely on the full history features or full-fledged address transaction networks, thus cannot meet the requirements of early malicious address detection, which is urgent but seldom discussed by existing studies. To detect fraud behaviors of malicious addresses in the early stage, we present Evolve Path Tracer, which consists of Evolve Path Encoder LSTM, Evolve Path Graph GCN, and Hierarchical Survival Predictor. Specifically, in addition to the general address features, we propose asset transfer paths and corresponding path graphs to characterize early transaction patterns. Further, since the transaction patterns are changing rapidly during the early stage, we propose Evolve Path Encoder LSTM and Evolve Path Graph GCN to encode asset transfer path and path graph under an evolving structure setting. Hierarchical Survival Predictor then predicts addresses' labels with nice scalability and faster prediction speed. We investigate the effectiveness and versatility of Evolve Path Tracer on three real-world illicit bitcoin datasets. Our experimental results demonstrate that Evolve Path Tracer outperforms the state-of-the-art methods. Extensive scalability experiments demonstrate the model's adaptivity under a dynamic prediction setting. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05412,January 2023,433,Spoofing keystroke dynamics authentication through synthetic typing pattern extracted from screen-recorded video,Chrisando Ryan Pardomuan Siahaan and Andry Chowanda,"As the inter-connectivity in cyberspace continues to increase exponentially, privacy and security have become two of the most concerning issues that need to be tackled in today’s state of technology. Therefore...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00662-8,21 November 2022,
434,0.000678989392134464,434,Knowledge Enhancement for Contrastive Multi-Behavior Recommendation,"Hongrui Xuan, Yi Liu, Bohan Li, Hongzhi Yin","A well-designed recommender system can accurately capture the attributes of users and items, reflecting the unique preferences of individuals. Traditional recommendation techniques usually focus on modeling the singular type of behaviors between users and items. However, in many practical recommendation scenarios (e.g., social media, e-commerce), there exist multi-typed interactive behaviors in user-item relationships, such as click, tag-as-favorite, and purchase in online shopping platforms. Thus, how to make full use of multi-behavior information for recommendation is of great importance to the existing system, which presents challenges in two aspects that need to be explored: (1) Utilizing users' personalized preferences to capture multi-behavioral dependencies; (2) Dealing with the insufficient recommendation caused by sparse supervision signal for target behavior. In this work, we propose a Knowledge Enhancement Multi-Behavior Contrastive Learning Recommendation (KMCLR) framework, including two Contrastive Learning tasks and three functional modules to tackle the above challenges, respectively. In particular, we design the multi-behavior learning module to extract users' personalized behavior information for user-embedding enhancement, and utilize knowledge graph in the knowledge enhancement module to derive more robust knowledge-aware representations for items. In addition, in the optimization stage, we model the coarse-grained commonalities and the fine-grained differences between multi-behavior of users to further improve the recommendation effect. Extensive experiments and ablation tests on the three real-world datasets indicate our KMCLR outperforms various state-of-the-art recommendation methods and verify the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05403,January 2023,434,“Dear future woman of STEM”: letters of advice from women in STEM,"Gili Freedman, Melanie C. Green, Mia Kussman, Mason Drusano and Melissa M. Moore","Although a large body of research has identified challenges faced by women in STEM fields and strategies to improve the experience for women in STEM, little of this research has examined which strategies under...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00411-0,10 March 2023,
435,0.000678989392134464,435,Toward Theoretical Guidance for Two Common Questions in Practical Cross-Validation based Hyperparameter Selection,"Parikshit Ram, Alexander G. Gray, Horst C. Samulowitz, Gregory Bramble","We show, to our knowledge, the first theoretical treatments of two common questions in cross-validation based hyperparameter selection: (1) After selecting the best hyperparameter using a held-out set, we train the final model using {\em all} of the training data -- since this may or may not improve future generalization error, should one do this? (2) During optimization such as via SGD (stochastic gradient descent), we must set the optimization tolerance $ρ$ -- since it trades off predictive accuracy with computation cost, how should one set it? Toward these problems, we introduce the {\em hold-in risk} (the error due to not using the whole training data), and the {\em model class mis-specification risk} (the error due to having chosen the wrong model class) in a theoretical view which is simple, general, and suggests heuristics that can be used when faced with a dataset instance. In proof-of-concept studies in synthetic data where theoretical quantities can be controlled, we show that these heuristics can, respectively, (1) always perform at least as well as always performing retraining or never performing retraining, (2) either improve performance or reduce computational overhead by $2\times$ with no loss in predictive performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.05131,January 2023,435,Global and Latin American female participation in evidence-based software engineering: a systematic mapping study,"Katia Romero Felizardo, Amanda Möhring Ramos, Claudia de O. Melo, Érica Ferreira de Souza, Nandamudi L. Vijaykumar and Elisa Yumi Nakagawa","While the digital economy requires a new generation of technology for scientists and practitioners, the software engineering (SE) field faces a gender crisis. SE research is a global enterprise that requires t...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00109-7,19 March 2021,
436,0.000678989392134464,436,LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification,"Wenjie Xi, Arnav Jain, Li Zhang, Jessica Lin","Time series classification is an important data mining task that has received a lot of interest in the past two decades. Due to the label scarcity in practice, semi-supervised time series classification with only a few labeled samples has become popular. Recently, Similarity-aware Time Series Classification (SimTSC) is proposed to address this problem by using a graph neural network classification model on the graph generated from pairwise Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy and outperforms state-of-the-art deep learning models in several few-label settings. However, since SimTSC relies on pairwise DTW distances, the quadratic complexity of DTW limits its usability to only reasonably sized datasets. To address this challenge, we propose a new efficient semi-supervised time series classification technique, LB-SimTSC, with a new graph construction module. Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to approximate the dissimilarity between instances in linear time, while retaining the relative proximity relationships one would have obtained via computing DTW. We construct the pairwise distance matrix using LB_Keogh and build a graph for the graph neural network. We apply this approach to the ten largest datasets from the well-known UCR time series classification archive. The results demonstrate that this approach can be up to 104x faster than SimTSC when constructing the graph on large datasets without significantly decreasing classification accuracy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.04838,January 2023,436,A Novel Prostate Cancer Classification Technique Using Intermediate Memory Tabu Search,"Muhammad Atif Tahir, Ahmed Bouridane, Fatih Kurugollu and Abbes Amira","The introduction of multispectral imaging in pathology problems such as the identification of prostatic cancer is recent. Unlike conventional RGB color space, it allows the acquisition of a large number of spe...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2241,25 August 2005,
437,0.000678989392134464,437,Blockchain For Mobile Health Applications: Acceleration With GPU Computing,"Georgios Drakopoulos, Michail Marountas, Xenophon Liapakis, Giannis Tzimas, Phivos Mylonas, Spyros Sioutas","Blockchain is a linearly linked, distributed, and very robust data structure. Originally proposed as part of the Bitcoin distributed stack, it found a number of applications in a number of fields, most notably in smart contracts, social media, secure IoT, and cryptocurrency mining. It ensures data integrity by distributing strongly encrypted data in widely redundant segments. Each new insertion requires verification and approval by the majority of the users of the blockchain. Both encryption and verification are computationally intensive tasks which cannot be solved with ordinary off-the-shelf CPUs. This has resulted in a renewed scientific interest in secure distributed communication and coordination protocols. Mobile health applications are growing progressively popular and have the enormous advantage of timely diagnosis of certain conditions. However, privacy concerns have been raised as mobile health application by default have access to highly sensitive personal data. This chapter presents concisely how blockchain can be applied to mobile health applications in order to enhance privacy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.04725,January 2023,437,An improved task assignment scheme for Hadoop running in the clouds,Wei Dai and Mostafa Bassiouni,"Nowadays, data-intensive problems are so prevalent that numerous organizations in various industries have to face them in their business operation. It is often crucial for enterprises to have the capability of...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-2-23,18 December 2013,
438,0.000678989392134464,438,Word-Graph2vec: An efficient word embedding approach on word co-occurrence graph using random walk sampling,"Wenting Li, Jiahong Xue, Xi Zhang, Huacan Chen, Zeyu Chen, Yuanzhe Cai","Word embedding has become ubiquitous and is widely used in various text mining and natural language processing (NLP) tasks, such as information retrieval, semantic analysis, and machine translation, among many others. Unfortunately, it is prohibitively expensive to train the word embedding in a relatively large corpus. We propose a graph-based word embedding algorithm, called Word-Graph2vec, which converts the large corpus into a word co-occurrence graph, then takes the word sequence samples from this graph by randomly traveling and trains the word embedding on this sampling corpus in the end. We posit that because of the stable vocabulary, relative idioms, and fixed expressions in English, the size and density of the word co-occurrence graph change slightly with the increase in the training corpus. So that Word-Graph2vec has stable runtime on the large scale data set, and its performance advantage becomes more and more obvious with the growth of the training corpus. Extensive experiments conducted on real-world datasets show that the proposed algorithm outperforms traditional Skip-Gram by four-five times in terms of efficiency, while the error generated by the random walk sampling is small. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.04312,January 2023,438,Students’ perceptions on distance education: A multinational study,"Patricia Fidalgo, Joan Thormann, Oleksandr Kulyk and José Alberto Lencastre",Many universities offer Distance Education (DE) courses and programs to address the diverse educational needs of students and to stay current with advancing technology. Some Institutions of Higher Education (I...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00194-2,20 May 2020,
439,0.000678989392134464,439,Analogical Relevance Index,"Suryani Lim, Henri Prade, Gilles Richard","Focusing on the most significant features of a dataset is useful both in machine learning (ML) and data mining. In ML, it can lead to a higher accuracy, a faster learning process, and ultimately a simpler and more understandable model. In data mining, identifying significant features is essential not only for gaining a better understanding of the data but also for visualization. In this paper, we demonstrate a new way of identifying significant features inspired by analogical proportions. Such a proportion is of the form of ""a is to b as c is to d"", comparing two pairs of items (a, b) and (c, d) in terms of similarities and dissimilarities. In a classification context, if the similarities/dissimilarities between a and b correlate with the fact that a and b have different labels, this knowledge can be transferred to c and d, inferring that c and d also have different labels. From a feature selection perspective, observing a huge number of such pairs (a, b) where a and b have different labels provides a hint about the importance of the features where a and b differ. Following this idea, we introduce the Analogical Relevance Index (ARI), a new statistical test of the significance of a given feature with respect to the label. ARI is a filter-based method. Filter-based methods are ML-agnostic but generally unable to handle feature redundancy. However, ARI can detect feature redundancy. Our experiments show that ARI is effective and outperforms well-known methods on a variety of artificial and some real datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.04134,January 2023,439,The impact of spatial data redundancy on SOLAP query performance,"Thiago Luís Lopes Siqueira, Cristina Dutra de Aguiar Ciferri, Valéria Cesário Times, Anjolina Grisi de Oliveira and Ricardo Rodrigues Ciferri","Geographic Data Warehouses (GDW) are one of the main technologies used in decision-making processes and spatial analysis, and the literature proposes several conceptual and logical data models for GDW. However...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194499,June 2009,
440,0.000678989392134464,440,"Causal Inference for Recommendation: Foundations, Methods and Applications","Shuyuan Xu, Jianchao Ji, Yunqi Li, Yingqiang Ge, Juntao Tan, Yongfeng Zhang","Recommender systems are important and powerful tools for various personalized services. Traditionally, these systems use data mining and machine learning techniques to make recommendations based on correlations found in the data. However, relying solely on correlation without considering the underlying causal mechanism may lead to various practical issues such as fairness, explainability, robustness, bias, echo chamber and controllability problems. Therefore, researchers in related area have begun incorporating causality into recommendation systems to address these issues. In this survey, we review the existing literature on causal inference in recommender systems. We discuss the fundamental concepts of both recommender systems and causal inference as well as their relationship, and review the existing work on causal methods for different problems in recommender systems. Finally, we discuss open problems and future directions in the field of causal inference for recommendations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.04016,January 2023,440,Model-based evolution of collaborative agent-based systems,"Shawn A. Bohner, Denis Graĉanin, Michael G. Hinchey and Mohamed Eltoweissy","As demands for behaviorally sophisticated software grow, agent-based systems are increasingly being employed. Software agents are frequently applied to large, complex systems that involve interdisciplinary dev...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194254,December 2007,
441,0.000678989392134464,441,Mining Healthcare Procurement Data Using Text Mining and Natural Language Processing -- Reflection From An Industrial Project,"Ziqi Zhang, Tomas Jasaitis, Richard Freeman, Rowida Alfrjani, Adam Funk","While text mining and NLP research has been established for decades, there remain gaps in the literature that reports the use of these techniques in building real-world applications. For example, they typically look at single and sometimes simplified tasks, and do not discuss in-depth data heterogeneity and inconsistency that is common in real-world problems or their implication on the development of their methods. Also, few prior work has focused on the healthcare domain. In this work, we describe an industry project that developed text mining and NLP solutions to mine millions of heterogeneous, multilingual procurement documents in the healthcare sector. We extract structured procurement contract data that is used to power a platform for dynamically assessing supplier risks. Our work makes unique contributions in a number of ways. First, we deal with highly heterogeneous, multilingual data and we document our approach to tackle these challenges. This is mainly based on a method that effectively uses domain knowledge and generalises to multiple text mining and NLP tasks and languages. Second, applying this method to mine millions of procurement documents, we develop the first structured procurement contract database that will help facilitate the tendering process. Second, Finally, we discuss lessons learned for practical text mining/NLP development, and make recommendations for future research and practice. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.03458,January 2023,441,Model-based evolution of collaborative agent-based systems,"Shawn A. Bohner, Denis Graĉanin, Michael G. Hinchey and Mohamed Eltoweissy","As demands for behaviorally sophisticated software grow, agent-based systems are increasingly being employed. Software agents are frequently applied to large, complex systems that involve interdisciplinary dev...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194254,December 2007,
442,0.000678989392134464,442,Machine Learning Algorithms for Depression Detection and Their Comparison,"Danish Muzafar, Furqan Yaqub Khan, Mubashir Qayoom","Textual emotional intelligence is playing a ubiquitously important role in leveraging human emotions on social media platforms. Social media platforms are privileged with emotional content and are leveraged for various purposes like opinion mining, emotion mining, and sentiment analysis. This data analysis is also levered for the prevention of online bullying, suicide prevention, and depression detection among social media users. In this article, we have designed an automatic depression detection of online social media users by analyzing their social media behavior. The designed depression detection classification can be effectively used to mine user's social media interactions and one can determine whether a social media user is suffering from depression or not. The underlying classifier is made using state-of-art technology in emotional artificial intelligence which includes LSTM (Long Short Term Memory) and other machine learning classifiers. The highest accuracy of the classifier is around 70% of LSTM and for SVM the highest accuracy is 81.79%. We trained the classifier on the datasets that are widely used in literature for emotion mining tasks. A confusion matrix of results is also given. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.03222,January 2023,442,A systematic literature review of personalized learning terms,Atikah Shemshack and Jonathan Michael Spector,"Learning is a natural human activity that is shaped by personal experiences, cognitive awareness, personal bias, opinions, cultural background, and environment. Learning has been defined as a stable and persis...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00140-9,23 October 2020,
443,0.000678989392134464,443,Learning the Relation between Similarity Loss and Clustering Loss in Self-Supervised Learning,"Jidong Ge, Yuxiang Liu, Jie Gui, Lanting Fang, Ming Lin, James Tin-Yau Kwok, LiGuo Huang, Bin Luo","Self-supervised learning enables networks to learn discriminative features from massive data itself. Most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. By utilizing the consistency of two augmentations, the burden of manual annotations can be freed. Contrastive learning exploits instance-level information to learn robust features. However, the learned information is probably confined to different views of the same instance. In this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. In contrast to instance-level information, the similarity between two distinct images may provide more useful information. Besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. These two losses are essential for most deep learning methods. However, the relation between these two losses is not clear. Similarity loss helps obtain instance-level representation, while feature-level cross-entropy loss helps mine the similarity between two distinct images. We provide theoretical analyses and experiments to show that a suitable combination of these two losses can get state-of-the-art results. Code is available at https://github.com/guijiejie/ICCL. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.03041,January 2023,443,Statistical learning approaches for discriminant features selection,"Gilson A. Giraldi, Paulo S. Rodrigues, Edson C. Kitani, João R. Sato and Carlos E. Thomaz",Supervised statistical learning covers important models like Support Vector Machines (SVM) and Linear Discriminant Analysis (LDA). In this paper we describe the idea of using the discriminant weights given by ...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192556,June 2008,
444,0.000678989392134464,444,Network Sparsification via Degree- and Subgraph-based Edge Sampling,"Zhen Su, Jürgen Kurths, Henning Meyerhenke","Network (or graph) sparsification compresses a graph by removing inessential edges. By reducing the data volume, it accelerates or even facilitates many downstream analyses. Still, the accuracy of many sparsification methods, with filtering-based edge sampling being the most typical one, heavily relies on an appropriate definition of edge importance. Instead, we propose a different perspective with a generalized local-property-based sampling method, which preserves (scaled) local \emph{node} characteristics. Apart from degrees, these local node characteristics we use are the expected (scaled) number of wedges and triangles a node belongs to. Through such a preservation, main complex structural properties are preserved implicitly. We adapt a game-theoretic framework from uncertain graph sampling by including a threshold for faster convergence (at least $4$ times faster empirically) to approximate solutions. Extensive experimental studies on functional climate networks show the effectiveness of this method in preserving macroscopic to mesoscopic and microscopic network structural properties. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.03032,January 2023,444,Bibliometric analysis of fourth industrial revolution applied to heritage studies based on web of science and scopus databases from 2016 to 2021,"Anibal Alviz-Meza, Manuel H. Vásquez-Coronado, Jorge G. Delgado-Caramutti and Daniel J. Blanco-Victorio","Using past material and spiritual remains, cultural heritage examines communities’ identity formation across time. Cultural heritage requires public and private institutions to care about its restoration, main...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-022-00821-3,22 November 2022,
445,0.000678989392134464,445,Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering,"Huibing Wang, Mingze Yao, Guangqi Jiang, Zetian Mi, Xianping Fu","Unsupervised hashing methods have attracted widespread attention with the explosive growth of large-scale data, which can greatly reduce storage and computation by learning compact binary codes. Existing unsupervised hashing methods attempt to exploit the valuable information from samples, which fails to take the local geometric structure of unlabeled samples into consideration. Moreover, hashing based on auto-encoders aims to minimize the reconstruction loss between the input data and binary codes, which ignores the potential consistency and complementarity of multiple sources data. To address the above issues, we propose a hashing algorithm based on auto-encoders for multi-view binary clustering, which dynamically learns affinity graphs with low-rank constraints and adopts collaboratively learning between auto-encoders and affinity graphs to learn a unified binary code, called Graph-Collaborated Auto-Encoder Hashing for Multi-view Binary Clustering (GCAE). Specifically, we propose a multi-view affinity graphs learning model with low-rank constraint, which can mine the underlying geometric information from multi-view data. Then, we design an encoder-decoder paradigm to collaborate the multiple affinity graphs, which can learn a unified binary code effectively. Notably, we impose the decorrelation and code balance constraints on binary codes to reduce the quantization errors. Finally, we utilize an alternating iterative optimization scheme to obtain the multi-view clustering results. Extensive experimental results on $5$ public datasets are provided to reveal the effectiveness of the algorithm and its superior performance over other state-of-the-art alternatives. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.02484,January 2023,445,"Systematic review and meta-analysis of augmented reality in medicine, retail, and games","Pranav Parekh, Shireen Patel, Nivedita Patel and Manan Shah",This paper presents a detailed review of the applications of augmented reality (AR) in three important fields where AR use is currently increasing. The objective of this study is to highlight how AR improves a...,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00057-7,16 September 2020,
446,0.000678989392134464,446,Deep Biological Pathway Informed Pathology-Genomic Multimodal Survival Prediction,"Lin Qiu, Aminollah Khormali, Kai Liu","The integration of multi-modal data, such as pathological images and genomic data, is essential for understanding cancer heterogeneity and complexity for personalized treatments, as well as for enhancing survival predictions. Despite the progress made in integrating pathology and genomic data, most existing methods cannot mine the complex inter-modality relations thoroughly. Additionally, identifying explainable features from these models that govern preclinical discovery and clinical prediction is crucial for cancer diagnosis, prognosis, and therapeutic response studies. We propose PONET- a novel biological pathway-informed pathology-genomic deep model that integrates pathological images and genomic data not only to improve survival prediction but also to identify genes and pathways that cause different survival rates in patients. Empirical results on six of The Cancer Genome Atlas (TCGA) datasets show that our proposed method achieves superior predictive performance and reveals meaningful biological interpretations. The proposed method establishes insight into how to train biologically informed deep networks on multimodal biomedical data which will have general applicability for understanding diseases and predicting response and resistance to treatment. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.02383,January 2023,446,Survey of feature selection and extraction techniques for stock market prediction,"Htet Htet Htun, Michael Biehl and Nicolai Petkov","In stock market forecasting, the identification of critical features that affect the performance of machine learning (ML) models is crucial to achieve accurate stock price predictions. Several review papers in...",https://www.springeropen.com//jfin-swufe.springeropen.com/articles/10.1186/s40854-022-00441-7,12 January 2023,
447,0.000678989392134464,447,TrojanPuzzle: Covertly Poisoning Code-Suggestion Models,"Hojjat Aghakhani, Wei Dai, Andre Manoel, Xavier Fernandes, Anant Kharkar, Christopher Kruegel, Giovanni Vigna, David Evans, Ben Zorn, Robert Sim","With tools like GitHub Copilot, automatic code suggestion is no longer a dream in software engineering. These tools, based on large language models, are typically trained on massive corpora of code mined from unvetted public sources. As a result, these models are susceptible to data poisoning attacks where an adversary manipulates the model's training or fine-tuning phases by injecting malicious data. Poisoning attacks could be designed to influence the model's suggestions at run time for chosen contexts, such as inducing the model into suggesting insecure code payloads. To achieve this, prior poisoning attacks explicitly inject the insecure code payload into the training data, making the poisoning data detectable by static analysis tools that can remove such malicious data from the training set. In this work, we demonstrate two novel data poisoning attacks, COVERT and TROJANPUZZLE, that can bypass static analysis by planting malicious poisoning data in out-of-context regions such as docstrings. Our most novel attack, TROJANPUZZLE, goes one step further in generating less suspicious poisoning data by never including certain (suspicious) parts of the payload in the poisoned data, while still inducing a model that suggests the entire payload when completing code (i.e., outside docstrings). This makes TROJANPUZZLE robust against signature-based dataset-cleansing methods that identify and filter out suspicious sequences from the training data. Our evaluation against two model sizes demonstrates that both COVERT and TROJANPUZZLE have significant implications for how practitioners should select code used to train or tune code-suggestion models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.02344,January 2023,447,Predicting pathways to optional summer science experiences by socioeconomic status and the impact on science attitudes and skills,Allison S. Liu and Christian D. Schunn,"Large achievement and motivation gaps exist in science between students from higher and lower socioeconomic status (SES) backgrounds. Middle and high school are an important time to address these disparities, ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00247-y,25 September 2020,
448,0.000678989392134464,448,Dynamical Data Mining Captures Disc-Halo Couplings that Structure Galaxies,"Alexander Johnson, Michael S. Petersen, Kathryn V. Johnston, Martin D. Weinberg","Studying coupling between different galactic components is a challenging problem in galactic dynamics. Using basis function expansions (BFEs) and multichannel singular spectrum analysis (mSSA) as a means of dynamical data mining, we discover evidence for two multi-component disc-halo dipole modes in a Milky-Way-like simulated galaxy. One of the modes grows throughout the simulation, while the other decays throughout the simulation. The multi-component disc-halo modes are driven primarily by the halo, and have implications for the structural evolution of galaxies, including observations of lopsidedness and other non-axisymmetric structure. In our simulation, the modes create surface density features up to 10 per cent relative to the equilibrium model stellar disc. While the simulated galaxy was constructed to be in equilibrium, BFE+mSSA also uncovered evidence of persistent periodic signals incited by aphysical initial conditions disequilibrium, including rings and weak two-armed spirals, both at the 1 per cent level. The method is sensitive to distinct evolutionary features at and even below the 1 per cent level of surface density variation. The use of mSSA produced clean signals for both modes and disequilibrium, efficiently removing variance owing to estimator noise from the input BFE time series. The discovery of multi-component halo-disc modes is strong motivation for application of BFE+mSSA to the rich zoo of dynamics of multi-component interacting galaxies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.02256,January 2023,448,"A longitudinal analysis of developing marine science identity in a place-based, undergraduate research experience",Christine M. Ambrosino and Malia Ana J. Rivera,"Students from historically excluded groups face many pedagogical, societal, and institutional barriers that lead to disproportionately lower levels of entering and higher levels of attrition from Science, Tech...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00386-4,23 November 2022,
449,0.000678989392134464,449,Comparing Ordering Strategies For Process Discovery Using Synthesis Rules,"Tsung-Hao Huang, Wil M. P. van der Aalst","Process discovery aims to learn process models from observed behaviors, i.e., event logs, in the information systems.The discovered models serve as the starting point for process mining techniques that are used to address performance and compliance problems. Compared to the state-of-the-art Inductive Miner, the algorithm applying synthesis rules from the free-choice net theory discovers process models with more flexible (non-block) structures while ensuring the same desirable soundness and free-choiceness properties. Moreover, recent development in this line of work shows that the discovered models have compatible quality. Following the synthesis rules, the algorithm incrementally modifies an existing process model by adding the activities in the event log one at a time. As the applications of rules are highly dependent on the existing model structure, the model quality and computation time are significantly influenced by the order of adding activities. In this paper, we investigate the effect of different ordering strategies on the discovered models (w.r.t. fitness and precision) and the computation time using real-life event data. The results show that the proposed ordering strategy can improve the quality of the resulting process models while requiring less time compared to the ordering strategy solely based on the frequency of activities. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.02182,January 2023,449,A conceptual framework for teaching computational thinking in personalized OERs,"Jewoong Moon, Jaewoo Do, Daeyeoul Lee and Gi Woong Choi","Interests towards teaching programming skills have risen recently in the realm of computing education. Learning how to program not only enables learners to develop computing applications, but it can also enhan...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-019-0108-z,13 February 2020,
450,0.000678989392134464,450,PMP: Privacy-Aware Matrix Profile against Sensitive Pattern Inference for Time Series,"Li Zhang, Jiahao Ding, Yifeng Gao, Jessica Lin","Recent rapid development of sensor technology has allowed massive fine-grained time series (TS) data to be collected and set the foundation for the development of data-driven services and applications. During the process, data sharing is often involved to allow the third-party modelers to perform specific time series data mining (TSDM) tasks based on the need of data owner. The high resolution of TS brings new challenges in protecting privacy. While meaningful information in high-resolution TS shifts from concrete point values to local shape-based segments, numerous research have found that long shape-based patterns could contain more sensitive information and may potentially be extracted and misused by a malicious third party. However, the privacy issue for TS patterns is surprisingly seldom explored in privacy-preserving literature. In this work, we consider a new privacy-preserving problem: preventing malicious inference on long shape-based patterns while preserving short segment information for the utility task performance. To mitigate the challenge, we investigate an alternative approach by sharing Matrix Profile (MP), which is a non-linear transformation of original data and a versatile data structure that supports many data mining tasks. We found that while MP can prevent concrete shape leakage, the canonical correlation in MP index can still reveal the location of sensitive long pattern. Based on this observation, we design two attacks named Location Attack and Entropy Attack to extract the pattern location from MP. To further protect MP from these two attacks, we propose a Privacy-Aware Matrix Profile (PMP) via perturbing the local correlation and breaking the canonical correlation in MP index vector. We evaluate our proposed PMP against baseline noise-adding methods through quantitative analysis and real-world case studies to show the effectiveness of the proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.01838,January 2023,450,Multi-Agent Framework in Visual Sensor Networks,"M. A. Patricio, J. Carbó, O. Pérez, J. García and J. M. Molina","The recent interest in the surveillance of public, military, and commercial scenarios is increasing the need to develop and deploy intelligent and/or automated distributed visual surveillance systems. Many app...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/98639,1 December 2006,
451,0.000678989392134464,451,Significant Digits: Using Large-Scale Blockchain Data to Predict Fraudulent Addresses,"Jared Gridley, Oshani Seneviratne","Blockchain systems and cryptocurrencies have exploded in popularity over the past decade, and with this growing user base, the number of cryptocurrency scams has also surged. Given the graphical structure of blockchain networks and the abundance of data generated on these networks, we use graph mining techniques to extract essential information on transactions and apply Benford's Law to extract distributional information on address transactions. We then apply a gradient-boosting tree model to predict fraudulent addresses. Our results show that our method can detect scams with reasonable accuracy and that the features generated based on Benford's Law are the most significant features. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.01809,January 2023,451,Path to European quantum unicorns,"Markku Räsänen, Henrikki Mäkynen, Mikko Möttönen and Jan Goetz","Quantum computing holds the potential to deliver great economic prosperity to the European Union (EU). However, the creation of successful business in the field is challenging owing to the required extensive i...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00095-x,25 February 2021,
452,0.000678989392134464,452,Cluster-guided Contrastive Graph Clustering Network,"Xihong Yang, Yue Liu, Sihang Zhou, Siwei Wang, Wenxuan Tu, Qun Zheng, Xinwang Liu, Liming Fang, En Zhu","Benefiting from the intrinsic supervision information exploitation capability, contrastive learning has achieved promising performance in the field of deep graph clustering recently. However, we observe that two drawbacks of the positive and negative sample construction mechanisms limit the performance of existing algorithms from further improvement. 1) The quality of positive samples heavily depends on the carefully designed data augmentations, while inappropriate data augmentations would easily lead to the semantic drift and indiscriminative positive samples. 2) The constructed negative samples are not reliable for ignoring important clustering information. To solve these problems, we propose a Cluster-guided Contrastive deep Graph Clustering network (CCGC) by mining the intrinsic supervision information in the high-confidence clustering results. Specifically, instead of conducting complex node or edge perturbation, we construct two views of the graph by designing special Siamese encoders whose weights are not shared between the sibling sub-networks. Then, guided by the high-confidence clustering information, we carefully select and construct the positive samples from the same high-confidence cluster in two views. Moreover, to construct semantic meaningful negative sample pairs, we regard the centers of different high-confidence clusters as negative samples, thus improving the discriminative capability and reliability of the constructed sample pairs. Lastly, we design an objective function to pull close the samples from the same cluster while pushing away those from other clusters by maximizing and minimizing the cross-view cosine similarity between positive and negative samples. Extensive experimental results on six datasets demonstrate the effectiveness of CCGC compared with the existing state-of-the-art algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.01098,January 2023,452,Architecture for animation of affective behaviors in pedagogical agents,"Ronaldo Motola, Patrícia Augustin Jaques, Margarete Axt and Rosa Vicari",This article introduces an open-source module responsible for the presentation of verbal (speech) and corporal (animation) behaviors of animated pedagogical agents. This module can be inserted into any learnin...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194509,December 2009,
453,0.000678989392134464,453,Fine-Grained Hard Negative Mining: Generalizing Mitosis Detection with a Fifth of the MIDOG 2022 Dataset,"Maxime W. Lafarge, Viktor H. Koelzer","Making histopathology image classifiers robust to a wide range of real-world variability is a challenging task. Here, we describe a candidate deep learning solution for the Mitosis Domain Generalization Challenge 2022 (MIDOG) to address the problem of generalization for mitosis detection in images of hematoxylin-eosin-stained histology slides under high variability (scanner, tissue type and species variability). Our approach consists in training a rotation-invariant deep learning model using aggressive data augmentation with a training set enriched with hard negative examples and automatically selected negative examples from the unlabeled part of the challenge dataset. To optimize the performance of our models, we investigated a hard negative mining regime search procedure that lead us to train our best model using a subset of image patches representing 19.6% of our training partition of the challenge dataset. Our candidate model ensemble achieved a F1-score of .697 on the final test set after automated evaluation on the challenge platform, achieving the third best overall score in the MIDOG 2022 Challenge. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.01079,January 2023,453,Advances in Subspace-Based Techniques for Signal Processing and Communications,"Kostas Berberidis, Benoit Champagne, George V. Moustakides, H. Vincent Poor and Peter Stoica",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/48612,1 December 2006,
454,0.000678989392134464,454,Continual Causal Effect Estimation: Challenges and Opportunities,"Zhixuan Chu, Sheng Li","A further understanding of cause and effect within observational data is critical across many domains, such as economics, health care, public policy, web mining, online advertising, and marketing campaigns. Although significant advances have been made to overcome the challenges in causal effect estimation with observational data, such as missing counterfactual outcomes and selection bias between treatment and control groups, the existing methods mainly focus on source-specific and stationary observational data. Such learning strategies assume that all observational data are already available during the training phase and from only one source. This practical concern of accessibility is ubiquitous in various academic and industrial applications. That's what it boiled down to: in the era of big data, we face new challenges in causal inference with observational data, i.e., the extensibility for incrementally available observational data, the adaptability for extra domain adaptation problem except for the imbalance between treatment and control groups, and the accessibility for an enormous amount of data. In this position paper, we formally define the problem of continual treatment effect estimation, describe its research challenges, and then present possible solutions to this problem. Moreover, we will discuss future research directions on this topic. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.01026,January 2023,454,On equalization of fundamental education in Tibet: a case study on the trend of conditions of primary and middle schools running,Xuewen Zhou,"Based on public data such as the Educational Statistics Yearbook and the National Statistics Yearbook, this paper analyzes the equalization trend of fundamental education in the Tibet Autonomous Region (herein...",https://www.springeropen.com//ijae.springeropen.com/articles/10.1186/s41257-022-00078-5,16 December 2022,
455,0.000678989392134464,455,Fusing Models for Prognostics and Health Management of Lithium-Ion Batteries Based on Physics-Informed Neural Networks,"Pengfei Wen, Zhi-Sheng Ye, Yong Li, Shaowei Chen, Shuai Zhao","For Prognostics and Health Management (PHM) of Lithium-ion (Li-ion) batteries, many models have been established to characterize their degradation process. The existing empirical or physical models can reveal important information regarding the degradation dynamics. However, there is no general and flexible methods to fuse the information represented by those models. Physics-Informed Neural Network (PINN) is an efficient tool to fuse empirical or physical dynamic models with data-driven models. To take full advantage of various information sources, we propose a model fusion scheme based on PINN. It is implemented by developing a semi-empirical semi-physical Partial Differential Equation (PDE) to model the degradation dynamics of Li-ion-batteries. When there is little prior knowledge about the dynamics, we leverage the data-driven Deep Hidden Physics Model (DeepHPM) to discover the underlying governing dynamic models. The uncovered dynamics information is then fused with that mined by the surrogate neural network in the PINN framework. Moreover, an uncertainty-based adaptive weighting method is employed to balance the multiple learning tasks when training the PINN. The proposed methods are verified on a public dataset of Li-ion Phosphate (LFP)/graphite batteries. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00776,January 2023,455,"A structural equation modeling analysis of relationships among university students’ readiness for e-learning, self-regulation skills, satisfaction, and academic achievement",Nuh Yavuzalp and Eralp Bahcivan,"The aim of this study is to examine the relationships of readiness for e-learning with self-regulation skills, satisfaction, and academic achievement in university students taking campus-based courses via dist...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-021-00162-y,24 May 2021,
456,0.000678989392134464,456,IRT2: Inductive Linking and Ranking in Knowledge Graphs of Varying Scale,"Felix Hamann, Adrian Ulges, Maurice Falk","We address the challenge of building domain-specific knowledge models for industrial use cases, where labelled data and taxonomic information is initially scarce. Our focus is on inductive link prediction models as a basis for practical tools that support knowledge engineers with exploring text collections and discovering and linking new (so-called open-world) entities to the knowledge graph. We argue that - though neural approaches to text mining have yielded impressive results in the past years - current benchmarks do not reflect the typical challenges encountered in the industrial wild properly. Therefore, our first contribution is an open benchmark coined IRT2 (inductive reasoning with text) that (1) covers knowledge graphs of varying sizes (including very small ones), (2) comes with incidental, low-quality text mentions, and (3) includes not only triple completion but also ranking, which is relevant for supporting experts with discovery tasks. We investigate two neural models for inductive link prediction, one based on end-to-end learning and one that learns from the knowledge graph and text data in separate steps. These models compete with a strong bag-of-words baseline. The results show a significant advance in performance for the neural approaches as soon as the available graph data decreases for linking. For ranking, the results are promising, and the neural approaches outperform the sparse retriever by a wide margin. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00716,January 2023,456,Multimodal learning analytics of collaborative patterns during pair programming in higher education,"Weiqi Xu, Yajuan Wu and Fan Ouyang","Pair programming (PP), as a mode of collaborative problem solving (CPS) in computer programming education, asks two students work in a pair to co-construct knowledge and solve problems. Considering the complex...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00377-z,8 February 2023,
457,0.000678989392134464,457,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,"Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, Deva Ramanan, Peter Carr, James Hays","We introduce Argoverse 2 (AV2) - a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions between the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for ""scored actors"" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry - sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00493,January 2023,457,Correction: Towards a better understanding of the characteristics of fractal networks,"Enikö Zakar-Polyák, Marcell Nagy and Roland Molontay",Theoriginal articlewas published inApplied Network Science20238:17,https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-023-00545-8,22 May 2023,
458,0.000678989392134464,458,A principled distributional approach to trajectory similarity measurement,"Yufan Wang, Kai Ming Ting, Yuanyi Shang","Existing measures and representations for trajectories have two longstanding fundamental shortcomings, i.e., they are computationally expensive and they can not guarantee the `uniqueness' property of a distance function: dist(X,Y) = 0 if and only if X=Y, where $X$ and $Y$ are two trajectories. This paper proposes a simple yet powerful way to represent trajectories and measure the similarity between two trajectories using a distributional kernel to address these shortcomings. It is a principled approach based on kernel mean embedding which has a strong theoretical underpinning. It has three distinctive features in comparison with existing approaches. (1) A distributional kernel is used for the very first time for trajectory representation and similarity measurement. (2) It does not rely on point-to-point distances which are used in most existing distances for trajectories. (3) It requires no learning, unlike existing learning and deep learning approaches. We show the generality of this new approach in three applications: (a) trajectory anomaly detection, (b) anomalous sub-trajectory detection, and (c) trajectory pattern mining. We identify that the distributional kernel has (i) a unique data-dependent property and the above uniqueness property which are the key factors that lead to its superior task-specific performance; and (ii) runtime orders of magnitude faster than existing distance measures. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00393,January 2023,458,Structure and properties of soft organic-aqueous interfaces,Michael Grunze and Alexander Pertsin,"The interfaces formed by water and aqueous solutions with organic soft matter play an important part in nature and technological applications. Good examples are provided by water in contact with surfactants, s...",https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.2977734,September 2008,
459,0.000678989392134464,459,DiRaC-I: Identifying Diverse and Rare Training Classes for Zero-Shot Learning,"Sandipan Sarma, Arijit Sur","Inspired by strategies like Active Learning, it is intuitive that intelligently selecting the training classes from a dataset for Zero-Shot Learning (ZSL) can improve the performance of existing ZSL methods. In this work, we propose a framework called Diverse and Rare Class Identifier (DiRaC-I) which, given an attribute-based dataset, can intelligently yield the most suitable ""seen classes"" for training ZSL models. DiRaC-I has two main goals - constructing a diversified set of seed classes, followed by a visual-semantic mining algorithm initialized by these seed classes that acquires the classes capturing both diversity and rarity in the object domain adequately. These classes can then be used as ""seen classes"" to train ZSL models for image classification. We adopt a real-world scenario where novel object classes are available to neither DiRaC-I nor the ZSL models during training and conducted extensive experiments on two benchmark data sets for zero-shot image classification - CUB and SUN. Our results demonstrate DiRaC-I helps ZSL models to achieve significant classification accuracy improvements. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00236,January 2023,459,Cooperative object manipulation in collaborative virtual environments,"Marcio S. Pinho, Doug A. Bowman and Carla M. Dal Sasso Freitas","Cooperative manipulation refers to the simultaneous manipulation of a virtual object by multiple users in an immersive virtual environment (VE). In this work, we present techniques for cooperative manipulation...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192559,June 2008,
460,0.000678989392134464,460,Behave-XAI: Deep Explainable Learning of Behavioral Representational Data,"Rossi Kamal, Zuzana Kubincova","According to the latest trend of artificial intelligence, AI-systems needs to clarify regarding general,specific decisions,services provided by it. Only consumer is satisfied, with explanation , for example, why any classification result is the outcome of any given time. This actually motivates us using explainable or human understandable AI for a behavioral mining scenario, where users engagement on digital platform is determined from context, such as emotion, activity, weather, etc. However, the output of AI-system is not always systematically correct, and often systematically correct, but apparently not-perfect and thereby creating confusions, such as, why the decision is given? What is the reason underneath? In this context, we first formulate the behavioral mining problem in deep convolutional neural network architecture. Eventually, we apply a recursive neural network due to the presence of time-series data from users physiological and environmental sensor-readings. Once the model is developed, explanations are presented with the advent of XAI models in front of users. This critical step involves extensive trial with users preference on explanations over conventional AI, judgement of credibility of explanation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2301.00016,January 2023,460,Fast two-step segmentation of natural color scenes using hierarchical region-growing and a Color-Gradient Network,"Aldo von Wangenheim, Rafael F. Bertoldi, Daniel D. Abdala, Michael M. Richter, Lutz Priese and Frank Schmitt","We present evaluation results with focus on combined image and efficiency performance of the Gradient Network Method to segment color images, especially images showing outdoor scenes. A brief review of the tec...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192570,December 2008,
461,0.000678989392134464,461,Topical Hidden Genome: Discovering Latent Cancer Mutational Topics using a Bayesian Multilevel Context-learning Approach,"Saptarshi Chakraborty, Zoe Guan, Colin B. Begg, Ronglai Shen","Statistical inference on the cancer-site specificities of collective ultra-rare whole genome somatic mutations is an open problem. Traditional statistical methods cannot handle whole-genome mutation data due to their ultra-high-dimensionality and extreme data sparsity -- e.g., >30 million unique variants are observed in the ~1700 whole-genome tumor dataset considered herein, of which >99% variants are encountered only once. To harness information in these rare variants we have recently proposed the ""hidden genome model"", a formal multilevel multi-logistic model that mines information in ultra-rare somatic variants to characterize tumor types. The model condenses signals in rare variants through a hierarchical layer leveraging contexts of individual mutations. The model is currently implemented using consistent, scalable point estimation techniques that can handle 10s of millions of variants detected across thousands of tumors. Our recent publications have evidenced its impressive accuracy and attributability at scale. However, principled statistical inference from the model is infeasible due to the volume, correlation, and non-interpretability of the mutation contexts. In this paper we propose a novel framework that leverages topic models from the field of computational linguistics to induce an *interpretable dimension reduction* of the mutation contexts used in the model. The proposed model is implemented using an efficient MCMC algorithm that permits rigorous full Bayesian inference at a scale that is orders of magnitude beyond the capability of out-of-the-box high-dimensional multi-class regression methods and software. We employ our model on the Pan Cancer Analysis of Whole Genomes (PCAWG) dataset, and our results reveal interesting novel insights. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.14567,December 2022,461,The varied experience of undergraduate students during the transition to mandatory online chem lab during the initial lockdown of the COVID-19 pandemic,"Joseph Watts, Kent J. Crippen, Corey Payne, Lorelie Imperial and Melanie Veige","The radical global shift to online teaching that resulted from the initial lockdown of the COVID-19 pandemic forced many science educators into the predicament of translating courses, including teaching labora...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00055-0,13 April 2022,
462,0.000678989392134464,462,HUSP-SP: Faster Utility Mining on Sequence Data,"Chunkai Zhang, Yuting Yang, Zilin Du, Wensheng Gan, Philip S. Yu","High-utility sequential pattern mining (HUSPM) has emerged as an important topic due to its wide application and considerable popularity. However, due to the combinatorial explosion of the search space when the HUSPM problem encounters a low utility threshold or large-scale data, it may be time-consuming and memory-costly to address the HUSPM problem. Several algorithms have been proposed for addressing this problem, but they still cost a lot in terms of running time and memory usage. In this paper, to further solve this problem efficiently, we design a compact structure called sequence projection (seqPro) and propose an efficient algorithm, namely discovering high-utility sequential patterns with the seqPro structure (HUSP-SP). HUSP-SP utilizes the compact seq-array to store the necessary information in a sequence database. The seqPro structure is designed to efficiently calculate candidate patterns' utilities and upper bound values. Furthermore, a new upper bound on utility, namely tighter reduced sequence utility (TRSU) and two pruning strategies in search space, are utilized to improve the mining performance of HUSP-SP. Experimental results on both synthetic and real-life datasets show that HUSP-SP can significantly outperform the state-of-the-art algorithms in terms of running time, memory usage, search space pruning efficiency, and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.14255,December 2022,462,"A longitudinal analysis of developing marine science identity in a place-based, undergraduate research experience",Christine M. Ambrosino and Malia Ana J. Rivera,"Students from historically excluded groups face many pedagogical, societal, and institutional barriers that lead to disproportionately lower levels of entering and higher levels of attrition from Science, Tech...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00386-4,23 November 2022,
463,0.000678989392134464,463,"High Resolution Modeling and Analysis of Cryptocurrency Mining's Impact on Power Grids: Carbon Footprint, Reliability, and Electricity Price","Ali Menati, Xiangtian Zheng, Kiyeob Lee, Ranyu Shi, Pengwei Du, Chanan Singh, Le Xie","Blockchain technologies are considered one of the most disruptive innovations of the last decade, enabling secure decentralized trust-building. However, in recent years, with the rapid increase in the energy consumption of blockchain-based computations for cryptocurrency mining, there have been growing concerns about their sustainable operation in electric grids. This paper investigates the tri-factor impact of such large loads on carbon footprint, grid reliability, and electricity market price in the Texas grid. We release open-source high-resolution data to enable high-resolution modeling of influencing factors such as location and flexibility. We reveal that the per-megawatt-hour carbon footprint of cryptocurrency mining loads across locations can vary by as much as 50% of the crude system average estimate. We show that the flexibility of mining loads can significantly mitigate power shortages and market disruptions that can result from the deployment of mining loads. These findings suggest policymakers to facilitate the participation of large mining facilities in wholesale markets and require them to provide mandatory demand response. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.14189,December 2022,463,Constrained Texture Restoration,"A. Rareş, M. J. T. Reinders and J. Biemond","A method is proposed for filling in missing areas of degraded images through explicit structure reconstruction, followed by texture synthesis. The structure being reconstructed represents meaningful edges from...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2758,23 October 2005,
464,0.000678989392134464,464,Towards Disentangling Relevance and Bias in Unbiased Learning to Rank,"Yunan Zhang, Le Yan, Zhen Qin, Honglei Zhuang, Jiaming Shen, Xuanhui Wang, Michael Bendersky, Marc Najork","Unbiased learning to rank (ULTR) studies the problem of mitigating various biases from implicit user feedback data such as clicks, and has been receiving considerable attention recently. A popular ULTR approach for real-world applications uses a two-tower architecture, where click modeling is factorized into a relevance tower with regular input features, and a bias tower with bias-relevant inputs such as the position of a document. A successful factorization will allow the relevance tower to be exempt from biases. In this work, we identify a critical issue that existing ULTR methods ignored - the bias tower can be confounded with the relevance tower via the underlying true relevance. In particular, the positions were determined by the logging policy, i.e., the previous production model, which would possess relevance information. We give both theoretical analysis and empirical results to show the negative effects on relevance tower due to such a correlation. We then propose three methods to mitigate the negative confounding effects by better disentangling relevance and bias. Empirical results on both controlled public datasets and a large-scale industry dataset show the effectiveness of the proposed approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13937,December 2022,464,A Statistical and Spectral Model for Representing Noisy Sounds with Short-Time Sinusoids,Pierre Hanna and Myriam Desainte-Catherine,"We propose an original model for noise analysis, transformation, and synthesis: the CNSS model. Noisy sounds are represented with short-time sinusoids whose frequencies and phases are random variables. This sp...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1794,28 July 2005,
465,0.000678989392134464,465,Automatic Recognition and Classification of Future Work Sentences from Academic Articles in a Specific Domain,"Chengzhi Zhang, Yi Xiang, Wenke Hao, Zhicheng Li, Yuchen Qian, Yuzhuo Wang","Future work sentences (FWS) are the particular sentences in academic papers that contain the author's description of their proposed follow-up research direction. This paper presents methods to automatically extract FWS from academic papers and classify them according to the different future directions embodied in the paper's content. FWS recognition methods will enable subsequent researchers to locate future work sentences more accurately and quickly and reduce the time and cost of acquiring the corpus. The current work on automatic identification of future work sentences is relatively small, and the existing research cannot accurately identify FWS from academic papers, and thus cannot conduct data mining on a large scale. Furthermore, there are many aspects to the content of future work, and the subdivision of the content is conducive to the analysis of specific development directions. In this paper, Nature Language Processing (NLP) is used as a case study, and FWS are extracted from academic papers and classified into different types. We manually build an annotated corpus with six different types of FWS. Then, automatic recognition and classification of FWS are implemented using machine learning models, and the performance of these models is compared based on the evaluation metrics. The results show that the Bernoulli Bayesian model has the best performance in the automatic recognition task, with the Macro F1 reaching 90.73%, and the SCIBERT model has the best performance in the automatic classification task, with the weighted average F1 reaching 72.63%. Finally, we extract keywords from FWS and gain a deep understanding of the key content described in FWS, and we also demonstrate that content determination in FWS will be reflected in the subsequent research work by measuring the similarity between future work sentences and the abstracts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13860,December 2022,465,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0039-1,3 September 2011,
466,0.000678989392134464,466,A Compositional Approach to Creating Architecture Frameworks with an Application to Distributed AI Systems,"Hans-Martin Heyn, Eric Knauss, Patrizio Pelliccione","Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks. Then, we derive from the identified challenges four rules and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project ``Very efficient deep learning in the IoT"" (VEDLIoT) in the form of a case study. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13570,December 2022,466,"Birth, growth and computation of pi to ten trillion digits","Ravi P Agarwal, Hans Agarwal and Syamal K Sen","The universal real constant pi, the ratio of the circumference of any circle and its diameter, has no exact numerical representation in a finite number of digits in any number/radix system. It has conjured up ...",https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/1687-1847-2013-100,11 April 2013,
467,0.000678989392134464,467,Semi-Supervised Semantic Segmentation Methods for UW-OCTA Diabetic Retinopathy Grade Assessment,"Zhuoyi Tan, Hizmawati Madzin, Zeyu Ding","People with diabetes are more likely to develop diabetic retinopathy (DR) than healthy people. However, DR is the leading cause of blindness. At present, the diagnosis of diabetic retinopathy mainly relies on the experienced clinician to recognize the fine features in color fundus images. This is a time-consuming task. Therefore, in this paper, to promote the development of UW-OCTA DR automatic detection, we propose a novel semi-supervised semantic segmentation method for UW-OCTA DR image grade assessment. This method, first, uses the MAE algorithm to perform semi-supervised pre-training on the UW-OCTA DR grade assessment dataset to mine the supervised information in the UW-OCTA images, thereby alleviating the need for labeled data. Secondly, to more fully mine the lesion features of each region in the UW-OCTA image, this paper constructs a cross-algorithm ensemble DR tissue segmentation algorithm by deploying three algorithms with different visual feature processing strategies. The algorithm contains three sub-algorithms, namely pre-trained MAE, ConvNeXt, and SegFormer. Based on the initials of these three sub-algorithms, the algorithm can be named MCS-DRNet. Finally, we use the MCS-DRNet algorithm as an inspector to check and revise the results of the preliminary evaluation of the DR grade evaluation algorithm. The experimental results show that the mean dice similarity coefficient of MCS-DRNet v1 and v2 are 0.5161 and 0.5544, respectively. The quadratic weighted kappa of the DR grading evaluation is 0.7559. Our code will be released soon. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13486,December 2022,467,On equalization of fundamental education in Tibet: a case study on the trend of conditions of primary and middle schools running,Xuewen Zhou,"Based on public data such as the Educational Statistics Yearbook and the National Statistics Yearbook, this paper analyzes the equalization trend of fundamental education in the Tibet Autonomous Region (herein...",https://www.springeropen.com//ijae.springeropen.com/articles/10.1186/s41257-022-00078-5,16 December 2022,
468,0.000678989392134464,468,NEEDED: Introducing Hierarchical Transformer to Eye Diseases Diagnosis,"Xu Ye, Meng Xiao, Zhiyuan Ning, Weiwei Dai, Wenjuan Cui, Yi Du, Yuanchun Zhou","With the development of natural language processing techniques(NLP), automatic diagnosis of eye diseases using ophthalmology electronic medical records (OEMR) has become possible. It aims to evaluate the condition of both eyes of a patient respectively, and we formulate it as a particular multi-label classification task in this paper. Although there are a few related studies in other diseases, automatic diagnosis of eye diseases exhibits unique characteristics. First, descriptions of both eyes are mixed up in OEMR documents, with both free text and templated asymptomatic descriptions, resulting in sparsity and clutter of information. Second, OEMR documents contain multiple parts of descriptions and have long document lengths. Third, it is critical to provide explainability to the disease diagnosis model. To overcome those challenges, we present an effective automatic eye disease diagnosis framework, NEEDED. In this framework, a preprocessing module is integrated to improve the density and quality of information. Then, we design a hierarchical transformer structure for learning the contextualized representations of each sentence in the OEMR document. For the diagnosis part, we propose an attention-based predictor that enables traceable diagnosis by obtaining disease-specific information. Experiments on the real dataset and comparison with several baseline models show the advantage and explainability of our framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13408,December 2022,468,Image-Based 3D Face Modeling System,"In Kyu Park, Hui Zhang and Vladimir Vezhnevets","This paper describes an automatic system for 3D face modeling using frontal and profile images taken by an ordinary digital camera. The system consists of four subsystems including frontal feature detection, p...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2072,15 August 2005,
469,0.0130975937317491,469,Traceable Automatic Feature Transformation via Cascading Actor-Critic Agents,"Meng Xiao, Dongjie Wang, Min Wu, Ziyue Qiao, Pengfei Wang, Kunpeng Liu, Yuanchun Zhou, Yanjie Fu","Feature transformation for AI is an essential task to boost the effectiveness and interpretability of machine learning (ML). Feature transformation aims to transform original data to identify an optimal feature space that enhances the performances of a downstream ML model. Existing studies either combines preprocessing, feature selection, and generation skills to empirically transform data, or automate feature transformation by machine intelligence, such as reinforcement learning. However, existing studies suffer from: 1) high-dimensional non-discriminative feature space; 2) inability to represent complex situational states; 3) inefficiency in integrating local and global feature information. To fill the research gap, we formulate the feature transformation task as an iterative, nested process of feature generation and selection, where feature generation is to generate and add new features based on original features, and feature selection is to remove redundant features to control the size of feature space. Finally, we present extensive experiments and case studies to illustrate 24.7\% improvements in F1 scores compared with SOTAs and robustness in high-dimensional data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13402,December 2022,469,Editorial,"Sergio Palazzo, Leandros Tassiulas and Lang Tong",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN.2005.607,12 December 2005,
470,0.000678989392134464,470,Artificial Intelligence to Enhance Mission Science Output for In-situ Observations: Dealing with the Sparse Data Challenge,"M. I. Sitnov, G. K. Stephens, V. G. Merkin, C. -P. Wang, D. Turner, K. Genestreti, M. Argall, T. Y. Chen, A. Y. Ukhorskiy, S. Wing, Y. -H. Liu","In the Earth's magnetosphere, there are fewer than a dozen dedicated probes beyond low-Earth orbit making in-situ observations at any given time. As a result, we poorly understand its global structure and evolution, the mechanisms of its main activity processes, magnetic storms, and substorms. New Artificial Intelligence (AI) methods, including machine learning, data mining, and data assimilation, as well as new AI-enabled missions will need to be developed to meet this Sparse Data challenge. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.13289,December 2022,470,Rule-Driven Object Tracking in Clutter and Partial Occlusion with Model-Based Snakes,"Gabriel Tsechpenakis, Konstantinos Rapantzikos, Nicolas Tsapatsoulis and Stefanos Kollias",In the last few years it has been made clear to the research community that further improvements in classic approaches for solving low-level computer vision and image/video understanding tasks are difficult to...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704401103,15 June 2004,
471,0.000678989392134464,471,Mining the Factor Zoo: Estimation of Latent Factor Models with Sufficient Proxies,"Runzhe Wan, Yingying Li, Wenbin Lu, Rui Song","Latent factor model estimation typically relies on either using domain knowledge to manually pick several observed covariates as factor proxies, or purely conducting multivariate analysis such as principal component analysis. However, the former approach may suffer from the bias while the latter can not incorporate additional information. We propose to bridge these two approaches while allowing the number of factor proxies to diverge, and hence make the latent factor model estimation robust, flexible, and statistically more accurate. As a bonus, the number of factors is also allowed to grow. At the heart of our method is a penalized reduced rank regression to combine information. To further deal with heavy-tailed data, a computationally attractive penalized robust reduced rank regression method is proposed. We establish faster rates of convergence compared with the benchmark. Extensive simulations and real examples are used to illustrate the advantages. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.12845,December 2022,471,Exploring the key influencing factors on college students’ computational thinking skills through flipped-classroom instruction,"Di Gong, Harrison H. Yang and Jin Cai","To better understand students’ computational thinking skills (CTS) within the context of flipped-classroom instruction, a structural equation modeling analysis is employed to examine the key factors that influ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00196-0,8 June 2020,
472,0.000678989392134464,472,Automated Gadget Discovery in Science,"Lea M. Trenkwalder, Andrea López Incera, Hendrik Poulsen Nautrup, Fulvio Flamini, Hans J. Briegel","In recent years, reinforcement learning (RL) has become increasingly successful in its application to science and the process of scientific discovery in general. However, while RL algorithms learn to solve increasingly complex problems, interpreting the solutions they provide becomes ever more challenging. In this work, we gain insights into an RL agent's learned behavior through a post-hoc analysis based on sequence mining and clustering. Specifically, frequent and compact subroutines, used by the agent to solve a given task, are distilled as gadgets and then grouped by various metrics. This process of gadget discovery develops in three stages: First, we use an RL agent to generate data, then, we employ a mining algorithm to extract gadgets and finally, the obtained gadgets are grouped by a density-based clustering algorithm. We demonstrate our method by applying it to two quantum-inspired RL environments. First, we consider simulated quantum optics experiments for the design of high-dimensional multipartite entangled states where the algorithm finds gadgets that correspond to modern interferometer setups. Second, we consider a circuit-based quantum computing environment where the algorithm discovers various gadgets for quantum information processing, such as quantum teleportation. This approach for analyzing the policy of a learned agent is agent and environment agnostic and can yield interesting insights into any agent's policy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.12743,December 2022,472,Applying 4IRs in education technology to science pedagogy: effects and students’ experience,Segun Michael Ojetunde and Umesh Ramnarain,"Education, technology, and economic growth are not only linked but synchronised to achieve holistic global development. An instance is the adoption of online platforms for learning and promoting economic activ...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00251-z,27 June 2023,
473,0.000678989392134464,473,The contribution of the modern amateur astronomer to the science of astronomy,Filipp Romanov,"An amateur astronomer in the modern world has the opportunity not only to make visual observations for own interest, but can make scientific astronomical observations and new discoveries in astronomy. In my example, as amateur astronomer and only through self-education, I inform about my discoveries: of the possible dwarf nova on the old digitized photographic plates and of new variable stars from sky surveys data by means of data mining; how I discovered (in the images of the sky surveys): astronomical transients, supernovae, planetary nebula candidates and new binary systems in the data of Gaia DR2; I describe my discoveries of three novae in the Andromeda Galaxy. I report about some of my scientific observations using remote telescopes: of superhumps of cataclysmic variable stars; of echo outburst of AM CVn star; of maximum brightness of blazars; of optical afterglows of gamma-ray bursts (including GRB 221009A); of microlensing events; of rotation of near-Earth asteroid 2022 AB. I also describe my photometric follow-up observations of novae (including V1405 Cas and V1674 Her) and my astrometric observations of Solar System objects (including the confirmation of objects posted at the Confirmation Pages of the Minor Planet Center) including observations of comet 2I/Borisov, asteroids 2020 AV2 and (65803) Didymos. I also describe some of my observations of occultations: of the star by asteroid (159) Aemilia, of the star by Saturn's moon Titan and of Uranus by the Moon during total lunar eclipse on November 8, 2022; and visual observations of variable stars, meteors and sunspots (including during the transit of Venus in 2012). Some of my data already used in scientific papers, others were sent to the databases. I share my experience of discovery and research of astronomical objects and in my example, I show that an amateur astronomer can make a real contribution to the science. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.12543,December 2022,473,Properties of Orthogonal Gaussian-Hermite Moments and Their Applications,Youfu Wu and Jun Shen,"Moments are widely used in pattern recognition, image processing, and computer vision and multiresolution analysis. In this paper, we first point out some properties of the orthogonal Gaussian-Hermite moments,...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.588,30 March 2005,
474,0.000678989392134464,474,Word Embedding Neural Networks to Advance Knee Osteoarthritis Research,"Soheyla Amirian, Husam Ghazaleh, Mehdi Assefi, Hilal Maradit Kremers, Hamid R. Arabnia, Johannes F. Plate, Ahmad P. Tafti","Osteoarthritis (OA) is the most prevalent chronic joint disease worldwide, where knee OA takes more than 80% of commonly affected joints. Knee OA is not a curable disease yet, and it affects large columns of patients, making it costly to patients and healthcare systems. Etiology, diagnosis, and treatment of knee OA might be argued by variability in its clinical and physical manifestations. Although knee OA carries a list of well-known terminology aiming to standardize the nomenclature of the diagnosis, prognosis, treatment, and clinical outcomes of the chronic joint disease, in practice there is a wide range of terminology associated with knee OA across different data sources, including but not limited to biomedical literature, clinical notes, healthcare literacy, and health-related social media. Among these data sources, the scientific articles published in the biomedical literature usually make a principled pipeline to study disease. Rapid yet, accurate text mining on large-scale scientific literature may discover novel knowledge and terminology to better understand knee OA and to improve the quality of knee OA diagnosis, prevention, and treatment. The present works aim to utilize artificial neural network strategies to automatically extract vocabularies associated with knee OA diseases. Our finding indicates the feasibility of developing word embedding neural networks for autonomous keyword extraction and abstraction of knee OA. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.11933,December 2022,474,Structuring reflective middleware using meta-informationmanagement: The Meta-ORB approach and prototypes,Fábio M. Costa and Bruno da Silva Santos,"Reflection is now an established technique for achieving dynamic adaptability of middleware platforms. It provides a clean and comprehensive way to access the internals of a platform implementation, allowing i...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192353,February 2004,
475,0.000678989392134464,475,Model Based Co-clustering of Mixed Numerical and Binary Data,"Aichetou Bouchareb, Marc Boullé, Fabrice Clérot, Fabrice Rossi","Co-clustering is a data mining technique used to extract the underlying block structure between the rows and columns of a data matrix. Many approaches have been studied and have shown their capacity to extract such structures in continuous, binary or contingency tables. However, very little work has been done to perform co-clustering on mixed type data. In this article, we extend the latent block models based co-clustering to the case of mixed data (continuous and binary variables). We then evaluate the effectiveness of the proposed approach on simulated data and we discuss its advantages and potential limits. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.11725,December 2022,475,Factorial design analysis applied to the performance of parallel evolutionary algorithms,"Mônica S Pais, Igor S Peretta, Keiji Yamanaka and Edmilson R Pinto","Parallel computing is a powerful way to reduce computation time and to improve the quality of solutions of evolutionary algorithms (EAs). At first, parallel EAs (PEAs) ran on very expensive and not easily avai...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/1678-4804-20-6,3 February 2014,
476,0.000678989392134464,476,Exploring Student Engagement and Outcomes: Experiences from Three Cycles of an Undergraduate Module,"Robert D. Macredie, Martin Shepperd, Tommaso Turchi, Terry Young","Many studies in educational data mining address specific learner groups, such as first-in-family to attend Higher Education, or focus on differences in characteristics such as gender or ethnicity, with the aim of predicting performance and designing interventions to improve outcomes. For Higher Education, this is reflected in significant interest in institutional-level analysis of student cohorts and in tools being promoted to Higher Education Institutions to support collection, integration and analysis of data. For those leading modules/units on degree programmes, however, the reality can be far removed from the seemingly well-supported and increasingly sophisticated approaches advocated in centrally-led data analysis. Module leaders often find themselves working with a number of student-data systems that are not integrated, may contain conflicting data and where significant effort is required to extract, clean and meaningfully analyse the data. This paper suggests that important lessons may be learned from experiences at module level in this context and from subsequent analysis of related data collected across multiple years. The changes made each year are described and a range of data analysis methods are applied, post hoc, to identify findings in relation to the four areas of focus. The key findings are that non-engagement with the Virtual Learning Environment in the first three weeks was the strongest predictor of failure and that early engagement correlated most strongly with final grade. General recommendations are drawn from the findings which should be valuable to module leaders in environments where access to integrated, up-to-date student information remains a day-to-day challenge, and insights will be presented into how such bottom-up activities might inform institutional/top-down planning in the use of relevant technologies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.11682,December 2022,476,An improved particle filter for sparse environments,"Edson Prestes, Marcus Ritt and Gustavo Führ","In this paper, we combine a path planner based on Boundary Value Problems (BVP) and Monte Carlo Localization (MCL) to solve the wake-up robot problem in a sparse environment. This problem is difficult since la...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194506,September 2009,
477,0.000678989392134464,477,Multi-queue Momentum Contrast for Microvideo-Product Retrieval,"Yali Du, Yinwei Wei, Wei Ji, Fan Liu, Xin Luo, Liqiang Nie","The booming development and huge market of micro-videos bring new e-commerce channels for merchants. Currently, more micro-video publishers prefer to embed relevant ads into their micro-videos, which not only provides them with business income but helps the audiences to discover their interesting products. However, due to the micro-video recording by unprofessional equipment, involving various topics and including multiple modalities, it is challenging to locate the products related to micro-videos efficiently, appropriately, and accurately. We formulate the microvideo-product retrieval task, which is the first attempt to explore the retrieval between the multi-modal and multi-modal instances. A novel approach named Multi-Queue Momentum Contrast (MQMC) network is proposed for bidirectional retrieval, consisting of the uni-modal feature and multi-modal instance representation learning. Moreover, a discriminative selection strategy with a multi-queue is used to distinguish the importance of different negatives based on their categories. We collect two large-scale microvideo-product datasets (MVS and MVS-large) for evaluation and manually construct the hierarchical category ontology, which covers sundry products in daily life. Extensive experiments show that MQMC outperforms the state-of-the-art baselines. Our replication package (including code, dataset, etc.) is publicly available at https://github.com/duyali2000/MQMC. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.11471,December 2022,477,"Infeasible paths in the context of data flow based testing criteria: Identification, classification and prediction","Silvia Regina Vergilio, José Carlos Maldonado and Mario Jino","Infeasible paths constitute a bottleneck for the complete automation of software testing, one of the most expensive activities of software quality assurance. Research efforts have been spent on infeasible path...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192389,February 2006,
478,0.000678989392134464,478,Self-supervised Hypergraph Representation Learning for Sociological Analysis,"Xiangguo Sun, Hong Cheng, Bo Liu, Jia Li, Hongyang Chen, Guandong Xu, Hongzhi Yin","Modern sociology has profoundly uncovered many convincing social criteria for behavioural analysis. Unfortunately, many of them are too subjective to be measured and presented in online social networks. On the other hand, data mining techniques can better find data patterns but many of them leave behind unnatural understanding. In this paper, we propose a fundamental methodology to support the further fusion of data mining techniques and sociological behavioral criteria. Our highlights are three-fold: First, we propose an effective hypergraph awareness and a fast line graph construction framework. The hypergraph can more profoundly indicate the interactions between individuals and their environments because each edge in the hypergraph (a.k.a hyperedge) contains more than two nodes, which is perfect to describe social environments. A line graph treats each social environment as a super node with the underlying influence between different environments. In this way, we go beyond traditional pair-wise relations and explore richer patterns under various sociological criteria; Second, we propose a novel hypergraph-based neural network to learn social influence flowing from users to users, users to environments, environment to users, and environments to environments. The neural network can be learned via a task-free method, making our model very flexible to support various data mining tasks and sociological analysis; Third, we propose both qualitative and quantitive solutions to effectively evaluate the most common sociological criteria like social conformity, social equivalence, environmental evolving and social polarization. Our extensive experiments show that our framework can better support both data mining tasks for online user behaviours and sociological analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.11440,December 2022,478,A Tutorial on Text-Independent Speaker Verification,"Frédéric Bimbot, Jean-François Bonastre, Corinne Fredouille, Guillaume Gravier, Ivan Magrin-Chagnolleau, Sylvain Meignier, Teva Merlin, Javier Ortega-García, Dijana Petrovska-Delacrétaz and Douglas A. Reynolds","This paper presents an overview of a state-of-the-art text-independent speaker verification system. First, an introduction proposes a modular scheme of the training and test phases of a speaker verification sy...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704310024,21 April 2004,
479,0.000678989392134464,479,Automatic Semantic Modeling for Structural Data Source with the Prior Knowledge from Knowledge Base,"Jiakang Xu, Wolfgang Mayer, HongYu Zhang, Keqing He, Zaiwen Feng","A critical step in sharing semantic content online is to map the structural data source to a public domain ontology. This problem is denoted as the Relational-To-Ontology Mapping Problem (Rel2Onto). A huge effort and expertise are required for manually modeling the semantics of data. Therefore, an automatic approach for learning the semantics of a data source is desirable. Most of the existing work studies the semantic annotation of source attributes. However, although critical, the research for automatically inferring the relationships between attributes is very limited. In this paper, we propose a novel method for semantically annotating structured data sources using machine learning, graph matching and modified frequent subgraph mining to amend the candidate model. In our work, Knowledge graph is used as prior knowledge. Our evaluation shows that our approach outperforms two state-of-the-art solutions in tricky cases where only a few semantic models are known. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10915,December 2022,479,Editorial,"Kiyoharu Aizawa, Thomas Huang, Stefanos Kollias, Petros Maragos and Ralf Schäfer",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002707,15 June 2004,
480,0.000678989392134464,480,Ensemble learning techniques for intrusion detection system in the context of cybersecurity,"Andricson Abeline Moreira, Carlos A. C. Tojeiro, Carlos J. Reis, Gustavo Henrique Massaro, Igor Andrade Brito e Kelton A. P. da Costa","Recently, there has been an interest in improving the resources available in Intrusion Detection System (IDS) techniques. In this sense, several studies related to cybersecurity show that the environment invasions and information kidnapping are increasingly recurrent and complex. The criticality of the business involving operations in an environment using computing resources does not allow the vulnerability of the information. Cybersecurity has taken on a dimension within the universe of indispensable technology in corporations, and the prevention of risks of invasions into the environment is dealt with daily by Security teams. Thus, the main objective of the study was to investigate the Ensemble Learning technique using the Stacking method, supported by the Support Vector Machine (SVM) and k-Nearest Neighbour (kNN) algorithms aiming at an optimization of the results for DDoS attack detection. For this, the Intrusion Detection System concept was used with the application of the Data Mining and Machine Learning Orange tool to obtain better results △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10913,December 2022,480,Preprocessing in a Tiered Sensor Network for Habitat Monitoring,"Hanbiao Wang, Deborah Estrin and Lewis Girod",We investigate task decomposition and collaboration in a two-tiered sensor network for habitat monitoring. The system recognizes and localizes a specified type of birdcalls. The system has a few powerful macro...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703212087,30 March 2003,
481,0.000678989392134464,481,Mining User-aware Multi-relations for Fake News Detection in Large Scale Online Social Networks,"Xing Su, Jian Yang, Jia Wu, Yuchen Zhang","Users' involvement in creating and propagating news is a vital aspect of fake news detection in online social networks. Intuitively, credible users are more likely to share trustworthy news, while untrusted users have a higher probability of spreading untrustworthy news. In this paper, we construct a dual-layer graph (i.e., the news layer and the user layer) to extract multiple relations of news and users in social networks to derive rich information for detecting fake news. Based on the dual-layer graph, we propose a fake news detection model named Us-DeFake. It learns the propagation features of news in the news layer and the interaction features of users in the user layer. Through the inter-layer in the graph, Us-DeFake fuses the user signals that contain credibility information into the news features, to provide distinctive user-aware embeddings of news for fake news detection. The training process conducts on multiple dual-layer subgraphs obtained by a graph sampler to scale Us-DeFake in large scale social networks. Extensive experiments on real-world datasets illustrate the superiority of Us-DeFake which outperforms all baselines, and the users' credibility signals learned by interaction relation can notably improve the performance of our model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10778,December 2022,481,"Past, present, and future of smart learning: a topic-based bibliometric analysis","Xieling Chen, Di Zou, Haoran Xie and Fu Lee Wang",Innovative information and communication technologies have reformed higher education from the traditional way to smart learning. Smart learning applies technological and social developments and facilitates eff...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00239-6,15 January 2021,
482,0.000678989392134464,482,AgAsk: An Agent to Help Answer Farmer's Questions From Scientific Documents,"Bevan Koopman, Ahmed Mourad, Hang Li, Anton van der Vegt, Shengyao Zhuang, Simon Gibson, Yash Dang, David Lawrence, Guido Zuccon","Decisions in agriculture are increasingly data-driven; however, valuable agricultural knowledge is often locked away in free-text reports, manuals and journal articles. Specialised search systems are needed that can mine agricultural information to provide relevant answers to users' questions. This paper presents AgAsk -- an agent able to answer natural language agriculture questions by mining scientific documents. We carefully survey and analyse farmers' information needs. On the basis of these needs we release an information retrieval test collection comprising real questions, a large collection of scientific documents split in passages, and ground truth relevance assessments indicating which passages are relevant to each question. We implement and evaluate a number of information retrieval models to answer farmers questions, including two state-of-the-art neural ranking models. We show that neural rankers are highly effective at matching passages to questions in this context. Finally, we propose a deployment architecture for AgAsk that includes a client based on the Telegram messaging platform and retrieval model deployed on commodity hardware. The test collection we provide is intended to stimulate more research in methods to match natural language to answers in scientific documents. While the retrieval models were evaluated in the agriculture domain, they are generalisable and of interest to others working on similar problems. The test collection is available at: \url{https://github.com/ielab/agvaluate}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10762,December 2022,482,Generic Multimedia Multimodal Agents Paradigms and Their Dynamic Reconfiguration at the Architectural Level,"H. Djenidi, S. Benarif, A. Ramdane-Cherif, C. Tadj and N. Levy",The multimodal fusion for natural human-computer interaction involves complex intelligent architectures which are subject to the unexpected errors and mistakes of users. These architectures should react to eve...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704402212,18 September 2004,
483,0.000678989392134464,483,Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval,"John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor Berg-Kirkpatrick","Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in $N$ languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval -- the last of which we introduce in this paper. Overall, our Variational Multilingual Source-Separation Transformer (VMSST) model outperforms both a strong contrastive and generative baseline on these tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10726,December 2022,483,The lived experiences of pre-service science teachers designing and teaching socioscientific issues-based units,Engin Karahan,"The purpose of this study was to explore the experiences of secondary science preservice teachers while designing and implementing SSI-based instructional processes, as well as their interpretations from these...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00064-z,1 July 2022,
484,0.000678989392134464,484,AutoMESC: Automatic Framework for Mining and Classifying Ethereum Smart Contract Vulnerabilities and Their Fixes,"Majd Soud, Ilham Qasse, Grischa Liebel, Mohammad Hamdaqa","Due to the risks associated with vulnerabilities in smart contracts, their security has gained significant attention in recent years. However, there is a lack of open datasets on smart contract vulnerabilities and their fixes that allows for data-driven research. Towards this end, we propose an automated method for mining and classifying Ethereum's smart contract vulnerabilities and their corresponding fixes from GitHub and from the Common Vulnerabilities and Exposures (CVE) records in the National Vulnerability Database. We implemented the proposed method in a fully automated framework, which we call AutoMESC. AutoMESC uses seven of the most well-known smart contract security tools to classify and label the collected vulnerabilities based on vulnerability types. Furthermore, it collects metadata that can be used in data-intensive smart contract security research (e.g., vulnerability detection, vulnerability classification, severity prediction, and automated repair). We used AutoMESC to construct a sample dataset and made it publicly available. Currently, the dataset contains 6.7K smart contracts' vulnerability-fix pairs written in Solidity. We assess the quality of the constructed dataset in terms of accuracy, provenance, and relevance, and compare it with existing datasets. AutoMESC is designed to collect data continuously and keep the corresponding dataset up-to-date with newly discovered smart contract vulnerabilities and their fixes from GitHub and CVE records. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10660,December 2022,484,Breakpoint Tuning in DCT-Based Nonlinear Layered Video Codecs,"Pedro Cuenca, Luis Orozco-Barbosa, Francisco Delicado and Antonio Garrido","Many studies have been conducted to evaluate the benefits of using layered video coding schemes as a means to improve the robustness of video communications systems. In this paper, we study a frame-aware nonli...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704404156,2 December 2004,
485,0.000678989392134464,485,Towards Sequence Utility Maximization under Utility Occupancy Measure,"Gengsen Huang, Wensheng Gan, Philip S. Yu","The discovery of utility-driven patterns is a useful and difficult research topic. It can extract significant and interesting information from specific and varied databases, increasing the value of the services provided. In practice, the measure of utility is often used to demonstrate the importance, profit, or risk of an object or a pattern. In the database, although utility is a flexible criterion for each pattern, it is a more absolute criterion due to the neglect of utility sharing. This leads to the derived patterns only exploring partial and local knowledge from a database. Utility occupancy is a recently proposed model that considers the problem of mining with high utility but low occupancy. However, existing studies are concentrated on itemsets that do not reveal the temporal relationship of object occurrences. Therefore, this paper towards sequence utility maximization. We first define utility occupancy on sequence data and raise the problem of High Utility-Occupancy Sequential Pattern Mining (HUOSPM). Three dimensions, including frequency, utility, and occupancy, are comprehensively evaluated in HUOSPM. An algorithm called Sequence Utility Maximization with Utility occupancy measure (SUMU) is proposed. Furthermore, two data structures for storing related information about a pattern, Utility-Occupancy-List-Chain (UOL-Chain) and Utility-Occupancy-Table (UO-Table) with six associated upper bounds, are designed to improve efficiency. Empirical experiments are carried out to evaluate the novel algorithm's efficiency and effectiveness. The influence of different upper bounds and pruning strategies is analyzed and discussed. The comprehensive results suggest that the work of our algorithm is intelligent and effective. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10452,December 2022,485,What is a quantum simulator?,"Tomi H Johnson, Stephen R Clark and Dieter Jaksch","Quantum simulators are devices that actively use quantum effects to answer questions about model systems and, through them, real systems. In this review we expand on this definition by answering several fundam...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt10,23 July 2014,
486,0.000678989392134464,486,Peer-reviewed theory does not help predict the cross-section of stock returns,"Andrew Y. Chen, Alejandro Lopez-Lira, Tom Zimmermann","To examine whether theory helps predict the cross-section of returns, we combine text analysis of publications with out-of-sample tests. Based on the original texts, only 18% predictors are attributed to risk-based theory. 59% are attributed to mispricing and 23% have uncertain origins. Post-publication, risk-based predictability decays by 65% (p-value < 0.1%), compared to 50% for non-risk predictors. Out-of-sample, risk-based predictors fail to outperform data-mined accounting predictors that are matched on in-sample summary statistics. Published and data-mined returns rise before in-sample periods end and fall out-of-sample at similar rates. Overall, peer-reviewed research adds little information about future mean returns above naive back testing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10317,December 2022,486,Structural Analysis of Single-Point Mutations Given an RNA Sequence: A Case Study with RNAMute,Alexander Churkin and Danny Barash,"We introduce here for the first time the RNAMute package, a pattern-recognition-based utility to perform mutational analysis and detect vulnerable spots within an RNA sequence that affect structure. Mutations ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/56246,1 December 2006,
487,0.000678989392134464,487,Personalized PageRank on Evolving Graphs with an Incremental Index-Update Scheme,"Guanhao Hou, Qintian Guo, Fangyuan Zhang, Sibo Wang, Zhewei Wei","{\em Personalized PageRank (PPR)} stands as a fundamental proximity measure in graph mining. Since computing an exact SSPPR query answer is prohibitive, most existing solutions turn to approximate queries with guarantees. The state-of-the-art solutions for approximate SSPPR queries are index-based and mainly focus on static graphs, while real-world graphs are usually dynamically changing. However, existing index-update schemes can not achieve a sub-linear update time. Motivated by this, we present an efficient indexing scheme to maintain indexed random walks in expected $O(1)$ time after each graph update. To reduce the space consumption, we further propose a new sampling scheme to remove the auxiliary data structure for vertices while still supporting $O(1)$ index update cost on evolving graphs. Extensive experiments show that our update scheme achieves orders of magnitude speed-up on update performance over existing index-based dynamic schemes without sacrificing the query efficiency. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10288,December 2022,487,Toward the formulation of a proposed frame for the formal and structural specifications of the modern parametric digital architecture,Amal R. Tantawy,This research paper contributes to presenting a proposed framework for the formal and structural specifications of parametric digital architecture in order to increase knowledge and know-how in this field clos...,https://www.springeropen.com//bjbas.springeropen.com/articles/10.1186/s43088-022-00195-2,15 January 2022,
488,0.000678989392134464,488,MDL-based Compressing Sequential Rules,"Xinhong Chen, Wensheng Gan, Shicheng Wan, Tianlong Gu","Nowadays, with the rapid development of the Internet, the era of big data has come. The Internet generates huge amounts of data every day. However, extracting meaningful information from massive data is like looking for a needle in a haystack. Data mining techniques can provide various feasible methods to solve this problem. At present, many sequential rule mining (SRM) algorithms are presented to find sequential rules in databases with sequential characteristics. These rules help people extract a lot of meaningful information from massive amounts of data. How can we achieve compression of mined results and reduce data size to save storage space and transmission time? Until now, there has been little research on the compression of SRM. In this paper, combined with the Minimum Description Length (MDL) principle and under the two metrics (support and confidence), we introduce the problem of compression of SRM and also propose a solution named ComSR for MDL-based compressing of sequential rules based on the designed sequential rule coding scheme. To our knowledge, we are the first to use sequential rules to encode an entire database. A heuristic method is proposed to find a set of compact and meaningful sequential rules as much as possible. ComSR has two trade-off algorithms, ComSR_non and ComSR_ful, based on whether the database can be completely compressed. Experiments done on a real dataset with different thresholds show that a set of compact and meaningful sequential rules can be found. This shows that the proposed method works. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.10252,December 2022,488,Nanomaterials and nanoparticles: Sources and toxicity,"Cristina Buzea, Ivan I. Pacheco and Kevin Robbie","This review is presented as a common foundation for scientists interested in nanoparticles, their origin, activity, and biological toxicity. It is written with the goal of rationalizing and informing public he...",https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.2815690,December 2007,
489,0.000678989392134464,489,Graph-based Semantical Extractive Text Analysis,Mina Samizadeh,"In the past few decades, there has been an explosion in the amount of available data produced from various sources with different topics. The availability of this enormous data necessitates us to adopt effective computational tools to explore the data. This leads to an intense growing interest in the research community to develop computational methods focused on processing this text data. A line of study focused on condensing the text so that we are able to get a higher level of understanding in a shorter time. The two important tasks to do this are keyword extraction and text summarization. In keyword extraction, we are interested in finding the key important words from a text. This makes us familiar with the general topic of a text. In text summarization, we are interested in producing a short-length text which includes important information about the document. The TextRank algorithm, an unsupervised learning method that is an extension of the PageRank (algorithm which is the base algorithm of Google search engine for searching pages and ranking them) has shown its efficacy in large-scale text mining, especially for text summarization and keyword extraction. this algorithm can automatically extract the important parts of a text (keywords or sentences) and declare them as the result. However, this algorithm neglects the semantic similarity between the different parts. In this work, we improved the results of the TextRank algorithm by incorporating the semantic similarity between parts of the text. Aside from keyword extraction and text summarization, we develop a topic clustering algorithm based on our framework which can be used individually or as a part of generating the summary to overcome coverage problems. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.09701,December 2022,489,Per-Sample Multiple Kernel Approach for Visual Concept Learning,"Jingjing Yang, Yuanning Li, Yonghong Tian, Ling-Yu Duan and Wen Gao",Learning visual concepts from images is an important yet challenging problem in computer vision and multimedia research areas. Multiple kernel learning (MKL) methods have shown great advantages in visual conce...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2010/461450,16 March 2010,
490,0.000678989392134464,490,Data mining techniques on astronomical spectra data. II : Classification Analysis,"Haifeng Yang, Lichan Zhou, Jianghui Cai, Chenhui Shi, Yuqing Yang, Xujun Zhao, Juncheng Duan, Xiaona Yin","Classification is valuable and necessary in spectral analysis, especially for data-driven mining. Along with the rapid development of spectral surveys, a variety of classification techniques have been successfully applied to astronomical data processing. However, it is difficult to select an appropriate classification method in practical scenarios due to the different algorithmic ideas and data characteristics. Here, we present the second work in the data mining series - a review of spectral classification techniques. This work also consists of three parts: a systematic overview of current literature, experimental analyses of commonly used classification algorithms and source codes used in this paper. Firstly, we carefully investigate the current classification methods in astronomical literature and organize these methods into ten types based on their algorithmic ideas. For each type of algorithm, the analysis is organized from the following three perspectives. (1) their current applications and usage frequencies in spectral classification are summarized; (2) their basic ideas are introduced and preliminarily analysed; (3) the advantages and caveats of each type of algorithm are discussed. Secondly, the classification performance of different algorithms on the unified data sets is analysed. Experimental data are selected from the LAMOST survey and SDSS survey. Six groups of spectral data sets are designed from data characteristics, data qualities and data volumes to examine the performance of these algorithms. Then the scores of nine basic algorithms are shown and discussed in the experimental analysis. Finally, nine basic algorithms source codes written in python and manuals for usage and improvement are provided. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.09286,December 2022,490,A case study of a novel summer bridge program to prepare transfer students for research in biological sciences,"Austin L. Zuckerman, Ashley L. Juavinett, Eduardo R. Macagno, Brenda L. Bloodgood, Terry Gaasterland, David Artis and Stanley M. Lo","Undergraduate research experiences enculturate students into the research community by providing support networks to explore advanced professional opportunities. However, transfer students are at a considerabl...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00067-w,20 December 2022,
491,0.000678989392134464,491,Very Large Language Model as a Unified Methodology of Text Mining,Meng Jiang,"Text data mining is the process of deriving essential information from language text. Typical text mining tasks include text categorization, text clustering, topic modeling, information extraction, and text summarization. Various data sets are collected and various algorithms are designed for the different types of tasks. In this paper, I present a blue sky idea that very large language model (VLLM) will become an effective unified methodology of text mining. I discuss at least three advantages of this new methodology against conventional methods. Finally I discuss the challenges in the design and development of VLLM techniques for text mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.09271,December 2022,491,Correction: Can cloudlet coordination support cloud computing infrastructure?,Abdullah Alsaleh,Theoriginal articlewas published inJournal of Cloud Computing20187:8,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00393-y,3 February 2023,
492,0.000678989392134464,492,Text2Struct: A Machine Learning Pipeline for Mining Structured Data from Text,"Chaochao Zhou, Bo Yang","Many analysis and prediction tasks require the extraction of structured data from unstructured texts. However, an annotation scheme and a training dataset have not been available for training machine learning models to mine structured data from text without special templates and patterns. To solve it, this paper presents an end-to-end machine learning pipeline, Text2Struct, including a text annotation scheme, training data processing, and machine learning implementation. We formulated the mining problem as the extraction of metrics and units associated with numerals in the text. Text2Struct was trained and evaluated using an annotated text dataset collected from abstracts of medical publications regarding thrombectomy. In terms of prediction performance, a dice coefficient of 0.82 was achieved on the test dataset. By random sampling, most predicted relations between numerals and entities were well matched to the ground-truth annotations. These results show that Text2Struct is viable for the mining of structured data from text without special templates or patterns. It is anticipated to further improve the pipeline by expanding the dataset and investigating other machine learning models. A code demonstration can be found at: https://github.com/zcc861007/CourseProject △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.09044,December 2022,492,Network-based data classification: combiningK-associated optimal graphs and high-level prediction,"Murillo G Carneiro, João LG Rosa, Alneu A Lopes and Liang Zhao","Traditional data classification techniques usually divide the data space into sub-spaces, each representing a class. Such a division is carried out considering only physical attributes of the training data (e....",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/1678-4804-20-14,17 June 2014,
493,0.00991669352293754,493,Graph Learning and Its Applications: A Holistic Survey,"Shaopeng Wei, Yu Zhao, Xingyan Chen, Qing Li, Fuzhen Zhuang, Ji Liu, Gang Kou","Graph learning is a prevalent domain that endeavors to learn the intricate relationships among nodes and the topological structure of graphs. Over the years, graph learning has transcended from graph theory to graph data mining. With the advent of representation learning, it has attained remarkable performance in diverse scenarios. Owing to its extensive application prospects, graph learning attracts copious attention. While some researchers have accomplished impressive surveys on graph learning, they failed to connect related objectives, methods, and applications in a more coherent way. As a result, they did not encompass current ample scenarios and challenging problems due to the rapid expansion of graph learning. Particularly, large language models have recently had a disruptive effect on human life, but they also show relative weakness in structured scenarios. The question of how to make these models more powerful with graph learning remains open. Different from previous surveys on graph learning, we provide a holistic review that analyzes current works from the perspective of graph structure, and discusses the latest applications, trends, and challenges in graph learning. Specifically, we commence by proposing a taxonomy and then summarize the methods employed in graph learning. We then provide a detailed elucidation of mainstream applications. Finally, we propose future directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.08966,December 2022,493,Machine and cognitive intelligence for human health: systematic review,"Xieling Chen, Gary Cheng, Fu Lee Wang, Xiaohui Tao, Haoran Xie and Lingling Xu",Brain informatics is a novel interdisciplinary area that focuses on scientifically studying the mechanisms of human brain information processing by integrating experimental cognitive neuroscience with advanced...,https://www.springeropen.com//braininformatics.springeropen.com/articles/10.1186/s40708-022-00153-9,12 February 2022,
494,0.000678989392134464,494,Data mining techniques on astronomical spectra data. I : Clustering Analysis,"Haifeng Yang, Chenhui Shi, Jianghui Cai, Lichan Zhou, Yuqing Yang, Xujun Zhao, Yanting He, Jing Hao","Clustering is an effective tool for astronomical spectral analysis, to mine clustering patterns among data. With the implementation of large sky surveys, many clustering methods have been applied to tackle spectroscopic and photometric data effectively and automatically. Meanwhile, the performance of clustering methods under different data characteristics varies greatly. With the aim of summarizing astronomical spectral clustering algorithms and laying the foundation for further research, this work gives a review of clustering methods applied to astronomical spectra data in three parts. First, many clustering methods for astronomical spectra are investigated and analysed theoretically, looking at algorithmic ideas, applications, and features. Secondly, experiments are carried out on unified datasets constructed using three criteria (spectra data type, spectra quality, and data volume) to compare the performance of typical algorithms; spectra data are selected from the Large Sky Area Multi-Object Fibre Spectroscopic Telescope (LAMOST) survey and Sloan Digital Sky Survey (SDSS). Finally, source codes of the comparison clustering algorithms and manuals for usage and improvement are provided on GitHub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.08419,December 2022,494,Big data actionable intelligence architecture,"Tian J. Ma, Rudy J. Garcia, Forest Danford, Laura Patrizi, Jennifer Galasso and Jason Loyd","The amount of data produced by sensors, social and digital media, and Internet of Things (IoTs) are rapidly increasing each day. Decision makers often need to sift through a sea of Big Data to utilize informat...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00378-7,23 November 2020,
495,0.000678989392134464,495,Multi-Level Association Rule Mining for Wireless Network Time Series Data,"Chen Zhu, Chengbo Qiu, Shaoyu Dou, Minghao Liao","Key performance indicators(KPIs) are of great significance in the monitoring of wireless network service quality. The network service quality can be improved by adjusting relevant configuration parameters(CPs) of the base station. However, there are numerous CPs and different cells may affect each other, which bring great challenges to the association analysis of wireless network data. In this paper, we propose an adjustable multi-level association rule mining framework, which can quantitatively mine association rules at each level with environmental information, including engineering parameters and performance management(PMs), and it has interpretability at each level. Specifically, We first cluster similar cells, then quantify KPIs and CPs, and integrate expert knowledge into the association rule mining model, which improve the robustness of the model. The experimental results in real world dataset prove the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.07860,December 2022,495,"A survey on deep learning tools dealing with data scarcity: definitions, challenges, solutions, tips, and applications","Laith Alzubaidi, Jinshuai Bai, Aiman Al-Sabaawi, Jose Santamaría, A. S. Albahri, Bashar Sami Nayyef Al-dabbagh, Mohammed A. Fadhel, Mohamed Manoufali, Jinglan Zhang, Ali H. Al-Timemy, Ye Duan, Amjed Abdullah, Laith Farhan, Yi Lu, Ashish Gupta, Felix Albu…","Data scarcity is a major challenge when training deep learning (DL) models. DL demands a large amount of data to achieve exceptional performance. Unfortunately, many applications have small or inadequate data ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00727-2,14 April 2023,
496,0.000678989392134464,496,TED: Towards Discovering Top-k Edge-Diversified Patterns in a Graph Database,"Kai Huang, Haibo Hu, Qingqing Ye, Kai Tian, Bolong Zheng, Xiaofang Zhou","With an exponentially growing number of graphs from disparate repositories, there is a strong need to analyze a graph database containing an extensive collection of small- or medium-sized data graphs (e.g., chemical compounds). Although subgraph enumeration and subgraph mining have been proposed to bring insights into a graph database by a set of subgraph structures, they often end up with similar or homogenous topologies, which is undesirable in many graph applications. To address this limitation, we propose the Top-k Edge-Diversified Patterns Discovery problem to retrieve a set of subgraphs that cover the maximum number of edges in a database. To efficiently process such query, we present a generic and extensible framework called Ted which achieves a guaranteed approximation ratio to the optimal result. Two optimization strategies are further developed to improve the performance. Experimental studies on real-world datasets demonstrate the superiority of Ted to traditional techniques. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.07612,December 2022,496,Predicting scientific success based on coauthorship networks,"Emre Sarigöl, René Pfitzner, Ingo Scholtes, Antonios Garas and Frank Schweitzer","We address the question to what extent the success of scientific articles is due to social influence. Analyzing a data set of over 100,000 publications from the field of Computer Science, we study how centrali...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-014-0009-x,25 September 2014,
497,0.000678989392134464,497,"Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection","John Lambert, James Hays","High-definition (HD) map change detection is the task of determining when sensor data and map data are no longer in agreement with one another due to real-world changes. We collect the first dataset for the task, which we entitle the Trust, but Verify (TbV) dataset, by mining thousands of hours of data from over 9 months of autonomous vehicle fleet operations. We present learning-based formulations for solving the problem in the bird's eye view and ego-view. Because real map changes are infrequent and vector maps are easy to synthetically manipulate, we lean on simulated data to train our model. Perhaps surprisingly, we show that such models can generalize to real world distributions. The dataset, consisting of maps and logs collected in six North American cities, is one of the largest AV datasets to date with more than 7.8 million images. We make the data available to the public at https://www.argoverse.org/av2.html#mapchange-link, along with code and models at https://github.com/johnwlambert/tbv under the the CC BY-NC-SA 4.0 license. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.07312,December 2022,497,"Search the Audio, Browse the Video—A Generic Paradigm for Video Collections","Arnon Amir, Savitha Srinivasan and Alon Efrat","The amount of digital video being shot, captured, and stored is growing at a rate faster than ever before. The large amount of stored video is not penetrable without efficient video indexing, retrieval, and br...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570321012X,25 February 2003,
498,0.000678989392134464,498,NLIP: Noise-robust Language-Image Pre-training,"Runhui Huang, Yanxin Long, Jianhua Han, Hang Xu, Xiwen Liang, Chunjing Xu, Xiaodan Liang","Large-scale cross-modal pre-training paradigms have recently shown ubiquitous success on a wide range of downstream tasks, e.g., zero-shot classification, retrieval and image captioning. However, their successes highly rely on the scale and quality of web-crawled data that naturally contain incomplete and noisy information (e.g., wrong or irrelevant content). Existing works either design manual rules to clean data or generate pseudo-targets as auxiliary signals for reducing noise impact, which do not explicitly tackle both the incorrect and incomplete challenges simultaneously. In this paper, to automatically mitigate the impact of noise by solely mining over existing data, we propose a principled Noise-robust Language-Image Pre-training framework (NLIP) to stabilize pre-training via two schemes: noise-harmonization and noise-completion. First, in noise-harmonization scheme, NLIP estimates the noise probability of each pair according to the memorization effect of cross-modal transformers, then adopts noise-adaptive regularization to harmonize the cross-modal alignments with varying degrees. Second, in noise-completion scheme, to enrich the missing object information of text, NLIP injects a concept-conditioned cross-modal decoder to obtain semantic-consistent synthetic captions to complete noisy ones, which uses the retrieved visual concepts (i.e., objects' names) for the corresponding image to guide captioning generation. By collaboratively optimizing noise-harmonization and noise-completion schemes, our NLIP can alleviate the common noise effects during image-text pre-training in a more efficient way. Extensive experiments show the significant performance improvements of our NLIP using only 26M data over existing pre-trained models (e.g., CLIP, FILIP and BLIP) on 12 zero-shot classification datasets, MSCOCO image captioning and zero-shot image-text retrieval tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.07086,December 2022,498,Advanced bridge visual inspection using real-time machine learning in edge devices,"Mahta Zakaria, Enes Karaaslan and F. Necati Catbas",Conventional methods for bridge inspection are labor intensive and highly subjective. This study introduces an optimized approach using real-time learning-based computer vision algorithms on edge devices to as...,https://www.springeropen.com//aben.springeropen.com/articles/10.1186/s43251-022-00073-y,28 December 2022,
499,0.000678989392134464,499,Earthquake Impact Analysis Based on Text Mining and Social Media Analytics,"Zhe Zheng, Hong-Zheng Shi, Yu-Cheng Zhou, Xin-Zheng Lu, Jia-Rui Lin","Earthquakes have a deep impact on wide areas, and emergency rescue operations may benefit from social media information about the scope and extent of the disaster. Therefore, this work presents a text miningbased approach to collect and analyze social media data for early earthquake impact analysis. First, disasterrelated microblogs are collected from the Sina microblog based on crawler technology. Then, after data cleaning a series of analyses are conducted including (1) the hot words analysis, (2) the trend of the number of microblogs, (3) the trend of public opinion sentiment, and (4) a keyword and rule-based text classification for earthquake impact analysis. Finally, two recent earthquakes with the same magnitude and focal depth in China are analyzed to compare their impacts. The results show that the public opinion trend analysis and the trend of public opinion sentiment can estimate the earthquake's social impact at an early stage, which will be helpful to decision-making and rescue management. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06765,December 2022,499,Arduino-based wireless spectrometer: a practical application,Jaeseong Shin and Han-Kyu Choi,We provide a technical report on designing an Arduino-based wireless spectrometer using an old spectrometer. The designed spectrometer is environmentally friendly and cost-effective and can be used in physical...,https://www.springeropen.com//jast-journal.springeropen.com/articles/10.1186/s40543-022-00353-2,17 November 2022,
500,0.000678989392134464,500,Differentially Private Tree-Based Redescription Mining,"Matej Mihelčić, Pauli Miettinen","Differential privacy provides a strong form of privacy and allows preserving most of the original characteristics of the dataset. Utilizing these benefits requires one to design specific differentially private data analysis algorithms. In this work, we present three tree-based algorithms for mining redescriptions while preserving differential privacy. Redescription mining is an exploratory data analysis method for finding connections between two views over the same entities, such as phenotypes and genotypes of medical patients, for example. It has applications in many fields, including some, like health care informatics, where privacy-preserving access to data is desired. Our algorithms are the first differentially private redescription mining algorithms, and we show via experiments that, despite the inherent noise in differential privacy, it can return trustworthy results even in smaller datasets where noise typically has a stronger effect. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06630,December 2022,500,Applications of AI in classical software engineering,"Marco Barenkamp, Jonas Rebstadt and Oliver Thomas","Although Artificial Intelligence (AI) has become a buzzword for self-organizing IT applications, its relevance to software engineering has hardly been analyzed systematically. This study combines a systematic ...",https://www.springeropen.com//aiperspectives.springeropen.com/articles/10.1186/s42467-020-00005-4,26 July 2020,
501,0.000678989392134464,501,Automatic ESG Assessment of Companies by Mining and Evaluating Media Coverage Data: NLP Approach and Tool,"Jannik Fischbach, Max Adam, Victor Dzhagatspanyan, Daniel Mendez, Julian Frattini, Oleksandr Kosenkov, Parisa Elahidoost","Context: Sustainable corporate behavior is increasingly valued by society and impacts corporate reputation and customer trust. Hence, companies regularly publish sustainability reports to shed light on their impact on environmental, social, and governance (ESG) factors. Problem: Sustainability reports are written by companies themselves and are therefore considered a company-controlled source. Contrary, studies reveal that non-corporate channels (e.g., media coverage) represent the main driver for ESG transparency. However, analysing media coverage regarding ESG factors is challenging since (1) the amount of published news articles grows daily, (2) media coverage data does not necessarily deal with an ESG-relevant topic, meaning that it must be carefully filtered, and (3) the majority of media coverage data is unstructured. Research Goal: We aim to extract ESG-relevant information from textual media reactions automatically to calculate an ESG score for a given company. Our goal is to reduce the cost of ESG data collection and make ESG information available to the general public. Contribution: Our contributions are three-fold: First, we publish a corpus of 432,411 news headlines annotated as being environmental-, governance-, social-related, or ESG-irrelevant. Second, we present our tool-supported approach called ESG-Miner capable of analyzing and evaluating headlines on corporate ESG-performance automatically. Third, we demonstrate the feasibility of our approach in an experiment and apply the ESG-Miner on 3000 manually labeled headlines. Our approach processes 96.7 % of the headlines correctly and shows a great performance in detecting environmental-related headlines along with their correct sentiment. We encourage fellow researchers and practitioners to use the ESG-Miner at https://www.esg-miner.com. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06540,December 2022,501,Computational aspects of the Helly property: a survey,"Mitre C. Dourado, Fábio Protti and Jayme L. Szwarcfiter","In 1923, Eduard Helly published his celebrated theorem, which originated the well known Helly property. Say that a family of subsets has the Helly property when every subfamily of it, formed by pairwise inters...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192385,February 2006,
502,0.000678989392134464,502,Interactive Process Identification and Selection from SAP ERP,"Julian Weber, Alessandro Berti, Gyunam Park, Majid Rafiei, Wil van der Aalst","SAP ERP is one of the most popular information systems supporting various organizational processes, e.g., O2C and P2P. However, the amount of processes and data contained in SAP ERP is enormous. Thus, the identification of the processes that are contained in a specific SAP instance, and the creation of a list of related tables is a significant challenge. Eventually, one needs to extract an event log for process mining purposes from SAP ERP. This demo paper shows the tool Interactive SAP Explorer that tackles the process identification and selection problem by encoding the relational structure of SAP ERP in a labeled property graph. Our approach allows asking complex process-related queries along with advanced representations of the relational structure. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06514,December 2022,502,Optimizing metric access methods for querying and mining complex data types,"Jessica Andressa de Souza, Humberto Luiz Razente and Maria Camila N Barioni","There are several application scenarios that can take advantage from the efficient processing of similarity operations in complex data types, such as multimedia data. Among them, it is possible to mention the ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-014-0017-5,13 September 2014,
503,0.000678989392134464,503,Graph Convolutional Networks for Traffic Forecasting with Missing Values,"Jingwei Zuo, Karine Zeitouni, Yehia Taher, Sandra Garcia-Rodriguez","Traffic forecasting has attracted widespread attention recently. In reality, traffic data usually contains missing values due to sensor or communication errors. The Spatio-temporal feature in traffic data brings more challenges for processing such missing values, for which the classic techniques (e.g., data imputations) are limited: 1) in temporal axis, the values can be randomly or consecutively missing; 2) in spatial axis, the missing values can happen on one single sensor or on multiple sensors simultaneously. Recent models powered by Graph Neural Networks achieved satisfying performance on traffic forecasting tasks. However, few of them are applicable to such a complex missing-value context. To this end, we propose GCN-M, a Graph Convolutional Network model with the ability to handle the complex missing values in the Spatio-temporal context. Particularly, we jointly model the missing value processing and traffic forecasting tasks, considering both local Spatio-temporal features and global historical patterns in an attention-based memory network. We propose as well a dynamic graph learning module based on the learned local-global features. The experimental results on real-life datasets show the reliability of our proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06419,December 2022,503,Industry quantum computing applications,"Andreas Bayerstadler, Guillaume Becquin, Julia Binder, Thierry Botter, Hans Ehm, Thomas Ehmer, Marvin Erdmann, Norbert Gaus, Philipp Harbach, Maximilian Hess, Johannes Klepsch, Martin Leib, Sebastian Luber, Andre Luckow, Maximilian Mansky, Wolfgang Mauerer…","Quantum computing promises to overcome computational limitations with better and faster solutions for optimization, simulation, and machine learning problems. Europe and Germany are in the process of successfu...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-021-00114-x,13 November 2021,
504,0.000678989392134464,504,"The cosmic DANCe of Perseus I: Membership, phase-space structure, mass, and energy distributions","J. Olivares, H. Bouy, N. Miret-Roig, P. A. B. Galli, L. M. Sarro, E. Moraux, A. Berihuete","Context. Star-forming regions are excellent benchmarks for testing and validating theories of star formation and stellar evolution. The Perseus star-forming region being one of the youngest (<10 Myr), closest (280-320 pc), and most studied in the literature, is a fundamental benchmark. Aims. We aim to study the membership, phase-space structure, mass, and energy (kinetic plus potential) distribution of the Perseus star-forming region using public catalogues (Gaia, APOGEE, 2MASS, PanSTARRS). Methods. We use Bayesian methodologies accounting for extinction to identify the Perseus physical groups in the phase-space, retrieve their candidate members, derive their properties (age, mass, 3D positions, 3D velocities, and energy), and attempt to reconstruct their origin. Results. We identify 1052 candidate members in seven physical groups (one of them new) with ages between 3 and 10 Myr, dynamical super-virial states, and large fractions of energetically unbound stars. Their mass distributions are broadly compatible with that of Chabrier for masses >0.1 $M_\odot$ and do not show hints of over-abundance of low-mass stars in NGC1333 with respect to IC348. These groups' ages, spatial structure, and kinematics are compatible with at least three generations of stars. Future work is still needed to clarify if the formation of the youngest was triggered by the oldest. Conclusions. The exquisite Gaia data complemented with public archives and mined with comprehensive Bayesian methodologies allow us to identify 31% more members than in previous studies, discover a new physical group (Gorgophone: 7 Myr, 191 members, and 145 $M_\odot$), and confirm that the spatial, kinematic, and energy distributions of these groups support the hierarchical star-formation scenario. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06291,December 2022,504,Water at solid surfaces: A review of selected theoretical aspects and experiments on the subject,M. Maccarini,"This review summarizes select aspects of the research on solid/water interfaces. Despite the considerable amount of work, the way water molecules are organized at the interface is still a source of debate. The...",https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.2768902,21 August 2007,
505,0.000678989392134464,505,Matrix Profile XXVII: A Novel Distance Measure for Comparing Long Time Series,"Audrey Der, Chin-Chia Michael Yeh, Renjie Wu, Junpeng Wang, Yan Zheng, Zhongfang Zhuang, Liang Wang, Wei Zhang, Eamonn Keogh","The most useful data mining primitives are distance measures. With an effective distance measure, it is possible to perform classification, clustering, anomaly detection, segmentation, etc. For single-event time series Euclidean Distance and Dynamic Time Warping distance are known to be extremely effective. However, for time series containing cyclical behaviors, the semantic meaningfulness of such comparisons is less clear. For example, on two separate days the telemetry from an athlete workout routine might be very similar. The second day may change the order in of performing push-ups and squats, adding repetitions of pull-ups, or completely omitting dumbbell curls. Any of these minor changes would defeat existing time series distance measures. Some bag-of-features methods have been proposed to address this problem, but we argue that in many cases, similarity is intimately tied to the shapes of subsequences within these longer time series. In such cases, summative features will lack discrimination ability. In this work we introduce PRCIS, which stands for Pattern Representation Comparison in Series. PRCIS is a distance measure for long time series, which exploits recent progress in our ability to summarize time series with dictionaries. We will demonstrate the utility of our ideas on diverse tasks and datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.06146,December 2022,505,Medusa: A Novel Stream-Scheduling Scheme for Parallel Video Servers,"Hai Jin, Dafu Deng and Liping Pang",Parallel video servers provide highly scalable video-on-demand service for a huge number of clients. The conventional stream-scheduling scheme does not use I/O and network bandwidth efficiently. Some other sch...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704310115,4 March 2004,
506,0.000678989392134464,506,Text Mining-Based Patent Analysis for Automated Rule Checking in AEC,"Zhe Zheng, Bo-Rui Kang, Qi-Tian Yuan, Yu-Cheng Zhou, Xin-Zheng Lu, Jia-Rui Lin","Automated rule checking (ARC), which is expected to promote the efficiency of the compliance checking process in the architecture, engineering, and construction (AEC) industry, is gaining increasing attention. Throwing light on the ARC application hotspots and forecasting its trends are useful to the related research and drive innovations. Therefore, this study takes the patents from the database of the Derwent Innovations Index database (DII) and China national knowledge infrastructure (CNKI) as data sources and then carried out a three-step analysis including (1) quantitative characteristics (i.e., annual distribution analysis) of patents, (2) identification of ARC topics using a latent Dirichlet allocation (LDA) and, (3) SNA-based co-occurrence analysis of ARC topics. The results show that the research hotspots and trends of Chinese and English patents are different. The contributions of this study have three aspects: (1) an approach to a comprehensive analysis of patents by integrating multiple text mining methods (i.e., SNA and LDA) is introduced ; (2) the application hotspots and development trends of ARC are reviewed based on patent analysis; and (3) a signpost for technological development and innovation of ARC is provided. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.05891,December 2022,506,Interdisciplinary technology assessment of service robots: the psychological/work science perspective,Martin Fischer,The article sheds light on psychological and work science aspects of the design and utilization of service robots. An initial presentation of the characteristics of man–robot interaction is followed by a discu...,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s10202-012-0113-6,27 November 2012,
507,0.000678989392134464,507,Scaling pattern mining through non-overlapping variable partitioning,"Leonardo Alexandre, Rafael S. Costa, Rui Henriques","Biclustering algorithms play a central role in the biotechnological and biomedical domains. The knowledge extracted supports the extraction of putative regulatory modules, essential to understanding diseases, aiding therapy research, and advancing biological knowledge. However, given the NP-hard nature of the biclustering task, algorithms with optimality guarantees tend to scale poorly in the presence of high-dimensionality data. To this end, we propose a pipeline for clustering-based vertical partitioning that takes into consideration both parallelization and cross-partition pattern merging needs. Given a specific type of pattern coherence, these clusters are built based on the likelihood that variables form those patterns. Subsequently, the extracted patterns per cluster are then merged together into a final set of closed patterns. This approach is evaluated using five published datasets. Results show that in some of the tested data, execution times yield statistically significant improvements when variables are clustered together based on the likelihood to form specific types of patterns, as opposed to partitions based on dissimilarity or randomness. This work offers a departuring step on the efficiency impact of vertical partitioning criteria along the different stages of pattern mining and biclustering algorithms. Availability: All the code is freely available at https://github.com/JupitersMight/pattern_merge under the MIT license. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.05340,December 2022,507,Enhancing spontaneous interaction in opportunistic mobile social networks,"Bin Guo, Daqing Zhang, Zhiwen Yu, Xingshe Zhou and Zhangbing Zhou","With the development of advanced mobile devices and social network services, mobile social network (MSN) has become popular in recent years. This paper we present a new perspective of MSN, the opportunistic MS...",https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-1-6,22 September 2012,
508,0.000678989392134464,508,Neural Bandits for Data Mining: Searching for Dangerous Polypharmacy,"Alexandre Larouche, Audrey Durand, Richard Khoury, Caroline Sirois","Polypharmacy, most often defined as the simultaneous consumption of five or more drugs at once, is a prevalent phenomenon in the older population. Some of these polypharmacies, deemed inappropriate, may be associated with adverse health outcomes such as death or hospitalization. Considering the combinatorial nature of the problem as well as the size of claims database and the cost to compute an exact association measure for a given drug combination, it is impossible to investigate every possible combination of drugs. Therefore, we propose to optimize the search for potentially inappropriate polypharmacies (PIPs). To this end, we propose the OptimNeuralTS strategy, based on Neural Thompson Sampling and differential evolution, to efficiently mine claims datasets and build a predictive model of the association between drug combinations and health outcomes. We benchmark our method using two datasets generated by an internally developed simulator of polypharmacy data containing 500 drugs and 100 000 distinct combinations. Empirically, our method can detect up to 72% of PIPs while maintaining an average precision score of 99% using 30 000 time steps. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.05190,December 2022,508,Towards a unified English technology-based writing curriculum in the Arabian Gulf countries: the case of Oman,Hussain Al Sharoufi,"This study investigates the efficacy of a new testing tool, a Web-based application known as the Academic Writing Wizard (AWW), in creating a unified English technology-based writing curriculum in the Arabian ...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-022-00178-1,31 August 2022,
509,0.000678989392134464,509,Mining CryptoNight-Haven on the Varium C1100 Blockchain Accelerator Card,"Lucas Bex, Furkan Turan, Michiel Van Beirendonck, Ingrid Verbauwhede",Cryptocurrency mining is an energy-intensive process that presents a prime candidate for hardware acceleration. This work-in-progress presents the first coprocessor design for the ASIC-resistant CryptoNight-Haven Proof of Work (PoW) algorithm. We construct our hardware accelerator as a Xilinx Run Time (XRT) RTL kernel targeting the Xilinx Varium C1100 Blockchain Accelerator Card. The design employs deeply pipelined computation and High Bandwidth Memory (HBM) for the underlying scratchpad data. We aim to compare our accelerator to existing CPU and GPU miners to show increased throughput and energy efficiency of its hash computations △ Less,https://arxiv.orghttps://arxiv.org/abs/2212.05033,December 2022,509,Climbing up from the abyss of problem gambling: a story of an international student in New Zealand,John Wong,Unknown,https://www.springeropen.com//ajgiph.springeropen.com/articles/10.1186/2195-3007-3-5,1 March 2013,
510,0.000678989392134464,510,Closed pattern mining of interval data and distributional data,"Henry Soldano, Guillaume Santini, Stella Zevio","We discuss pattern languages for closed pattern mining and learning of interval data and distributional data. We first introduce pattern languages relying on pairs of intersection-based constraints or pairs of inclusion based constraints, or both, applied to intervals. We discuss the encoding of such interval patterns as itemsets thus allowing to use closed itemsets mining and formal concept analysis programs. We experiment these languages on clustering and supervised learning tasks. Then we show how to extend the approach to address distributional data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.04849,December 2022,510,Quantum encryption with quantum permutation pad in IBMQ systems,Randy Kuang and Maria Perepechaenko,Quantum permutation pad or QPP is a quantum-safe symmetric cryptographic algorithm proposed by Kuang and Bettenburg in 2020. The theoretical foundation of QPP leverages the linear algebraic representations of ...,https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00145-y,12 October 2022,
511,0.000678989392134464,511,Introduction of an Assistance System to Support Domain Experts in Programming Low-code to Leverage Industry 5.0,"Eva-Maria Neumann, Birgit Vogel-Heuser, Fabian Haben, Marius Krueger, Timotheus Wieringa","The rapid technological leaps of Industry 4.0 increase the pressure and demands on humans working in automation, which is one of the main motivators of Industry 5.0. In particular, automation software development for mechatronic systems becomes increasingly challenging, as both domain knowledge and programming skills are required for high-quality, maintainable software. Especially for small companies from automation and robotics without dedicated software engineering departments, domain-specific low-code platforms become indispensable that enable domain experts to develop code intuitively using visual programming languages, e.g., for tasks such as retrofitting mobile machines. However, for extensive functionalities, visual programs may become overwhelming due to the scaling-up problem. In addition, the ever-shortening time-to-market increases the time pressure on programmers. Thus, an assistance system concept is introduced that can be implemented by low-code platform suppliers based on combining data mining and static code analysis. Domain experts are supported in developing low-code by targeted recommendations, metric-based complexity measurement, and reducing complexity by encapsulating functionalities. The concept is implemented for the industrial low-code platform HAWE eDesign to program hydraulic components in mobile machines, and its benefits are confirmed in a user study and an industrial expert workshop. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.04830,December 2022,511,From mechanism-based to data-driven approaches in materials science,Stefan Hiemer and Stefano Zapperi,A time-honored approach in theoretical materials science revolves around the search for basic mechanisms that should incorporate key feature of the phenomenon under investigation. Recent years have witnessed a...,https://www.springeropen.com//materialstheory.springeropen.com/articles/10.1186/s41313-021-00027-3,1 September 2021,
512,0.000678989392134464,512,Mining Explainable Predictive Features for Water Quality Management,"Conor Muldoon, Levent Görgü, John J. O'Sullivan, Wim G. Meijer, Gregory M. P. O'Hare","With water quality management processes, identifying and interpreting relationships between features, such as location and weather variable tuples, and water quality variables, such as levels of bacteria, is key to gaining insights and identifying areas where interventions should be made. There is a need for a search process to identify the locations and types of phenomena that are influencing water quality and a need to explain how the quality is being affected and which factors are most relevant. This paper addresses both of these issues. A process is developed for collecting data for features that represent a variety of variables over a spatial region and which are used for training models and inference. An analysis of the performance of the features is undertaken using the models and Shapley values. Shapley values originated in cooperative game theory and can be used to aid in the interpretation of machine learning results. Evaluations are performed using several machine learning algorithms and water quality data from the Dublin Grand Canal basin. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.04419,December 2022,512,Learning-Based Nonparametric Image Super-Resolution,"Shyamsundar Rajaram, Mithun Das Gupta, Nemanja Petrovic and Thomas S. Huang","We present a novel learning-based framework for zooming and recognizing images of digits obtained from vehicle registration plates, which have been blurred using an unknown kernel. We model the image as an und...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/51306,1 December 2006,
513,0.00673579331412596,513,MIME: Human-Aware 3D Scene Generation,"Hongwei Yi, Chun-Hao P. Huang, Shashank Tripathi, Lea Hering, Justus Thies, Michael J. Black","Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a ""scanner"" of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement. Code and data will be available for research at https://mime.is.tue.mpg.de. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.04360,December 2022,513,Tools for Protecting the Privacy of Specific Individuals in Video,"Datong Chen, Yi Chang, Rong Yan and Jie Yang","This paper presents a system for protecting the privacy of specific individuals in video recordings. We address the following two problems: automatic people identification with limited labeled data, and human ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/75427,1 December 2007,
514,0.000678989392134464,514,Persona-Based Conversational AI: State of the Art and Challenges,"Junfeng Liu, Christopher Symons, Ranga Raju Vatsavai","Conversational AI has become an increasingly prominent and practical application of machine learning. However, existing conversational AI techniques still suffer from various limitations. One such limitation is a lack of well-developed methods for incorporating auxiliary information that could help a model understand conversational context better. In this paper, we explore how persona-based information could help improve the quality of response generation in conversations. First, we provide a literature review focusing on the current state-of-the-art methods that utilize persona information. We evaluate two strong baseline methods, the Ranking Profile Memory Network and the Poly-Encoder, on the NeurIPS ConvAI2 benchmark dataset. Our analysis elucidates the importance of incorporating persona information into conversational systems. Additionally, our study highlights several limitations with current state-of-the-art methods and outlines challenges and future research directions for advancing personalized conversational AI technology. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.03699,December 2022,514,FPGA-Based Configurable Systolic Architecture for Window-Based Image Processing,César Torres-Huitzil and Miguel Arias-Estrada,Image processing requires more computational power and data throughput than most conventional processors can provide. Designing specific hardware can improve execution time and achieve better performance per u...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1024,22 May 2005,
515,0.000678989392134464,515,Time series numerical association rule mining variants in smart agriculture,"Iztok Fister Jr., Dušan Fister, Iztok Fister, Vili Podgorelec, Sancho Salcedo-Sanz","Numerical association rule mining offers a very efficient way of mining association rules, where algorithms can operate directly with categorical and numerical attributes. These methods are suitable for mining different transaction databases, where data are entered sequentially. However, little attention has been paid to the time series numerical association rule mining, which offers a new technique for extracting association rules from time series data. This paper presents a new algorithmic method for time series numerical association rule mining and its application in smart agriculture. We offer a concept of a hardware environment for monitoring plant parameters and a novel data mining method with practical experiments. The practical experiments showed the method's potential and opened the door for further extension. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.03669,December 2022,515,Can an AI learn political theory?,Stephen J. DeCanio,"Alan Turing’s 1950 paper, “Computing Machinery and Intelligence,” contains much more than its proposal of the “Turing Test.” Turing imagined the development of what we today call AI by a process akin to the ed...",https://www.springeropen.com//aiperspectives.springeropen.com/articles/10.1186/s42467-020-00007-2,7 October 2020,
516,0.000678989392134464,516,Robust convex biclustering with a tuning-free method,"Yifan Chen, Chunyin Lei, Chuanquan Li, Haiqiang Ma","Biclustering is widely used in different kinds of fields including gene information analysis, text mining, and recommendation system by effectively discovering the local correlation between samples and features. However, many biclustering algorithms will collapse when facing heavy-tailed data. In this paper, we propose a robust version of convex biclustering algorithm with Huber loss. Yet, the newly introduced robustification parameter brings an extra burden to selecting the optimal parameters. Therefore, we propose a tuning-free method for automatically selecting the optimal robustification parameter with high efficiency. The simulation study demonstrates the more fabulous performance of our proposed method than traditional biclustering methods when encountering heavy-tailed noise. A real-life biomedical application is also presented. The R package RcvxBiclustr is available at https://github.com/YifanChen3/RcvxBiclustr. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.03122,December 2022,516,Letter from the guest editors,"Manoel Mendonca, H. Guilherme Travassos and José Carlos Maldonado",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194491,October 2006,
517,0.000678989392134464,517,Causal Inference via Style Transfer for Out-of-distribution Generalisation,"Toan Nguyen, Kien Do, Duc Thanh Nguyen, Bao Duong, Thin Nguyen","Out-of-distribution (OOD) generalisation aims to build a model that can generalise well on an unseen target domain using knowledge from multiple source domains. To this end, the model should seek the causal dependence between inputs and labels, which may be determined by the semantics of inputs and remain invariant across domains. However, statistical or non-causal methods often cannot capture this dependence and perform poorly due to not considering spurious correlations learnt from model training via unobserved confounders. A well-known existing causal inference method like back-door adjustment cannot be applied to remove spurious correlations as it requires the observation of confounders. In this paper, we propose a novel method that effectively deals with hidden confounders by successfully implementing front-door adjustment (FA). FA requires the choice of a mediator, which we regard as the semantic information of images that helps access the causal mechanism without the need for observing confounders. Further, we propose to estimate the combination of the mediator with other observed images in the front-door formula via style transfer algorithms. Our use of style transfer to estimate FA is novel and sensible for OOD generalisation, which we justify by extensive experimental results on widely used benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.03063,December 2022,517,Correction to: A cloud-oriented siamese network object tracking algorithm with attention network and adaptive loss function,Jinping Sun and Dan Li,Theoriginal articlewas published inJournal of Cloud Computing202312:51,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00439-1,15 May 2023,
518,0.000678989392134464,518,Trajectory Flow Map: Graph-based Approach to Analysing Temporal Evolution of Aggregated Traffic Flows in Large-scale Urban Networks,"Jiwon Kim, Kai Zheng, Jonathan Corcoran, Sanghyung Ahn, Marty Papamanolis","This paper proposes a graph-based approach to representing spatio-temporal trajectory data that allows an effective visualization and characterization of city-wide traffic dynamics. With the advance of sensor, mobile, and Internet of Things (IoT) technologies, vehicle and passenger trajectories are being increasingly collected on a massive scale and are becoming a critical source of insight into traffic pattern and traveller behaviour. To leverage such trajectory data to better understand traffic dynamics in a large-scale urban network, this study develops a trajectory-based network traffic analysis method that converts individual trajectory data into a sequence of graphs that evolve over time (known as dynamic graphs or time-evolving graphs) and analyses network-wide traffic patterns in terms of a compact and informative graph-representation of aggregated traffic flows. First, we partition the entire network into a set of cells based on the spatial distribution of data points in individual trajectories, where the cells represent spatial regions between which aggregated traffic flows can be measured. Next, dynamic flows of moving objects are represented as a time-evolving graph, where regions are graph vertices and flows between them are treated as weighted directed edges. Given a fixed set of vertices, edges can be inserted or removed at every time step depending on the presence of traffic flows between two regions at a given time window. Once a dynamic graph is built, we apply graph mining algorithms to detect change-points in time, which represent time points where the graph exhibits significant changes in its overall structure and, thus, correspond to change-points in city-wide mobility pattern throughout the day (e.g., global transition points between peak and off-peak periods). △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.02927,December 2022,518,Correction to: Severity: a QoS-aware approach to cloud application elasticity,"Andreas Tsagkaropoulos, Yiannis Verginadis, Nikos Papageorgiou, Fotis Paraskevopoulos, Dimitris Apostolou and Gregoris Mentzas",Theoriginal articlewas published inJournal of Cloud Computing202110:45,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-021-00266-2,13 September 2021,
519,0.000678989392134464,519,Enhancing Data-Awareness of Object-Centric Event Logs,"Alexandre Goossens, Johannes De Smedt, Jan Vanthienen, Wil van der Aalst","When multiple objects are involved in a process, there is an opportunity for processes to be discovered from different angles with new information that previously might not have been analyzed from a single object point of view. This does require that all the information of event/object attributes and their values are stored within logs including attributes that have a list of values or attributes with values that change over time. It also requires that attributes can unambiguously be linked to an object, an event or both. As such, object-centric event logs are an interesting development in process mining as they support the presence of multiple types of objects. First, this paper shows that the current object-centric event log formats do not support the aforementioned aspects to their full potential since the possibility to support dynamic object attributes (attributes with changing values) is not supported by existing formats. Next, this paper introduces a novel enriched object-centric event log format tackling the aforementioned issues alongside an algorithm that automatically translates XES logs to this Data-aware OCEL (DOCEL) format. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.02858,December 2022,519,ESICM LIVES 2021: Part 2,Unknown,"This article is part of a Supplement:Volume 9
                                        Supplement 1",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-021-00415-6,11 October 2021,
520,0.000678989392134464,520,Interdisciplinary Discovery of Nanomaterials Based on Convolutional Neural Networks,"Tong Xie, Yuwei Wan, Weijian Li, Qingyuan Linghu, Shaozhou Wang, Yalun Cai, Han Liu, Chunyu Kit, Clara Grazian, Bram Hoex","The material science literature contains up-to-date and comprehensive scientific knowledge of materials. However, their content is unstructured and diverse, resulting in a significant gap in providing sufficient information for material design and synthesis. To this end, we used natural language processing (NLP) and computer vision (CV) techniques based on convolutional neural networks (CNN) to discover valuable experimental-based information about nanomaterials and synthesis methods in energy-material-related publications. Our first system, TextMaster, extracts opinions from texts and classifies them into challenges and opportunities, achieving 94% and 92% accuracy, respectively. Our second system, GraphMaster, realizes data extraction of tables and figures from publications with 98.3\% classification accuracy and 4.3% data extraction mean square error. Our results show that these systems could assess the suitability of materials for a certain application by evaluation of synthesis insights and case analysis with detailed references. This work offers a fresh perspective on mining knowledge from scientific literature, providing a wide swatch to accelerate nanomaterial research through CNN. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.02805,December 2022,520,Real-Time Multi-Step View Reconstruction for a Virtual Teleconference System,B. J. Lei and E. A. Hendriks,We propose a real-time multi-step view reconstruction algorithm and we tune its implementation to a virtual teleconference application. Theoretical motivations and practical implementation issues of the algori...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206071,22 October 2002,
521,0.000678989392134464,521,MUS-CDB: Mixed Uncertainty Sampling with Class Distribution Balancing for Active Annotation in Aerial Object Detection,"Dong Liang, Jing-Wei Zhang, Ying-Peng Tang, Sheng-Jun Huang","Recent aerial object detection models rely on a large amount of labeled training data, which requires unaffordable manual labeling costs in large aerial scenes with dense objects. Active learning is effective in reducing the data labeling cost by selectively querying the informative and representative unlabelled samples. However, existing active learning methods are mainly with class-balanced setting and image-based querying for generic object detection tasks, which are less applicable to aerial object detection scenario due to the long-tailed class distribution and dense small objects in aerial scenes. In this paper, we propose a novel active learning method for cost-effective aerial object detection. Specifically, both object-level and image-level informativeness are considered in the object selection to refrain from redundant and myopic querying. Besides, an easy-to-use class-balancing criterion is incorporated to favor the minority objects to alleviate the long-tailed class distribution problem in model training. To fully utilize the queried information, we further devise a training loss to mine the latent knowledge in the undiscovered image regions. Extensive experiments are conducted on the DOTA-v1.0 and DOTA-v2.0 benchmarks to validate the effectiveness of the proposed method. The results show that it can save more than 75% of the labeling cost to reach the same performance compared to the baselines and state-of-the-art active object detection methods. Code is available at \href{https://github.com/ZJW700/MUS-CDB}{\textit{https://github.com/ZJW700/MUS-CDB}}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.02804,December 2022,521,Mathematical e-Learning,"Ángel A. Juan, María Antonia Huertas, Hans Cuypers and Birgit Loch",Unknown,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.7238/rusc.v9i1.1431,19 December 2015,
522,0.000678989392134464,522,Approximate Order-Preserving Pattern Mining for Time Series,"Yan Li, Jin Liu, Yingchun Guo, Jing Liu, Youxi Wu","The order-preserving pattern mining can be regarded as discovering frequent trends in time series, since the same order-preserving pattern has the same relative order which can represent a trend. However, in the case where data noise is present, the relative orders of many meaningful patterns are usually similar rather than the same. To mine similar relative orders in time series, this paper addresses an approximate order-preserving pattern (AOP) mining method based on (delta-gamma) distance to effectively measure the similarity, and proposes an algorithm called AOP-Miner to mine AOPs according to global and local approximation parameters. AOP-Miner adopts a pattern fusion strategy to generate candidate patterns generation and employs the screening strategy to calculate the supports of candidate patterns. Experimental results validate that AOP-Miner outperforms other competitive methods and can find more similar trends in time series. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.01995,December 2022,522,Link weights recovery in heterogeneous information networks,Hong-Lan Botterman and Robin Lamarche-Perrin,"Socio-technical systems usually consist of many intertwined networks, each connecting different types of objects or actors through a variety of means. As these networks are co-dependent, one can take advantage...",https://www.springeropen.com//computationalsocialnetworks.springeropen.com/articles/10.1186/s40649-020-00083-8,23 March 2021,
523,0.000678989392134464,523,GraphGDP: Generative Diffusion Processes for Permutation Invariant Graph Generation,"Han Huang, Leilei Sun, Bowen Du, Yanjie Fu, Weifeng Lv","Graph generative models have broad applications in biology, chemistry and social science. However, modelling and understanding the generative process of graphs is challenging due to the discrete and high-dimensional nature of graphs, as well as permutation invariance to node orderings in underlying graph distributions. Current leading autoregressive models fail to capture the permutation invariance nature of graphs for the reliance on generation ordering and have high time complexity. Here, we propose a continuous-time generative diffusion process for permutation invariant graph generation to mitigate these issues. Specifically, we first construct a forward diffusion process defined by a stochastic differential equation (SDE), which smoothly converts graphs within the complex distribution to random graphs that follow a known edge probability. Solving the corresponding reverse-time SDE, graphs can be generated from newly sampled random graphs. To facilitate the reverse-time SDE, we newly design a position-enhanced graph score network, capturing the evolving structure and position information from perturbed graphs for permutation equivariant score estimation. Under the evaluation of comprehensive metrics, our proposed generative diffusion process achieves competitive performance in graph distribution learning. Experimental results also show that GraphGDP can generate high-quality graphs in only 24 function evaluations, much faster than previous autoregressive models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.01842,December 2022,523,Ergonomic risk factors and musculoskeletal disorders in bank staff: an interventional follow-up study in Iran,"Majid Motamedzadeh, Mahdi Jalali, Rostam Golmohammadi, Javad Faradmal, Hamid Reza Zakeri and Iman Nasiri","Long-term use of computer in a static mode may cause musculoskeletal disorders (MSDs) in bank staff. Considering the high number of bank employees in different countries, such as Iran, the risk factors of thes...",https://www.springeropen.com//jepha.springeropen.com/articles/10.1186/s42506-021-00097-8,11 December 2021,
524,0.000678989392134464,524,Agent Miner: An Algorithm for Discovering Agent Systems from Event Data,"Andrei Tour, Artem Polyvyanyy, Anna Kalenkova, Arik Senderovich","The problem of process discovery in process mining studies ways to construct process models that encode business processes that induced event data recorded by IT systems. Most existing discovery algorithms are concerned with constructing models that represent the control flow of the processes. Agent system mining argues that business processes often emerge from interactions of autonomous agents and uses event data to construct models of the agents and their interactions. This paradigm shift from the control flow to agent system discovery proves beneficial when interacting agents have produced the underlying data. This paper presents an algorithm, called Agent Miner, for discovering models of agents and their interactions that compose the system that has generated the business processes recorded in the input event data. The conducted evaluation using our open-source implementation of Agent Miner over publicly available industrial datasets confirms that the approach can unveil insights into the process participants and their interaction patterns and often discovers models that describe the data more accurately in terms of precision and recall and are smaller in size than the corresponding models discovered using conventional discovery algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.01454,December 2022,524,Editorial,Riccardo Poli and Stefano Cagnoni,Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703002841,21 July 2003,
525,0.000678989392134464,525,Twitter Data Analysis: Izmir Earthquake Case,"Özgür Agrali, Hakan Sökün, Enis Karaarslan","Türkiye is located on a fault line; earthquakes often occur on a large and small scale. There is a need for effective solutions for gathering current information during disasters. We can use social media to get insight into public opinion. This insight can be used in public relations and disaster management. In this study, Twitter posts on Izmir Earthquake that took place on October 2020 are analyzed. We question if this analysis can be used to make social inferences on time. Data mining and natural language processing (NLP) methods are used for this analysis. NLP is used for sentiment analysis and topic modelling. The latent Dirichlet Allocation (LDA) algorithm is used for topic modelling. We used the Bidirectional Encoder Representations from Transformers (BERT) model working with Transformers architecture for sentiment analysis. It is shown that the users shared their goodwill wishes and aimed to contribute to the initiated aid activities after the earthquake. The users desired to make their voices heard by competent institutions and organizations. The proposed methods work effectively. Future studies are also discussed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.01453,December 2022,525,A Machine Learning Approach to Automatic Music Genre Classification,"Carlos N. Silla, Alessandro L. Koerich and Celso A. A. Kaestner","This paper presents a non-conventional approach for the automatic music genre classification problem. The proposed approach uses multiple feature vectors and a pattern recognition ensemble approach, according ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192561,September 2008,
526,0.000678989392134464,526,DeepFT: Fault-Tolerant Edge Computing using a Self-Supervised Deep Surrogate Model,"Shreshth Tuli, Giuliano Casale, Ludmila Cherkasova, Nicholas R. Jennings","The emergence of latency-critical AI applications has been supported by the evolution of the edge computing paradigm. However, edge solutions are typically resource-constrained, posing reliability challenges due to heightened contention for compute and communication capacities and faulty application behavior in the presence of overload conditions. Although a large amount of generated log data can be mined for fault prediction, labeling this data for training is a manual process and thus a limiting factor for automation. Due to this, many companies resort to unsupervised fault-tolerance models. Yet, failure models of this kind can incur a loss of accuracy when they need to adapt to non-stationary workloads and diverse host characteristics. To cope with this, we propose a novel modeling approach, called DeepFT, to proactively avoid system overloads and their adverse effects by optimizing the task scheduling and migration decisions. DeepFT uses a deep surrogate model to accurately predict and diagnose faults in the system and co-simulation based self-supervised learning to dynamically adapt the model in volatile settings. It offers a highly scalable solution as the model size scales by only 3 and 1 percent per unit increase in the number of active tasks and hosts. Extensive experimentation on a Raspberry-Pi based edge cluster with DeFog benchmarks shows that DeepFT can outperform state-of-the-art baseline methods in fault-detection and QoS metrics. Specifically, DeepFT gives the highest F1 scores for fault-detection, reducing service deadline violations by up to 37\% while also improving response time by up to 9%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.01302,December 2022,526,Automated Quality Assurance Applied to Mammographic Imaging,"Lilian Blot, Anne Davis, Mike Holubinka, Robert Martí and Reyer Zwiggelaar","Quality control in mammography is based upon subjective interpretation of the image quality of a test phantom. In order to suppress subjectivity due to the human observer, automated computer analysis of the Le...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702203029,24 July 2002,
527,0.000678989392134464,527,Explainable Artificial Intelligence for Improved Modeling of Processes,"Riza Velioglu, Jan Philip Göpfert, André Artelt, Barbara Hammer","In modern business processes, the amount of data collected has increased substantially in recent years. Because this data can potentially yield valuable insights, automated knowledge extraction based on process mining has been proposed, among other techniques, to provide users with intuitive access to the information contained therein. At present, the majority of technologies aim to reconstruct explicit business process models. These are directly interpretable but limited concerning the integration of diverse and real-valued information sources. On the other hand, Machine Learning (ML) benefits from the vast amount of data available and can deal with high-dimensional sources, yet it has rarely been applied to being used in processes. In this contribution, we evaluate the capability of modern Transformer architectures as well as more classical ML technologies of modeling process regularities, as can be quantitatively evaluated by their prediction capability. In addition, we demonstrate the capability of attentional properties and feature relevance determination by highlighting features that are crucial to the processes' predictive abilities. We demonstrate the efficacy of our approach using five benchmark datasets and show that the ML models are capable of predicting critical outcomes and that the attention mechanisms or XAI components offer new insights into the underlying processes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.00695,December 2022,527,The interrelationship between concepts about agency and students’ use of teachable-agent learning technology,"Christopher Brett Jaeger, Alicia M. Hymel, Daniel T. Levin, Gautam Biswas, Natalie Paul and John Kinnebrew","To successfully interact with software agents, people must call upon basic concepts about goals and intentionality and strategically deploy these concepts in a range of circumstances where specific entailments...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-019-0163-6,18 April 2019,
528,0.000678989392134464,528,An Empirical Study on the Bugs Found while Reusing Pre-trained Natural Language Processing Models,"Rangeet Pan, Sumon Biswas, Mohna Chakraborty, Breno Dantas Cruz, Hridesh Rajan","In NLP, reusing pre-trained models instead of training from scratch has gained popularity; however, NLP models are mostly black boxes, very large, and often require significant resources. To ease, models trained with large corpora are made available, and developers reuse them for different problems. In contrast, developers mostly build their models from scratch for traditional DL-related problems. By doing so, they have control over the choice of algorithms, data processing, model structure, tuning hyperparameters, etc. Whereas in NLP, due to the reuse of the pre-trained models, NLP developers are limited to little to no control over such design decisions. They either apply tuning or transfer learning on pre-trained models to meet their requirements. Also, NLP models and their corresponding datasets are significantly larger than the traditional DL models and require heavy computation. Such reasons often lead to bugs in the system while reusing the pre-trained models. While bugs in traditional DL software have been intensively studied, the nature of extensive reuse and black-box structure motivates us to understand the different types of bugs that occur while reusing NLP models? What are the root causes of those bugs? How do these bugs affect the system? To answer these questions, We studied the bugs reported while reusing the 11 popular NLP models. We mined 9,214 issues from GitHub repositories and identified 984 bugs. We created a taxonomy with bug types, root causes, and impacts. Our observations led to several findings, including limited access to model internals resulting in a lack of robustness, lack of input validation leading to the propagation of algorithmic and data bias, and high-resource consumption causing more crashes, etc. Our observations suggest several bug patterns, which would greatly facilitate further efforts in reducing bugs in pre-trained models and code reuse. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.00105,December 2022,528,Exploring the impact of Artificial Intelligence and robots on higher education through literature-based design fictions,A. M. Cox,"Artificial Intelligence (AI) and robotics are likely to have a significant long-term impact on higher education (HE). The scope of this impact is hard to grasp partly because the literature is siloed, as well ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00237-8,18 January 2021,
529,0.000678989392134464,529,Resolving Uncertain Case Identifiers in Interaction Logs: A User Study,"Marco Pegoraro, Merih Seran Uysal, Tom-Hendrik Hülsmann, Wil M. P. van der Aalst","Modern software systems are able to record vast amounts of user actions, stored for later analysis. One of the main types of such user interaction data is click data: the digital trace of the actions of a user through the graphical elements of an application, website or software. While readily available, click data is often missing a case notion: an attribute linking events from user interactions to a specific process instance in the software. In this paper, we propose a neural network-based technique to determine a case notion for click data, thus enabling process mining and other process analysis techniques on user interaction data. We describe our method, show its scalability to datasets of large dimensions, and we validate its efficacy through a user study based on the segmented event log resulting from interaction data of a mobility sharing company. Interviews with domain experts in the company demonstrate that the case notion obtained by our method can lead to actionable process insights. △ Less",https://arxiv.orghttps://arxiv.org/abs/2212.00009,December 2022,529,A Method for Assessment of Segmentation Success Considering Uncertainty in the Edge Positions,"Rubén Usamentiaga, Daniel F García, Carlos López and Diego González",A method for segmentation assessment is proposed. The technique is based on a comparison of the segmentation produced by an algorithm with an ideal segmentation. The procedure to obtain the ideal segmentation ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/21746,1 December 2006,
530,0.000678989392134464,530,AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning,"Ling Luo, Chih-Hsuan Wei, Po-Ting Lai, Robert Leaman, Qingyu Chen, Zhiyong Lu","Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favorably to other state-of-the-art approaches such as multi-task learning. We further demonstrate the practical utility of AIONER in three independent tasks to recognize entity types not previously seen in training data, as well as the advantages of AIONER over existing methods for processing biomedical text at a large scale (e.g., the entire PubMed data). △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.16944,November 2022,530,Technological innovation in architecture and engineering education - an investigation on three generations from Turkey,Hasan Gokberk Bayhan and Ece Karaca,"The developments in technology have caused many radical changes in the curriculum of architecture and engineering (a/e) disciplines. At the same time, generations and their personal characteristics are in cont...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00207-0,7 August 2020,
531,0.000678989392134464,531,I Will Survive: An Online Conformance Checking Algorithm Using Decay Time,"Kristo Raun, Ahmed Awad","Process executions in organizations generate a large variety of data. Process mining is a data-driven analytical approach for analyzing this data from a business process point of view. Online conformance checking deals with finding discrepancies between real-life and modeled process behavior on data streams. The current state-of-the-art output of online conformance checking is a prefix-alignment, which is used for pinpointing the exact deviations in terms of the trace and the model while accommodating a trace's unknown termination in a streaming setting. However, producing prefix-alignments entails a state space search to find the shortest path from a common start state to a common end state between the trace and the model. This is computationally expensive and makes the method infeasible in an online setting. Previously, the trie data structure has been shown to be efficient for constructing alignments, utilizing a proxy log representing the process model in a finite way. This paper introduces a new approximate algorithm (IWS) on top of the trie for online conformance checking. The algorithm is shown to be fast, memory-efficient, and able to output both a prefix and a complete alignment event-by-event while keeping track of previously seen cases and their state. Comparative analysis against the current state-of-the-art algorithm for finding prefix-alignments shows that the IWS algorithm achieves, in some cases, an order of magnitude faster execution time while having a smaller error cost. In extreme cases, the IWS finds prefix-alignments roughly three orders of magnitude faster than the current state of the art. The IWS algorithm includes a discounted decay time setting for efficient memory usage and a look-ahead limit for improving computation time. Finally, the algorithm is stress tested for performance using a simulation of high-traffic event streams. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.16702,November 2022,531,Methodological Considerations for Furthering the Understanding of Constraints in Applied Sports,"Peter Browne, Alice J. Sweeting, Carl T. Woods and Sam Robertson","Commonly classified as individual, task or environmental, constraints are boundaries which shape the emergence of functional movement solutions. In applied sport, an ongoing challenge is to improve the measure...",https://www.springeropen.com//sportsmedicine-open.springeropen.com/articles/10.1186/s40798-021-00313-x,1 April 2021,
532,0.000678989392134464,532,Is Twitter Enough? Investigating Situational Awareness in Social and Print Media during the Second COVID-19 Wave in India,"Ishita Vohra, Meher Shashwat Nigam, Aryan Sakaria, Amey Kudari, Nimmi Rangaswamy","The pandemic required efficient allocation of public resources and transforming existing ways of societal functions. To manage any crisis, governments and public health researchers exploit the information available to them in order to make informed decisions, also defined as situational awareness. Gathering situational awareness using social media has been functional to manage epidemics. Previous research focused on using discussions during periods of epidemic crises on social media platforms like Twitter, Reddit, or Facebook and developing NLP techniques to filter out relevant discussions from a huge corpus of messages and posts. Social media usage varies with internet penetration and other socioeconomic factors, which might induce disparity in analyzing discussions across different geographies. However, print media is a ubiquitous information source, irrespective of geography. Further, topics discussed in news articles are already newsworthy, while on social media newsworthiness is a product of techno-social processes. Developing this fundamental difference, we study Twitter data during the second wave in India focused on six high-population cities with varied macroeconomic factors. Through a mixture of qualitative and quantitative methods, we further analyze two Indian newspapers during the same period and compare topics from both Twitter and the newspapers to evaluate situational awareness around the second phase of COVID on each of these platforms. We conclude that factors like internet penetration and GDP in a specific city influence the discourse surrounding situational updates on social media. Thus, augmenting information from newspapers with information extracted from social media would provide a more comprehensive perspective in resource deficit cities. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.16360,November 2022,532,Correction to: Framework for automatically suggesting remedial actions to help students at risk based on explainable ML and rulebased models,"Balqis Albreiki, Tetiana Habuza and Nazar Zaki",Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education202219:49,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00367-1,7 October 2022,
533,0.000678989392134464,533,How Many Tweets DoWe Need?: Efficient Mining of Short-Term Polarized Topics on Twitter: A Case Study From Japan,"Tomoki Fukuma, Koki Noda, Hiroki Kumagai, Hiroki Yamamoto, Yoshiharu Ichikawa, Kyosuke Kambe, Yu Maubuchi, Fujio Toriumi","In recent years, social media has been criticized for yielding polarization. Identifying emerging disagreements and growing polarization is important for journalists to create alerts and provide more balanced coverage. While recent studies have shown the existence of polarization on social media, they primarily focused on limited topics such as politics with a large volume of data collected in the long term, especially over months or years. While these findings are helpful, they are too late to create an alert immediately. To address this gap, we develop a domain-agnostic mining method to identify polarized topics on Twitter in a short-term period, namely 12 hours. As a result, we find that daily Japanese news-related topics in early 2022 were polarized by 31.6\% within a 12-hour range. We also analyzed that they tend to construct information diffusion networks with a relatively high average degree, and half of the tweets are created by a relatively small number of people. However, it is very costly and impractical to collect a large volume of tweets daily on many topics and monitor the polarization due to the limitations of the Twitter API. To make it more cost-efficient, we also develop a prediction method using machine learning techniques to estimate the polarization level using randomly collected tweets leveraging the network information. Extensive experiments show a significant saving in collection costs compared to baseline methods. In particular, our approach achieves F-score of 0.85, requiring 4,000 tweets, 4x savings than the baseline. To the best of our knowledge, our work is the first to predict the polarization level of the topics with low-resource tweets. Our findings have profound implications for the news media, allowing journalists to detect and disseminate polarizing information quickly and efficiently. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.16305,November 2022,533,Learning data science in elementary school mathematics: a comparative curriculum analysis,"Yook Kit Ow-Yeong, Ibrahim H. Yeter and Farhan Ali",Data literacy is increasingly important in today’s data-driven world. Students across many educational systems first formally learn about data in elementary school not as a separate subject but via the mathema...,https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00397-9,30 January 2023,
534,0.000678989392134464,534,DCDetector: An IoT terminal vulnerability mining system based on distributed deep ensemble learning under source code representation,Wen Zhou,"Context: The IoT system infrastructure platform facility vulnerability attack has become the main battlefield of network security attacks. Most of the traditional vulnerability mining methods rely on vulnerability detection tools to realize vulnerability discovery. However, due to the inflexibility of tools and the limitation of file size, its scalability It is relatively low and cannot be applied to large-scale power big data fields. Objective: The goal of the research is to intelligently detect vulnerabilities in source codes of high-level languages such as C/C++. This enables us to propose a code representation of sensitive sentence-related slices of source code, and to detect vulnerabilities by designing a distributed deep ensemble learning model. Method: In this paper, a new directional vulnerability mining method of parallel ensemble learning is proposed to solve the problem of large-scale data vulnerability mining. By extracting sensitive functions and statements, a sensitive statement library of vulnerable codes is formed. The AST stream-based vulnerability code slice with higher granularity performs doc2vec sentence vectorization on the source code through the random sampling module, obtains different classification results through distributed training through the Bi-LSTM trainer, and obtains the final classification result by voting. Results: This method designs and implements a distributed deep ensemble learning system software vulnerability mining system called DCDetector. It can make accurate predictions by using the syntactic information of the code, and is an effective method for analyzing large-scale vulnerability data. Conclusion: Experiments show that this method can reduce the false positive rate of traditional static analysis and improve the performance and accuracy of machine learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.16235,November 2022,534,A systematic review on trends in using Moodle for teaching and learning,"Sithara H. P. W. Gamage, Jennifer R. Ayres and Monica B. Behrend","The Moodle Learning Management System (LMS) is widely used in online teaching and learning, especially in STEM education. However, educational research on using Moodle is scattered throughout the literature. T...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-021-00323-x,25 January 2022,
535,0.000678989392134464,535,A Survey of Relevant Text Mining Technology,"Claudia Peersman, Matthew Edwards, Emma Williams, Awais Rashid","Recent advances in text mining and natural language processing technology have enabled researchers to detect an authors identity or demographic characteristics, such as age and gender, in several text genres by automatically analysing the variation of linguistic characteristics. However, applying such techniques in the wild, i.e., in both cybercriminal and regular online social media, differs from more general applications in that its defining characteristics are both domain and process dependent. This gives rise to a number of challenges of which contemporary research has only scratched the surface. More specifically, a text mining approach applied on social media communications typically has no control over the dataset size, the number of available communications will vary across users. Hence, the system has to be robust towards limited data availability. Additionally, the quality of the data cannot be guaranteed. As a result, the approach needs to be tolerant to a certain degree of linguistic noise (for example, abbreviations, non-standard language use, spelling variations and errors). Finally, in the context of cybercriminal fora, it has to be robust towards deceptive or adversarial behaviour, i.e. offenders who attempt to hide their criminal intentions (obfuscation) or who assume a false digital persona (imitation), potentially using coded language. In this work we present a comprehensive survey that discusses the problems that have already been addressed in current literature and review potential solutions. Additionally, we highlight which areas need to be given more attention. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.15784,November 2022,535,Secure Steganography in Multimedia Content,Miroslav Goljan and Andreas Westfeld,Unknown,https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1155/2009/257131,5 May 2009,
536,0.000678989392134464,536,Analysis on English Vocabulary Appearance Pattern in Korean CSAT,"Byunghyun Ban, Jejong Lee, Hyeonmok Hwang","A text-mining-based word class categorization method and LSTM-based vocabulary pattern prediction method are introduced in this paper. A preprocessing method based on simple text appearance frequency analysis is first described. This method was developed as a data screening tool but showed 4.35 ~ 6.21 times higher than previous works. An LSTM deep learning method is also suggested for vocabulary appearance pattern prediction method. AI performs a regression with various size of data window of previous exams to predict the probabilities of word appearance in the next exam. Predicted values of AI over various data windows are processed into a single score as a weighted sum, which we call an ""AI-Score"", which represents the probability of word appearance in next year's exam. Suggested method showed 100% accuracy at the range 100-score area and showed only 1.7% error of prediction in the section where the scores were over 60 points. All source codes are freely available at the authors' Git Hub repository. (https://github.com/needleworm/bigdata_voca) △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.15426,November 2022,536,A new insight to the analysis of co-authorship in Google Scholar,"Ghazal Kalhor, Amin Asadi Sarijalou, Niloofar Sharifi Sadr and Behnam Bahrak","Google Scholar is a search engine for scholarly literature which indexes most academic papers, dissertations, and books that are available online. This paper aims to analyze the characteristics of the manually...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-022-00460-4,8 April 2022,
537,0.000678989392134464,537,Automating Systematic Literature Reviews with Natural Language Processing and Text Mining: a Systematic Literature Review,"Girish Sundaram, Daniel Berleant","Objectives: An SLR is presented focusing on text mining based automation of SLR creation. The present review identifies the objectives of the automation studies and the aspects of those steps that were automated. In so doing, the various ML techniques used, challenges, limitations and scope of further research are explained. Methods: Accessible published literature studies that primarily focus on automation of study selection, study quality assessment, data extraction and data synthesis portions of SLR. Twenty-nine studies were analyzed. Results: This review identifies the objectives of the automation studies, steps within the study selection, study quality assessment, data extraction and data synthesis portions that were automated, the various ML techniques used, challenges, limitations and scope of further research. Discussion: We describe uses of NLP/TM techniques to support increased automation of systematic literature reviews. This area has attracted increase attention in the last decade due to significant gaps in the applicability of TM to automate steps in the SLR process. There are significant gaps in the application of TM and related automation techniques in the areas of data extraction, monitoring, quality assessment and data synthesis. There is thus a need for continued progress in this area, and this is expected to ultimately significantly facilitate the construction of systematic literature reviews. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.15397,November 2022,537,Computational modeling for parallel grid-based recursive Bayesian estimation: parallel computation using graphics processing unit,"Xianqiao Tong, Tomonari Furukawa and Hugh Durrant-Whyte","This paper presents the performance modeling of the real-time grid-based recursive Bayesian estimation (RBE), particularly the parallel computation using graphics processing unit (GPU). The proposed modeling f...",https://www.springeropen.com//juaa-journal.springeropen.com/articles/10.1186/2195-5468-1-15,16 December 2013,
538,0.000678989392134464,538,GADMSL: Graph Anomaly Detection on Attributed Networks via Multi-scale Substructure Learning,"Jingcan Duan, Siwei Wang, Xinwang Liu, Haifang Zhou, Jingtao Hu, Hu Jin","Recently, graph anomaly detection has attracted increasing attention in data mining and machine learning communities. Apart from existing attribute anomalies, graph anomaly detection also captures suspicious topological-abnormal nodes that differ from the major counterparts. Although massive graph-based detection approaches have been proposed, most of them focus on node-level comparison while pay insufficient attention on the surrounding topology structures. Nodes with more dissimilar neighborhood substructures have more suspicious to be abnormal. To enhance the local substructure detection ability, we propose a novel Graph Anomaly Detection framework via Multi-scale Substructure Learning (GADMSL for abbreviation). Unlike previous algorithms, we manage to capture anomalous substructures where the inner similarities are relatively low in dense-connected regions. Specifically, we adopt a region proposal module to find high-density substructures in the network as suspicious regions. Their inner-node embedding similarities indicate the anomaly degree of the detected substructures. Generally, a lower degree of embedding similarities means a higher probability that the substructure contains topology anomalies. To distill better embeddings of node attributes, we further introduce a graph contrastive learning scheme, which observes attribute anomalies in the meantime. In this way, GADMSL can detect both topology and attribute anomalies. Ultimately, extensive experiments on benchmark datasets show that GADMSL greatly improves detection performance (up to 7.30% AUC and 17.46% AUPRC gains) compared to state-of-the-art attributed networks anomaly detection algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.15255,November 2022,538,Feature Extraction Methods for Real-Time Face Detection and Classification,"David Masip, Marco Bressan and Jordi Vitrià",We propose a complete scheme for face detection and recognition. We have used a Bayesian classifier for face detection and a nearest neighbor approach for face classification. To improve the performance of the...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2061,15 August 2005,
539,0.000678989392134464,539,Long-tail Cross Modal Hashing,"Zijun Gao, Jun Wang, Guoxian Yu, Zhongmin Yan, Carlotta Domeniconi, Jinglin Zhang","Existing Cross Modal Hashing (CMH) methods are mainly designed for balanced data, while imbalanced data with long-tail distribution is more general in real-world. Several long-tail hashing methods have been proposed but they can not adapt for multi-modal data, due to the complex interplay between labels and individuality and commonality information of multi-modal data. Furthermore, CMH methods mostly mine the commonality of multi-modal data to learn hash codes, which may override tail labels encoded by the individuality of respective modalities. In this paper, we propose LtCMH (Long-tail CMH) to handle imbalanced multi-modal data. LtCMH firstly adopts auto-encoders to mine the individuality and commonality of different modalities by minimizing the dependency between the individuality of respective modalities and by enhancing the commonality of these modalities. Then it dynamically combines the individuality and commonality with direct features extracted from respective modalities to create meta features that enrich the representation of tail labels, and binaries meta features to generate hash codes. LtCMH significantly outperforms state-of-the-art baselines on long-tail datasets and holds a better (or comparable) performance on datasets with balanced labels. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.15162,November 2022,539,Spatio-temporal Background Models for Outdoor Surveillance,Robert Pless,"Video surveillance in outdoor areas is hampered by consistent background motion which defeats systems that use motion to identify intruders. While algorithms exist for masking out regions with motion, a better...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2281,25 August 2005,
540,0.000678989392134464,540,Estimation of a Causal Directed Acyclic Graph Process using Non-Gaussianity,"Aref Einizade, Sepideh Hajipour Sardouie","Numerous approaches have been proposed to discover causal dependencies in machine learning and data mining; among them, the state-of-the-art VAR-LiNGAM (short for Vector Auto-Regressive Linear Non-Gaussian Acyclic Model) is a desirable approach to reveal both the instantaneous and time-lagged relationships. However, all the obtained VAR matrices need to be analyzed to infer the final causal graph, leading to a rise in the number of parameters. To address this issue, we propose the CGP-LiNGAM (short for Causal Graph Process-LiNGAM), which has significantly fewer model parameters and deals with only one causal graph for interpreting the causal relations by exploiting Graph Signal Processing (GSP). △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.13800,November 2022,540,Embedded Real-Time Architecture for Level-Set-Based Active Contours,Eva Dejnožková and Petr Dokládal,"Methods described by partial differential equations have gained a considerable interest because of undoubtful advantages such as an easy mathematical description of the underlying physics phenomena, subpixel p...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2788,23 October 2005,
541,0.000678989392134464,541,Learning Invariant Rules from Data for Interpretable Anomaly Detection,"Cheng Feng, Pingge Hu","In the research area of anomaly detection, novel and promising methods are frequently developed. However, most existing studies exclusively focus on the detection task only and ignore the interpretability of the underlying models as well as their detection results. Nevertheless, anomaly interpretation, which aims to provide explanation of why specific data instances are identified as anomalies, is an equally important task in many real-world applications. In this work, we propose a novel framework which synergizes several machine learning and data mining techniques to automatically learn invariant rules that are consistently satisfied in a given dataset. The learned invariant rules can provide explicit explanation of anomaly detection results in the inference phase and thus are extremely useful for subsequent decision-making regarding reported anomalies. Furthermore, our empirical evaluation shows that the proposed method can also achieve comparable or even better performance in terms of AUC and partial AUC on public benchmark datasets across various application domains compared with start-of-the-art anomaly detection models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.13577,November 2022,541,Image data model optimization method based on cloud computing,"Jingyu Liu, Jing Wu, Linan Sun and Hailong Zhu","In the current age of data explosion, the amount of data has reached incredible proportions. Digital image data constitute most of these data. With the development of science and technology, the demand for net...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-020-00178-7,15 June 2020,
542,0.000678989392134464,542,TSGP: Two-Stage Generative Prompting for Unsupervised Commonsense Question Answering,"Yueqing Sun, Yu Zhang, Le Qi, Qi Shi","Unsupervised commonsense question answering requires mining effective commonsense knowledge without the rely on the labeled task data. Previous methods typically retrieved from traditional knowledge bases or used pre-trained language models (PrLMs) to generate fixed types of knowledge, which have poor generalization ability. In this paper, we aim to address the above limitation by leveraging the implicit knowledge stored in PrLMs and propose a two-stage prompt-based unsupervised commonsense question answering framework (TSGP). Specifically, we first use knowledge generation prompts to generate the knowledge required for questions with unlimited types and possible candidate answers independent of specified choices. Then, we further utilize answer generation prompts to generate possible candidate answers independent of specified choices. Experimental results and analysis on three different commonsense reasoning tasks, CommonsenseQA, OpenBookQA, and SocialIQA, demonstrate that TSGP significantly improves the reasoning ability of language models in unsupervised settings. Our code is available at: https://github.com/Yueqing-Sun/TSGP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.13515,November 2022,542,A DSP Based POD Implementation for High Speed Multimedia Communications,"Chang Nian Zhang, Hua Li, Nuannuan Zhang and Jiesheng Xie","In the cable network services, the audio/video entertainment contents should be protected from unauthorized copying, intercepting, and tampering. Point-of-deployment (POD) security module, proposed by",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702204138,1 September 2002,
543,0.000678989392134464,543,Semi-Supervised Lifelong Language Learning,"Yingxiu Zhao, Yinhe Zheng, Bowen Yu, Zhiliang Tian, Dongkyu Lee, Jian Sun, Haiyang Yu, Yongbin Li, Nevin L. Zhang","Lifelong learning aims to accumulate knowledge and alleviate catastrophic forgetting when learning tasks sequentially. However, existing lifelong language learning methods only focus on the supervised learning setting. Unlabeled data, which can be easily accessed in real-world scenarios, are underexplored. In this paper, we explore a novel setting, semi-supervised lifelong language learning (SSLL), where a model learns sequentially arriving language tasks with both labeled and unlabeled data. We propose an unlabeled data enhanced lifelong learner to explore SSLL. Specially, we dedicate task-specific modules to alleviate catastrophic forgetting and design two modules to exploit unlabeled data: (1) a virtual supervision enhanced task solver is constructed on a teacher-student framework to mine the underlying knowledge from unlabeled data; and (2) a backward augmented learner is built to encourage knowledge transfer from newly arrived unlabeled data to previous tasks. Experimental results on various language tasks demonstrate our model's effectiveness and superiority over competitive baselines under the new setting SSLL. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.13050,November 2022,543,Multimedia learning principles in different learning environments: a systematic review,Burç Çeken and Nazım Taşkın,Current literature mainly focused on one or two multimedia learning principles in traditional learning environments. Studies on multimedia learning principles in AR and VR environments are also limited. To rev...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-022-00200-2,13 April 2022,
544,0.000678989392134464,544,Petroleum prices prediction using data mining techniques -- A Review,"Kiplang'at Weldon, John Ngechu, Ngatho Everlyne, Nancy Njambi, Kinyua Gikunda","Over the past 20 years, Kenya's demand for petroleum products has proliferated. This is mainly because this particular commodity is used in many sectors of the country's economy. Exchange rates are impacted by constantly shifting prices, which also impact Kenya's industrial output of commodities. The cost of other items produced and even the expansion of the economy is significantly impacted by any change in the price of petroleum products. Therefore, accurate petroleum price forecasting is critical for devising policies that are suitable to curb fuel-related shocks. Data mining techniques are the tools used to find valuable patterns in data. Data mining techniques used in petroleum price prediction, including artificial neural networks (ANNs), support vector machines (SVMs), and intelligent optimization techniques like the genetic algorithm (GA), have grown increasingly popular. This study provides a comprehensive review of the existing data mining techniques for making predictions on petroleum prices. The data mining techniques are classified into regression models, deep neural network models, fuzzy sets and logic, and hybrid models. A detailed discussion of how these models are developed and the accuracy of the models is provided. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12964,November 2022,544,Explicability of humanitarian AI: a matter of principles,"Giulio Coppi, Rebeca Moreno Jimenez and Sofia Kyriazi","In the debate on how to improve efficiencies in the humanitarian sector and better meet people’s needs, the argument for the use of artificial intelligence (AI) and automated decision-making (ADMs) systems has...",https://www.springeropen.com//jhumanitarianaction.springeropen.com/articles/10.1186/s41018-021-00096-6,6 October 2021,
545,0.000678989392134464,545,Cambrian Explosion Algorithm for Multi-Objective Association Rules Mining,"Théophile Berteloot, Richard Khoury, Audrey Durand","Association rule mining is one of the most studied research fields of data mining, with applications ranging from grocery basket problems to highly explainable classification systems. Classical association rule mining algorithms have several flaws especially with regards to their execution times, memory usage and number of rules produced. An alternative is the use of meta-heuristics, which have been used on several optimisation problems. This paper has two objectives. First, we provide a comparison of the performances of state-of-the-art meta-heuristics on the association rule mining problem. We use the multi-objective versions of those algorithms using support, confidence and cosine. Second, we propose a new algorithm designed to mine rules efficiently from massive datasets by exploring a large variety of solutions, akin to the explosion of species diversity of the Cambrian Explosion. We compare our algorithm to 20 benchmark algorithms on 22 real-world data-sets, and show that our algorithm present good results and outperform several state-of-the-art algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12767,November 2022,545,Scientific production and thematic breakthroughs in smart learning environments: a bibliometric analysis,"Friday Joseph Agbo, Solomon Sunday Oyelere, Jarkko Suhonen and Markku Tukiainen","This study examines the research landscape of smart learning environments by conducting a comprehensive bibliometric analysis of the field over the years. The study focused on the research trends, scholar’s pr...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00145-4,15 January 2021,
546,0.000678989392134464,546,Progressive Learning with Cross-Window Consistency for Semi-Supervised Semantic Segmentation,"Bo Dang, Yansheng Li, Yongjun Zhang, Jiayi Ma","Semi-supervised semantic segmentation focuses on the exploration of a small amount of labeled data and a large amount of unlabeled data, which is more in line with the demands of real-world image understanding applications. However, it is still hindered by the inability to fully and effectively leverage unlabeled images. In this paper, we reveal that cross-window consistency (CWC) is helpful in comprehensively extracting auxiliary supervision from unlabeled data. Additionally, we propose a novel CWC-driven progressive learning framework to optimize the deep network by mining weak-to-strong constraints from massive unlabeled data. More specifically, this paper presents a biased cross-window consistency (BCC) loss with an importance factor, which helps the deep network explicitly constrain confidence maps from overlapping regions in different windows to maintain semantic consistency with larger contexts. In addition, we propose a dynamic pseudo-label memory bank (DPM) to provide high-consistency and high-reliability pseudo-labels to further optimize the network. Extensive experiments on three representative datasets of urban views, medical scenarios, and satellite scenes demonstrate our framework consistently outperforms the state-of-the-art methods with a large margin. Code will be available publicly. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12425,November 2022,546,Domain engineering to ensure flexibility on interaction laws of multi-agent systems,"Gustavo R. Carvalho, Rodrigo B. Paes, Carlos J. P. Lucena and Ricardo Choren",Law enforcement approaches have been proposed to promote dependability in open multi-agent systems. Interaction laws are defined and then enforced to promote predictability. As new software demands and require...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192406,June 2007,
547,0.000678989392134464,547,"Method for Determining the Similarity of Text Documents for the Kazakh language, Taking Into Account Synonyms: Extension to TF-IDF",Bakhyt Bakiyev,"The task of determining the similarity of text documents has received considerable attention in many areas such as Information Retrieval, Text Mining, Natural Language Processing (NLP) and Computational Linguistics. Transferring data to numeric vectors is a complex task where algorithms such as tokenization, stopword filtering, stemming, and weighting of terms are used. The term frequency - inverse document frequency (TF-IDF) is the most widely used term weighting method to facilitate the search for relevant documents. To improve the weighting of terms, a large number of TF-IDF extensions are made. In this paper, another extension of the TF-IDF method is proposed where synonyms are taken into account. The effectiveness of the method is confirmed by experiments on functions such as Cosine, Dice and Jaccard to measure the similarity of text documents for the Kazakh language. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12364,November 2022,547,A method for model based test harness generation for component testing,Camila Ribeiro Rocha and Eliane Martins,"We present a model-based testing approach that allows the automatic generation of test artifacts for component testing. A component interacts with its clients through provided interfaces, and request services ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192549,March 2008,
548,0.000678989392134464,548,A Combined Approach of Process Mining and Rule-based AI for Study Planning and Monitoring in Higher Education,"Miriam Wagner, Hayyan Helal, Rene Roepke, Sven Judel, Jens Doveren, Sergej Goerzen, Pouya Soudmand, Gerhard Lakemeyer, Ulrik Schroeder, Wil van der Aalst","This paper presents an approach of using methods of process mining and rule-based artificial intelligence to analyze and understand study paths of students based on campus management system data and study program models. Process mining techniques are used to characterize successful study paths, as well as to detect and visualize deviations from expected plans. These insights are combined with recommendations and requirements of the corresponding study programs extracted from examination regulations. Here, event calculus and answer set programming are used to provide models of the study programs which support planning and conformance checking while providing feedback on possible study plan violations. In its combination, process mining and rule-based artificial intelligence are used to support study planning and monitoring by deriving rules and recommendations for guiding students to more suitable study paths with higher success rates. Two applications will be implemented, one for students and one for study program designers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12190,November 2022,548,"Examining the relationships between students’ perceptions of technology, pedagogy, and cognition: the case of immersive virtual reality mini games to foster computational thinking in higher education","Friday Joseph Agbo, Sunday Adewale Olaleye, Matt Bower and Solomon Sunday Oyelere","Researchers are increasingly exploring educational games in immersive virtual reality (IVR) environments to facilitate students’ learning experiences. Mainly, the effect of IVR on learning outcomes has been th...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00233-1,23 February 2023,
549,0.000678989392134464,549,Dynamic Acoustic Compensation and Adaptive Focal Training for Personalized Speech Enhancement,"Xiaofeng Ge, Jiangyu Han, Haixin Guan, Yanhua Long","Recently, more and more personalized speech enhancement systems (PSE) with excellent performance have been proposed. However, two critical issues still limit the performance and generalization ability of the model: 1) Acoustic environment mismatch between the test noisy speech and target speaker enrollment speech; 2) Hard sample mining and learning. In this paper, dynamic acoustic compensation (DAC) is proposed to alleviate the environment mismatch, by intercepting the noise or environmental acoustic segments from noisy speech and mixing it with the clean enrollment speech. To well exploit the hard samples in training data, we propose an adaptive focal training (AFT) strategy by assigning adaptive loss weights to hard and non-hard samples during training. A time-frequency multi-loss training is further introduced to improve and generalize our previous work sDPCCN for PSE. The effectiveness of proposed methods are examined on the DNS4 Challenge dataset. Results show that, the DAC brings large improvements in terms of multiple evaluation metrics, and AFT reduces the hard sample rate significantly and produces obvious MOS score improvement. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12097,November 2022,549,Experiences tracking agile projects: an empirical study,"Danilo Sato, Dairton Bassi, Mariana Bravo, Alfredo Goldman and Fabio Kon","In this article, we gather results from several projects we conducted recently that use some kind of agile method. We analyze both academic and governmental software development projects, some of them using ag...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194495,October 2006,
550,0.000678989392134464,550,Variation-based Cause Effect Identification,"Mohamed Amine ben Salem, Karim Said Barsim, Bin Yang","Mining genuine mechanisms underlying the complex data generation process in real-world systems is a fundamental step in promoting interpretability of, and thus trust in, data-driven models. Therefore, we propose a variation-based cause effect identification (VCEI) framework for causal discovery in bivariate systems from a single observational setting. Our framework relies on the principle of independence of cause and mechanism (ICM) under the assumption of an existing acyclic causal link, and offers a practical realization of this principle. Principally, we artificially construct two settings in which the marginal distributions of one covariate, claimed to be the cause, are guaranteed to have non-negligible variations. This is achieved by re-weighting samples of the marginal so that the resultant distribution is notably distinct from this marginal according to some discrepancy measure. In the causal direction, such variations are expected to have no impact on the effect generation mechanism. Therefore, quantifying the impact of these variations on the conditionals reveals the genuine causal direction. Moreover, we formulate our approach in the kernel-based maximum mean discrepancy, lifting all constraints on the data types of cause-and-effect covariates, and rendering such artificial interventions a convex optimization problem. We provide a series of experiments on real and synthetic data showing that VCEI is, in principle, competitive to other cause effect identification frameworks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.12016,November 2022,550,A novel taxonomy of student-generated video styles,"Rosa Arruabarrena, Ana Sánchez, César Domínguez and Arturo Jaime",Video is a medium increasingly used in education. The styles of videos produced for academic purposes have been studied in the literature based mainly on those initially designed by instructors for use in MOOC...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00295-6,8 November 2021,
551,0.000678989392134464,551,Disentangled Representation Learning,"Xin Wang, Hong Chen, Si'ao Tang, Zihao Wu, Wenwu Zhu","Disentangled Representation Learning (DRL) aims to learn a model capable of identifying and disentangling the underlying factors hidden in the observable data in representation form. The process of separating underlying factors of variation into variables with semantic meaning benefits in learning explainable representations of data, which imitates the meaningful understanding process of humans when observing an object or relation. As a general learning strategy, DRL has demonstrated its power in improving the model explainability, controlability, robustness, as well as generalization capacity in a wide range of scenarios such as computer vision, natural language processing, data mining etc. In this article, we comprehensively review DRL from various aspects including motivations, definitions, methodologies, evaluations, applications and model designs. We discuss works on DRL based on two well-recognized definitions, i.e., Intuitive Definition and Group Theory Definition. We further categorize the methodologies for DRL into four groups, i.e., Traditional Statistical Approaches, Variational Auto-encoder Based Approaches, Generative Adversarial Networks Based Approaches, Hierarchical Approaches and Other Approaches. We also analyze principles to design different DRL models that may benefit different tasks in practical applications. Finally, we point out challenges in DRL as well as potential research directions deserving future investigations. We believe this work may provide insights for promoting the DRL research in the community. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.11695,November 2022,551,"Trends in Texas high school student enrollment in mathematics, science, and CTE-STEM courses",So Yoon Yoon and Johannes Strobel,"In the context of Texas of the USA, House Bill 5 signifies a major policy shift requiring entering high school students starting in fall 2014 to choose an endorsement, like science, technology, engineering, an...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-017-0063-6,22 May 2017,
552,0.000678989392134464,552,Unsupervised Domain Adaptation via Deep Hierarchical Optimal Transport,"Yingxue Xu, Guihua Wen, Yang Hu, Pei Yang","Unsupervised domain adaptation is a challenging task that aims to estimate a transferable model for unlabeled target domain by exploiting source labeled data. Optimal Transport (OT) based methods recently have been proven to be a promising direction for domain adaptation due to their competitive performance. However, most of these methods coarsely aligned source and target distributions, leading to the over-aligned problem where the category-discriminative information is mixed up although domain-invariant representations can be learned. In this paper, we propose a Deep Hierarchical Optimal Transport method (DeepHOT) for unsupervised domain adaptation. The main idea is to use hierarchical optimal transport to learn both domain-invariant and category-discriminative representations by mining the rich structural correlations among domain data. The DeepHOT framework consists of a domain-level OT and an image-level OT, where the latter is used as the ground distance metric for the former. The image-level OT captures structural associations of local image regions that are beneficial to image classification, while the domain-level OT learns domain-invariant representations by leveraging the underlying geometry of domains. However, due to the high computational complexity, the optimal transport based models are limited in some scenarios. To this end, we propose a robust and efficient implementation of the DeepHOT framework by approximating origin OT with sliced Wasserstein distance in image-level OT and using a mini-batch unbalanced optimal transport for domain-level OT. Extensive experiments show that DeepHOT surpasses the state-of-the-art methods in four benchmark datasets. Code will be released on GitHub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.11424,November 2022,552,The impact of virtual reality meditation on college students’ exam performance,"Regina Kaplan-Rakowski, Karen R. Johnson and Tomasz Wojdynski","Advocates of meditation claim that it can improve various aspects of life, including health, attention, thinking, and learning. The purpose of this empirical, quantitative, between-subject study was twofold. F...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00166-7,16 October 2021,
553,0.000678989392134464,553,Blind Knowledge Distillation for Robust Image Classification,"Timo Kaiser, Lukas Ehmann, Christoph Reinders, Bodo Rosenhahn","Optimizing neural networks with noisy labels is a challenging task, especially if the label set contains real-world noise. Networks tend to generalize to reasonable patterns in the early training stages and overfit to specific details of noisy samples in the latter ones. We introduce Blind Knowledge Distillation - a novel teacher-student approach for learning with noisy labels by masking the ground truth related teacher output to filter out potentially corrupted knowledge and to estimate the tipping point from generalizing to overfitting. Based on this, we enable the estimation of noise in the training data with Otsus algorithm. With this estimation, we train the network with a modified weighted cross-entropy loss function. We show in our experiments that Blind Knowledge Distillation detects overfitting effectively during training and improves the detection of clean and noisy labels on the recently published CIFAR-N dataset. Code is available at GitHub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.11355,November 2022,553,A novel human activity recognition architecture: using residual inception ConvLSTM layer,"Sarah Khater, Mayada Hadhoud and Magda B. Fayek",Human activity recognition (HAR) is a very challenging problem that requires identifying an activity performed by a single individual or a group of people observed from spatiotemporal data. Many computer visio...,https://www.springeropen.com//jeas.springeropen.com/articles/10.1186/s44147-022-00098-0,21 May 2022,
554,0.000678989392134464,554,Explainable Model-specific Algorithm Selection for Multi-Label Classification,"Ana Kostovska, Carola Doerr, Sašo Džeroski, Dragi Kocev, Panče Panov, Tome Eftimov","Multi-label classification (MLC) is an ML task of predictive modeling in which a data instance can simultaneously belong to multiple classes. MLC is increasingly gaining interest in different application domains such as text mining, computer vision, and bioinformatics. Several MLC algorithms have been proposed in the literature, resulting in a meta-optimization problem that the user needs to address: which MLC approach to select for a given dataset? To address this algorithm selection problem, we investigate in this work the quality of an automated approach that uses characteristics of the datasets - so-called features - and a trained algorithm selector to choose which algorithm to apply for a given task. For our empirical evaluation, we use a portfolio of 38 datasets. We consider eight MLC algorithms, whose quality we evaluate using six different performance metrics. We show that our automated algorithm selector outperforms any of the single MLC algorithms, and this is for all evaluated performance measures. Our selection approach is explainable, a characteristic that we exploit to investigate which meta-features have the largest influence on the decisions made by the algorithm selector. Finally, we also quantify the importance of the most significant meta-features for various domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.11227,November 2022,554,A progressive vector map browser for the web,"José Augusto Sapienza Ramos, Claudio Esperança and Esteban Walter Gonzales Clua","With the increasing popularity of web-based map browsers, remotely obtaining a high quality depiction of cartographic information has become commonplace. Most web mapping systems, however, rely on high-capacit...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194500,June 2009,
555,0.000678989392134464,555,A review of laser scanning for geological and geotechnical applications in underground mining,"Sarvesh Kumar Singh, Bikram Pratap Banerjee, Simit Raval","Laser scanning can provide timely assessments of mine sites despite adverse challenges in the operational environment. Although there are several published articles on laser scanning, there is a need to review them in the context of underground mining applications. To this end, a holistic review of laser scanning is presented including progress in 3D scanning systems, data capture/processing techniques and primary applications in underground mines. Laser scanning technology has advanced significantly in terms of mobility and mapping, but there are constraints in coherent and consistent data collection at certain mines due to feature deficiency, dynamics, and environmental influences such as dust and water. Studies suggest that laser scanning has matured over the years for change detection, clearance measurements and structure mapping applications. However, there is scope for improvements in lithology identification, surface parameter measurements, logistic tracking and autonomous navigation. Laser scanning has the potential to provide real-time solutions but the lack of infrastructure in underground mines for data transfer, geodetic networking and processing capacity remain limiting factors. Nevertheless, laser scanners are becoming an integral part of mine automation thanks to their affordability, accuracy and mobility, which should support their widespread usage in years to come. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.11181,November 2022,555,Contextual Information and Covariance Descriptors for People Surveillance: An Application for Safety of Construction Workers,"Giovanni Gualdi, Andrea Prati and Rita Cucchiara","In computer science, contextual information can be used both to reduce computations and to increase accuracy. This paper discusses how it can be exploited for people surveillance in very cluttered environments...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2011/684819,15 December 2010,
556,0.00461519317491824,556,DeepGAR: Deep Graph Learning for Analogical Reasoning,"Chen Ling, Tanmoy Chowdhury, Junji Jiang, Junxiang Wang, Xuchao Zhang, Haifeng Chen, Liang Zhao","Analogical reasoning is the process of discovering and mapping correspondences from a target subject to a base subject. As the most well-known computational method of analogical reasoning, Structure-Mapping Theory (SMT) abstracts both target and base subjects into relational graphs and forms the cognitive process of analogical reasoning by finding a corresponding subgraph (i.e., correspondence) in the target graph that is aligned with the base graph. However, incorporating deep learning for SMT is still under-explored due to several obstacles: 1) the combinatorial complexity of searching for the correspondence in the target graph; 2) the correspondence mining is restricted by various cognitive theory-driven constraints. To address both challenges, we propose a novel framework for Analogical Reasoning (DeepGAR) that identifies the correspondence between source and target domains by assuring cognitive theory-driven constraints. Specifically, we design a geometric constraint embedding space to induce subgraph relation from node embeddings for efficient subgraph search. Furthermore, we develop novel learning and optimization strategies that could end-to-end identify correspondences that are strictly consistent with constraints driven by the cognitive theory. Extensive experiments are conducted on synthetic and real-world datasets to demonstrate the effectiveness of the proposed DeepGAR over existing methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10821,November 2022,556,Robust assertions and fail-bounded behavior,"Paula Prata, Mario Rela, Henrique Madeira and João Gabriel Silva",In this paper the behavior of assertion-based error detection mechanisms is characterized under faults injected according to a quite general fault model. Assertions based on the knowledge of the application ca...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192363,October 2004,
557,0.000678989392134464,557,Knowledge Graph Contrastive Learning Based on Relation-Symmetrical Structure,"Ke Liang, Yue Liu, Sihang Zhou, Wenxuan Tu, Yi Wen, Xihong Yang, Xiangjun Dong, Xinwang Liu","Knowledge graph embedding (KGE) aims at learning powerful representations to benefit various artificial intelligence applications. Meanwhile, contrastive learning has been widely leveraged in graph learning as an effective mechanism to enhance the discriminative capacity of the learned representations. However, the complex structures of KG make it hard to construct appropriate contrastive pairs. Only a few attempts have integrated contrastive learning strategies with KGE. But, most of them rely on language models ( e.g., Bert) for contrastive pair construction instead of fully mining information underlying the graph structure, hindering expressive ability. Surprisingly, we find that the entities within a relational symmetrical structure are usually similar and correlated. To this end, we propose a knowledge graph contrastive learning framework based on relation-symmetrical structure, KGE-SymCL, which mines symmetrical structure information in KGs to enhance the discriminative ability of KGE models. Concretely, a plug-and-play approach is proposed by taking entities in the relation-symmetrical positions as positive pairs. Besides, a self-supervised alignment loss is designed to pull together positive pairs. Experimental results on link prediction and entity classification datasets demonstrate that our KGE-SymCL can be easily adopted to various KGE models for performance improvements. Moreover, extensive experiments show that our model could outperform other state-of-the-art baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10738,November 2022,557,Design and Implementation of Digital Linear Control Systems on Reconfigurable Hardware,"Marcus Bednara, Klaus Danne, Markus Deppe, Oliver Oberschelp, Frank Slomka and Jürgen Teich","The implementation of large linear control systems requires a high amount of digital signal processing. Here, we show that reconfigurable hardware allows the design of fast yet flexible control systems. After ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703301040,20 May 2003,
558,0.000678989392134464,558,Multi-timescale Event Detection in Nonintrusive Load Monitoring based on MDL Principle,"Bo Liu, Jianfeng Zhang, Wenpeng Luan, Zishuai Liu, Yixin Yu","Load event detection is the fundamental step for the event-based non-intrusive load monitoring (NILM). However, existing event detection methods with fixed parameters may fail in coping with the inherent multi-timescale characteristics of events and their event detection accuracy is easily affected by the load fluctuation. In this regard, this paper extends our previously designed two-stage event detection framework, and proposes a novel multi-timescale event detection method based on the principle of minimum description length (MDL). Following the completion of step-like event detection in the first stage, a long-transient event detection scheme with variable-length sliding window is designed for the second stage, which is intended to provide the observation and characterization of the same event at different time scales. In that, the context information in the aggregated load data is mined by motif discovery, and then based on the MDL principle, the proper observation scales are selected for different events and the corresponding detection results are determined. In the post-processing step, a load fluctuation location method based on voice activity detection (VAD) is proposed to identify and remove the unreasonable events caused by fluctuations. Based on newly proposed evaluation metrics, the comparison tests on public and private datasets demonstrate that our method achieves higher detection accuracy and integrity for events of various appliances across different scenarios. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10721,November 2022,558,Adaptive complementary filtering algorithm for mobile robot localization,"Armando Alves Neto, Douglas Guimarães Macharet, Víctor Costa da Silva Campos and Mario Fernando Montenegro Campos","As a mobile robot navigates through an indoor environment, the condition of the floor is of low (or no) relevance to its decisions. In an outdoor environment, however, terrain characteristics play a major role...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194503,September 2009,
559,0.000678989392134464,559,Evaluating COVID-19 Sequence Data Using Nearest-Neighbors Based Network Model,Sarwan Ali,"The SARS-CoV-2 coronavirus is the cause of the COVID-19 disease in humans. Like many coronaviruses, it can adapt to different hosts and evolve into different lineages. It is well-known that the major SARS-CoV-2 lineages are characterized by mutations that happen predominantly in the spike protein. Understanding the spike protein structure and how it can be perturbed is vital for understanding and determining if a lineage is of concern. These are crucial to identifying and controlling current outbreaks and preventing future pandemics. Machine learning (ML) methods are a viable solution to this effort, given the volume of available sequencing data, much of which is unaligned or even unassembled. However, such ML methods require fixed-length numerical feature vectors in Euclidean space to be applicable. Similarly, euclidean space is not considered the best choice when working with the classification and clustering tasks for biological sequences. For this purpose, we design a method that converts the protein (spike) sequences into the sequence similarity network (SSN). We can then use SSN as an input for the classical algorithms from the graph mining domain for the typical tasks such as classification and clustering to understand the data. We show that the proposed alignment-free method is able to outperform the current SOTA method in terms of clustering results. Similarly, we are able to achieve higher classification accuracy using well-known Node2Vec-based embedding compared to other baseline embedding approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10546,November 2022,559,Hemorrhage semantic segmentation in fundus images for the diagnosis of diabetic retinopathy by using a convolutional neural network,"Ayoub Skouta, Abdelali Elmoufidi, Said Jai-Andaloussi and Ouail Ouchetto","Because retinal hemorrhage is one of the earliest symptoms of diabetic retinopathy, its accurate identification is essential for early diagnosis. One of the major obstacles ophthalmologists face in making a qu...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00632-0,13 June 2022,
560,0.000678989392134464,560,"Social media mining for toxicovigilance of prescription medications: End-to-end pipeline, challenges and future work",Abeed Sarker,"Substance use, substance use disorder, and overdoses related to substance use are major public health problems globally and in the United States. A key aspect of addressing these problems from a public health standpoint is improved surveillance. Traditional surveillance systems are laggy, and social media are potentially useful sources of timely data. However, mining knowledge from social media is challenging, and requires the development of advanced artificial intelligence, specifically natural language processing (NLP) and machine learning methods. We developed a sophisticated end-to-end pipeline for mining information about nonmedical prescription medication use from social media, namely Twitter and Reddit. Our pipeline employs supervised machine learning and NLP for filtering out noise and characterizing the chatter. In this paper, we describe our end-to-end pipeline developed over four years. In addition to describing our data mining infrastructure, we discuss existing challenges in social media mining for toxicovigilance, and possible future research directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10443,November 2022,560,Negotiating and enacting contracts for business networks,Peter Rittgen,Flexibility is a major issue in business today and one answer to that is to organize work in networks of companies rather than within a single business. Cooperation between organizations is more easily establi...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192542,September 2007,
561,0.000678989392134464,561,"Evident: a Development Methodology and a Knowledge Base Topology for Data Mining, Machine Learning and General Knowledge Management","Mingwu, Gao, Samer Haidar","Software has been developed for knowledge discovery, prediction and management for over 30 years. However, there are still unresolved pain points when using existing project development and artifact management methodologies. Historically, there has been a lack of applicable methodologies. Further, methodologies that have been applied, such as Agile, have several limitations including scientific unfalsifiability that reduce their applicability. Evident, a development methodology rooted in the philosophy of logical reasoning and EKB, a knowledge base topology, are proposed. Many pain points in data mining, machine learning and general knowledge management are alleviated conceptually. Evident can be extended potentially to accelerate philosophical exploration, science discovery, education as well as knowledge sharing & retention across the globe. EKB offers one solution of storing information as knowledge, a granular level above data. Related topics in computer history, software engineering, database, sensor, philosophy, and project & organization & military managements are also discussed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10291,November 2022,561,"Automatic engagement estimation in smart education/learning settings: a systematic review of engagement definitions, datasets, and methods",Shofiyati Nur Karimah and Shinobu Hasegawa,Recognizing learners’ engagement during learning processes is important for providing personalized pedagogical support and preventing dropouts. As learning processes shift from traditional offline classrooms t...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-022-00212-y,12 November 2022,
562,0.000678989392134464,562,Pedestrian Spatio-Temporal Information Fusion For Video Anomaly Detection,"Chao Hu, Liqiang Zhu","Aiming at the problem that the current video anomaly detection cannot fully use the temporal information and ignore the diversity of normal behavior, an anomaly detection method is proposed to integrate the spatiotemporal information of pedestrians. Based on the convolutional autoencoder, the input frame is compressed and restored through the encoder and decoder. Anomaly detection is realized according to the difference between the output frame and the true value. In order to strengthen the characteristic information connection between continuous video frames, the residual temporal shift module and the residual channel attention module are introduced to improve the modeling ability of the network on temporal information and channel information, respectively. Due to the excessive generalization of convolutional neural networks, in the memory enhancement modules, the hopping connections of each codec layer are added to limit autoencoders' ability to represent abnormal frames too vigorously and improve the anomaly detection accuracy of the network. In addition, the objective function is modified by a feature discretization loss, which effectively distinguishes different normal behavior patterns. The experimental results on the CUHK Avenue and ShanghaiTech datasets show that the proposed method is superior to the current mainstream video anomaly detection methods while meeting the real-time requirements. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.10052,November 2022,562,The impact of artificial intelligence on learner–instructor interaction in online learning,"Kyoungwon Seo, Joice Tang, Ido Roll, Sidney Fels and Dongwook Yoon","Artificial intelligence (AI) systems offer effective support for online learning and teaching, including personalizing learning for students, automating instructors’ routine tasks, and powering adaptive assess...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00292-9,26 October 2021,
563,0.000678989392134464,563,Absolute frequency measurement of the $6s^2~^1S_0 \to 6s6p~^3P_1$ $F=3/2\to F'=5/2$ $^{\text{201}}$Hg transition with background-free saturation spectroscopy,"Adam Linek, Piotr Morzyński, Marcin Witkowski","We report the development of a method for eliminating background-induced systematic shifts affecting precise measurements of saturation absorption signals. With this technique, we measured the absolute frequency of the $6s^2~^1\text{S}_0 \to 6s6p~^3\text{P}_1$ transition in $^{201a}\text{Hg}$ ($F=3/2\to F'=5/2$) to be $1181541111051(83)$~kHz. The measurement was referenced with an optical frequency comb synchronized to the frequency of the local representation of the UTC. This specific atomic line is situated on the steep slope of the Doppler background at room temperature, which results in frequency systematic shift. We determined the dependence of this shift on the properties of both the spectral line and the background of the measured signal. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.09197,November 2022,563,From “satisfaction of search” to “subsequent search misses”: a review of multiple-target search errors across radiology and cognitive science,"Stephen H. Adamo, Brian J. Gereke, Sarah Shomstein and Joseph Schmidt","For over 50 years, the satisfaction of search effect has been studied within the field of radiology. Defined as a decrease in detection rates for a subsequent target when an initial target is found within the ...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-021-00318-w,28 August 2021,
564,0.000678989392134464,564,A Unified Multimodal De- and Re-coupling Framework for RGB-D Motion Recognition,"Benjia Zhou, Pichao Wang, Jun Wan, Yanyan Liang, Fan Wang","Motion recognition is a promising direction in computer vision, but the training of video classification models is much harder than images due to insufficient data and considerable parameters. To get around this, some works strive to explore multimodal cues from RGB-D data. Although improving motion recognition to some extent, these methods still face sub-optimal situations in the following aspects: (i) Data augmentation, i.e., the scale of the RGB-D datasets is still limited, and few efforts have been made to explore novel data augmentation strategies for videos; (ii) Optimization mechanism, i.e., the tightly space-time-entangled network structure brings more challenges to spatiotemporal information modeling; And (iii) cross-modal knowledge fusion, i.e., the high similarity between multimodal representations caused to insufficient late fusion. To alleviate these drawbacks, we propose to improve RGB-D-based motion recognition both from data and algorithm perspectives in this paper. In more detail, firstly, we introduce a novel video data augmentation method dubbed ShuffleMix, which acts as a supplement to MixUp, to provide additional temporal regularization for motion recognition. Secondly, a Unified Multimodal De-coupling and multi-stage Re-coupling framework, termed UMDR, is proposed for video representation learning. Finally, a novel cross-modal Complement Feature Catcher (CFCer) is explored to mine potential commonalities features in multimodal information as the auxiliary fusion stream, to improve the late fusion results. The seamless combination of these novel designs forms a robust spatiotemporal representation and achieves better performance than state-of-the-art methods on four public motion datasets. Specifically, UMDR achieves unprecedented improvements of +4.5% on the Chalearn IsoGD dataset. Our code is available at https://github.com/zhoubenjia/MotionRGBD-PAMI. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.09146,November 2022,564,RETRACTED ARTICLE: Practical application of wireless communication network multimedia courseware in college basketball teaching,Wengang Chen and Fang Wang,"With the acceleration of informatization and the coverage of wireless networks, homes, conferences, schools and other places have a higher pursuit of the wireless transmission capabilities of electronic device...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-021-01943-1,6 April 2021,
565,0.000678989392134464,565,Coronavirus statistics causes emotional bias: a social media text mining perspective,"Linjiang Guo, Zijian Feng, Yuxue Chi, Mingzhu Wang, Yijun Liu","While COVID-19 has impacted humans for a long time, people search the web for pandemic-related information, causing anxiety. From a theoretic perspective, previous studies have confirmed that the number of COVID-19 cases can cause negative emotions, but how statistics of different dimensions, such as the number of imported cases, the number of local cases, and the number of government-designated lockdown zones, stimulate people's emotions requires detailed understanding. In order to obtain the views of people on COVID-19, this paper first proposes a deep learning model which classifies texts related to the pandemic from text data with place labels. Next, it conducts a sentiment analysis based on multi-task learning. Finally, it carries out a fixed-effect panel regression with outputs of the sentiment analysis. The performance of the algorithm shows a promising result. The empirical study demonstrates while the number of local cases is positively associated with risk perception, the number of imported cases is negatively associated with confidence levels, which explains why citizens tend to ascribe the protracted pandemic to foreign factors. Besides, this study finds that previous pandemic hits cities recover slowly from the suffering, while local governments' spending on healthcare can improve the situation. Our study illustrates the reasons for risk perception and confidence based on different sources of statistical information due to cognitive bias. It complements the knowledge related to epidemic information. It also contributes to a framework that combines sentiment analysis using advanced deep learning technology with the empirical regression method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08644,November 2022,565,Reliable Communications over Rapidly Time-Varying Channels,"Geert Leus, Georgios Giannakis, Jean-Paul Linnartz, Xiaoli Ma, Ananthram Swami and Cihan Tepedelenlioğlu",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/39672,1 December 2006,
566,0.000678989392134464,566,Graph Sequential Neural ODE Process for Link Prediction on Dynamic and Sparse Graphs,"Linhao Luo, Reza Haffari, Shirui Pan","Link prediction on dynamic graphs is an important task in graph mining. Existing approaches based on dynamic graph neural networks (DGNNs) typically require a significant amount of historical data (interactions over time), which is not always available in practice. The missing links over time, which is a common phenomenon in graph data, further aggravates the issue and thus creates extremely sparse and dynamic graphs. To address this problem, we propose a novel method based on the neural process, called Graph Sequential Neural ODE Process (GSNOP). Specifically, GSNOP combines the advantage of the neural process and neural ordinary differential equation that models the link prediction on dynamic graphs as a dynamic-changing stochastic process. By defining a distribution over functions, GSNOP introduces the uncertainty into the predictions, making it generalize to more situations instead of overfitting to the sparse data. GSNOP is also agnostic to model structures that can be integrated with any DGNN to consider the chronological and geometrical information for link prediction. Extensive experiments on three dynamic graph datasets show that GSNOP can significantly improve the performance of existing DGNNs and outperform other neural process variants. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08568,November 2022,566,Can artificial intelligence transform higher education?,"Tony Bates, Cristóbal Cobo, Olga Mariño and Steve Wheeler",Unknown,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00218-x,15 June 2020,
567,0.000678989392134464,567,Incorporating Pre-training Paradigm for Antibody Sequence-Structure Co-design,"Kaiyuan Gao, Lijun Wu, Jinhua Zhu, Tianbo Peng, Yingce Xia, Liang He, Shufang Xie, Tao Qin, Haiguang Liu, Kun He, Tie-Yan Liu","Antibodies are versatile proteins that can bind to pathogens and provide effective protection for human body. Recently, deep learning-based computational antibody design has attracted popular attention since it automatically mines the antibody patterns from data that could be complementary to human experiences. However, the computational methods heavily rely on high-quality antibody structure data, which is quite limited. Besides, the complementarity-determining region (CDR), which is the key component of an antibody that determines the specificity and binding affinity, is highly variable and hard to predict. Therefore, the data limitation issue further raises the difficulty of CDR generation for antibodies. Fortunately, there exists a large amount of sequence data of antibodies that can help model the CDR and alleviate the reliance on structure data. By witnessing the success of pre-training models for protein modeling, in this paper, we develop the antibody pre-training language model and incorporate it into the (antigen-specific) antibody design model in a systemic way. Specifically, we first pre-train an antibody language model based on the sequence data, then propose a one-shot way for sequence and structure generation of CDR to avoid the heavy cost and error propagation from an autoregressive manner, and finally leverage the pre-trained antibody model for the antigen-specific antibody generation model with some carefully designed modules. Through various experiments, we show that our method achieves superior performances over previous baselines on different tasks, such as sequence and structure generation and antigen-binding CDR-H3 design. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08406,November 2022,567,Ambiguity in utopian XR-games,Wolfgang Höhl,Utopian images in XR-games are often ambiguous. How can ambiguity be consciously designed in virtual worlds? What are the design principles for game designers? Ambiguity arises from discontinuity and decontext...,https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-023-00218-w,10 June 2023,
568,0.000678989392134464,568,FolkScope: Intention Knowledge Graph Construction for E-commerce Commonsense Discovery,"Changlong Yu, Weiqi Wang, Xin Liu, Jiaxin Bai, Yangqiu Song, Zheng Li, Yifan Gao, Tianyu Cao, Bing Yin","Understanding users' intentions in e-commerce platforms requires commonsense knowledge. In this paper, we present FolkScope, an intention knowledge graph construction framework to reveal the structure of humans' minds about purchasing items. As commonsense knowledge is usually ineffable and not expressed explicitly, it is challenging to perform information extraction. Thus, we propose a new approach that leverages the generation power of large language models~(LLMs) and human-in-the-loop annotation to semi-automatically construct the knowledge graph. LLMs first generate intention assertions via e-commerce-specific prompts to explain shopping behaviors, where the intention can be an open reason or a predicate falling into one of 18 categories aligning with ConceptNet, e.g., IsA, MadeOf, UsedFor, etc. Then we annotate plausibility and typicality labels of sampled intentions as training data in order to populate human judgments to all automatic generations. Last, to structurize the assertions, we propose pattern mining and conceptualization to form more condensed and abstract knowledge. Extensive evaluations and studies demonstrate that our constructed knowledge graph can well model e-commerce knowledge and have many potential applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08316,November 2022,568,Artificial intelligence applications in Latin American higher education: a systematic review,Sdenka Zobeida Salas-Pilco and Yuqin Yang,"Over the last decade, there has been great research interest in the application of artificial intelligence (AI) in various fields, such as medicine, finance, and law. Recently, there has been a research focus ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00326-w,18 April 2022,
569,0.000678989392134464,569,Matrix Factorization for Cache Optimization in Content Delivery Networks (CDN),"Adolf Kamuzora, Wadie Skaf, Ermiyas Birihanu, Jiyan Mahmud, Péter Kiss, Tamás Jursonovics, Peter Pogrzeba, Imre Lendák, Tomáš Horváth","Content delivery networks (CDNs) are key components of high throughput, low latency services on the internet. CDN cache servers have limited storage and bandwidth and implement state-of-the-art cache admission and eviction algorithms to select the most popular and relevant content for the customers served. The aim of this study was to utilize state-of-the-art recommender system techniques for predicting ratings for cache content in CDN. Matrix factorization was used in predicting content popularity which is valuable information in content eviction and content admission algorithms run on CDN edge servers. A custom implemented matrix factorization class and MyMediaLite were utilized. The input CDN logs were received from a European telecommunication service provider. We built a matrix factorization model with that data and utilized grid search to tune its hyper-parameters. Experimental results indicate that there is promise about the proposed approaches and we showed that a low root mean square error value can be achieved on the real-life CDN log data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08273,November 2022,569,Attitudes and awareness of regional Pacific Island students towards e-learning,"Joel B. Johnson, Pritika Reddy, Ronil Chand and Mani Naiker","The rise of online modes of content delivery, termed e-learning, has increased student convenience and provided geographically remote students with more options for tertiary education. However, its efficacy re...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00248-z,11 March 2021,
570,0.000678989392134464,570,Supporting the Task-driven Skill Identification in Open Source Project Issue Tracking Systems,Fabio Santos,"Selecting an appropriate task is challenging for contributors to Open Source Software (OSS), mainly for those who are contributing for the first time. Therefore, researchers and OSS projects have proposed various strategies to aid newcomers, including labeling tasks. We investigate the automatic labeling of open issues strategy to help the contributors to pick a task to contribute. We label the issues with API-domains--categories of APIs parsed from the source code used to solve the issues. We plan to add social network analysis metrics from the issues conversations as new predictors. By identifying the skills, we claim the contributor candidates should pick a task more suitable. We analyzed interview transcripts and the survey's open-ended questions to comprehend the strategies used to assist in onboarding contributors and used to pick up an issue. We applied quantitative studies to analyze the relevance of the labels in an experiment and compare the strategies' relative importance. We also mined issue data from OSS repositories to predict the API-domain labels with comparable precision, recall, and F-measure with the state-of-art. We plan to use a skill ontology to assist the matching process between contributors and tasks. By analyzing the confidence level of the matching instances in ontologies describing contributors' skills and tasks, we might recommend issues for contribution. So far, the results showed that organizing the issues--which includes assigning labels is seen as an essential strategy for diverse roles in OSS communities. The API-domain labels are relevant for experienced practitioners. The predictions have an average precision of 75.5%. Labeling the issues indicates the skills involved in an issue. The labels represent possible skills in the source code related to an issue. By investigating this research topic, we expect to assist the new contributors in finding a task. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08143,November 2022,570,Labor order under digital control: research on labor control of take-out platform riders,Long Chen,"Following Marx’s analysis of technical control, this article studies the labor process of take-out riders from the perspectives of organizational and scientific technology. On the one hand, by redistributing c...",https://www.springeropen.com//journalofchinesesociology.springeropen.com/articles/10.1186/s40711-022-00171-4,3 November 2022,
571,0.000678989392134464,571,Persian Emotion Detection using ParsBERT and Imbalanced Data Handling Approaches,"Amirhossein Abaskohi, Nazanin Sabri, Behnam Bahrak","Emotion recognition is one of the machine learning applications which can be done using text, speech, or image data gathered from social media spaces. Detecting emotion can help us in different fields, including opinion mining. With the spread of social media, different platforms like Twitter have become data sources, and the language used in these platforms is informal, making the emotion detection task difficult. EmoPars and ArmanEmo are two new human-labeled emotion datasets for the Persian language. These datasets, especially EmoPars, are suffering from inequality between several samples between two classes. In this paper, we evaluate EmoPars and compare them with ArmanEmo. Throughout this analysis, we use data augmentation techniques, data re-sampling, and class-weights with Transformer-based Pretrained Language Models(PLMs) to handle the imbalance problem of these datasets. Moreover, feature selection is used to enhance the models' performance by emphasizing the text's specific features. In addition, we provide a new policy for selecting data from EmoPars, which selects the high-confidence samples; as a result, the model does not see samples that do not have specific emotion during training. Our model reaches a Macro-averaged F1-score of 0.81 and 0.76 on ArmanEmo and EmoPars, respectively, which are new state-of-the-art results in these benchmarks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.08029,November 2022,571,Realities of data sharing using the genome wars as case study - an historical perspective and commentary,Barbara R Jasny,"The importance of data sharing has become a mantra within the science research community. However, sharing has not been as easy (or as readily adopted) as advocates have suggested. Questions of privacy, indivi...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds13,12 February 2013,
572,0.000678989392134464,572,A Metaheuristic Approach for Mining Gradual Patterns,"Dickson Odhiambo Owuor, Thomas Runkler, Anne Laurent","Swarm intelligence is a discipline that studies the collective behavior that is produced by local interactions of a group of individuals with each other and with their environment. In Computer Science domain, numerous swarm intelligence techniques are applied to optimization problems that seek to efficiently find best solutions within a search space. Gradual pattern mining is another Computer Science field that could benefit from the efficiency of swarm based optimization techniques in the task of finding gradual patterns from a huge search space. A gradual pattern is a rule-based correlation that describes the gradual relationship among the attributes of a data set. For example, given attributes {G,H} of a data set a gradual pattern may take the form: ""the less G, the more H"". In this paper, we propose a numeric encoding for gradual pattern candidates that we use to define an effective search space. In addition, we present a systematic study of several meta-heuristic optimization techniques as efficient solutions to the problem of finding gradual patterns using our search space. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.07940,November 2022,572,PROMIS® Health Organization (PHO) 2020 Conference Toward Patient-Centered Care: PROMIS Implementations and Advances Abstracts,Unknown,"This article is part of a Supplement:Volume 4
                                        Supplement 2",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-020-00250-5,18 November 2020,
573,0.000678989392134464,573,Conversational Pattern Mining using Motif Detection,"Nicolle Garber, Vukosi Marivate","The subject of conversational mining has become of great interest recently due to the explosion of social and other online media. Supplementing this explosion of text is the advancement in pre-trained language models which have helped us to leverage these sources of information. An interesting domain to analyse is conversations in terms of complexity and value. Complexity arises due to the fact that a conversation can be asynchronous and can involve multiple parties. It is also computationally intensive to process. We use unsupervised methods in our work in order to develop a conversational pattern mining technique which does not require time consuming, knowledge demanding and resource intensive labelling exercises. The task of identifying repeating patterns in sequences is well researched in the Bioinformatics field. In our work, we adapt this to the field of Natural Language Processing and make several extensions to a motif detection algorithm. In order to demonstrate the application of the algorithm on a dynamic, real world data set; we extract motifs from an open-source film script data source. We run an exploratory investigation into the types of motifs we are able to mine. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.06846,November 2022,573,Anticipating complex network vulnerabilities through abstraction-based analysis,Richard Colbaugh and Kristin Glass,"Large, complex networks are ubiquitous in nature and society, and there is great interest in developing rigorous, scalable methods for identifying and characterizing their vulnerabilities. This paper presents ...",https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-1-9,22 August 2012,
574,0.0130975937317491,574,A Pipeline for Business Intelligence and Data-Driven Root Cause Analysis on Categorical Data,"Shubham Thakar, Dhananjay Kalbande","Business intelligence (BI) is any knowledge derived from existing data that may be strategically applied within a business. Data mining is a technique or method for extracting BI from data using statistical data modeling. Finding relationships or correlations between the various data items that have been collected can be used to boost business performance or at the very least better comprehend what is going on. Root cause analysis (RCA) is discovering the root causes of problems or events to identify appropriate solutions. RCA can show why an event occurred and this can help in avoiding occurrences of an issue in the future. This paper proposes a new clustering + association rule mining pipeline for getting business insights from data. The results of this pipeline are in the form of association rules having consequents, antecedents, and various metrics to evaluate these rules. The results of this pipeline can help in anchoring important business decisions and can also be used by data scientists for updating existing models or while developing new ones. The occurrence of any event is explained by its antecedents in the generated rules. Hence this output can also help in data-driven root cause analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.06717,November 2022,574,Attack induced cascading breakdown in complex networks,"Liang Zhao, Kwangho Park, Ying-Cheng Lai and Thiago Henrique Cupertino","The possibility that a complex network can be brought down by attack on a single or very few nodes through the process of cascading failures is of significant concern. In this paper, we investigate cascading f...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192546,September 2007,
575,0.000678989392134464,575,Mining Mathematical Documents for Question Answering via Unsupervised Formula Labeling,"Philipp Scharpf, Moritz Schubotz, Bela Gipp","The increasing number of questions on Question Answering (QA) platforms like Math Stack Exchange (MSE) signifies a growing information need to answer math-related questions. However, there is currently very little research on approaches for an open data QA system that retrieves mathematical formulae using their concept names or querying formula identifier relationships from knowledge graphs. In this paper, we aim to bridge the gap by presenting data mining methods and benchmark results to employ Mathematical Entity Linking (MathEL) and Unsupervised Formula Labeling (UFL) for semantic formula search and mathematical question answering (MathQA) on the arXiv preprint repository, Wikipedia, and Wikidata, which is part of the Wikimedia ecosystem of free knowledge. Based on different types of information needs, we evaluate our system in 15 information need modes, assessing over 7,000 query results. Furthermore, we compare its performance to a commercial knowledge-base and calculation-engine (Wolfram Alpha) and search-engine (Google). The open source system is hosted by Wikimedia at https://mathqa.wmflabs.org. A demovideo is available at purl.org/mathqa. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.06664,November 2022,575,Automated formal specification generation and refinement from requirement documents,Gustavo Cabral and Augusto Sampaio,"The automatic generation of formal specifications from requirements suppresses the complexity of formal models manual creation and reveals the immediate benefits of its usage, such as the possibility to carry ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192554,March 2008,
576,0.000678989392134464,576,Speech-to-Speech Translation For A Real-world Unwritten Language,"Peng-Jen Chen, Kevin Tran, Yilin Yang, Jingfei Du, Justine Kao, Yu-An Chung, Paden Tomasello, Paul-Ambroise Duquenne, Holger Schwenk, Hongyu Gong, Hirofumi Inaguma, Sravya Popuri, Changhan Wang, Juan Pino, Wei-Ning Hsu, Ann Lee","We study speech-to-speech translation (S2ST) that translates speech from one language into another language and focuses on building systems to support languages without standard text writing systems. We use English-Taiwanese Hokkien as a case study, and present an end-to-end solution from training data collection, modeling choices to benchmark dataset release. First, we present efforts on creating human annotated data, automatically mining data from large unlabeled speech datasets, and adopting pseudo-labeling to produce weakly supervised data. On the modeling, we take advantage of recent advances in applying self-supervised discrete representations as target for prediction in S2ST and show the effectiveness of leveraging additional text supervision from Mandarin, a language similar to Hokkien, in model training. Finally, we release an S2ST benchmark set to facilitate future research in this field. The demo can be found at https://huggingface.co/spaces/facebook/Hokkien_Translation . △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.06474,November 2022,576,Implementing ICT in classroom practice: what else matters besides the ICT infrastructure?,"Catalina Lomos, J. W. (Hans) Luyten and Sabine Tieck",The large-scale International Computer and Information Literacy Study (2018) has an interesting finding concerning Luxembourg teachers. Luxembourg has one of the highest reported level of technology-related re...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-022-00144-6,13 January 2023,
577,0.000678989392134464,577,GeoAI for Knowledge Graph Construction: Identifying Causality Between Cascading Events to Support Environmental Resilience Research,"Yuanyuan Tian, Wenwen Li","Knowledge graph technology is considered a powerful and semantically enabled solution to link entities, allowing users to derive new knowledge by reasoning data according to various types of reasoning rules. However, in building such a knowledge graph, events modeling, such as that of disasters, is often limited to single, isolated events. The linkages among cascading events are often missing in existing knowledge graphs. This paper introduces our GeoAI (Geospatial Artificial Intelligence) solutions to identify causality among events, in particular, disaster events, based on a set of spatially and temporally-enabled semantic rules. Through a use case of causal disaster events modeling, we demonstrated how these defined rules, including theme-based identification of correlated events, spatiotemporal co-occurrence constraint, and text mining of event metadata, enable the automatic extraction of causal relationships between different events. Our solution enriches the event knowledge base and allows for the exploration of linked cascading events in large knowledge graphs, therefore empowering knowledge query and discovery. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.06011,November 2022,577,"Research in computing-intensive simulations for nature-oriented civil-engineering and related scientific fields, using machine learning and big data: an overview of open problems","Zoran Babović, Branislav Bajat, Vladan Đokić, Filip Đorđević, Dražen Drašković, Nenad Filipović, Borko Furht, Nikola Gačić, Igor Ikodinović, Marija Ilić, Ayhan Irfanoglu, Branislav Jelenković, Aleksandar Kartelj, Gerhard Klimeck, Nenad Korolija, Miloš Kotlar…",This article presents a taxonomy and represents a repository of open problems in computing for numerically and logically intensive problems in a number of disciplines that have to synergize for the best perfor...,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00731-6,22 May 2023,
578,0.000678989392134464,578,Outcome-Oriented Prescriptive Process Monitoring Based on Temporal Logic Patterns,"Ivan Donadello, Chiara Di Francescomarino, Fabrizio Maria Maggi, Francesco Ricci, Aladdin Shikhizada","Prescriptive Process Monitoring systems recommend, during the execution of a business process, interventions that, if followed, prevent a negative outcome of the process. Such interventions have to be reliable, that is, they have to guarantee the achievement of the desired outcome or performance, and they have to be flexible, that is, they have to avoid overturning the normal process execution or forcing the execution of a given activity. Most of the existing Prescriptive Process Monitoring solutions, however, while performing well in terms of recommendation reliability, provide the users with very specific (sequences of) activities that have to be executed without caring about the feasibility of these recommendations. In order to face this issue, we propose a new Outcome-Oriented Prescriptive Process Monitoring system recommending temporal relations between activities that have to be guaranteed during the process execution in order to achieve a desired outcome. This softens the mandatory execution of an activity at a given point in time, thus leaving more freedom to the user in deciding the interventions to put in place. Our approach defines these temporal relations with Linear Temporal Logic over finite traces patterns that are used as features to describe the historical process data recorded in an event log by the information systems supporting the execution of the process. Such encoded log is used to train a Machine Learning classifier to learn a mapping between the temporal patterns and the outcome of a process execution. The classifier is then queried at runtime to return as recommendations the most salient temporal patterns to be satisfied to maximize the likelihood of a certain outcome for an input ongoing process execution. The proposed system is assessed using a pool of 22 real-life event logs that have already been used as a benchmark in the Process Mining community. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04880,November 2022,578,Arrhythmic Pulses Detection Using Lempel-Ziv Complexity Analysis,"Lisheng Xu, David Zhang, Kuanquan Wang and Lu Wang",Computerized pulse analysis based on traditional Chinese medicine (TCM) is relatively new in the field of automatic physiological signal analysis and diagnosis. Considerable researches have been done on the au...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/18268,1 December 2006,
579,0.000678989392134464,579,Discrimination and Class Imbalance Aware Online Naive Bayes,"Maryam Badar, Marco Fisichella, Vasileios Iosifidis, Wolfgang Nejdl","Fairness-aware mining of massive data streams is a growing and challenging concern in the contemporary domain of machine learning. Many stream learning algorithms are used to replace humans at critical decision-making points e.g., hiring staff, assessing credit risk, etc. This calls for handling massive incoming information with minimum response delay while ensuring fair and high quality decisions. Recent discrimination-aware learning methods are optimized based on overall accuracy. However, the overall accuracy is biased in favor of the majority class; therefore, state-of-the-art methods mainly diminish discrimination by partially or completely ignoring the minority class. In this context, we propose a novel adaptation of Naïve Bayes to mitigate discrimination embedded in the streams while maintaining high predictive performance for both the majority and minority classes. Our proposed algorithm is simple, fast, and attains multi-objective optimization goals. To handle class imbalance and concept drifts, a dynamic instance weighting module is proposed, which gives more importance to recent instances and less importance to obsolete instances based on their membership in minority or majority class. We conducted experiments on a range of streaming and static datasets and deduced that our proposed methodology outperforms existing state-of-the-art fairness-aware methods in terms of both discrimination score and balanced accuracy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04812,November 2022,579,Perspectives on cloud computing: interviews with five leading scientists from the cloud community,"Gordon Blair, Fabio Kon, Walfredo Cirne, Dejan Milojicic, Raghu Ramakrishnan, Dan Reed and Dilma Silva","Cloud computing is currently one of the major topics in distributed systems, with large numbers of papers being written on the topic, with major players in the industry releasing a range of software platforms ...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-011-0023-1,3 June 2011,
580,0.000678989392134464,580,SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations,"Paul-Ambroise Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswani, Changhan Wang, Juan Pino, Benoît Sagot, Holger Schwenk","We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models are freely available. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04508,November 2022,580,"Exploration of issues, challenges and latest developments in autonomous cars","B. Padmaja, CH. V. K. N. S. N. Moorthy, N. Venkateswarulu and Myneni Madhu Bala","Autonomous cars have achieved exceptional growth in the automotive industry in the last century in terms of reliability, safety and affordability. Due to significant advancements in computing, communication an...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00701-y,6 May 2023,
581,0.000678989392134464,581,Extracting and Pre-Processing Event Logs,Dirk Fahland,"Event data is the basis for all process mining analysis. Most process mining techniques assume their input to be an event log. However, event data is rarely recorded in an event log format, but has to be extracted from raw data. Event log extraction itself is an act of modeling as the analyst has to consciously choose which features of the raw data are used for describing which behavior of which entities. Being aware of these choices and subtle but important differences in concepts such as trace, case, activity, event, table, and log is crucial for mastering advanced process mining analyses. This text provides fundamental concepts and formalizations and discusses design decisions in event log extraction from a raw event table and for event log pre-processing. It is intended as study material for an advanced lecture in a process mining course. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04338,November 2022,581,Perspectives on cloud computing: interviews with five leading scientists from the cloud community,"Gordon Blair, Fabio Kon, Walfredo Cirne, Dejan Milojicic, Raghu Ramakrishnan, Dan Reed and Dilma Silva","Cloud computing is currently one of the major topics in distributed systems, with large numbers of papers being written on the topic, with major players in the industry releasing a range of software platforms ...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-011-0023-1,3 June 2011,
582,0.000678989392134464,582,TimeKit: A Time-series Forecasting-based Upgrade Kit for Collaborative Filtering,"Seoyoung Hong, Minju Jo, Seungji Kook, Jaeeun Jung, Hyowon Wi, Noseong Park, Sung-Bae Cho","Recommender systems are a long-standing research problem in data mining and machine learning. They are incremental in nature, as new user-item interaction logs arrive. In real-world applications, we need to periodically train a collaborative filtering algorithm to extract user/item embedding vectors and therefore, a time-series of embedding vectors can be naturally defined. We present a time-series forecasting-based upgrade kit (TimeKit), which works in the following way: it i) first decides a base collaborative filtering algorithm, ii) extracts user/item embedding vectors with the base algorithm from user-item interaction logs incrementally, e.g., every month, iii) trains our time-series forecasting model with the extracted time-series of embedding vectors, and then iv) forecasts the future embedding vectors and recommend with their dot-product scores owing to a recent breakthrough in processing complicated time-series data, i.e., neural controlled differential equations (NCDEs). Our experiments with four real-world benchmark datasets show that the proposed time-series forecasting-based upgrade kit can significantly enhance existing popular collaborative filtering algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04266,November 2022,582,Measurement in STEM education research: a systematic literature review of trends in the psychometric evidence of scales,"Danka Maric, Grant A. Fore, Samuel Cornelius Nyarko and Pratibha Varma-Nelson","The objective of this systematic review is to identify characteristics, trends, and gaps in measurement in Science, Technology, Engineering, and Mathematics (STEM) education research.",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00430-x,2 June 2023,
583,0.000678989392134464,583,Control-Flow-Based Querying of Process Executions from Partially Ordered Event Data,"Daniel Schuster, Michael Martini, Sebastiaan J. van Zelst, Wil M. P. van der Aalst","Event logs, as viewed in process mining, contain event data describing the execution of operational processes. Most process mining techniques take an event log as input and generate insights about the underlying process by analyzing the data provided. Consequently, handling large volumes of event data is essential to apply process mining successfully. Traditionally, individual process executions are considered sequentially ordered process activities. However, process executions are increasingly viewed as partially ordered activities to more accurately reflect process behavior observed in reality, such as simultaneous execution of activities. Process executions comprising partially ordered activities may contain more complex activity patterns than sequence-based process executions. This paper presents a novel query language to call up process executions from event logs containing partially ordered activities. The query language allows users to specify complex ordering relations over activities, i.e., control flow constraints. Evaluating a query for a given log returns process executions satisfying the specified constraints. We demonstrate the implementation of the query language in a process mining tool and evaluate its performance on real-life event logs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04146,November 2022,583,"Perception, awareness, and attitude toward digital dentistry among pre-dental students: an observational survey","Lina Sharab, Mohamed Adel, Rahaf Abualsoud, Brandi Hall, Suheil Albaree, Reny de Leeuw and Ahmad Kutkut",Pre-dental students’ perception and awareness about current technology in dentistry can influence their motivation and maturity during the learning experience and affect their dental education outcome. This ob...,https://www.springeropen.com//BNRC.springeropen.com/articles/10.1186/s42269-022-00937-3,23 September 2022,
584,0.000678989392134464,584,An Incremental Phase Mapping Approach for X-ray Diffraction Patterns using Binary Peak Representations,"Dipendra Jha, K. V. L. V. Narayanachari, Ruifeng Zhang, Justin Liao, Denis T. Keane, Wei-keng Liao, Alok Choudhary, Yip-Wah Chung, Michael Bedzyk, Ankit Agrawal","Despite the huge advancement in knowledge discovery and data mining techniques, the X-ray diffraction (XRD) analysis process has mostly remained untouched and still involves manual investigation, comparison, and verification. Due to the large volume of XRD samples from high-throughput XRD experiments, it has become impossible for domain scientists to process them manually. Recently, they have started leveraging standard clustering techniques, to reduce the XRD pattern representations requiring manual efforts for labeling and verification. Nevertheless, these standard clustering techniques do not handle problem-specific aspects such as peak shifting, adjacent peaks, background noise, and mixed phases; hence, resulting in incorrect composition-phase diagrams that complicate further steps. Here, we leverage data mining techniques along with domain expertise to handle these issues. In this paper, we introduce an incremental phase mapping approach based on binary peak representations using a new threshold based fuzzy dissimilarity measure. The proposed approach first applies an incremental phase computation algorithm on discrete binary peak representation of XRD samples, followed by hierarchical clustering or manual merging of similar pure phases to obtain the final composition-phase diagram. We evaluate our method on the composition space of two ternary alloy systems- Co-Ni-Ta and Co-Ti-Ta. Our results are verified by domain scientists and closely resembles the manually computed ground-truth composition-phase diagrams. The proposed approach takes us closer towards achieving the goal of complete end-to-end automated XRD analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.04011,November 2022,584,Using an artificial life simulation to enhance reflective critical thinking among student teachers,Byron Bunt and Grantt Gouws,This paper presents a methodology for developing critical reflection among student teachers by using a novel artificial life simulation called Creatures. The paper is a report of an initial investigation that ...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00119-6,15 April 2020,
585,0.000678989392134464,585,Stochastic Solutions for Dense Subgraph Discovery in Multilayer Networks,"Yasushi Kawase, Atsushi Miyauchi, Hanna Sumita","Network analysis has played a key role in knowledge discovery and data mining. In many real-world applications in recent years, we are interested in mining multilayer networks, where we have a number of edge sets called layers, which encode different types of connections and/or time-dependent connections over the same set of vertices. Among many network analysis techniques, dense subgraph discovery, aiming to find a dense component in a network, is an essential primitive with a variety of applications in diverse domains. In this paper, we introduce a novel optimization model for dense subgraph discovery in multilayer networks. Our model aims to find a stochastic solution, i.e., a probability distribution over the family of vertex subsets, rather than a single vertex subset, whereas it can also be used for obtaining a single vertex subset. For our model, we design an LP-based polynomial-time exact algorithm. Moreover, to handle large-scale networks, we also devise a simple, scalable preprocessing algorithm, which often reduces the size of the input networks significantly and results in a substantial speed-up. Computational experiments demonstrate the validity of our model and the effectiveness of our algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.03306,November 2022,585,Indoor versus Outdoor Scene Classification Using Probabilistic Neural Network,"Lalit Gupta, Vinod Pathangay, Arpita Patra, A. Dyana and Sukhendu Das",We propose a method for indoor versus outdoor scene classification using a probabilistic neural network (PNN). The scene is initially segmented (unsupervised) using fuzzy...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/94298,1 December 2006,
586,0.000678989392134464,586,Applying Association Rules Mining to Investigate Pedestrian Fatal and Injury Crash Patterns Under Different Lighting Conditions,"Ahmed Hossain, Xiaoduan Sun, Raju Thapa, Julius Codjoe","The pattern of pedestrian crashes varies greatly depending on lighting circumstances, emphasizing the need of examining pedestrian crashes in various lighting conditions. Using Louisiana pedestrian fatal and injury crash data (2010-2019), this study applied Association Rules Mining (ARM) to identify the hidden pattern of crash risk factors according to three different lighting conditions (daylight, dark-with-streetlight, and dark-no-streetlight). Based on the generated rules, the results show that daylight pedestrian crashes are associated with children (less than 15 years), senior pedestrians (greater than 64 years), older drivers (>64 years), and other driving behaviors such as failure to yield, inattentive/distracted, illness/fatigue/asleep. Additionally, young drivers (15-24 years) are involved in severe pedestrian crashes in daylight conditions. This study also found pedestrian alcohol/drug involvement as the most frequent item in the dark-with-streetlight condition. This crash type is particularly associated with pedestrian action (crossing intersection/midblock), driver age (55-64 years), speed limit (30-35 mph), and specific area type (business with mixed residential area). Fatal pedestrian crashes are found to be associated with roadways with high-speed limits (>50 mph) during the dark without streetlight condition. Some other risk factors linked with high-speed limit related crashes are pedestrians walking with/against the traffic, presence of pedestrian dark clothing, pedestrian alcohol/drug involvement. The research findings are expected to provide an improved understanding of the underlying relationships between pedestrian crash risk factors and specific lighting conditions. Highway safety experts can utilize these findings to conduct a decision-making process for selecting effective countermeasures to reduce pedestrian crashes strategically. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.03187,November 2022,586,Kinetics of the hydroxymethylation of phenol II: values of rate parameters and results of simulation experiments,"Mitsuo Higuchi, Shoichi Nohno, Mitsuhiro Morita and Shin-ichiro Tohmura","The reaction course of the sodium hydroxidecatalyzed hydroxymethylation of phenol was analyzed by use of high-performance liquid chromatography (HPLC), and the rate constants for the seven reactions taking pla...",https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/BF00833495,1 August 1999,
587,0.000678989392134464,587,Improved Target-specific Stance Detection on Social Media Platforms by Delving into Conversation Threads,"Yupeng Li, Haorui He, Shaonan Wang, Francis C. M. Lau, Yunya Song","Target-specific stance detection on social media, which aims at classifying a textual data instance such as a post or a comment into a stance class of a target issue, has become an emerging opinion mining paradigm of importance. An example application would be to overcome vaccine hesitancy in combating the coronavirus pandemic. However, existing stance detection strategies rely merely on the individual instances which cannot always capture the expressed stance of a given target. In response, we address a new task called conversational stance detection which is to infer the stance towards a given target (e.g., COVID-19 vaccination) when given a data instance and its corresponding conversation thread. To tackle the task, we first propose a benchmarking conversational stance detection (CSD) dataset with annotations of stances and the structures of conversation threads among the instances based on six major social media platforms in Hong Kong. To infer the desired stances from both data instances and conversation threads, we propose a model called Branch-BERT that incorporates contextual information in conversation threads. Extensive experiments on our CSD dataset show that our proposed model outperforms all the baseline models that do not make use of contextual information. Specifically, it improves the F1 score by 10.3% compared with the state-of-the-art method in the SemEval-2016 Task 6 competition. This shows the potential of incorporating rich contextual information on detecting target-specific stances on social media platforms and implies a more practical way to construct future stance detection tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.03061,November 2022,587,Selection and evaluation of fire related scenarios in multifunctional buildings considering antagonistic attacks,"Martin Nilsson, Håkan Frantzich and Patrick van Hees",Multifunctional buildings have become more common in the last years. At the same time the threat from antagonistic attacks has increased. This presents challenges for the fire safety systems in multifunctional...,https://www.springeropen.com//firesciencereviews.springeropen.com/articles/10.1186/2193-0414-2-3,5 July 2013,
588,0.000678989392134464,588,Learning Product Graphs from Spectral Templates,"Aref Einizade, Sepideh Hajipour Sardouie","Graph Learning (GL) is at the core of inference and analysis of connections in data mining and machine learning (ML). By observing a dataset of graph signals, and considering specific assumptions, Graph Signal Processing (GSP) tools can provide practical constraints in the GL approach. One applicable constraint can infer a graph with desired frequency signatures, i.e., spectral templates. However, a severe computational burden is a challenging barrier, especially for inference from high-dimensional graph signals. To address this issue and in the case of the underlying graph having graph product structure, we propose learning product (high dimensional) graphs from product spectral templates with significantly reduced complexity rather than learning them directly from high-dimensional graph signals, which, to the best of our knowledge, has not been addressed in the related areas. In contrast to the rare current approaches, our approach can learn all types of product graphs (with more than two graphs) without knowing the type of graph products and has fewer parameters. Experimental results on both the synthetic and real-world data, i.e., brain signal analysis and multi-view object images, illustrate explainable and meaningful factor graphs supported by expert-related research, as well as outperforming the rare current restricted approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02893,November 2022,588,Transport behavior-mining from smartphones: a review,"Valentino Servizi, Francisco C. Pereira, Marie K. Anderson and Otto A. Nielsen","Although people and smartphones have become almost inseparable, especially during travel, smartphones still represent a small fraction of a complex multi-sensor platform enabling the passive collection of user...",https://www.springeropen.com//etrr.springeropen.com/articles/10.1186/s12544-021-00516-z,6 November 2021,
589,0.000678989392134464,589,"One Person, One Model--Learning Compound Router for Sequential Recommendation","Zhiding Liu, Mingyue Cheng, Zhi Li, Qi Liu, Enhong Chen","Deep learning has brought significant breakthroughs in sequential recommendation (SR) for capturing dynamic user interests. A series of recent research revealed that models with more parameters usually achieve optimal performance for SR tasks, inevitably resulting in great challenges for deploying them in real systems. Following the simple assumption that light networks might already suffice for certain users, in this work, we propose CANet, a conceptually simple yet very scalable framework for assigning adaptive network architecture in an input-dependent manner to reduce unnecessary computation. The core idea of CANet is to route the input user behaviors with a light-weighted router module. Specifically, we first construct the routing space with various submodels parameterized in terms of multiple model dimensions such as the number of layers, hidden size and embedding size. To avoid extra storage overhead of the routing space, we employ a weight-slicing schema to maintain all the submodels in exactly one network. Furthermore, we leverage several solutions to solve the discrete optimization issues caused by the router module. Thanks to them, CANet could adaptively adjust its network architecture for each input in an end-to-end manner, in which the user preference can be effectively captured. To evaluate our work, we conduct extensive experiments on benchmark datasets. Experimental results show that CANet reduces computation by 55 ~ 65% while preserving the accuracy of the original model. Our codes are available at https://github.com/icantnamemyself/CANet. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02824,November 2022,589,Ranking in collaboration networks using a group based metric,Vinícius P. Freire and Daniel R. Figueiredo,Collaboration networks are social networks in which relationships represent some kind of professional collaboration. The study of collaboration networks can help identify individuals or groups that are importa...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0041-7,12 October 2011,
590,0.000678989392134464,590,KB4VA: A Knowledge Base of Visualization Designs for Visual Analytics,"Dazhen Deng, Aoyu Wu, Haotian Li, Ji Lan, Yong Wang, Huamin Qu, Yingcai Wu","Visual analytics (VA) systems have been widely used to facilitate decision-making and analytical reasoning in various application domains. VA involves visual designs, interaction designs, and data mining, which is a systematic and complex paradigm. In this work, we focus on the design of effective visualizations for complex data and analytical tasks, which is a critical step in designing a VA system. This step is challenging because it requires extensive knowledge about domain problems and visualization to design effective encodings. Existing visualization designs published in top venues are valuable resources to inspire designs for problems with similar data structures and tasks. However, those designs are hard to understand, parse, and retrieve due to the lack of specifications. To address this problem, we build KB4VA, a knowledge base of visualization designs in VA systems with comprehensive labels about their analytical tasks and visual encodings. Our labeling scheme is inspired by a workshop study with 12 VA researchers to learn user requirements in understanding and retrieving professional visualization designs in VA systems. The theme extends Vega-Lite specifications for describing advanced and composited visualization designs in a declarative manner, thus facilitating human understanding and automatic indexing. To demonstrate the usefulness of our knowledge base, we present a user study about design inspirations for VA tasks. In summary, our work opens new perspectives for enhancing the accessibility and reusability of professional visualization designs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02567,November 2022,590,Geneticus Investigatio:a technology-enhanced learning environment for scaffolding complex learning in genetics,"Anurag Deep, Sahana Murthy and Jayadeva Bhat",Bioscientists such as geneticists and molecular biologists regularly demonstrate the integration of domain concepts and science inquiry practices/skills while explaining a natural phenomenon. The complexity of...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-020-00145-5,27 November 2020,
591,0.000678989392134464,591,"Lidar-level localization with radar? The CFEAR approach to accurate, fast and robust large-scale radar odometry in diverse environments","Daniel Adolfsson, Martin Magnusson, Anas Alhashimi, Achim J. Lilienthal, Henrik Andreasson","This paper presents an accurate, highly efficient, and learning-free method for large-scale odometry estimation using spinning radar, empirically found to generalize well across very diverse environments -- outdoors, from urban to woodland, and indoors in warehouses and mines - without changing parameters. Our method integrates motion compensation within a sweep with one-to-many scan registration that minimizes distances between nearby oriented surface points and mitigates outliers with a robust loss function. Extending our previous approach CFEAR, we present an in-depth investigation on a wider range of data sets, quantifying the importance of filtering, resolution, registration cost and loss functions, keyframe history, and motion compensation. We present a new solving strategy and configuration that overcomes previous issues with sparsity and bias, and improves our state-of-the-art by 38%, thus, surprisingly, outperforming radar SLAM and approaching lidar SLAM. The most accurate configuration achieves 1.09% error at 5Hz on the Oxford benchmark, and the fastest achieves 1.79% error at 160Hz. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02445,November 2022,591,ESICM LIVES 2019,Unknown,"This article is part of a Supplement:Volume 7
                                        Supplement 3",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-019-0265-y,27 September 2019,
592,0.000678989392134464,592,Neural RELAGGS,"Lukas Pensel, Stefan Kramer","Multi-relational databases are the basis of most consolidated data collections in science and industry today. Most learning and mining algorithms, however, require data to be represented in a propositional form. While there is a variety of specialized machine learning algorithms that can operate directly on multi-relational data sets, propositionalization algorithms transform multi-relational databases into propositional data sets, thereby allowing the application of traditional machine learning and data mining algorithms without their modification. One prominent propositionalization algorithm is RELAGGS by Krogel and Wrobel, which transforms the data by nested aggregations. We propose a new neural network based algorithm in the spirit of RELAGGS that employs trainable composite aggregate functions instead of the static aggregate functions used in the original approach. In this way, we can jointly train the propositionalization with the prediction model, or, alternatively, use the learned aggegrations as embeddings in other algorithms. We demonstrate the increased predictive performance by comparing N-RELAGGS with RELAGGS and multiple other state-of-the-art algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02363,November 2022,592,Pre-processing for noise detection in gene expression classification data,"Giampaolo Luiz Libralon, André Carlos Ponce de Leon Ferreira de Carvalho and Ana Carolina Lorena","Due to the imprecise nature of biological experiments, biological data is often characterized by the presence of redundant and noisy data. This may be due to errors that occurred during data collection, such a...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192573,March 2009,
593,0.000678989392134464,593,Towards Asteroid Detection in Microlensing Surveys with Deep Learning,"Preeti Cowan, Ian A. Bond, Napoleon H. Reyes","Asteroids are an indelible part of most astronomical surveys though only a few surveys are dedicated to their detection. Over the years, high cadence microlensing surveys have amassed several terabytes of data while scanning primarily the Galactic Bulge and Magellanic Clouds for microlensing events and thus provide a treasure trove of opportunities for scientific data mining. In particular, numerous asteroids have been observed by visual inspection of selected images. This paper presents novel deep learning-based solutions for the recovery and discovery of asteroids in the microlensing data gathered by the MOA project. Asteroid tracklets can be clearly seen by combining all the observations on a given night and these tracklets inform the structure of the dataset. Known asteroids were identified within these composite images and used for creating the labelled datasets required for supervised learning. Several custom CNN models were developed to identify images with asteroid tracklets. Model ensembling was then employed to reduce the variance in the predictions as well as to improve the generalisation error, achieving a recall of 97.67%. Furthermore, the YOLOv4 object detector was trained to localize asteroid tracklets, achieving a mean Average Precision (mAP) of 90.97%. These trained networks will be applied to 16 years of MOA archival data to find both known and unknown asteroids that have been observed by the survey over the years. The methodologies developed can be adapted for use by other surveys for asteroid recovery and discovery. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02239,November 2022,593,Exploring the relationship between computational thinking and learning satisfaction for non-STEM college students,"Chien Hsiang Liao, Chang-Tang Chiang, I-Chuan Chen and Kevin R. Parker","While various studies have focused on the significance of computational thinking (CT) for the future career paths of individuals in science, technology, engineering, and mathematics (STEM), few studies have fo...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00347-5,15 August 2022,
594,0.000678989392134464,594,Privacy-preserving Deep Learning based Record Linkage,"Thilina Ranbaduge, Dinusha Vatsalan, Ming Ding","Deep learning-based linkage of records across different databases is becoming increasingly useful in data integration and mining applications to discover new insights from multiple sources of data. However, due to privacy and confidentiality concerns, organisations often are not willing or allowed to share their sensitive data with any external parties, thus making it challenging to build/train deep learning models for record linkage across different organizations' databases. To overcome this limitation, we propose the first deep learning-based multi-party privacy-preserving record linkage (PPRL) protocol that can be used to link sensitive databases held by multiple different organisations. In our approach, each database owner first trains a local deep learning model, which is then uploaded to a secure environment and securely aggregated to create a global model. The global model is then used by a linkage unit to distinguish unlabelled record pairs as matches and non-matches. We utilise differential privacy to achieve provable privacy protection against re-identification attacks. We evaluate the linkage quality and scalability of our approach using several large real-world databases, showing that it can achieve high linkage quality while providing sufficient privacy protection against existing attacks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02161,November 2022,594,Robust Recognition and Segmentation of Human Actions Using HMMs with Missing Observations,"Patrick Peursum, Hung H. Bui, Svetha Venkatesh and Geoff West",This paper describes the integration of missing observation data with hidden Markov models to create a framework that is able to segment and classify individual actions from a stream of human motion using an i...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2110,15 August 2005,
595,0.000678989392134464,595,"A unique, ring-like radio source with quadrilateral structure detected with machine learning","Michelle Lochner, Lawrence Rudnick, Ian Heywood, Kenda Knowles, Stanislav S. Shabala","We report the discovery of a unique object in the MeerKAT Galaxy Cluster Legacy Survey (MGCLS) using the machine learning anomaly detection framework Astronomaly. This strange, ring-like source is 30' from the MGCLS field centred on Abell 209, and is not readily explained by simple physical models. With an assumed host galaxy at redshift 0.55, the luminosity (10^25 W/Hz) is comparable to powerful radio galaxies. The source consists of a ring of emission 175 kpc across, quadrilateral enhanced brightness regions bearing resemblance to radio jets, two ""ears"" separated by 368 kpc, and a diffuse envelope. All of the structures appear spectrally steep, ranging from -1.0 to -1.5. The ring has high polarization (25%) except on the bright patches (<10%). We compare this source to the Odd Radio Circles recently discovered in ASKAP data and discuss several possible physical models, including a termination shock from starburst activity, an end-on radio galaxy, and a supermassive black hole merger event. No simple model can easily explain the observed structure of the source. This work, as well as other recent discoveries, demonstrates the power of unsupervised machine learning in mining large datasets for scientifically interesting sources. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.02062,November 2022,595,"Review of deep learning: concepts, CNN architectures, challenges, applications, future directions","Laith Alzubaidi, Jinglan Zhang, Amjad J. Humaidi, Ayad Al-Dujaili, Ye Duan, Omran Al-Shamma, J. Santamaría, Mohammed A. Fadhel, Muthana Al-Amidie and Laith Farhan","In the last few years, the deep learning (DL) computing paradigm has been deemed the Gold Standard in the machine learning (ML) community. Moreover, it has gradually become the most widely used computational a...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00444-8,31 March 2021,
596,0.000678989392134464,596,Fast Noise Removal in Hyperspectral Images via Representative Coefficient Total Variation,"Jiangjun Peng, Hailin Wang, Xiangyong Cao, Xinlin Liu, Xiangyu Rui, Deyu Meng","Mining structural priors in data is a widely recognized technique for hyperspectral image (HSI) denoising tasks, whose typical ways include model-based methods and data-based methods. The model-based methods have good generalization ability, while the runtime cannot meet the fast processing requirements of the practical situations due to the large size of an HSI data $ \mathbf{X} \in \mathbb{R}^{MN\times B}$. For the data-based methods, they perform very fast on new test data once they have been trained. However, their generalization ability is always insufficient. In this paper, we propose a fast model-based HSI denoising approach. Specifically, we propose a novel regularizer named Representative Coefficient Total Variation (RCTV) to simultaneously characterize the low rank and local smooth properties. The RCTV regularizer is proposed based on the observation that the representative coefficient matrix $\mathbf{U}\in\mathbb{R}^{MN\times R} (R\ll B)$ obtained by orthogonally transforming the original HSI $\mathbf{X}$ can inherit the strong local-smooth prior of $\mathbf{X}$. Since $R/B$ is very small, the HSI denoising model based on the RCTV regularizer has lower time complexity. Additionally, we find that the representative coefficient matrix $\mathbf{U}$ is robust to noise, and thus the RCTV regularizer can somewhat promote the robustness of the HSI denoising model. Extensive experiments on mixed noise removal demonstrate the superiority of the proposed method both in denoising performance and denoising speed compared with other state-of-the-art methods. Remarkably, the denoising speed of our proposed method outperforms all the model-based techniques and is comparable with the deep learning-based approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.01825,November 2022,596,Abstracts of the 11th DACH+ Conference on Energy Informatics,Unknown,"This article is part of a Supplement:Volume 5
                                        Supplement 2",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-022-00215-6,7 September 2022,
597,0.00673579331412596,597,Stock Trading Volume Prediction with Dual-Process Meta-Learning,"Ruibo Chen, Wei Li, Zhiyuan Zhang, Ruihan Bao, Keiko Harimoto, Xu Sun","Volume prediction is one of the fundamental objectives in the Fintech area, which is helpful for many downstream tasks, e.g., algorithmic trading. Previous methods mostly learn a universal model for different stocks. However, this kind of practice omits the specific characteristics of individual stocks by applying the same set of parameters for different stocks. On the other hand, learning different models for each stock would face data sparsity or cold start problems for many stocks with small capitalization. To take advantage of the data scale and the various characteristics of individual stocks, we propose a dual-process meta-learning method that treats the prediction of each stock as one task under the meta-learning framework. Our method can model the common pattern behind different stocks with a meta-learner, while modeling the specific pattern for each stock across time spans with stock-dependent parameters. Furthermore, we propose to mine the pattern of each stock in the form of a latent variable which is then used for learning the parameters for the prediction module. This makes the prediction procedure aware of the data pattern. Extensive experiments on volume predictions show that our method can improve the performance of various baseline models. Further analyses testify the effectiveness of our proposed meta-learning framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.01762,November 2022,597,ECR 2020 Book of Abstracts,Unknown,"This article is part of a Supplement:Volume 11
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-020-00851-0,5 May 2020,
598,0.000678989392134464,598,Ground Plane Matters: Picking Up Ground Plane Prior in Monocular 3D Object Detection,"Fan Yang, Xinhao Xu, Hui Chen, Yuchen Guo, Jungong Han, Kai Ni, Guiguang Ding","The ground plane prior is a very informative geometry clue in monocular 3D object detection (M3OD). However, it has been neglected by most mainstream methods. In this paper, we identify two key factors that limit the applicability of ground plane prior: the projection point localization issue and the ground plane tilt issue. To pick up the ground plane prior for M3OD, we propose a Ground Plane Enhanced Network (GPENet) which resolves both issues at one go. For the projection point localization issue, instead of using the bottom vertices or bottom center of the 3D bounding box (BBox), we leverage the object's ground contact points, which are explicit pixels in the image and easy for the neural network to detect. For the ground plane tilt problem, our GPENet estimates the horizon line in the image and derives a novel mathematical expression to accurately estimate the ground plane equation. An unsupervised vertical edge mining algorithm is also proposed to address the occlusion of the horizon line. Furthermore, we design a novel 3D bounding box deduction method based on a dynamic back projection algorithm, which could take advantage of the accurate contact points and the ground plane equation. Additionally, using only M3OD labels, contact point and horizon line pseudo labels can be easily generated with NO extra data collection and label annotation cost. Extensive experiments on the popular KITTI benchmark show that our GPENet can outperform other methods and achieve state-of-the-art performance, well demonstrating the effectiveness and the superiority of the proposed approach. Moreover, our GPENet works better than other methods in cross-dataset evaluation on the nuScenes dataset. Our code and models will be published. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.01556,November 2022,598,Robust Face Detection in Airports,"Jimmy Liu Jiang, Kia-Fock Loe and Hong Jiang Zhang","Robust face detection in complex airport environment is a challenging task. The complexity in such detection systems stems from the variances in image background, view, illumination, articulation, and facial e...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704310206,21 April 2004,
599,0.000678989392134464,599,Offline RL With Realistic Datasets: Heteroskedasticity and Support Constraints,"Anikait Singh, Aviral Kumar, Quan Vuong, Yevgen Chebotar, Sergey Levine","Offline reinforcement learning (RL) learns policies entirely from static datasets, thereby avoiding the challenges associated with online data collection. Practical applications of offline RL will inevitably require learning from datasets where the variability of demonstrated behaviors changes non-uniformly across the state space. For example, at a red light, nearly all human drivers behave similarly by stopping, but when merging onto a highway, some drivers merge quickly, efficiently, and safely, while many hesitate or merge dangerously. Both theoretically and empirically, we show that typical offline RL methods, which are based on distribution constraints fail to learn from data with such non-uniform variability, due to the requirement to stay close to the behavior policy to the same extent across the state space. Ideally, the learned policy should be free to choose per state how closely to follow the behavior policy to maximize long-term return, as long as the learned policy stays within the support of the behavior policy. To instantiate this principle, we reweight the data distribution in conservative Q-learning (CQL) to obtain an approximate support constraint formulation. The reweighted distribution is a mixture of the current policy and an additional policy trained to mine poor actions that are likely under the behavior policy. Our method, CQL (ReDS), is simple, theoretically motivated, and improves performance across a wide range of offline RL problems in Atari games, navigation, and pixel-based manipulation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.01052,November 2022,599,State-of-the-art and annual progress of bridge engineering in 2021,"Renda Zhao, Kaifeng Zheng, Xing Wei, Hongyu Jia, Xiaozhen Li, Qinghua Zhang, Guoji Xu, Yulin Zhan, Ruili Shen, Fang Zhang, Qianhui Pu, Hongye Gou and Chuanjin Yu","Bridge construction is one of the cores of traffic infrastructure construction. To better develop relevant bridge science, this paper introduces the main research progress in China and abroad in 2021 from 12 a...",https://www.springeropen.com//aben.springeropen.com/articles/10.1186/s43251-022-00070-1,30 December 2022,
600,0.00673579331412596,600,A survey on the development status and application prospects of knowledge graph in smart grids,"Jian Wang, Xi Wang, Chaoqun Ma, Lei Kou","With the advent of the electric power big data era, semantic interoperability and interconnection of power data have received extensive attention. Knowledge graph technology is a new method describing the complex relationships between concepts and entities in the objective world, which is widely concerned because of its robust knowledge inference ability. Especially with the proliferation of measurement devices and exponential growth of electric power data empowers, electric power knowledge graph provides new opportunities to solve the contradictions between the massive power resources and the continuously increasing demands for intelligent applications. In an attempt to fulfil the potential of knowledge graph and deal with the various challenges faced, as well as to obtain insights to achieve business applications of smart grids, this work first presents a holistic study of knowledge-driven intelligent application integration. Specifically, a detailed overview of electric power knowledge mining is provided. Then, the overview of the knowledge graph in smart grids is introduced. Moreover, the architecture of the big knowledge graph platform for smart grids and critical technologies are described. Furthermore, this paper comprehensively elaborates on the application prospects leveraged by knowledge graph oriented to smart grids, power consumer service, decision-making in dispatching, and operation and maintenance of power equipment. Finally, issues and challenges are summarised. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.00901,November 2022,600,Cold atoms in space: community workshop summary and proposed road-map,"Iván Alonso, Cristiano Alpigiani, Brett Altschul, Henrique Araújo, Gianluigi Arduini, Jan Arlt, Leonardo Badurina, Antun Balaž, Satvika Bandarupally, Barry C. Barish, Michele Barone, Michele Barsanti, Steven Bass, Angelo Bassi, Baptiste Battelier, Charles F. A. Baynham…","We summarise the discussions at a virtual Community Workshop on Cold Atoms in Space concerning the status of cold atom technologies, the prospective scientific and societal opportunities offered by their deplo...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00147-w,20 November 2022,
601,0.000678989392134464,601,Position-Aware Subgraph Neural Networks with Data-Efficient Learning,"Chang Liu, Yuwen Yang, Zhe Xie, Hongtao Lu, Yue Ding","Data-efficient learning on graphs (GEL) is essential in real-world applications. Existing GEL methods focus on learning useful representations for nodes, edges, or entire graphs with ``small'' labeled data. But the problem of data-efficient learning for subgraph prediction has not been explored. The challenges of this problem lie in the following aspects: 1) It is crucial for subgraphs to learn positional features to acquire structural information in the base graph in which they exist. Although the existing subgraph neural network method is capable of learning disentangled position encodings, the overall computational complexity is very high. 2) Prevailing graph augmentation methods for GEL, including rule-based, sample-based, adaptive, and automated methods, are not suitable for augmenting subgraphs because a subgraph contains fewer nodes but richer information such as position, neighbor, and structure. Subgraph augmentation is more susceptible to undesirable perturbations. 3) Only a small number of nodes in the base graph are contained in subgraphs, which leads to a potential ``bias'' problem that the subgraph representation learning is dominated by these ``hot'' nodes. By contrast, the remaining nodes fail to be fully learned, which reduces the generalization ability of subgraph representation learning. In this paper, we aim to address the challenges above and propose a Position-Aware Data-Efficient Learning framework for subgraph neural networks called PADEL. Specifically, we propose a novel node position encoding method that is anchor-free, and design a new generative subgraph augmentation method based on a diffused variational subgraph autoencoder, and we propose exploratory and exploitable views for subgraph contrastive learning. Extensive experiment results on three real-world datasets show the superiority of our proposed method over state-of-the-art baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.00572,November 2022,601,Evolutionary Computation for Sensor Planning: The Task Distribution Plan,Enrique Dunn and Gustavo Olague,"Autonomous sensor planning is a problem of interest to scientists in the fields of computer vision, robotics, and photogrammetry. In automated visual tasks, a sensing planner must make complex and critical dec...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703303075,21 July 2003,
602,0.000678989392134464,602,Leveraging Graph-based Cross-modal Information Fusion for Neural Sign Language Translation,"Jiangbin Zheng, Siyuan Li, Cheng Tan, Chong Wu, Yidong Chen, Stan Z. Li","Sign Language (SL), as the mother tongue of the deaf community, is a special visual language that most hearing people cannot understand. In recent years, neural Sign Language Translation (SLT), as a possible way for bridging communication gap between the deaf and the hearing people, has attracted widespread academic attention. We found that the current mainstream end-to-end neural SLT models, which tries to learning language knowledge in a weakly supervised manner, could not mine enough semantic information under the condition of low data resources. Therefore, we propose to introduce additional word-level semantic knowledge of sign language linguistics to assist in improving current end-to-end neural SLT models. Concretely, we propose a novel neural SLT model with multi-modal feature fusion based on the dynamic graph, in which the cross-modal information, i.e. text and video, is first assembled as a dynamic graph according to their correlation, and then the graph is processed by a multi-modal graph encoder to generate the multi-modal embeddings for further usage in the subsequent neural translation models. To the best of our knowledge, we are the first to introduce graph neural networks, for fusing multi-modal information, into neural sign language translation models. Moreover, we conducted experiments on a publicly available popular SLT dataset RWTH-PHOENIX-Weather-2014T. and the quantitative experiments show that our method can improve the model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.00526,November 2022,602,Exploring the relationship between process data and contextual variables among Scandinavian students on PISA 2012 mathematics tasks,Denise Reis Costa and Chia-Wen Chen,"Given the ongoing development of computer-based tasks, there has been increasing interest in modelling students’ behaviour indicators from log file data with contextual variables collected via questionnaires. ...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-023-00155-x,8 February 2023,
603,0.000678989392134464,603,High-Level Event Mining: A Framework,"Bianka Bakullari, Wil M. P. van der Aalst","Process mining methods often analyze processes in terms of the individual end-to-end process runs. Process behavior, however, may materialize as a general state of many involved process components, which can not be captured by looking at the individual process instances. A more holistic state of the process can be determined by looking at the events that occur close in time and share common process capacities. In this work, we conceptualize such behavior using high-level events and propose a new framework for detecting and logging such high-level events. The output of our method is a new high-level event log, which collects all generated high-level events together with the newly assigned event attributes: activity, case, and timestamp. Existing process mining techniques can then be applied on the produced high-level event log to obtain further insights. Experiments on both simulated and real-life event data show that our method is able to automatically discover how system-level patterns such as high traffic and workload emerge, propagate and dissolve throughout the process. △ Less",https://arxiv.orghttps://arxiv.org/abs/2211.00006,November 2022,603,"What works where? The relationship between instructional variables and schools' mean scores in mathematics and science in low-, medium-, and high-achieving countries",Ruth Zuzovsky,"The association between frequent use of certain instructional practices in mathematics and science and learning outcomes in schools in low-, medium-, and high-achieving countries is the focus of this study. It...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/2196-0739-1-2,9 July 2013,
604,0.0162784939405607,604,kt-Safety: Graph Release via k-Anonymity and t-Closeness (Technical Report),"Weilong Ren, Kambiz Ghazinour, Xiang Lian","In a wide spectrum of real-world applications, it is very important to analyze and mine graph data such as social networks, communication networks, citation networks, and so on. However, the release of such graph data often raises privacy issue, and the graph privacy preservation has recently drawn much attention from the database community. While prior works on graph privacy preservation mainly focused on protecting the privacy of either the graph structure only or vertex attributes only, in this paper, we propose a novel mechanism for graph privacy preservation by considering attacks from both graph structures and vertex attributes, which transforms the original graph to a so-called kt-safe graph, via k-anonymity and t-closeness. We prove that the generation of a kt-safe graph is NP-hard, therefore, we propose a feasible framework for effectively and efficiently anonymizing a graph with low anonymization cost. In particular, we design a cost-model-based graph partitioning approach to enable our proposed divide-and-conquer strategy for the graph anonymization, and propose effective optimization techniques such as pruning method and a tree synopsis to improve the anonymization efficiency over large-scale graphs. Extensive experiments have been conducted to verify the efficiency and effectiveness of our proposed kt-safe graph generation approach on both real and synthetic data sets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.17479,October 2022,604,The challenges of modern computing and new opportunities for optics,"Chong Li, Xiang Zhang, Jingwei Li, Tao Fang and Xiaowen Dong","In recent years, the explosive development of artificial intelligence implementing by artificial neural networks (ANNs) creates inconceivable demands for computing hardware. However, conventional computing har...",https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-021-00042-0,9 September 2021,
605,0.000678989392134464,605,Using Locality-sensitive Hashing for Rendezvous Search,"Guann-Yng Jiang, Cheng-Shang Chang","The multichannel rendezvous problem is a fundamental problem for neighbor discovery in many IoT applications. The existing works in the literature focus mostly on improving the worst-case performance, and the average-case performance is often not as good as that of the random algorithm. As IoT devices (users) are close to each other, their available channel sets, though they might be different, are similar. Using the locality-sensitive hashing (LSH) technique in data mining, we propose channel hopping algorithms that exploit the similarity between the two available channel sets to increase the rendezvous probability. For the synchronous setting, our algorithms have the expected time-to-rendezvous (ETTR) inversely proportional to a well-known similarity measure called the Jaccard index. For the asynchronous setting, we use dimensionality reduction to speed up the rendezvous process. Our numerical results show that our algorithms can outperform the random algorithm in terms of ETTR. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.17203,October 2022,605,Global Motion Model for Stereovision-Based Motion Analysis,"Jia Wang, Zhencheng Hu, Keiichi Uchimura and Hanqing Lu","An advantage of stereovision-based motion analysis is that the depth information is available, thus motion can be estimated more precisely inD s...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/53691,1 December 2006,
606,0.000678989392134464,606,Mining Word Boundaries in Speech as Naturally Annotated Word Segmentation Data,"Lei Zhang, Shilin Zhou, Chen Gong, Zhenghua Li, Zhefeng Wang, Baoxing Huai, Min Zhang","Chinese word segmentation (CWS) models have achieved very high performance when the training data is sufficient and in-domain. However, the performance drops drastically when shifting to cross-domain and low-resource scenarios due to data sparseness issues. Considering that constructing large-scale manually annotated data is time-consuming and labor-intensive, in this work, we for the first time propose to mine word boundary information from pauses in speech to efficiently obtain large-scale CWS naturally annotated data. We present a simple yet effective complete-then-train method to utilize these natural annotations from speech for CWS model training. Extensive experiments demonstrate that the CWS performance in cross-domain and low-resource scenarios can be significantly improved by leveraging our naturally annotated data extracted from speech. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.17122,October 2022,606,Planning and managing disruptive business models for RFID-startups-The disruptor's arrow of time,Rod King,This paper presents a one-page visual tool for adaptively planning and managing potentially disruptive business models such as in RFID-Startups. The visual tool is called “The Disruptor’s Arrow of Time.” The m...,https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-1-5,13 September 2012,
607,0.000678989392134464,607,STN: a new tensor network method to identify stimulus category from brain activity pattern,"Chunyu Liu, Jiacai Zhang","Neural decoding is still a challenge and hot topic in neurocomputing science. Recently, many studies have shown that brain network patterns containing rich spatial and temporal structure information, which represents the activation information of brain under external stimuli. %Therefore, the research of decoding stimuli from brain network received extensive more attention. The traditional method extracts brain network features directly from the common machine learning method, then puts these features into the classifier, and realizes to decode external stimuli. However, this method cannot effectively extract the multi-dimensional structural information, which is hidden in the brain network. The tensor researchers show that the tensor decomposition model can fully mine unique spatio-temporal structure characteristics in multi-dimensional structure data. This research proposed a stimulus constrained tensor brain model(STN)which involves the tensor decomposition idea and stimulus category constraint information. The model was verified on the real neuroimaging data sets (MEG and fMRI). The experimental results show that the STN model achieves more than 11.06% and 18.46% on accuracy matrix compared with others methods on two modal data sets. These results imply the superiority of extracting discriminative characteristics about STN model, especially for decoding object stimuli with semantic information. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16993,October 2022,607,Measurement in STEM education research: a systematic literature review of trends in the psychometric evidence of scales,"Danka Maric, Grant A. Fore, Samuel Cornelius Nyarko and Pratibha Varma-Nelson","The objective of this systematic review is to identify characteristics, trends, and gaps in measurement in Science, Technology, Engineering, and Mathematics (STEM) education research.",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00430-x,2 June 2023,
608,0.000678989392134464,608,A Pipeline for Analysing Grant Applications,"Shuaiqun Pan, Sergio J. Rodríguez Méndez, Kerry Taylor","Data mining techniques can transform massive amounts of unstructured data into quantitative data that quickly reveal insights, trends, and patterns behind the original data. In this paper, a data mining model is applied to analyse the 2019 grant applications submitted to an Australian Government research funding agency to investigate whether grant schemes successfully identifies innovative project proposals, as intended. The grant applications are peer-reviewed research proposals that include specific ``innovation and creativity'' (IC) scores assigned by reviewers. In addition to predicting the IC score for each research proposal, we are particularly interested in understanding the vocabulary of innovative proposals. In order to solve this problem, various data mining models and feature encoding algorithms are studied and explored. As a result, we propose a model with the best performance, a Random Forest (RF) classifier over documents encoded with features denoting the presence or absence of unigrams. In specific, the unigram terms are encoded by a modified Term Frequency - Inverse Document Frequency (TF-IDF) algorithm, which only implements the IDF part of TF-IDF. Besides the proposed model, this paper also presents a rigorous experimental pipeline for analysing grant applications, and the experimental results prove its feasibility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16843,October 2022,608,Skill profiles of Japanese English learners and reasons for uneven patterns,"Rie Koizumi, Toshie Agawa, Keiko Asano and Yo In’nami","Variations in the skill profiles of learners have become an important research area in recent years. However, there is a lack of empirical research on this topic in Japan. We conducted three studies to address...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-022-00203-3,26 November 2022,
609,0.000678989392134464,609,"Graph Fuzzy System: Concepts, Models and Algorithms","Fuping Hu, Zhaohong Deng, Zhenping Xie, Kup-Sze Choi, Shitong Wang","Fuzzy systems (FSs) have enjoyed wide applications in various fields, including pattern recognition, intelligent control, data mining and bioinformatics, which is attributed to the strong interpretation and learning ability. In traditional application scenarios, FSs are mainly applied to model Euclidean space data and cannot be used to handle graph data of non-Euclidean structure in nature, such as social networks and traffic route maps. Therefore, development of FS modeling method that is suitable for graph data and can retain the advantages of traditional FSs is an important research. To meet this challenge, a new type of FS for graph data modeling called Graph Fuzzy System (GFS) is proposed in this paper, where the concepts, modeling framework and construction algorithms are systematically developed. First, GFS related concepts, including graph fuzzy rule base, graph fuzzy sets and graph consequent processing unit (GCPU), are defined. A GFS modeling framework is then constructed and the antecedents and consequents of the GFS are presented and analyzed. Finally, a learning framework of GFS is proposed, in which a kernel K-prototype graph clustering (K2PGC) is proposed to develop the construction algorithm for the GFS antecedent generation, and then based on graph neural network (GNNs), consequent parameters learning algorithm is proposed for GFS. Specifically, three different versions of the GFS implementation algorithm are developed for comprehensive evaluations with experiments on various benchmark graph classification datasets. The results demonstrate that the proposed GFS inherits the advantages of both existing mainstream GNNs methods and conventional FSs methods while achieving better performance than the counterparts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16730,October 2022,609,Quality assessment for mobile media-enriched services: impact of video lengths,"Jose Oscar Fajardo, Ianire Taboada and Fidel Liberal","The inclusion of multimedia content in different web-based services has increased significantly. Through an extensive subjective testing campaign, we analyse the quality of experience concerning video transmis...",https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-1-2,1 February 2012,
610,0.000678989392134464,610,Linear Programs with Conjunctive Database Queries,"Florent Capelli, Nicolas Crosetti, Joachim Niehren, Jan Ramon","In this paper, we study the problem of optimizing a linear program whose variables are the answers to a conjunctive query. For this we propose the language LP(CQ) for specifying linear programs whose constraints and objective functions depend on the answer sets of conjunctive queries. We contribute an efficient algorithm for solving programs in a fragment of LP(CQ). The natural approach constructs a linear program having as many variables as there are elements in the answer set of the queries. Our approach constructs a linear program having the same optimal value but fewer variables. This is done by exploiting the structure of the conjunctive queries using generalized hypertree decompositions of small width to factorize elements of the answer set together. We illustrate the various applications of LP(CQ) programs on three examples: optimizing deliveries of resources, minimizing noise for differential privacy, and computing the s-measure of patterns in graphs as needed for data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16694,October 2022,610,Experiences with AR plots: design issues and recommendations for augmented reality based mobile games,"Dan Dixon, Saad Liaquat Kiani and Ahsan Ikram",Digital games have the potential for changing attitudes towards social issues such as climate change and sustainability. This paper presents the experience of developing a prototype locative game with an augme...,https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-2-1,29 January 2013,
611,0.000678989392134464,611,DisenPOI: Disentangling Sequential and Geographical Influence for Point-of-Interest Recommendation,"Yifang Qin, Yifan Wang, Fang Sun, Wei Ju, Xuyang Hou, Zhe Wang, Jia Cheng, Jun Lei, Ming Zhang","Point-of-Interest (POI) recommendation plays a vital role in various location-aware services. It has been observed that POI recommendation is driven by both sequential and geographical influences. However, since there is no annotated label of the dominant influence during recommendation, existing methods tend to entangle these two influences, which may lead to sub-optimal recommendation performance and poor interpretability. In this paper, we address the above challenge by proposing DisenPOI, a novel Disentangled dual-graph framework for POI recommendation, which jointly utilizes sequential and geographical relationships on two separate graphs and disentangles the two influences with self-supervision. The key novelty of our model compared with existing approaches is to extract disentangled representations of both sequential and geographical influences with contrastive learning. To be specific, we construct a geographical graph and a sequential graph based on the check-in sequence of a user. We tailor their propagation schemes to become sequence-/geo-aware to better capture the corresponding influences. Preference proxies are extracted from check-in sequence as pseudo labels for the two influences, which supervise the disentanglement via a contrastive loss. Extensive experiments on three datasets demonstrate the superiority of the proposed model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16591,October 2022,611,Human-centered AI and robotics,"Stephane Doncieux, Raja Chatila, Sirko Straube and Frank Kirchner","Robotics has a special place in AI as robots are connected to the real world and robots increasingly appear in humans everyday environment, from home to industry. Apart from cases were robots are expected to c...",https://www.springeropen.com//aiperspectives.springeropen.com/articles/10.1186/s42467-021-00014-x,28 January 2022,
612,0.000678989392134464,612,A Survey on Fundamental Concepts and Practical Challenges of Hyperspectral images,"Hasna Nhaila, Elkebir Sarhrouni, Ahmed Hammouch","The Remote sensing provides a synoptic view of land by detecting the energy reflected from Earth's surface. The Hyperspectral images (HSI) use perfect sensors that extract more than a hundred of images, with more detailed information than using traditional Multispectral data. In this paper, we aim to study this aspect of communication in the case of passive reception. First, a brief overview of acquisition process and treatment of Hyperspectral images is provided. Then, we explain representation spaces and the various analysis methods of these images. Furthermore, the factors influencing this analysis are investigated and some applications, in this area, are presented. Finally, we explain the relationship between Hyperspectral images and Datamining and we outline the open issues related to this area. So we consider the case study: HSI AVIRIS 92AV3C. This study serves as map of route for integrating classification methods in the higher dimensionality data. Keywords-component: Hyperspectral images, Passive Sensing,Classification, Data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16237,October 2022,612,Mobile communication for intellectually challenged people: a proposed set of requirements for interface design on touch screen devices,"Zelai Saenz de Urturi Breton, Fernando Jorge Hernández, Amaia Méndez Zorrilla and Begoña García Zapirain","The use of certain applications for touch-based mobile devices can prove to be difficult for intellectually challenged people. By performing tests and gathering user feedback, great difficulty has been detecte...",https://www.springeropen.com//muxjournal.springeropen.com/articles/10.1186/2192-1121-1-1,1 February 2012,
613,0.000678989392134464,613,Towards Reliable Neural Specifications,"Chuqin Geng, Nham Le, Xiaojie Xu, Zhaoyue Wang, Arie Gurfinkel, Xujie Si","Having reliable specifications is an unavoidable challenge in achieving verifiable correctness, robustness, and interpretability of AI systems. Existing specifications for neural networks are in the paradigm of data as specification. That is, the local neighborhood centering around a reference input is considered to be correct (or robust). While existing specifications contribute to verifying adversarial robustness, a significant problem in many research domains, our empirical study shows that those verified regions are somewhat tight, and thus fail to allow verification of test set inputs, making them impractical for some real-world applications. To this end, we propose a new family of specifications called neural representation as specification, which uses the intrinsic information of neural networks - neural activation patterns (NAPs), rather than input data to specify the correctness and/or robustness of neural network predictions. We present a simple statistical approach to mining neural activation patterns. To show the effectiveness of discovered NAPs, we formally verify several important properties, such as various types of misclassifications will never happen for a given NAP, and there is no ambiguity between different NAPs. We show that by using NAP, we can verify a significant region of the input space, while still recalling 84% of the data on MNIST. Moreover, we can push the verifiable bound to 10 times larger on the CIFAR10 benchmark. Thus, we argue that NAPs can potentially be used as a more reliable and extensible specification for neural network verification. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16114,October 2022,613,Learning science with YouTube videos and the impacts of Covid-19,Wayne Breslyn and Amy E. Green,This study investigates student and teacher use of online instructional YouTube chemistry videos in the context of the Covid-19 pandemic. Data were collected from a global sample of students (n= 1147) subscribed...,https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00051-4,11 April 2022,
614,0.000678989392134464,614,Semi-UFormer: Semi-supervised Uncertainty-aware Transformer for Image Dehazing,"Ming Tong, Yongzhen Wang, Peng Cui, Xuefeng Yan, Mingqiang Wei","Image dehazing is fundamental yet not well-solved in computer vision. Most cutting-edge models are trained in synthetic data, leading to the poor performance on real-world hazy scenarios. Besides, they commonly give deterministic dehazed images while neglecting to mine their uncertainty. To bridge the domain gap and enhance the dehazing performance, we propose a novel semi-supervised uncertainty-aware transformer network, called Semi-UFormer. Semi-UFormer can well leverage both the real-world hazy images and their uncertainty guidance information. Specifically, Semi-UFormer builds itself on the knowledge distillation framework. Such teacher-student networks effectively absorb real-world haze information for quality dehazing. Furthermore, an uncertainty estimation block is introduced into the model to estimate the pixel uncertainty representations, which is then used as a guidance signal to help the student network produce haze-free images more accurately. Extensive experiments demonstrate that Semi-UFormer generalizes well from synthetic to real-world images. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.16057,October 2022,614,A Rapid Prototyping Environment for Wireless Communication Embedded Systems,Bryan A. Jones and Joseph R. Cavallaro,This paper introduces a rapid prototyping methodology which overcomes important barriers in the design and implementation of digital signal processing (DSP) algorithms and systems on embedded hardware platform...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570330304X,20 May 2003,
615,0.000678989392134464,615,System Network Analytics: Evolution and Stable Rules of a State Series,"Animesh Chaturvedi, Aruna Tiwari, Nicolas Spyratos","System Evolution Analytics on a system that evolves is a challenge because it makes a State Series SS = {S1, S2... SN} (i.e., a set of states ordered by time) with several inter-connected entities changing over time. We present stability characteristics of interesting evolution rules occurring in multiple states. We defined an evolution rule with its stability as the fraction of states in which the rule is interesting. Extensively, we defined stable rule as the evolution rule having stability that exceeds a given threshold minimum stability (minStab). We also defined persistence metric, a quantitative measure of persistent entity-connections. We explain this with an approach and algorithm for System Network Analytics (SysNet-Analytics), which uses minStab to retrieve Network Evolution Rules (NERs) and Stable NERs (SNERs). The retrieved information is used to calculate a proposed System Network Persistence (SNP) metric. This work is automated as a SysNet-Analytics Tool to demonstrate application on real world systems including: software system, natural-language system, retail market system, and IMDb system. We quantified stability and persistence of entity-connections in a system state series. This results in evolution information, which helps in system evolution analytics based on knowledge discovery and data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15965,October 2022,615,A content analysis of alignment messages to the Next Generation Science Standards,Jamie Tanas and Gavin Fulmer,"Teachers are a critical component to standards-based reform systems, which require that reforms conceived at the national level pass through several layers of the educational system before impacting learning i...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-023-00073-6,3 April 2023,
616,0.000678989392134464,616,Student-centric Model of Learning Management System Activity and Academic Performance: from Correlation to Causation,"Varun Mandalapu, Lujie Karen Chen, Sushruta Shetty, Zhiyuan Chen, Jiaqi Gong","In recent years, there is a lot of interest in modeling students' digital traces in Learning Management System (LMS) to understand students' learning behavior patterns including aspects of meta-cognition and self-regulation, with the ultimate goal to turn those insights into actionable information to support students to improve their learning outcomes. In achieving this goal, however, there are two main issues that need to be addressed given the existing literature. Firstly, most of the current work is course-centered (i.e. models are built from data for a specific course) rather than student-centered; secondly, a vast majority of the models are correlational rather than causal. Those issues make it challenging to identify the most promising actionable factors for intervention at the student level where most of the campus-wide academic support is designed for. In this paper, we explored a student-centric analytical framework for LMS activity data that can provide not only correlational but causal insights mined from observational data. We demonstrated this approach using a dataset of 1651 computing major students at a public university in the US during one semester in the Fall of 2019. This dataset includes students' fine-grained LMS interaction logs and administrative data, e.g. demographics and academic performance. In addition, we expand the repository of LMS behavior indicators to include those that can characterize the time-of-the-day of login (e.g. chronotype). Our analysis showed that student login volume, compared with other login behavior indicators, is both strongly correlated and causally linked to student academic performance, especially among students with low academic performance. We envision that those insights will provide convincing evidence for college student support groups to launch student-centered and targeted interventions that are effective and scalable. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15430,October 2022,616,Video Analysis of Human Gait and Posture to Determine Neurological Disorders,"Howard Lee, Ling Guan and Ivan Lee",This paper investigates the application of digital image processing techniques to the detection of neurological disorder. Visual information extracted from the postures and movements of a human gait cycle can ...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/380867,6 April 2008,
617,0.000678989392134464,617,Domain Adaptive Object Detection for Autonomous Driving under Foggy Weather,"Jinlong Li, Runsheng Xu, Jin Ma, Qin Zou, Jiaqi Ma, Hongkai Yu","Most object detection methods for autonomous driving usually assume a consistent feature distribution between training and testing data, which is not always the case when weathers differ significantly. The object detection model trained under clear weather might not be effective enough in foggy weather because of the domain gap. This paper proposes a novel domain adaptive object detection framework for autonomous driving under foggy weather. Our method leverages both image-level and object-level adaptation to diminish the domain discrepancy in image style and object appearance. To further enhance the model's capabilities under challenging samples, we also come up with a new adversarial gradient reversal layer to perform adversarial mining for the hard examples together with domain adaptation. Moreover, we propose to generate an auxiliary domain by data augmentation to enforce a new domain-level metric regularization. Experimental results on public benchmarks show the effectiveness and accuracy of the proposed method. The code is available at https://github.com/jinlong17/DA-Detect. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15176,October 2022,617,Examining the usability and viability of using a simulated classroom environment to prepare preservice science teachers during and after the COVID-19 pandemic,"Jamie N. Mikeska, Heather Howell and Devon Kinsey","Educator preparation programs experienced extreme challenges during the COVID-19 pandemic, as many universities and K-12 schools moved to fully online or hybrid instructional models. These abrupt changes signi...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00054-1,15 June 2022,
618,0.000678989392134464,618,IDEAL: Improved DEnse locAL Contrastive Learning for Semi-Supervised Medical Image Segmentation,"Hritam Basak, Soumitri Chattopadhyay, Rohit Kundu, Sayan Nag, Rammohan Mallipeddi","Due to the scarcity of labeled data, Contrastive Self-Supervised Learning (SSL) frameworks have lately shown great potential in several medical image analysis tasks. However, the existing contrastive mechanisms are sub-optimal for dense pixel-level segmentation tasks due to their inability to mine local features. To this end, we extend the concept of metric learning to the segmentation task, using a dense (dis)similarity learning for pre-training a deep encoder network, and employing a semi-supervised paradigm to fine-tune for the downstream task. Specifically, we propose a simple convolutional projection head for obtaining dense pixel-level features, and a new contrastive loss to utilize these dense projections thereby improving the local representations. A bidirectional consistency regularization mechanism involving two-stream model training is devised for the downstream task. Upon comparison, our IDEAL method outperforms the SoTA methods by fair margins on cardiac MRI segmentation. Code available: https://github.com/hritam-98/IDEAL-ICASSP23 △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15075,October 2022,618,Modeling in Earth system science up to and beyond IPCC AR5,"Tomohiro Hajima, Michio Kawamiya, Michio Watanabe, Etsushi Kato, Kaoru Tachiiri, Masahiro Sugiyama, Shingo Watanabe, Hideki Okajima and Akinori Ito",Changes in the natural environment that are the result of human activities are becoming evident. Since these changes are interrelated and can not be investigated without interdisciplinary collaboration between...,https://www.springeropen.com//progearthplanetsci.springeropen.com/articles/10.1186/s40645-014-0029-y,18 December 2014,
619,0.000678989392134464,619,Privately Fine-Tuning Large Language Models with Differential Privacy,"Rouzbeh Behnia, Mohamamdreza Ebrahimi, Jason Pacheco, Balaji Padmanabhan","Pre-trained Large Language Models (LLMs) are an integral part of modern AI that have led to breakthrough performances in complex AI tasks. Major AI companies with expensive infrastructures are able to develop and train these large models with billions and millions of parameters from scratch. Third parties, researchers, and practitioners are increasingly adopting these pre-trained models and fine-tuning them on their private data to accomplish their downstream AI tasks. However, it has been shown that an adversary can extract/reconstruct the exact training samples from these LLMs, which can lead to revealing personally identifiable information. The issue has raised deep concerns about the privacy of LLMs. Differential privacy (DP) provides a rigorous framework that allows adding noise in the process of training or fine-tuning LLMs such that extracting the training data becomes infeasible (i.e., with a cryptographically small success probability). While the theoretical privacy guarantees offered in most extant studies assume learning models from scratch through many training iterations in an asymptotic setting, this assumption does not hold in fine-tuning scenarios in which the number of training iterations is significantly smaller. To address the gap, we present \ewtune, a DP framework for fine-tuning LLMs based on Edgeworth accountant with finite-sample privacy guarantees. Our results across four well-established natural language understanding (NLU) tasks show that while \ewtune~adds privacy guarantees to LLM fine-tuning process, it directly contributes to decreasing the induced noise to up to 5.6\% and improves the state-of-the-art LLMs performance by up to 1.1\% across all NLU tasks. We have open-sourced our implementations for wide adoption and public testing purposes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15042,October 2022,619,Robust Real-Time Tracking for Visual Surveillance,"David Thirde, Mark Borg, Josep Aguilera, Horst Wildenauer, James Ferryman and Martin Kampel",This paper describes a real-time multi-camera surveillance system that can be applied to a range of application domains. This integrated system is designed to observe crowded scenes and has mechanisms to impro...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/96568,1 December 2006,
620,0.000678989392134464,620,Relation of Observable Stellar Parameters to Mass-Loss Rate of AGB Stars in the LMC,"Henry A. Prager, Lee Anne Willson, Massimo Marengo, Michelle J. Creech-Eakman","Using the Riebel et al. (2012) data set for 6,889 pulsating AGB stars in the LMC, we have derived formulae for mass-loss rate as a function of luminosity and pulsation period or luminosity and mass in three ways, for each of five subsets of data: fundamental mode oxygen rich stars, first overtone mode oxygen rich stars stars, fundamental mode carbon stars, first overtone mode carbon stars, and extreme carbon stars. Using the distribution of the stars in period versus luminosity and mass versus luminosity, we are able to derive a power-law fit to the dependence of mass-loss rate on those quantities. This results in formulae that reproduce observed mass-loss rates and are in general agreement with the expectation from mass-loss models that the mass-loss rate is highly sensitive to luminosity, mass, and pulsation period. In the process of carrying out this analysis we have found radius-mass-luminosity and examined pulsation-mass-radius relations using published evolutionary and pulsation models. These allow us to derive mass and radius from the observed quantities luminosity and pulsation period. We also derived new mass-loss rate versus color relations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15039,October 2022,620,Multisensor Processing for Signal Extraction and Applications,"Chong-Yung Chi, Ta-Sung Lee, Zhi-Quan Luo, Yue Wang and Kung Yao",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/26508,1 December 2006,
621,0.000678989392134464,621,A Hierarchical Approach to Conditional Random Fields for System Anomaly Detection,"Srishti Mishra, Tvarita Jain, Dinkar Sitaram","Anomaly detection to recognize unusual events in large scale systems in a time sensitive manner is critical in many industries, eg. bank fraud, enterprise systems, medical alerts, etc. Large-scale systems often grow in size and complexity over time, and anomaly detection algorithms need to adapt to changing structures. A hierarchical approach takes advantage of the implicit relationships in complex systems and localized context. The features in complex systems may vary drastically in data distribution, capturing different aspects from multiple data sources, and when put together provide a more complete view of the system. In this paper, two datasets are considered, the 1st comprising of system metrics from machines running on a cloud service, and the 2nd of application metrics from a large-scale distributed software system with inherent hierarchies and interconnections amongst its system nodes. Comparing algorithms, across the changepoint based PELT algorithm, cognitive learning-based Hierarchical Temporal Memory algorithms, Support Vector Machines and Conditional Random Fields provides a basis for proposing a Hierarchical Global-Local Conditional Random Field approach to accurately capture anomalies in complex systems across various features. Hierarchical algorithms can learn both the intricacies of specific features, and utilize these in a global abstracted representation to detect anomalous patterns robustly across multi-source feature data and distributed systems. A graphical network analysis on complex systems can further fine-tune datasets to mine relationships based on available features, which can benefit hierarchical models. Furthermore, hierarchical solutions can adapt well to changes at a localized level, learning on new data and changing environments when parts of a system are over-hauled, and translate these learnings to a global view of the system over time. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.15030,October 2022,621,Non-iterative image reconstruction from sparse magnetic resonance imaging radial data without priors,Gengsheng L. Zeng and Edward V. DiBella,The state-of-the-art approaches for image reconstruction using under-sampled k-space data are compressed sensing based. They are iterative algorithms that optimize objective functions with spatial and/or tempo...,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00044-y,23 April 2020,
622,0.000678989392134464,622,TraVaS: Differentially Private Trace Variant Selection for Process Mining,"Majid Rafiei, Frederik Wangelik, Wil M. P. van der Aalst","In the area of industrial process mining, privacy-preserving event data publication is becoming increasingly relevant. Consequently, the trade-off between high data utility and quantifiable privacy poses new challenges. State-of-the-art research mainly focuses on differentially private trace variant construction based on prefix expansion methods. However, these algorithms face several practical limitations such as high computational complexity, introducing fake variants, removing frequent variants, and a bounded variant length. In this paper, we introduce a new approach for direct differentially private trace variant release which uses anonymized \textit{partition selection} strategies to overcome the aforementioned restraints. Experimental results on real-life event data show that our algorithm outperforms state-of-the-art methods in terms of both plain data utility and result utility preservation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.14951,October 2022,622,Voice Biometrics over the Internet in the Framework of COST Action 275,"Laurent Besacier, Aladdin M Ariyaeeinia, John S Mason, Jean-François Bonastre, Pedro Mayorga, Corinne Fredouille, Sylvain Meignier, Johann Siau, Nicholas WD Evans, Roland Auckenthaler and Robert Stapert",The emerging field of biometric authentication over the Internet requires both robust person authentication and secure computer network protocols. This paper presents investigations of vocal biometric person a...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704310012,21 April 2004,
623,0.00435011815751728,623,Personalized Federated Learning via Heterogeneous Modular Networks,"Tianchun Wang, Wei Cheng, Dongsheng Luo, Wenchao Yu, Jingchao Ni, Liang Tong, Haifeng Chen, Xiang Zhang","Personalized Federated Learning (PFL) which collaboratively trains a federated model while considering local clients under privacy constraints has attracted much attention. Despite its popularity, it has been observed that existing PFL approaches result in sub-optimal solutions when the joint distribution among local clients diverges. To address this issue, we present Federated Modular Network (FedMN), a novel PFL approach that adaptively selects sub-modules from a module pool to assemble heterogeneous neural architectures for different clients. FedMN adopts a light-weighted routing hypernetwork to model the joint distribution on each client and produce the personalized selection of the module blocks for each client. To reduce the communication burden in existing FL, we develop an efficient way to interact between the clients and the server. We conduct extensive experiments on the real-world test beds and the results show both the effectiveness and efficiency of the proposed FedMN over the baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.14830,October 2022,623,Evolutionary TBL template generation,"Ruy Luiz Milidiú, Julio Cesar Duarte and Cícero Nogueira dos Santos",Transformation Based Learning (TBL) is a Machine Learning technique frequently used in some Natural Language Processing (NLP) tasks. TBL uses rule templates to identify error-correcting patterns. A critical re...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194255,December 2007,
624,0.000678989392134464,624,A Case for Business Process-Specific Foundation Models,"Yara Rizk, Praveen Venkateswaran, Vatche Isahagian, Vinod Muthusamy","The inception of large language models has helped advance state-of-the-art performance on numerous natural language tasks. This has also opened the door for the development of foundation models for other domains and data modalities such as images, code, and music. In this paper, we argue that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making. These models should also tackle the unique challenges of applying AI to business processes which include data scarcity, multi-modal representations, domain specific terminology, and privacy concerns. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.14739,October 2022,624,TAnnotator: Towards Annotating Programming E-textbooks with Facts and Examples,Akhila Sri Manasa Venigalla and Sridhar Chimalakonda,"E-textbooks are one of the commonly used sources to learn programming, in the domain of computer science and engineering. Programming related textbooks provide examples related to syntax, but the number of exa...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00228-y,25 January 2023,
625,0.000678989392134464,625,Enhancing Product Safety in E-Commerce with NLP,"Kishaloy Halder, Josip Krapac, Dmitry Goryunov, Anthony Brew, Matti Lyra, Alsida Dizdari, William Gillett, Adrien Renahy, Sinan Tang","Ensuring safety of the products offered to the customers is of paramount importance to any e- commerce platform. Despite stringent quality and safety checking of products listed on these platforms, occasionally customers might receive a product that can pose a safety issue arising out of its use. In this paper, we present an innovative mechanism of how a large scale multinational e-commerce platform, Zalando, uses Natural Language Processing techniques to assist timely investigation of the potentially unsafe products mined directly from customer written claims in unstructured plain text. We systematically describe the types of safety issues that concern Zalando customers. We demonstrate how we map this core business problem into a supervised text classification problem with highly imbalanced, noisy, multilingual data in a AI-in-the-loop setup with a focus on Key Performance Indicator (KPI) driven evaluation. Finally, we present detailed ablation studies to show a comprehensive comparison between different classification techniques. We conclude the work with how this NLP model was deployed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.14363,October 2022,625,A modeling methodology for hierarchical control system and its aplication,"Paolo Lollini, Andrea Bondavalli and Felicita Di Giandomenico","Current and future computerized systems and infrastructures are going to be based on the layering of different systems, designed at different times, with different technologies and components and difficult to ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192366,October 2004,
626,0.000678989392134464,626,On Robust Incremental Learning over Many Multilingual Steps,"Karan Praharaj, Irina Matveeva","Recent work in incremental learning has introduced diverse approaches to tackle catastrophic forgetting from data augmentation to optimized training regimes. However, most of them focus on very few training steps. We propose a method for robust incremental learning over dozens of fine-tuning steps using data from a variety of languages. We show that a combination of data-augmentation and an optimized training regime allows us to continue improving the model even for as many as fifty training steps. Crucially, our augmentation strategy does not require retaining access to previous training data and is suitable in scenarios with privacy constraints. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.14307,October 2022,626,Editorial for the special issue on micro-/nanorobotics,"Hongsoo Choi, Wen J Li and Li Zhang",Unknown,https://www.springeropen.com//jrobio.springeropen.com/articles/10.1186/s40638-014-0025-0,7 March 2015,
627,0.000678989392134464,627,FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification,"Yulin Zhu, Liang Tong, Kai Zhou","Recently, a lot of research attention has been devoted to exploring Web security, a most representative topic is the adversarial robustness of graph mining algorithms. Especially, a widely deployed adversarial attacks formulation is the graph manipulation attacks by modifying the relational data to mislead the Graph Neural Networks' (GNNs) predictions. Naturally, an intrinsic question one would ask is whether we can accurately identify the manipulations over graphs - we term this problem as poisoned graph sanitation. In this paper, we present FocusedCleaner, a poisoned graph sanitation framework consisting of two modules: bi-level structural learning and victim node detection. In particular, the structural learning module will reserve the attack process to steadily sanitize the graph while the detection module provides the ""focus"" - a narrowed and more accurate search region - to structural learning. These two modules will operate in iterations and reinforce each other to sanitize a poisoned graph step by step. Extensive experiments demonstrate that FocusedCleaner outperforms the state-of-the-art baselines both on poisoned graph sanitation and improving robustness. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.13815,October 2022,627,On the parallel efficiency and scalability of the correntropy coefficient for image analysis,"Aluisio I Rêgo Fontes, Samuel Xavier-de-Souza, Adrião D Dória Neto and Luiz Felipe de Queiroz Silveira",Similarity measures have application in many scenarios of digital image processing. The correntropy is a robust and relatively new similarity measure that recently has been employed in various engineering appl...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-014-0018-4,16 October 2014,
628,0.000678989392134464,628,Causal Analysis on the Anchor Store Effect in a Location-based Social Network,"Anish K. Vallapuram, Young D. Kwon, Lik-Hang Lee, Fengli Xu, Pan Hui","A particular phenomenon of interest in Retail Economics is the spillover effect of anchor stores (specific stores with a reputable brand) to non-anchor stores in terms of customer traffic. Prior works in this area rely on small and survey-based datasets that are often confidential or expensive to collect on a large scale. Also, very few works study the underlying causal mechanisms between factors that underpin the spillover effect. In this work, we analyse the causal relationship between anchor stores and customer traffic to non-anchor stores and employ a propensity score matching framework to investigate this effect more efficiently. First of all, to demonstrate the effect, we leverage open and mobile data from London Datastore and Location-Based Social Networks (LBSNs) such as Foursquare. We then perform a large-scale empirical analysis on customer visit patterns from anchor stores to non-anchor stores(e.g., non-chain restaurants) located in the Greater London area as a case study. By studying over 600 neighbourhoods in the GreaterLondon Area, we find that anchor stores cause a 14.2-26.5% increase in customer traffic for the non-anchor stores reinforcing the established economic theory. Moreover, we evaluate the efficiency of our methodology by studying the confounder balance, dose difference and performance of matching framework on synthetic data. Through this work, we point decision-makers in the retail industry to a more systematic approach to estimate the anchor store effect and pave the way for further research to discover more complex causal relationships underlying this effect with open data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.13582,October 2022,628,PROMIS® Health Organization (PHO) 2022 Conference Abstracts,Unknown,"This article is part of a Supplement:Volume 7
                                        Supplement 1",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-023-00547-1,21 March 2023,
629,0.000678989392134464,629,Path association rule mining,Yuya Sasaki,"Graph association rule mining is a data mining technique used for discovering regularities in graph data. In this study, we propose a novel concept, {\it path association rule mining}, to discover the correlations of path patterns that frequently appear in a given graph. Reachability path patterns (i.e., existence of paths from a vertex to another vertex) are applied in our concept to discover diverse regularities. We show that the problem is NP-hard, and we develop an efficient algorithm in which the anti-monotonic property is used on path patterns. Subsequently, we develop approximation and parallelization techniques to efficiently and scalably discover rules. We use real-life graphs to experimentally verify the effective △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.13136,October 2022,629,Chemical composition of ethanol extract ofMacrotyloma uniflorum(Lam.) Verdc. using GC-MS spectroscopy,"Sneha Das, Neeru Vasudeva and Sunil Sharma","Macrotyloma uniflorumLinn (Fabaceae) is a herbaceous plant with annual branches. It is used in kidney stones, inflamed joints, fever, musculoskeletal disorders, sinus wounds and localized abdominal tumors. It is...",https://www.springeropen.com//orgmedchemlett.springeropen.com/articles/10.1186/s13588-014-0013-y,2 December 2014,
630,0.000678989392134464,630,Full-Text Argumentation Mining on Scientific Publications,"Arne Binder, Bhuvanesh Verma, Leonhard Hennig","Scholarly Argumentation Mining (SAM) has recently gained attention due to its potential to help scholars with the rapid growth of published scientific literature. It comprises two subtasks: argumentative discourse unit recognition (ADUR) and argumentative relation extraction (ARE), both of which are challenging since they require e.g. the integration of domain knowledge, the detection of implicit statements, and the disambiguation of argument structure. While previous work focused on dataset construction and baseline methods for specific document sections, such as abstract or results, full-text scholarly argumentation mining has seen little progress. In this work, we introduce a sequential pipeline model combining ADUR and ARE for full-text SAM, and provide a first analysis of the performance of pretrained language models (PLMs) on both subtasks. We establish a new SotA for ADUR on the Sci-Arg corpus, outperforming the previous best reported result by a large margin (+7% F1). We also present the first results for ARE, and thus for the full AM pipeline, on this benchmark dataset. Our detailed error analysis reveals that non-contiguous ADUs as well as the interpretation of discourse connectors pose major challenges and that data annotation needs to be more consistent. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.13084,October 2022,630,Digital implant placement accuracy: a clinical study on a fully-guided flapless single-unit immediate-loading protocol,"Parsa Pirooz, Faezeh Atri, Paria Gholami and Mohammad Bayat",The primary aim of the present study was to measure the discrepancy between the virtual and the actual position of the single-unit implants placed via a digitally-designed fully-guided surgical template using ...,https://www.springeropen.com//jkamprs.springeropen.com/articles/10.1186/s40902-023-00387-5,17 May 2023,
631,0.000678989392134464,631,Global Contrastive Batch Sampling via Optimization on Sample Permutations,"Vin Sachidananda, Ziyi Yang, Chenguang Zhu","Contrastive Learning has recently achieved state-of-the-art performance in a wide range of tasks. Many contrastive learning approaches use mined hard negatives to make batches more informative during training but these approaches are inefficient as they increase epoch length proportional to the number of mined negatives and require frequent updates of nearest neighbor indices or mining from recent batches. In this work, we provide an alternative to hard negative mining, Global Contrastive Batch Sampling (GCBS), an efficient approximation to the batch assignment problem that upper bounds the gap between the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$, in contrastive learning settings. Through experimentation we find GCBS improves state-of-the-art performance in sentence embedding and code-search tasks. Additionally, GCBS is easy to implement as it requires only a few additional lines of code, does not maintain external data structures such as nearest neighbor indices, is more computationally efficient than the most minimal hard negative mining approaches, and makes no changes to the model being trained. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12874,October 2022,631,Exploration of nature-based biomimetic approach in landscape architectural design: parametric study of candelabra model design,Biljana S. Jović and Anđela D. Mitić,This research provides an exploration of a biomimetic approach in the process of designing a candelabra model using linear shaped leaves of a Bell flower. The design process described in this research contains...,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00060-y,4 November 2020,
632,0.000678989392134464,632,Bootstrapping meaning through listening: Unsupervised learning of spoken sentence embeddings,"Jian Zhu, Zuoyu Tian, Yadong Liu, Cong Zhang, Chia-wen Lo","Inducing semantic representations directly from speech signals is a highly challenging task but has many useful applications in speech mining and spoken language understanding. This study tackles the unsupervised learning of semantic representations for spoken utterances. Through converting speech signals into hidden units generated from acoustic unit discovery, we propose WavEmbed, a multimodal sequential autoencoder that predicts hidden units from a dense representation of speech. Secondly, we also propose S-HuBERT to induce meaning through knowledge distillation, in which a sentence embedding model is first trained on hidden units and passes its knowledge to a speech encoder through contrastive learning. The best performing model achieves a moderate correlation (0.5~0.6) with human judgments, without relying on any labels or transcriptions. Furthermore, these models can also be easily extended to leverage textual transcriptions of speech to learn much better speech embeddings that are strongly correlated with human annotations. Our proposed methods are applicable to the development of purely data-driven systems for speech mining, indexing and search. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12857,October 2022,632,Applications of artificial intelligence and deep learning in molecular imaging and radiotherapy,Hossein Arabi and Habib Zaidi,"This brief review summarizes the major applications of artificial intelligence (AI), in particular deep learning approaches, in molecular imaging and radiation therapy research. To this end, the applications o...",https://www.springeropen.com//ejhi.springeropen.com/articles/10.1186/s41824-020-00086-8,23 September 2020,
633,0.000678989392134464,633,Deep-Learning-Based Precipitation Nowcasting with Ground Weather Station Data and Radar Data,"Jihoon Ko, Kyuhan Lee, Hyunjin Hwang, Kijung Shin","Recently, many deep-learning techniques have been applied to various weather-related prediction tasks, including precipitation nowcasting (i.e., predicting precipitation levels and locations in the near future). Most existing deep-learning-based approaches for precipitation nowcasting, however, consider only radar and/or satellite images as inputs, and meteorological observations collected from ground weather stations, which are sparsely located, are relatively unexplored. In this paper, we propose ASOC, a novel attentive method for effectively exploiting ground-based meteorological observations from multiple weather stations. ASOC is designed to capture temporal dynamics of the observations and also contextual relationships between them. ASOC is easily combined with existing image-based precipitation nowcasting models without changing their architectures. We show that such a combination improves the average critical success index (CSI) of predicting heavy (at least 10 mm/hr) and light (at least 1 mm/hr) rainfall events at 1-6 hr lead times by 5.7%, compared to the original image-based model, using the radar images and ground-based observations around South Korea collected from 2014 to 2020. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12853,October 2022,633,Abstracts from the 9th DACH+ Conference on Energy Informatics,Unknown,"This article is part of a Supplement:Volume 3
                                        Supplement 2",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-020-00113-9,30 October 2020,
634,0.000678989392134464,634,On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?,"Yuping Wu, Lifeng Han, Valerio Antonini, Goran Nenadic","Pre-trained language models (PLMs) have been deployed in many natural language processing (NLP) tasks and in various domains. Language model pre-training from general or mixed domain rich data plus fine-tuning using small amounts of available data in a low resource domain demonstrated beneficial results by researchers. In this work, we question this statement and verify if BERT-based PLMs from the biomedical domain can perform well in clinical text mining tasks via fine-tuning. We test the state-of-the-art models, i.e. Bioformer which is pre-trained on a large amount of biomedical data from PubMed corpus. We use a historical n2c2 clinical NLP challenge dataset for fine-tuning its task-adapted version (BioformerApt), and show that their performances are actually very low. We also present our own end-to-end model, TransformerCRF, which is developed using Transformer and conditional random fields (CRFs) as encoder and decoder. We further create a new variation model by adding a CRF layer on top of PLM Bioformer (BioformerCRF). We investigate the performances of TransformerCRF on clinical text mining tasks by training from scratch using a limited amount of data, as well as the model BioformerCRF. Experimental evaluation shows that, in a \textit{constrained setting}, all tested models are \textit{far from ideal} regarding extreme low-frequency special token recognition, even though they can achieve relatively higher accuracy on overall text tagging. Our models including source codes will be hosted at \url{https://github.com/poethan/TransformerCRF}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12770,October 2022,634,"Big Data and discrimination: perils, promises and solutions. A systematic review","Maddalena Favaretto, Eva De Clercq and Bernice Simone Elger","Big Data analytics such as credit scoring and predictive analytics offer numerous opportunities but also raise considerable concerns, among which the most pressing is the risk of discrimination. Although this ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0177-4,5 February 2019,
635,0.000678989392134464,635,Blockchain and Machine Learning for Fraud Detection: A Privacy-Preserving and Adaptive Incentive Based Approach,"Tahmid Hasan Pranto, Kazi Tamzid Akhter Md Hasib, Tahsinur Rahman, AKM Bahalul Haque, A. K. M. Najmul Islam, Rashedur M. Rahman","Financial fraud cases are on the rise even with the current technological advancements. Due to the lack of inter-organization synergy and because of privacy concerns, authentic financial transaction data is rarely available. On the other hand, data-driven technologies like machine learning need authentic data to perform precisely in real-world systems. This study proposes a blockchain and smart contract-based approach to achieve robust Machine Learning (ML) algorithm for e-commerce fraud detection by facilitating inter-organizational collaboration. The proposed method uses blockchain to secure the privacy of the data. Smart contract deployed inside the network fully automates the system. An ML model is incrementally upgraded from collaborative data provided by the organizations connected to the blockchain. To incentivize the organizations, we have introduced an incentive mechanism that is adaptive to the difficulty level in updating a model. The organizations receive incentives based on the difficulty faced in updating the ML model. A mining criterion has been proposed to mine the block efficiently. And finally, the blockchain network istested under different difficulty levels and under different volumes of data to test its efficiency. The model achieved 98.93% testing accuracy and 98.22% Fbeta score (recall-biased f measure) over eight incremental updates. Our experiment shows that both data volume and difficulty level of blockchain impacts the mining time. For difficulty level less than five, mining time and difficulty level has a positive correlation. For difficulty level two and three, less than a second is required to mine a block in our system. Difficulty level five poses much more difficulties to mine the blocks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12609,October 2022,635,Regularized implicit surface reconstruction from points and normals,"B. Mederos, M. Lage, S. Arouca, F. Petronetto, L. Velho, T. Lewiner and H. Lopes",We consider the problem of surface reconstruction of a geometric object from a finite set of sample points with normals. Our contribution is to present a new scheme for implicit surface reconstruction. Similar...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194253,December 2007,
636,0.000678989392134464,636,Factor Investing with a Deep Multi-Factor Model,"Zikai Wei, Bo Dai, Dahua Lin","Modeling and characterizing multiple factors is perhaps the most important step in achieving excess returns over market benchmarks. Both academia and industry are striving to find new factors that have good explanatory power for future stock returns and good stability of their predictive power. In practice, factor investing is still largely based on linear multi-factor models, although many deep learning methods show promising results compared to traditional methods in stock trend prediction and portfolio risk management. However, the existing non-linear methods have two drawbacks: 1) there is a lack of interpretation of the newly discovered factors, 2) the financial insights behind the mining process are unclear, making practitioners reluctant to apply the existing methods to factor investing. To address these two shortcomings, we develop a novel deep multi-factor model that adopts industry neutralization and market neutralization modules with clear financial insights, which help us easily build a dynamic and multi-relational stock graph in a hierarchical structure to learn the graph representation of stock relationships at different levels, e.g., industry level and universal level. Subsequently, graph attention modules are adopted to estimate a series of deep factors that maximize the cumulative factor returns. And a factor-attention module is developed to approximately compose the estimated deep factors from the input factors, as a way to interpret the deep factors explicitly. Extensive experiments on real-world stock market data demonstrate the effectiveness of our deep multi-factor model in the task of factor investing. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12462,October 2022,636,On-board Data Processing to Lower Bandwidth Requirements on an Infrared Astronomy Satellite: Case of Herschel-PACS Camera,"Ahmed Nabil Belbachir, Horst Bischof, Roland Ottensamer, Franz Kerschbaum and Christian Reimers","This paper presents a new data compression concept, ""on-board processing,"" for infrared astronomy, where space observatories have limited processing resources. The proposed approach has been developed and test...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2585,14 September 2005,
637,0.000678989392134464,637,DIGMN: Dynamic Intent Guided Meta Network for Differentiated User Engagement Forecasting in Online Professional Social Platforms,"Feifan Li, Lun Du, Qiang Fu, Shi Han, Yushu Du, Guangming Lu, Zi Li","User engagement prediction plays a critical role for designing interaction strategies to grow user engagement and increase revenue in online social platforms. Through the in-depth analysis of the real-world data from the world's largest professional social platforms, i.e., LinkedIn, we find that users expose diverse engagement patterns, and a major reason for the differences in user engagement patterns is that users have different intents. That is, people have different intents when using LinkedIn, e.g., applying for jobs, building connections, or checking notifications, which shows quite different engagement patterns. Meanwhile, user intents and the corresponding engagement patterns may change over time. Although such pattern differences and dynamics are essential for user engagement prediction, differentiating user engagement patterns based on user dynamic intents for better user engagement forecasting has not received enough attention in previous works. In this paper, we proposed a Dynamic Intent Guided Meta Network (DIGMN), which can explicitly model user intent varying with time and perform differentiated user engagement forecasting. Specifically, we derive some interpretable basic user intents as prior knowledge from data mining and introduce prior intents in explicitly modeling dynamic user intent. Furthermore, based on the dynamic user intent representations, we propose a meta predictor to perform differentiated user engagement forecasting. Through a comprehensive evaluation on LinkedIn anonymous user data, our method outperforms state-of-the-art baselines significantly, i.e., 2.96% and 3.48% absolute error reduction, on coarse-grained and fine-grained user engagement prediction tasks, respectively, demonstrating the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12402,October 2022,637,A Probabilistic Multimedia Retrieval Model and Its Evaluation,"Thijs Westerveld, Arjen P. de Vries, Alex van Ballegooij, Franciska de Jong and Djoerd Hiemstra",We present a probabilistic model for the retrieval of multimodal documents. The model is based on Bayesian decision theory and combines models for text-based search with models for visual search. The textual m...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570321101X,25 February 2003,
638,0.000678989392134464,638,The Devil is in the Conflict: Disentangled Information Graph Neural Networks for Fraud Detection,"Zhixun Li, Dingshuo Chen, Qiang Liu, Shu Wu","Graph-based fraud detection has heretofore received considerable attention. Owning to the great success of Graph Neural Networks (GNNs), many approaches adopting GNNs for fraud detection has been gaining momentum. However, most existing methods are based on the strong inductive bias of homophily, which indicates that the context neighbors tend to have same labels or similar features. In real scenarios, fraudsters often engage in camouflage behaviors in order to avoid detection system. Therefore, the homophilic assumption no longer holds, which is known as the inconsistency problem. In this paper, we argue that the performance degradation is mainly attributed to the inconsistency between topology and attribute. To address this problem, we propose to disentangle the fraud network into two views, each corresponding to topology and attribute respectively. Then we propose a simple and effective method that uses the attention mechanism to adaptively fuse two views which captures data-specific preference. In addition, we further improve it by introducing mutual information constraints for topology and attribute. To this end, we propose a Disentangled Information Graph Neural Network (DIGNN) model, which utilizes variational bounds to find an approximate solution to our proposed optimization objective function. Extensive experiments demonstrate that our model can significantly outperform stateof-the-art baselines on real-world fraud detection datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12384,October 2022,638,Learning science with YouTube videos and the impacts of Covid-19,Wayne Breslyn and Amy E. Green,This study investigates student and teacher use of online instructional YouTube chemistry videos in the context of the Covid-19 pandemic. Data were collected from a global sample of students (n= 1147) subscribed...,https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00051-4,11 April 2022,
639,0.000678989392134464,639,Feature selection intelligent algorithm with mutual information and steepest ascent strategy,"Elkebir Sarhrouni, Ahmed Hammouch, Driss Aboutajdine","Remote sensing is a higher technology to produce knowledge for data mining applications. In principle hyperspectral images (HSIs) is a remote sensing tool that provides precise classification of regions. The HSI contains more than a hundred of images of the ground truth (GT) map. Some images are carrying relevant information, but others describe redundant information, or they are affected by atmospheric noise. The aim is to reduce dimensionality of HSI. Many studies use mutual information (MI) or normalised forms of MI to select appropriate bands. In this paper we design an algorithm based also on MI, and we combine MI with steepest ascent algorithm, to improve a symmetric uncertainty coefficient-based strategy to select relevant bands for classification of HSI. This algorithm is a feature selection tool and a wrapper strategy. We perform our study on HSI AVIRIS 92AV3C. This is an artificial intelligent system to control redundancy; we had to clear the difference of the result's algorithm and the human decision, and this can be viewed as case study which human decision is perhaps different to an intelligent algorithm. Index Terms - Hyperspectral images, Classification, Fea-ture selection, Mutual Information, Redundancy, Steepest Ascent. Artificial Intelligence △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12296,October 2022,639,Efficient and robust adaptive consensus services based on oracles,"Lívia Sampaio, Francisco Brasileiro, Raul Ceretta Nunes and Ingrid Jansch-Pôrto","Due to their fundamental role in the design of fault-tolerant distributed systems, consensus protocols have been widely studied. Most of the research in this area has focused on providing ways for circumventin...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192364,October 2004,
640,0.000678989392134464,640,Auto-Encoder Neural Network Incorporating X-Ray Fluorescence Fundamental Parameters with Machine Learning,"Matthew Dirks, David Poole","We consider energy-dispersive X-ray Fluorescence (EDXRF) applications where the fundamental parameters method is impractical such as when instrument parameters are unavailable. For example, on a mining shovel or conveyor belt, rocks are constantly moving (leading to varying angles of incidence and distances) and there may be other factors not accounted for (like dust). Neural networks do not require instrument and fundamental parameters but training neural networks requires XRF spectra labelled with elemental composition, which is often limited because of its expense. We develop a neural network model that learns from limited labelled data and also benefits from domain knowledge by learning to invert a forward model. The forward model uses transition energies and probabilities of all elements and parameterized distributions to approximate other fundamental and instrument parameters. We evaluate the model and baseline models on a rock dataset from a lithium mineral exploration project. Our model works particularly well for some low-Z elements (Li, Mg, Al, and K) as well as some high-Z elements (Sn and Pb) despite these elements being outside the suitable range for common spectrometers to directly measure, likely owing to the ability of neural networks to learn correlations and non-linear relationships. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.12239,October 2022,640,Artificial intelligence and hybrid imaging: the best match for personalized medicine in oncology,"Martina Sollini, Francesco Bartoli, Andrea Marciano, Roberta Zanca, Riemer H. J. A. Slart and Paola A. Erba","Artificial intelligence (AI) refers to a field of computer science aimed to perform tasks typically requiring human intelligence. Currently, AI is recognized in the broader technology radar within the five key...",https://www.springeropen.com//ejhi.springeropen.com/articles/10.1186/s41824-020-00094-8,9 December 2020,
641,0.000678989392134464,641,AfroLID: A Neural Language Identification Tool for African Languages,"Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, Alcides Alcoba Inciarte","Language identification (LID) is a crucial precursor for NLP, especially for mining web data. Problematically, most of the world's 7000+ languages today are not covered by LID technologies. We address this pressing issue for Africa by introducing AfroLID, a neural LID toolkit for $517$ African languages and varieties. AfroLID exploits a multi-domain web dataset manually curated from across 14 language families utilizing five orthographic systems. When evaluated on our blind Test set, AfroLID achieves 95.89 F_1-score. We also compare AfroLID to five existing LID tools that each cover a small number of African languages, finding it to outperform them on most languages. We further show the utility of AfroLID in the wild by testing it on the acutely under-served Twitter domain. Finally, we offer a number of controlled case studies and perform a linguistically-motivated error analysis that allow us to both showcase AfroLID's powerful capabilities and limitations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.11744,October 2022,641,Artificial intelligence and hybrid imaging: the best match for personalized medicine in oncology,"Martina Sollini, Francesco Bartoli, Andrea Marciano, Roberta Zanca, Riemer H. J. A. Slart and Paola A. Erba","Artificial intelligence (AI) refers to a field of computer science aimed to perform tasks typically requiring human intelligence. Currently, AI is recognized in the broader technology radar within the five key...",https://www.springeropen.com//ejhi.springeropen.com/articles/10.1186/s41824-020-00094-8,9 December 2020,
642,0.00514534320972017,642,Finding the smallest or largest element of a tensor from its low-rank factors,"Nicholas D. Sidiropoulos, Paris Karakasis, Aritra Konar","We consider the problem of finding the smallest or largest entry of a tensor of order $N$ that is specified via its rank decomposition. Stated in a different way, we are given $N$ sets of $R$-dimensional vectors and we wish to select one vector from each set such that the sum of the Hadamard product of the selected vectors is minimized or maximized. This is a fundamental tensor problem with numerous applications in embedding similarity search, recommender systems, graph mining, multivariate probability, and statistics. We show that this discrete optimization problem is NP-hard for any tensor rank higher than one, but also provide an equivalent continuous problem reformulation which is amenable to disciplined non-convex optimization. We propose a suite of gradient-based approximation algorithms whose performance in preliminary experiments appears to be promising. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.11413,October 2022,642,From compressive sampling to compressive tasking: retrieving semantics in compressed domain with low bandwidth,"Zhihong Zhang, Bo Zhang, Xin Yuan, Siming Zheng, Xiongfei Su, Jinli Suo, David J. Brady and Qionghai Dai","High-throughput imaging is highly desirable in intelligent analysis of computer vision tasks. In conventional design, throughput is limited by the separation between physical image capture and digital post pro...",https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-022-00065-1,6 September 2022,
643,0.000678989392134464,643,Mathematical Justification of Hard Negative Mining via Isometric Approximation Theorem,"Albert Xu, Jhih-Yi Hsieh, Bhaskar Vundurthy, Eliana Cohen, Howie Choset, Lu Li","In deep metric learning, the Triplet Loss has emerged as a popular method to learn many computer vision and natural language processing tasks such as facial recognition, object detection, and visual-semantic embeddings. One issue that plagues the Triplet Loss is network collapse, an undesirable phenomenon where the network projects the embeddings of all data onto a single point. Researchers predominately solve this problem by using triplet mining strategies. While hard negative mining is the most effective of these strategies, existing formulations lack strong theoretical justification for their empirical success. In this paper, we utilize the mathematical theory of isometric approximation to show an equivalence between the Triplet Loss sampled by hard negative mining and an optimization problem that minimizes a Hausdorff-like distance between the neural network and its ideal counterpart function. This provides the theoretical justifications for hard negative mining's empirical efficacy. In addition, our novel application of the isometric approximation theorem provides the groundwork for future forms of hard negative mining that avoid network collapse. Our theory can also be extended to analyze other Euclidean space-based metric learning methods like Ladder Loss or Contrastive Learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.11173,October 2022,643,Non-iterative image reconstruction from sparse magnetic resonance imaging radial data without priors,Gengsheng L. Zeng and Edward V. DiBella,The state-of-the-art approaches for image reconstruction using under-sampled k-space data are compressed sensing based. They are iterative algorithms that optimize objective functions with spatial and/or tempo...,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00044-y,23 April 2020,
644,0.000678989392134464,644,User Value in Modern Payment Platforms: A Graph Approach,"Laura Arditti, Martino Trevisan, Luca Vassio, Alberto De Lazzari, Alberto Danese","Payment platforms have significantly evolved in recent years to keep pace with the proliferation of online and cashless payments. These platforms are increasingly aligned with online social networks, allowing users to interact with each other and transfer small amounts of money in a Peer-to-Peer fashion. This poses new challenges for analysing payment data, as traditional methods are only user-centric or business-centric and neglect the network users build during the interaction. This paper proposes a first methodology for measuring user value in modern payment platforms. We combine quantitative user-centric metrics with an analysis of the graph created by users' activities and its topological features inspired by the evolution of opinions in social networks. We showcase our approach using a dataset from a large operational payment platform and show how it can support business decisions and marketing campaign design, e.g., by targeting specific users. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.11168,October 2022,644,Editorial,Ronaldo Menezes and Hocine Cherifi,Unknown,https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-016-0002-3,1 June 2016,
645,0.000678989392134464,645,Machine and Deep Learning Methods with Manual and Automatic Labelling for News Classification in Bangla Language,"Istiak Ahmad, Fahad AlQurashi, Rashid Mehmood","Research in Natural Language Processing (NLP) has increasingly become important due to applications such as text classification, text mining, sentiment analysis, POS tagging, named entity recognition, textual entailment, and many others. This paper introduces several machine and deep learning methods with manual and automatic labelling for news classification in the Bangla language. We implemented several machine (ML) and deep learning (DL) algorithms. The ML algorithms are Logistic Regression (LR), Stochastic Gradient Descent (SGD), Support Vector Machine (SVM), Random Forest (RF), and K-Nearest Neighbour (KNN), used with Bag of Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), and Doc2Vec embedding models. The DL algorithms are Long Short-Term Memory (LSTM), Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and Convolutional Neural Network (CNN), used with Word2vec, Glove, and FastText word embedding models. We develop automatic labelling methods using Latent Dirichlet Allocation (LDA) and investigate the performance of single-label and multi-label article classification methods. To investigate performance, we developed from scratch Potrika, the largest and the most extensive dataset for news classification in the Bangla language, comprising 185.51 million words and 12.57 million sentences contained in 664,880 news articles in eight distinct categories, curated from six popular online news portals in Bangladesh for the period 2014-2020. GRU and Fasttext with 91.83% achieve the highest accuracy for manually-labelled data. For the automatic labelling case, KNN and Doc2Vec at 57.72% and 75% achieve the highest accuracy for single-label and multi-label data, respectively. The methods developed in this paper are expected to advance research in Bangla and other languages. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10903,October 2022,645,"Big Data and discrimination: perils, promises and solutions. A systematic review","Maddalena Favaretto, Eva De Clercq and Bernice Simone Elger","Big Data analytics such as credit scoring and predictive analytics offer numerous opportunities but also raise considerable concerns, among which the most pressing is the risk of discrimination. Although this ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0177-4,5 February 2019,
646,0.000678989392134464,646,Dodona: learn to code with a virtual co-teacher that supports active learning,"Charlotte Van Petegem, Rien Maertens, Niko Strijbol, Jorg Van Renterghem, Felix Van der Jeugt, Bram De Wever, Peter Dawyndt, Bart Mesuere","Dodona (dodona.ugent.be) is an intelligent tutoring system for computer programming. It bridges the gap between assessment and learning by providing real-time data and feedback to help students learn better, teachers teach better and educational technology become more effective. We demonstrate how Dodona can be used as a virtual co-teacher to stimulate active learning and support challenge-based education in open and collaborative learning environments. We also highlight some of the opportunities (automated feedback, learning analytics, educational data mining) and challenges (scalable feedback, open internet exams, plagiarism) we faced in practice. Dodona is free for use and has more than 36 thousand registered users across many educational and research institutes, of which 15 thousand new users registered last year. Lowering the barriers for such a broad adoption was achieved by following best practices and extensible approaches for software development, authentication, content management, assessment, security and interoperability, and by adopting a holistic view on computer-assisted learning and teaching that spans all aspects of managing courses that involve programming assignments. The source code of Dodona is available on GitHub under the permissive MIT open-source license. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10719,October 2022,646,A Feedback-Based Algorithm for Motion Analysis with Application to Object Tracking,Shesha Shah and P. S. Sastry,We present a motion detection algorithm which detects direction of motion at sufficient number of points and thus segregates the edge image into clusters of coherently moving points. Unlike most algorithms for...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/86064,1 December 2007,
647,0.000678989392134464,647,Depth Contrast: Self-Supervised Pretraining on 3DPM Images for Mining Material Classification,"Prakash Chandra Chhipa, Richa Upadhyay, Rajkumar Saini, Lars Lindqvist, Richard Nordenskjold, Seiichi Uchida, Marcus Liwicki","This work presents a novel self-supervised representation learning method to learn efficient representations without labels on images from a 3DPM sensor (3-Dimensional Particle Measurement; estimates the particle size distribution of material) utilizing RGB images and depth maps of mining material on the conveyor belt. Human annotations for material categories on sensor-generated data are scarce and cost-intensive. Currently, representation learning without human annotations remains unexplored for mining materials and does not leverage on utilization of sensor-generated data. The proposed method, Depth Contrast, enables self-supervised learning of representations without labels on the 3DPM dataset by exploiting depth maps and inductive transfer. The proposed method outperforms material classification over ImageNet transfer learning performance in fully supervised learning settings and achieves an F1 score of 0.73. Further, The proposed method yields an F1 score of 0.65 with an 11% improvement over ImageNet transfer learning performance in a semi-supervised setting when only 20% of labels are used in fine-tuning. Finally, the Proposed method showcases improved performance generalization on linear evaluation. The implementation of proposed method is available on GitHub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10633,October 2022,647,From face detection to emotion recognition on the framework of Raspberry pi and galvanic skin response sensor for visual and physiological biosignals,"Varsha Kiran Patil, Vijaya R. Pawar, Shreiya Randive, Rutika Rajesh Bankar, Dhanashree Yende and Aditya Kiran Patil",The facial and physiological sensor-based emotion recognition methods are two popular methods of emotion recognition. The proposed research is the first of its kind in real-time emotion recognition that combin...,https://www.springeropen.com//jesit.springeropen.com/articles/10.1186/s43067-023-00085-2,18 April 2023,
648,0.000678989392134464,648,HyperMiner: Topic Taxonomy Mining with Hyperbolic Embedding,"Yishi Xu, Dongsheng Wang, Bo Chen, Ruiying Lu, Zhibin Duan, Mingyuan Zhou","Embedded topic models are able to learn interpretable topics even with large and heavy-tailed vocabularies. However, they generally hold the Euclidean embedding space assumption, leading to a basic limitation in capturing hierarchical relations. To this end, we present a novel framework that introduces hyperbolic embeddings to represent words and topics. With the tree-likeness property of hyperbolic space, the underlying semantic hierarchy among words and topics can be better exploited to mine more interpretable topics. Furthermore, due to the superiority of hyperbolic geometry in representing hierarchical data, tree-structure knowledge can also be naturally injected to guide the learning of a topic hierarchy. Therefore, we further develop a regularization term based on the idea of contrastive learning to inject prior structural knowledge efficiently. Experiments on both topic taxonomy discovery and document representation demonstrate that the proposed framework achieves improved performance against existing embedded topic models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10625,October 2022,648,MPEG-4 Authoring Tool Using Moving Object Segmentation and Tracking in Video Shots,"Petros Daras, Ioannis Kompatsiaris, Ilias Grinias, Georgios Akrivas, Georgios Tziritas, Stefanos Kollias and Michael G. Strintzis",An Authoring tool for the MPEG-4 multimedia standard integrated with image sequence analysis algorithms is described. MPEG-4 offers numerous capabilities and is expected to be the future standard for multimedi...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703301052,18 August 2003,
649,0.000678989392134464,649,Community detection in edge-labeled graphs,"Iiro Kumpulainen, Nikolaj Tatti","Finding dense communities in networks is a widely-used tool for analysis in graph mining. A popular choice for finding such communities is to find subgraphs with a high average degree. While useful, interpreting such subgraphs may be difficult. On the other hand, many real-world networks have additional information, and we are specifically interested in networks that have labels on edges. In this paper, we study finding dense subgraphs that can be explained with the labels on edges. More specifically, we are looking for a set of labels so that the induced subgraph has a high average degree. There are many ways to induce a subgraph from a set of labels, and we study two cases: First, we study conjunctive-induced dense subgraphs, where the subgraph edges need to have all labels. Secondly, we study disjunctive-induced dense subgraphs, where the subgraph edges need to have at least one label. We show that both problems are $\textbf{NP}$-hard. Because of the hardness, we resort to greedy heuristics. We show that we can implement the greedy search efficiently: the respective running times for finding conjunctive-induced and disjunctive-induced dense subgraphs are in $\mathcal{O}(p \log k)$ and $\mathcal{O}(p \log^2 k)$, where $p$ is the number of edge-label pairs and $k$ is the number of labels. Our experimental evaluation demonstrates that we can find the ground truth in synthetic graphs and that we can find interpretable subgraphs from real-world networks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10583,October 2022,649,Examination of the usability of Tinkercad application in educational robotics teaching by eye tracking technique,"Rumeysa Erdogan, Zeynep Saglam, Gulay Cetintav and Fatma Gizem Karaoglan Yilmaz","Like all sectors, the education sector has been negatively affected by the Covid-19 pandemic. Considering the decision to conduct face-to-face training in schools remotely, teachers had many difficulties in mo...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00242-0,23 March 2023,
650,0.000678989392134464,650,Attribution and Obfuscation of Neural Text Authorship: A Data Mining Perspective,"Adaku Uchendu, Thai Le, Dongwon Lee","Two interlocking research questions of growing interest and importance in privacy research are Authorship Attribution (AA) and Authorship Obfuscation (AO). Given an artifact, especially a text t in question, an AA solution aims to accurately attribute t to its true author out of many candidate authors while an AO solution aims to modify t to hide its true authorship. Traditionally, the notion of authorship and its accompanying privacy concern is only toward human authors. However, in recent years, due to the explosive advancements in Neural Text Generation (NTG) techniques in NLP, capable of synthesizing human-quality open-ended texts (so-called ""neural texts""), one has to now consider authorships by humans, machines, or their combination. Due to the implications and potential threats of neural texts when used maliciously, it has become critical to understand the limitations of traditional AA/AO solutions and develop novel AA/AO solutions in dealing with neural texts. In this survey, therefore, we make a comprehensive review of recent literature on the attribution and obfuscation of neural text authorship from a Data Mining perspective, and share our view on their limitations and promising research directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10488,October 2022,650,A contemporary review of large-scale non-standard structural fire testing,"Luke Bisby, John Gales and Cristián Maluk","In recent years, large-scale structural fire testing has experienced something of a renaissance. After about a century with the standard fire resistance test being the predominant means to characterize the res...",https://www.springeropen.com//firesciencereviews.springeropen.com/articles/10.1186/2193-0414-2-1,21 May 2013,
651,0.000678989392134464,651,Discipline Reputation Evaluation Based on PhD Exchange Network,Shudong Yang,"When reputation evaluation indicators become targets, existing indicators will lose the role of indicating the true quality; At present, the evaluation of discipline reputation mostly focuses on subjective evaluation based on objective data, and there is a dispute about reliability and validity; Due to different indicators and weight settings, it is difficult to make horizontal comparison among disciplines; The evaluation also has a certain time lag. In order to solve the above four problems, this study explores a new method of discipline reputation evaluation. Taking the business administration discipline as an example, it collects data of 5848 doctoral graduates who first entered teaching posts, establishes a directed adjacency matrix from the employment unit to the doctoral degree awarding unit, and uses the theory and method of social network analysis to conduct quantitative analysis on the doctoral mutual employment network. The results show that: (1) PhD exchange network can explain discipline reputation and is a new indicator to measure discipline reputation; (2) From the perspective of employment behavior among colleges and universities, there is horizontal flow and downward flow between the head colleges and universities, and downward flow is mainly among the middle and lower colleges. There is a time lag between college talent recruitment and academic achievement output. Therefore, the mining of the structural characteristics and network evolution trend of the PhD exchange network based on the ""foot voting"" of doctoral graduates is faster than the discipline ranking based on the follow-up achievement indicators to reflect the changes in the discipline quality, which can be used to warn the changes in the discipline quality. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.10136,October 2022,651,Sound Synthesis of the Harpsichord Using a Computationally Efficient Physical Model,"Vesa Välimäki, Henri Penttinen, Jonte Knif, Mikael Laurson and Cumhur Erkut",A sound synthesis algorithm for the harpsichord has been developed by applying the principles of digital waveguide modeling. A modification to the loss filter of the string model is introduced that allows more...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570440211X,27 June 2004,
652,0.000678989392134464,652,DAGAD: Data Augmentation for Graph Anomaly Detection,"Fanzhen Liu, Xiaoxiao Ma, Jia Wu, Jian Yang, Shan Xue, Amin Beheshti, Chuan Zhou, Hao Peng, Quan Z. Sheng, Charu C. Aggarwal","Graph anomaly detection in this paper aims to distinguish abnormal nodes that behave differently from the benign ones accounting for the majority of graph-structured instances. Receiving increasing attention from both academia and industry, yet existing research on this task still suffers from two critical issues when learning informative anomalous behavior from graph data. For one thing, anomalies are usually hard to capture because of their subtle abnormal behavior and the shortage of background knowledge about them, which causes severe anomalous sample scarcity. Meanwhile, the overwhelming majority of objects in real-world graphs are normal, bringing the class imbalance problem as well. To bridge the gaps, this paper devises a novel Data Augmentation-based Graph Anomaly Detection (DAGAD) framework for attributed graphs, equipped with three specially designed modules: 1) an information fusion module employing graph neural network encoders to learn representations, 2) a graph data augmentation module that fertilizes the training set with generated samples, and 3) an imbalance-tailored learning module to discriminate the distributions of the minority (anomalous) and majority (normal) classes. A series of experiments on three datasets prove that DAGAD outperforms ten state-of-the-art baseline detectors concerning various mostly-used metrics, together with an extensive ablation study validating the strength of our proposed modules. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.09766,October 2022,652,Rigid Molecule Docking: FPGA Reconfiguration for Alternative Force Laws,"Tom VanCourt, Yongfeng Gu, Vikas Mundada and Martin Herbordt","Molecular docking is one of the primary computational methods used by pharmaceutical companies to try to reduce the cost of drug discovery. A common docking technique, used for low-resolution screening or as a...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/97950,1 December 2006,
653,0.000678989392134464,653,No Pairs Left Behind: Improving Metric Learning with Regularized Triplet Objective,"A. Ali Heydari, Naghmeh Rezaei, Daniel J. McDuff, Javier L. Prieto","We propose a novel formulation of the triplet objective function that improves metric learning without additional sample mining or overhead costs. Our approach aims to explicitly regularize the distance between the positive and negative samples in a triplet with respect to the anchor-negative distance. As an initial validation, we show that our method (called No Pairs Left Behind [NPLB]) improves upon the traditional and current state-of-the-art triplet objective formulations on standard benchmark datasets. To show the effectiveness and potentials of NPLB on real-world complex data, we evaluate our approach on a large-scale healthcare dataset (UK Biobank), demonstrating that the embeddings learned by our model significantly outperform all other current representations on tested downstream tasks. Additionally, we provide a new model-agnostic single-time health risk definition that, when used in tandem with the learned representations, achieves the most accurate prediction of subjects' future health complications. Our results indicate that NPLB is a simple, yet effective framework for improving existing deep metric learning models, showcasing the potential implications of metric learning in more complex applications, especially in the biological and healthcare domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.09506,October 2022,653,A Biologically Motivated Multiresolution Approach to Contour Detection,"Giuseppe Papari, Patrizio Campisi, Nicolai Petkov and Alessandro Neri","Standard edge detectors react to all local luminance changes, irrespective of whether they are due to the contours of the objects represented in a scene or due to natural textures like grass, foliage, water, a...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/71828,1 December 2007,
654,0.000678989392134464,654,Multi-granularity Argument Mining in Legal Texts,"Huihui Xu, Kevin Ashley","In this paper, we explore legal argument mining using multiple levels of granularity. Argument mining has usually been conceptualized as a sentence classification problem. In this work, we conceptualize argument mining as a token-level (i.e., word-level) classification problem. We use a Longformer model to classify the tokens. Results show that token-level text classification identifies certain legal argument elements more accurately than sentence-level text classification. Token-level classification also provides greater flexibility to analyze legal texts and to gain more insight into what the model focuses on when processing a large amount of input data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.09472,October 2022,654,Parameterized Facial Expression Synthesis Based on MPEG-4,"Amaryllis Raouzaiou, Nicolas Tsapatsoulis, Kostas Karpouzis and Stefanos Kollias","In the framework of MPEG-4, one can include applications where virtual agents, utilizing both textual and multisensory data, including facial expressions and nonverbal speech help systems become accustomed to ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206149,22 October 2002,
655,0.000678989392134464,655,Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints,"Omid Rohanian, Hannah Jauncey, Mohammadmahdi Nouriborji, Vinod Kumar Chauhan, Bronner P. Gonçalves, Christiana Kartsonaki, ISARIC Clinical Characterisation Group, Laura Merson, David Clifton","Processing information locked within clinical health records is a challenging task that remains an active area of research in biomedical NLP. In this work, we evaluate a broad set of machine learning techniques ranging from simple RNNs to specialised transformers such as BioBERT on a dataset containing clinical notes along with a set of annotations indicating whether a sample is cancer-related or not. Furthermore, we specifically employ efficient fine-tuning methods from NLP, namely, bottleneck adapters and prompt tuning, to adapt the models to our specialised task. Our evaluations suggest that fine-tuning a frozen BERT model pre-trained on natural language and with bottleneck adapters outperforms all other strategies, including full fine-tuning of the specialised BioBERT model. Based on our findings, we suggest that using bottleneck adapters in low-resource situations with limited access to labelled data or processing capacity could be a viable strategy in biomedical text mining. The code used in the experiments are going to be made available at https://github.com/omidrohanian/bottleneck-adapters. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.09440,October 2022,655,Evolution of metabolic networks: a computational frame-work,"Christoph Flamm, Alexander Ullrich, Heinz Ekker, Martin Mann, Daniel Högerl, Markus Rohrschneider, Sebastian Sauer, Gerik Scheuermann, Konstantin Klemm, Ivo L Hofacker and Peter F Stadler","The metabolic architectures of extant organisms share many key pathways such as the citric acid cycle, glycolysis, or the biosynthesis of most amino acids. Several competing hypotheses for the evolutionary mec...",https://www.springeropen.com//jsystchem.springeropen.com/articles/10.1186/1759-2208-1-4,18 August 2010,
656,0.000678989392134464,656,APGKT: Exploiting Associative Path on Skills Graph for Knowledge Tracing,"Haotian Zhang, Chenyang Bu, Fei Liu, Shuochen Liu, Yuhong Zhang, Xuegang Hu","Knowledge tracing (KT) is a fundamental task in educational data mining that mainly focuses on students' dynamic cognitive states of skills. The question-answering process of students can be regarded as a thinking process that considers the following two problems. One problem is which skills are needed to answer the question, and the other is how to use these skills in order. If a student wants to answer a question correctly, the student should not only master the set of skills involved in the question but also think and obtain the associative path on the skills graph. The nodes in the associative path refer to the skills needed and the path shows the order of using them. The associative path is referred to as the skill mode. Thus, obtaining the skill modes is the key to answering questions successfully. However, most existing KT models only focus on a set of skills, without considering the skill modes. We propose a KT model, called APGKT, that exploits skill modes. Specifically, we extract the subgraph topology of the skills involved in the question and combine the difficulty level of the skills to obtain the skill modes via encoding; then, through multi-layer recurrent neural networks, we obtain a student's higher-order cognitive states of skills, which is used to predict the student's future answering performance. Experiments on five benchmark datasets validate the effectiveness of the proposed model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.08971,October 2022,656,Local DNA sequence alignment in a cluster of workstations: Algorithms and tools,"Alba Cristina M. A. Melo, Maria Emilia M. T. Walter, Renata Cristina F. Melo, Marcelo N. P. Santana and Rodolfo B. Batista",Distributed Shared Memory systems allow the use of the shared memory programming paradigm in distributed architectures where no physically shared memory exist. Scope consistent software DSMs provide a relaxed ...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192360,June 2004,
657,0.000678989392134464,657,Macaw: The Machine Learning Magnetometer Calibration Workflow,"Jonathan Bader, Kevin Styp-Rekowski, Leon Doehler, Soeren Becker, Odej Kao","In Earth Systems Science, many complex data pipelines combine different data sources and apply data filtering and analysis steps. Typically, such data analysis processes are historically grown and implemented with many sequentially executed scripts. Scientific workflow management systems (SWMS) allow scientists to use their existing scripts and provide support for parallelization, reusability, monitoring, or failure handling. However, many scientists still rely on their sequentially called scripts and do not profit from the out-of-the-box advantages a SWMS can provide. In this work, we transform the data analysis processes of a Machine Learning-based approach to calibrate the platform magnetometers of non-dedicated satellites utilizing neural networks into a workflow called Macaw (MAgnetometer CAlibration Workflow). We provide details on the workflow and the steps needed to port these scripts to a scientific workflow. Our experimental evaluation compares the original sequential script executions on the original HPC cluster with our workflow implementation on a commodity cluster. Our results show that through porting, our implementation decreased the allocated CPU hours by 50.2% and the memory hours by 59.5%, leading to significantly less resource wastage. Further, through parallelizing single tasks, we reduced the runtime by 17.5%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.08897,October 2022,657,Correction to: Supplementary span-to-depth ratio expressions for one-way slab complying with ACI-318 and SBC-304 deflection limits,Hamdy A. Elgohary,Theoriginal articlewas published inJournal of Umm Al-Qura University for Engineering and Architecture202314:s43995-023-00015-3,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s43995-023-00020-6,21 April 2023,
658,0.000678989392134464,658,Improving the Intra-class Long-tail in 3D Detection via Rare Example Mining,"Chiyu Max Jiang, Mahyar Najibi, Charles R. Qi, Yin Zhou, Dragomir Anguelov","Continued improvements in deep learning architectures have steadily advanced the overall performance of 3D object detectors to levels on par with humans for certain tasks and datasets, where the overall performance is mostly driven by common examples. However, even the best performing models suffer from the most naive mistakes when it comes to rare examples that do not appear frequently in the training data, such as vehicles with irregular geometries. Most studies in the long-tail literature focus on class-imbalanced classification problems with known imbalanced label counts per class, but they are not directly applicable to the intra-class long-tail examples in problems with large intra-class variations such as 3D object detection, where instances with the same class label can have drastically varied properties such as shapes and sizes. Other works propose to mitigate this problem using active learning based on the criteria of uncertainty, difficulty, or diversity. In this study, we identify a new conceptual dimension - rareness - to mine new data for improving the long-tail performance of models. We show that rareness, as opposed to difficulty, is the key to data-centric improvements for 3D detectors, since rareness is the result of a lack in data support while difficulty is related to the fundamental ambiguity in the problem. We propose a general and effective method to identify the rareness of objects based on density estimation in the feature space using flow models, and propose a principled cost-aware formulation for mining rare object tracks, which improves overall model performance, but more importantly - significantly improves the performance for rare objects (by 30.97\% △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.08375,October 2022,658,A low-cost wireless endoscope camera: a preliminary report,J. M. Lazarus and M. Ncube,Technology currently used for surgical endoscopy was developed and is manufactured in high-income economies. The cost of this equipment makes technology transfer to resource constrained environments difficult....,https://www.springeropen.com//afju.springeropen.com/articles/10.1186/s12301-021-00127-z,12 February 2021,
659,0.000678989392134464,659,D.MCA: Outlier Detection with Explicit Micro-Cluster Assignments,"Shuli Jiang, Robson Leonardo Ferreira Cordeiro, Leman Akoglu","How can we detect outliers, both scattered and clustered, and also explicitly assign them to respective micro-clusters, without knowing apriori how many micro-clusters exist? How can we perform both tasks in-house, i.e., without any post-hoc processing, so that both detection and assignment can benefit simultaneously from each other? Presenting outliers in separate micro-clusters is informative to analysts in many real-world applications. However, a naïve solution based on post-hoc clustering of the outliers detected by any existing method suffers from two main drawbacks: (a) appropriate hyperparameter values are commonly unknown for clustering, and most algorithms struggle with clusters of varying shapes and densities; (b) detection and assignment cannot benefit from one another. In this paper, we propose D.MCA to $\underline{D}$etect outliers with explicit $\underline{M}$icro-$\underline{C}$luster $\underline{A}$ssignment. Our method performs both detection and assignment iteratively, and in-house, by using a novel strategy that prunes entire micro-clusters out of the training set to improve the performance of the detection. It also benefits from a novel strategy that avoids clustered outliers to mask each other, which is a well-known problem in the literature. Also, D.MCA is designed to be robust to a critical hyperparameter by employing a hyperensemble ""warm up"" phase. Experiments performed on 16 real-world and synthetic datasets demonstrate that D.MCA outperforms 8 state-of-the-art competitors, especially on the explicit outlier micro-cluster assignment task. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.08212,October 2022,659,Performance Evaluation in Image Processing,"Michael Wirth, Matteo Fraschini, Martin Masek and Michel Bruynooghe",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/45742,1 December 2006,
660,0.00461519317491824,660,Graph identification of proteins in tomograms (GRIP-Tomo),"August George, Doo Nam Kim, Trevor Moser, Ian T. Gildea, James E. Evans, Margaret S. Cheung","In this study, we present a method of pattern mining based on network theory that enables the identification of protein structures or complexes from synthetic volume densities, without the knowledge of predefined templates or human biases for refinement. We hypothesized that the topological connectivity of protein structures is invariant, and they are distinctive for the purpose of protein identification from distorted data presented in volume densities. Three-dimensional densities of a protein or a complex from simulated tomographic volumes were transformed into mathematical graphs as observables. We systematically introduced data distortion or defects such as missing fullness of data, the tumbling effect, and the missing wedge effect into the simulated volumes, and varied the distance cutoffs in pixels to capture the varying connectivity between the density cluster centroids in the presence of defects. A similarity score between the graphs from the simulated volumes and the graphs transformed from the physical protein structures in point data was calculated by comparing their network theory order parameters including node degrees, betweenness centrality, and graph densities. By capturing the essential topological features defining the heterogeneous morphologies of a network, we were able to accurately identify proteins and homo-multimeric complexes from ten topologically distinctive samples without realistic noise added. Our approach empowers future developments of tomogram processing by providing pattern mining with interpretability, to enable the classification of single-domain protein native topologies as well as distinct single-domain proteins from multimeric complexes within noisy volumes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.08194,October 2022,660,Editorial,"Min Wu, Nasir Memon, Touradj Ebrahimi and Ingemar J. Cox",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002641,28 October 2004,
661,0.000678989392134464,661,Communication-Efficient Adam-Type Algorithms for Distributed Data Mining,"Wenhan Xian, Feihu Huang, Heng Huang","Distributed data mining is an emerging research topic to effectively and efficiently address hard data mining tasks using big data, which are partitioned and computed on different worker nodes, instead of one centralized server. Nevertheless, distributed learning methods often suffer from the communication bottleneck when the network bandwidth is limited or the size of model is large. To solve this critical issue, many gradient compression methods have been proposed recently to reduce the communication cost for multiple optimization algorithms. However, the current applications of gradient compression to adaptive gradient method, which is widely adopted because of its excellent performance to train DNNs, do not achieve the same ideal compression rate or convergence rate as Sketched-SGD. To address this limitation, in this paper, we propose a class of novel distributed Adam-type algorithms (\emph{i.e.}, SketchedAMSGrad) utilizing sketching, which is a promising compression technique that reduces the communication cost from $O(d)$ to $O(\log(d))$ where $d$ is the parameter dimension. In our theoretical analysis, we prove that our new algorithm achieves a fast convergence rate of $O(\frac{1}{\sqrt{nT}} + \frac{1}{(k/d)^2 T})$ with the communication cost of $O(k \log(d))$ at each iteration. Compared with single-machine AMSGrad, our algorithm can achieve the linear speedup with respect to the number of workers $n$. The experimental results on training various DNNs in distributed paradigm validate the efficiency of our algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.07454,October 2022,661,Performance Evaluation in Image Processing,"Michael Wirth, Matteo Fraschini, Martin Masek and Michel Bruynooghe",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/45742,1 December 2006,
662,0.00461519317491824,662,G2A2: An Automated Graph Generator with Attributes and Anomalies,"Saikat Dey, Sonal Jha, Wu-chun Feng","Many data-mining applications use dynamic attributed graphs to represent relational information; but due to security and privacy concerns, there is a dearth of available datasets that can be represented as dynamic attributed graphs. Even when such datasets are available, they do not have ground truth that can be used to train deep-learning models. Thus, we present G2A2, an automated graph generator with attributes and anomalies, which encompasses (1) probabilistic models to generate a dynamic bipartite graph, representing time-evolving connections between two independent sets of entities, (2) realistic injection of anomalies using a novel algorithm that captures the general properties of graph anomalies across domains, and (3) a deep generative model to produce realistic attributes, learned from an existing real-world dataset. Using the maximum mean discrepancy (MMD) metric to evaluate the realism of a G2A2-generated graph against three real-world graphs, G2A2 outperforms Kronecker graph generation by reducing the MMD distance by up to six-fold (6x). △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.07449,October 2022,662,Editorial,"Min Wu, Nasir Memon, Touradj Ebrahimi and Ingemar J. Cox",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002641,28 October 2004,
663,0.00673579331412596,663,Deep Clustering With Consensus Representations,"Lukas Miklautz, Martin Teuffenbach, Pascal Weber, Rona Perjuci, Walid Durani, Christian Böhm, Claudia Plant","The field of deep clustering combines deep learning and clustering to learn representations that improve both the learned representation and the performance of the considered clustering method. Most existing deep clustering methods are designed for a single clustering method, e.g., k-means, spectral clustering, or Gaussian mixture models, but it is well known that no clustering algorithm works best in all circumstances. Consensus clustering tries to alleviate the individual weaknesses of clustering algorithms by building a consensus between members of a clustering ensemble. Currently, there is no deep clustering method that can include multiple heterogeneous clustering algorithms in an ensemble to update representations and clusterings together. To close this gap, we introduce the idea of a consensus representation that maximizes the agreement between ensemble members. Further, we propose DECCS (Deep Embedded Clustering with Consensus representationS), a deep consensus clustering method that learns a consensus representation by enhancing the embedded space to such a degree that all ensemble members agree on a common clustering result. Our contributions are the following: (1) We introduce the idea of learning consensus representations for heterogeneous clusterings, a novel notion to approach consensus clustering. (2) We propose DECCS, the first deep clustering method that jointly improves the representation and clustering results of multiple heterogeneous clustering algorithms. (3) We show in experiments that learning a consensus representation with DECCS is outperforming several relevant baselines from deep clustering and consensus clustering. Our code can be found at https://gitlab.cs.univie.ac.at/lukas/deccs △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.07063,October 2022,663,Hard Real-Time Performances in Multiprocessor-Embedded Systems Using ASMP-Linux,"Emiliano Betti, Daniel Pierre Bovet, Marco Cesati and Roberto Gioiosa","Multiprocessor systems, especially those based on multicore or multithreaded processors, and new operating system architectures can satisfy the ever increasing computational requirements of embedded systems. A...",https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/2008/582648,21 August 2007,
664,0.00673579331412596,664,Delta-Closure Structure for Studying Data Distribution,"Aleksey Buzmakov, Tatiana Makhalova, Sergei O. Kuznetsov, Amedeo Napoli","In this paper, we revisit pattern mining and study the distribution underlying a binary dataset thanks to the closure structure which is based on passkeys, i.e., minimum generators in equivalence classes robust to noise. We introduce $Δ$-closedness, a generalization of the closure operator, where $Δ$ measures how a closed set differs from its upper neighbors in the partial order induced by closure. A $Δ$-class of equivalence includes minimum and maximum elements and allows us to characterize the distribution underlying the data. Moreover, the set of $Δ$-classes of equivalence can be partitioned into the so-called $Δ$-closure structure. In particular, a $Δ$-class of equivalence with a high level demonstrates correlations among many attributes, which are supported by more observations when $Δ$ is large. In the experiments, we study the $Δ$-closure structure of several real-world datasets and show that this structure is very stable for large $Δ$ and does not substantially depend on the data sampling used for the analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.06926,October 2022,664,EM-Gaze: eye context correlation and metric learning for gaze estimation,"Jinchao Zhou, Guoan Li, Feng Shi, Xiaoyan Guo, Pengfei Wan and Miao Wang","In recent years, deep learning techniques have been used to estimate gaze—a significant task in computer vision and human-computer interaction. Previous studies have made significant achievements in predicting...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-023-00135-6,5 May 2023,
665,0.00673579331412596,665,Bug Analysis in Jupyter Notebook Projects: An Empirical Study,"Taijara Loiola de Santana, Paulo Anselmo da Mota Silveira Neto, Eduardo Santana de Almeida, Iftekhar Ahmed","Computational notebooks, such as Jupyter, have been widely adopted by data scientists to write code for analyzing and visualizing data. Despite their growing adoption and popularity, there has been no thorough study to understand Jupyter development challenges from the practitioners' point of view. This paper presents a systematic study of bugs and challenges that Jupyter practitioners face through a large-scale empirical investigation. We mined 14,740 commits from 105 GitHub open-source projects with Jupyter notebook code. Next, we analyzed 30,416 Stack Overflow posts which gave us insights into bugs that practitioners face when developing Jupyter notebook projects. Finally, we conducted nineteen interviews with data scientists to uncover more details about Jupyter bugs and to gain insights into Jupyter developers' challenges. We propose a bug taxonomy for Jupyter projects based on our results. We also highlight bug categories, their root causes, and the challenges that Jupyter practitioners face. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.06893,October 2022,665,"Radiology artificial intelligence, a systematic evaluation of methods (RAISE): a systematic review protocol","Brendan Kelly, Conor Judge, Stephanie M. Bollard, Simon M. Clifford, Gerard M. Healy, Kristen W. Yeom, Aonghus Lawlor and Ronan P. Killeen","There has been a recent explosion of research into the field of artificial intelligence as applied to clinical radiology with the advent of highly accurate computer vision technology. These studies, however, v...",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-020-00929-9,9 December 2020,
666,0.000678989392134464,666,"Blockchain for Unmanned Underwater Drones: Research Issues, Challenges, Trends and Future Directions","Neelu Jyoti Ahuja, Adarsh Kumar, Monika Thapliyal, Sarthika Dutt, Tanesh Kumar, Diego Augusto De Jesus Pacheco, Charalambos Konstantinou, Kim-Kwang Raymond Choo","Underwater drones have found a place in oceanography, oceanic research, bathymetric surveys, military, surveillance, monitoring, undersea exploration, mining, commercial diving, photography and several other activities. Drones housed with several sensors and complex propulsion systems help oceanographic scientists and undersea explorers to map the seabed, study waves, view dead zones, analyze fish counts, predict tidal wave behaviors, aid in finding shipwrecks, building windfarms, examine oil platforms located in deep seas and inspect nuclear reactors in the ship vessels. While drones can be explicitly programmed for specific missions, data security and privacy are crucial issues of serious concern. Blockchain has emerged as a key enabling technology, amongst other disruptive technological enablers, to address security, data sharing, storage, process tracking, collaboration and resource management. This study presents a comprehensive review on the utilization of Blockchain in different underwater applications, discussing use cases and detailing benefits. Potential challenges of underwater applications addressed by Blockchain have been detailed. This work identifies knowledge gaps between theoretical research and real-time Blockchain integration in realistic underwater drone applications. The key limitations for effective integration of Blockchain in real-time integration in UUD applications, along with directions for future research have been presented. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.06540,October 2022,666,University-level practical activities in bioinformatics benefit voluntary groups of pupils in the last 2 years of school,"Daniel Barker, Rosanna G. Alderson, James L. McDonagh, Heleen Plaisier, Muriel M. Comrie, Leigh Duncan, Gavin T. P. Muirhead and Stuart D. Sweeney","Bioinformatics—the use of computers in biology—is of major and increasing importance to biological sciences and medicine. We conducted a preliminary investigation of the value of bringing practical, university...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-015-0030-z,28 October 2015,
667,0.000678989392134464,667,Privacy of federated QR decomposition using additive secure multiparty computation,"Anne Hartebrodt, Richard Röttger","Federated learning (FL) is a privacy-aware data mining strategy keeping the private data on the owners' machine and thereby confidential. The clients compute local models and send them to an aggregator which computes a global model. In hybrid FL, the local parameters are additionally masked using secure aggregation, such that only the global aggregated statistics become available in clear text, not the client specific updates. Federated QR decomposition has not been studied extensively in the context of cross-silo federated learning. In this article, we investigate the suitability of three QR decomposition algorithms for cross-silo FL and suggest a privacy-aware QR decomposition scheme based on the Gram-Schmidt algorithm which does not blatantly leak raw data. We apply the algorithm to compute linear regression in a federated manner. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.06163,October 2022,667,Motion detection and classification: ultra-fast road user detection,"Risto Ojala, Jari Vepsäläinen and Kari Tammi","With the emerge of intelligent and connected transportation systems, driver perception and on-board safety systems could be extended with roadside camera units. Computer vision can be utilised to detect road u...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00581-8,7 March 2022,
668,0.000678989392134464,668,Towards Mining Creative Thinking Patterns from Educational Data,Nasrin Shabani,"Creativity, i.e., the process of generating and developing fresh and original ideas or products that are useful or effective, is a valuable skill in a variety of domains. Creativity is called an essential 21st-century skill that should be taught in schools. The use of educational technology to promote creativity is an active study field, as evidenced by several studies linking creativity in the classroom to beneficial learning outcomes. Despite the burgeoning body of research on adaptive technology for education, mining creative thinking patterns from educational data remains a challenging task. In this paper, to address this challenge, we put the first step towards formalizing educational knowledge by constructing a domain-specific Knowledge Base to identify essential concepts, facts, and assumptions in identifying creative patterns. We then introduce a pipeline to contextualize the raw educational data, such as assessments and class activities. Finally, we present a rule-based approach to learning from the Knowledge Base, and facilitate mining creative thinking patterns from contextualized data and knowledge. We evaluate our approach with real-world datasets and highlight how the proposed pipeline can help instructors understand creative thinking patterns from students' activities and assessment tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.06118,October 2022,668,Correction to: Investigation the performance of PV solar cells in extremely hot environments,"Mohamed K. Hassan, Ibrahim M. Alqurashi, Ahmed E. Salama and Ahmed F. Mohamed",Theoriginal articlewas published inJournal of Umm Al-Qura University for Engineering and Architecture202213:s43995-022-00005-x,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s43995-022-00007-9,10 October 2022,
669,0.000678989392134464,669,Adaptive Dual Channel Convolution Hypergraph Representation Learning for Technological Intellectual Property,"Yuxin Liu, Yawen Li, Yingxia Shao, Zeli Guan","In the age of big data, the demand for hidden information mining in technological intellectual property is increasing in discrete countries. Definitely, a considerable number of graph learning algorithms for technological intellectual property have been proposed. The goal is to model the technological intellectual property entities and their relationships through the graph structure and use the neural network algorithm to extract the hidden structure information in the graph. However, most of the existing graph learning algorithms merely focus on the information mining of binary relations in technological intellectual property, ignoring the higherorder information hidden in non-binary relations. Therefore, a hypergraph neural network model based on dual channel convolution is proposed. For the hypergraph constructed from technological intellectual property data, the hypergraph channel and the line expanded graph channel of the hypergraph are used to learn the hypergraph, and the attention mechanism is introduced to adaptively fuse the output representations of the two channels. The proposed model outperforms the existing approaches on a variety of datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.05947,October 2022,669,A systematic review on digital literacy,"Hasan Tinmaz, Yoo-Taek Lee, Mina Fanea-Ivanovici and Hasnan Baber","The purpose of this study is to discover the main themes and categories of the research studies regarding digital literacy. To serve this purpose, the databases of WoS/Clarivate Analytics, Proquest Central, Em...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-022-00204-y,8 June 2022,
670,0.000678989392134464,670,Adversarial Contrastive Learning for Evidence-aware Fake News Detection with Graph Neural Networks,"Junfei Wu, Weizhi Xu, Qiang Liu, Shu Wu, Liang Wang","The prevalence and perniciousness of fake news have been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on attention mechanisms. Despite their effectiveness, they still suffer from three weaknesses. Firstly, sequential models fail to integrate the relevant information that is scattered far apart in evidences. Secondly, they underestimate much redundant information in evidences may be useless or harmful. Thirdly, insufficient data utilization limits the separability and reliability of representations captured by the model. To solve these problems, we propose a unified Graph-based sEmantic structure mining framework with ConTRAstive Learning, namely GETRAL in short. Specifically, we first model claims and evidences as graph-structured data to capture the long-distance semantic dependency. Consequently, we reduce information redundancy by performing graph structure learning. Then the fine-grained semantic representations are fed into the claim-evidence interaction module for predictions. Finally, an adversarial contrastive learning module is applied to make full use of data and strengthen representation learning. Comprehensive experiments have demonstrated the superiority of GETRAL over the state-of-the-arts and validated the efficacy of semantic mining with graph structure and contrastive learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.05498,October 2022,670,Washback of IELTS on the Assumption College English Program,suchada Sanonguthai,"This paper reports and discusses how the IELTS test has brought its impact to Assumption College Thonburi English Program (ACTEP)'s exit testing, English courses offered and foreign teachers' teaching. Two dif...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/2229-0443-1-2-33,15 July 2011,
671,0.000678989392134464,671,COVID-19-related Nepali Tweets Classification in a Low Resource Setting,"Rabin Adhikari, Safal Thapaliya, Nirajan Basnet, Samip Poudel, Aman Shakya, Bishesh Khanal","Billions of people across the globe have been using social media platforms in their local languages to voice their opinions about the various topics related to the COVID-19 pandemic. Several organizations, including the World Health Organization, have developed automated social media analysis tools that classify COVID-19-related tweets into various topics. However, these tools that help combat the pandemic are limited to very few languages, making several countries unable to take their benefit. While multi-lingual or low-resource language-specific tools are being developed, they still need to expand their coverage, such as for the Nepali language. In this paper, we identify the eight most common COVID-19 discussion topics among the Twitter community using the Nepali language, set up an online platform to automatically gather Nepali tweets containing the COVID-19-related keywords, classify the tweets into the eight topics, and visualize the results across the period in a web-based dashboard. We compare the performance of two state-of-the-art multi-lingual language models for Nepali tweet classification, one generic (mBERT) and the other Nepali language family-specific model (MuRIL). Our results show that the models' relative performance depends on the data size, with MuRIL doing better for a larger dataset. The annotated data, models, and the web-based dashboard are open-sourced at https://github.com/naamiinepal/covid-tweet-classification. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.05425,October 2022,671,Better Flow Estimation from Color Images,Hui Ji and Cornelia Fermüller,"One of the difficulties in estimating optical flow is bias. Correcting the bias using the classical techniques is very difficult. The reason is that knowledge of the error statistics is required, which usually...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/53912,1 December 2007,
672,0.000678989392134464,672,"Reciprocity in Directed Hypergraphs: Measures, Findings, and Generators","Sunwoo Kim, Minyoung Choe, Jaemin Yoo, Kijung Shin","Group interactions are prevalent in a variety of areas. Many of them, including email exchanges, chemical reactions, and bitcoin transactions, are directional, and thus they are naturally modeled as directed hypergraphs, where each hyperarc consists of the set of source nodes and the set of destination nodes. For directed graphs, which are a special case of directed hypergraphs, reciprocity has played a key role as a fundamental graph statistic in revealing organizing principles of graphs and in solving graph learning tasks. For general directed hypergraphs, however, even no systematic measure of reciprocity has been developed. In this work, we investigate the reciprocity of 11 real-world hypergraphs. To this end, we first introduce eight axioms that any reasonable measure of reciprocity should satisfy. Second, we propose HyperRec, a family of principled measures of hypergraph reciprocity that satisfies all the axioms. Third, we develop Ferret, a fast and exact algorithm for computing the measure, whose search space is up to 10^{147}x smaller than that of naive computation. Fourth, using them, we examine 11 real-world hypergraphs and discover patterns that distinguish them from random hypergraphs. Lastly, we propose ReDi, an intuitive generative model for directed hypergraphs exhibiting the patterns. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.05328,October 2022,672,Efficient Measurement of Shape Dissimilarity between 3D Models Using Z-Buffer and Surface Roving Method,"In Kyu Park, Kyoung Mu Lee and Sang Uk Lee","Estimation of the shape dissimilarity between 3D models is a very important problem in both computer vision and graphics for 3D surface reconstruction, modeling, matching, and compression. In this paper, we pr...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702206022,22 October 2002,
673,0.000678989392134464,673,Mining Causality from Continuous-time Dynamics Models: An Application to Tsunami Forecasting,"Fan Wu, Sanghyun Hong, Donsub Rim, Noseong Park, Kookjin Lee","Continuous-time dynamics models, such as neural ordinary differential equations, have enabled the modeling of underlying dynamics in time-series data and accurate forecasting. However, parameterization of dynamics using a neural network makes it difficult for humans to identify causal structures in the data. In consequence, this opaqueness hinders the use of these models in the domains where capturing causal relationships carries the same importance as accurate predictions, e.g., tsunami forecasting. In this paper, we address this challenge by proposing a mechanism for mining causal structures from continuous-time models. We train models to capture the causal structure by enforcing sparsity in the weights of the input layers of the dynamics models. We first verify the effectiveness of our method in the scenario where the exact causal-structures of time-series are known as a priori. We next apply our method to a real-world problem, namely tsunami forecasting, where the exact causal-structures are difficult to characterize. Experimental results show that the proposed method is effective in learning physically-consistent causal relationships while achieving high forecasting accuracy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04958,October 2022,673,Success and luck in creative careers,"Milán Janosov, Federico Battiston and Roberta Sinatra","Luck is considered a crucial ingredient to achieve impact in all creative domains, despite their diversity. For instance, in science, the movie industry, music, and art, the occurrence of the highest impact wo...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00227-w,28 April 2020,
674,0.000678989392134464,674,Visually Similar Products Retrieval for Shopsy,"Prajit Nadkarni, Narendra Varma Dasararaju","Visual search is of great assistance in reseller commerce, especially for non-tech savvy users with affinity towards regional languages. It allows resellers to accurately locate the products that they seek, unlike textual search which recommends products from head brands. Product attributes available in e-commerce have a great potential for building better visual search systems as they capture fine grained relations between data points. In this work, we design a visual search system for reseller commerce using a multi-task learning approach. We also highlight and address the challenges like image compression, cropping, scribbling on the image, etc, faced in reseller commerce. Our model consists of three different tasks: attribute classification, triplet ranking and variational autoencoder (VAE). Masking technique is used for designing the attribute classification. Next, we introduce an offline triplet mining technique which utilizes information from multiple attributes to capture relative order within the data. This technique displays a better performance compared to the traditional triplet mining baseline, which uses single label/attribute information. We also compare and report incremental gain achieved by our unified multi-task model over each individual task separately. The effectiveness of our method is demonstrated using the in-house dataset of product images from the Lifestyle business-unit of Flipkart, India's largest e-commerce company. To efficiently retrieve the images in production, we use the Approximate Nearest Neighbor (ANN) index. Finally, we highlight our production environment constraints and present the design choices and experiments conducted to select a suitable ANN index. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04560,October 2022,674,Content-Aware Video Adaptation under Low-Bitrate Constraint,"Ming-Ho Hsiao, Yi-Wen Chen, Hua-Tsung Chen, Kuan-Hung Chou and Suh-Yin Lee","With the development of wireless network and the improvement of mobile device capability, video streaming is more and more widespread in such an environment. Under the condition of limited resource and inheren...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/17179,1 December 2007,
675,0.00673579331412596,675,Bridging CLIP and StyleGAN through Latent Alignment for Image Editing,"Wanfeng Zheng, Qiang Li, Xiaoyan Guo, Pengfei Wan, Zhongyuan Wang","Text-driven image manipulation is developed since the vision-language model (CLIP) has been proposed. Previous work has adopted CLIP to design a text-image consistency-based objective to address this issue. However, these methods require either test-time optimization or image feature cluster analysis for single-mode manipulation direction. In this paper, we manage to achieve inference-time optimization-free diverse manipulation direction mining by bridging CLIP and StyleGAN through Latent Alignment (CSLA). More specifically, our efforts consist of three parts: 1) a data-free training strategy to train latent mappers to bridge the latent space of CLIP and StyleGAN; 2) for more precise mapping, temporal relative consistency is proposed to address the knowledge distribution bias problem among different latent spaces; 3) to refine the mapped latent in s space, adaptive style mixing is also proposed. With this mapping scheme, we can achieve GAN inversion, text-to-image generation and text-driven image manipulation. Qualitative and quantitative comparisons are made to demonstrate the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04506,October 2022,675,“Weebles wobble but they also commit to lifelong relationships”: teachers’ transdisciplinary learning in computational play,"Brian E. Gravel, Amon Millner, Eli Tucker-Raymond, Maria C. Olivares and Aditi Wagh","Computational approaches in STEM foster creative extrapolations of ideas that extend the bounds of human perception, processing, and sense-making. Inviting teachers to explore computational approaches in STEM ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00373-9,20 September 2022,
676,0.000678989392134464,676,CrowdChecked: Detecting Previously Fact-Checked Claims in Social Media,"Momchil Hardalov, Anton Chernyavskiy, Ivan Koychev, Dmitry Ilvovsky, Preslav Nakov","While there has been substantial progress in developing systems to automate fact-checking, they still lack credibility in the eyes of the users. Thus, an interesting approach has emerged: to perform automatic fact-checking by verifying whether an input claim has been previously fact-checked by professional fact-checkers and to return back an article that explains their decision. This is a sensible approach as people trust manual fact-checking, and as many claims are repeated multiple times. Yet, a major issue when building such systems is the small number of known tweet--verifying article pairs available for training. Here, we aim to bridge this gap by making use of crowd fact-checking, i.e., mining claims in social media for which users have responded with a link to a fact-checking article. In particular, we mine a large-scale collection of 330,000 tweets paired with a corresponding fact-checking article. We further propose an end-to-end framework to learn from this noisy data based on modified self-adaptive training, in a distant supervision scenario. Our experiments on the CLEF'21 CheckThat! test set show improvements over the state of the art by two points absolute. Our code and datasets are available at https://github.com/mhardalov/crowdchecked-claims △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04447,October 2022,676,"What makes online teaching spatial? Examining the connections between K-12 teachers’ spatial skills, affect, and their use of spatial pedagogy during remote instruction","Kelsey Rocha, Catherine M. Lussier and Kinnari Atit",Spatial skills are critical for student success in K-12 STEM education. Teachers’ spatial skills and feelings about completing spatial tasks influence students’ spatial and STEM learning at both the primary an...,https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-022-00377-7,21 March 2022,
677,0.000678989392134464,677,Towards Training Graph Neural Networks with Node-Level Differential Privacy,"Qiuchen Zhang, Jing Ma, Jian Lou, Carl Yang, Li Xiong","Graph Neural Networks (GNNs) have achieved great success in mining graph-structured data. Despite the superior performance of GNNs in learning graph representations, serious privacy concerns have been raised for the trained models which could expose the sensitive information of graphs. We conduct the first formal study of training GNN models to ensure utility while satisfying the rigorous node-level differential privacy considering the private information of both node features and edges. We adopt the training framework utilizing personalized PageRank to decouple the message-passing process from feature aggregation during training GNN models and propose differentially private PageRank algorithms to protect graph topology information formally. Furthermore, we analyze the privacy degradation caused by the sampling process dependent on the differentially private PageRank results during model training and propose a differentially private GNN (DPGNN) algorithm to further protect node features and achieve rigorous node-level differential privacy. Extensive experiments on real-world graph datasets demonstrate the effectiveness of the proposed algorithms for providing node-level differential privacy while preserving good model utility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04442,October 2022,677,Development and Evaluation of High-Performance Decorrelation Algorithms for the Nonalternating 3D Wavelet Transform,"E. Moyano-Ávila, F. J. Quiles and L. Orozco-Barbosa",We introduce and evaluate the implementations of three parallel video-sequences decorrelation algorithms. The proposed algorithms are based on the nonalternating classic three-dimensional wavelet transform (3D...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/69384,1 December 2007,
678,0.000678989392134464,678,Modeling and Mining Multi-Aspect Graphs With Scalable Streaming Tensor Decomposition,Ekta Gujral,"Graphs emerge in almost every real-world application domain, ranging from online social networks all the way to health data and movie viewership patterns. Typically, such real-world graphs are big and dynamic, in the sense that they evolve over time. Furthermore, graphs usually contain multi-aspect information i.e. in a social network, we can have the ""means of communication"" between nodes, such as who messages whom, who calls whom, and who comments on whose timeline and so on. How can we model and mine useful patterns, such as communities of nodes in that graph, from such multi-aspect graphs? How can we identify dynamic patterns in those graphs, and how can we deal with streaming data, when the volume of data to be processed is very large? In order to answer those questions, in this thesis, we propose novel tensor-based methods for mining static and dynamic multi-aspect graphs. In general, a tensor is a higher-order generalization of a matrix that can represent high-dimensional multi-aspect data such as time-evolving networks, collaboration networks, and spatio-temporal data like Electroencephalography (EEG) brain measurements. The thesis is organized in two synergistic thrusts: First, we focus on static multi-aspect graphs, where the goal is to identify coherent communities and patterns between nodes by leveraging the tensor structure in the data. Second, as our graphs evolve dynamically, we focus on handling such streaming updates in the data without having to re-compute the decomposition, but incrementally update the existing results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04404,October 2022,678,An Attention-Driven Model for Grouping Similar Images with Image Retrieval Applications,"Oge Marques, Liam M Mayron, Gustavo B Borba and Humberto R Gamba",Recent work in the computational modeling of visual attention has demonstrated that a purely bottom-up approach to identifying salient regions within an image can be successfully applied to diverse and practic...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/43450,1 December 2006,
679,0.000678989392134464,679,Grow and Merge: A Unified Framework for Continuous Categories Discovery,"Xinwei Zhang, Jianwen Jiang, Yutong Feng, Zhi-Fan Wu, Xibin Zhao, Hai Wan, Mingqian Tang, Rong Jin, Yue Gao","Although a number of studies are devoted to novel category discovery, most of them assume a static setting where both labeled and unlabeled data are given at once for finding new categories. In this work, we focus on the application scenarios where unlabeled data are continuously fed into the category discovery system. We refer to it as the {\bf Continuous Category Discovery} ({\bf CCD}) problem, which is significantly more challenging than the static setting. A common challenge faced by novel category discovery is that different sets of features are needed for classification and category discovery: class discriminative features are preferred for classification, while rich and diverse features are more suitable for new category mining. This challenge becomes more severe for dynamic setting as the system is asked to deliver good performance for known classes over time, and at the same time continuously discover new classes from unlabeled data. To address this challenge, we develop a framework of {\bf Grow and Merge} ({\bf GM}) that works by alternating between a growing phase and a merging phase: in the growing phase, it increases the diversity of features through a continuous self-supervised learning for effective category mining, and in the merging phase, it merges the grown model with a static one to ensure satisfying performance for known classes. Our extensive studies verify that the proposed GM framework is significantly more effective than the state-of-the-art approaches for continuous category discovery. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04174,October 2022,679,Mobile Agent-Based Directed Diffusion in Wireless Sensor Networks,"Min Chen, Taekyoung Kwon, Yong Yuan, Yanghee Choi and Victor C.M. Leung","In the environments where the source nodes are close to one another and generate a lot of sensory data traffic with redundancy, transmitting all sensory data by individual nodes not only wastes the scarce wire...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/36871,1 December 2006,
680,0.00673579331412596,680,Multi-Objective Personalized Product Retrieval in Taobao Search,"Yukun Zheng, Jiang Bian, Guanghao Meng, Chao Zhang, Honggang Wang, Zhixuan Zhang, Sen Li, Tao Zhuang, Qingwen Liu, Xiaoyi Zeng","In large-scale e-commerce platforms like Taobao, it is a big challenge to retrieve products that satisfy users from billions of candidates. This has been a common concern of academia and industry. Recently, plenty of works in this domain have achieved significant improvements by enhancing embedding-based retrieval (EBR) methods, including the Multi-Grained Deep Semantic Product Retrieval (MGDSPR) model [16] in Taobao search engine. However, we find that MGDSPR still has problems of poor relevance and weak personalization compared to other retrieval methods in our online system, such as lexical matching and collaborative filtering. These problems promote us to further strengthen the capabilities of our EBR model in both relevance estimation and personalized retrieval. In this paper, we propose a novel Multi-Objective Personalized Product Retrieval (MOPPR) model with four hierarchical optimization objectives: relevance, exposure, click and purchase. We construct entire-space multi-positive samples to train MOPPR, rather than the single-positive samples for existing EBR models.We adopt a modified softmax loss for optimizing multiple objectives. Results of extensive offline and online experiments show that MOPPR outperforms the baseline MGDSPR on evaluation metrics of relevance estimation and personalized retrieval. MOPPR achieves 0.96% transaction and 1.29% GMV improvements in a 28-day online A/B test. Since the Double-11 shopping festival of 2021, MOPPR has been fully deployed in mobile Taobao search, replacing the previous MGDSPR. Finally, we discuss several advanced topics of our deeper explorations on multi-objective retrieval and ranking to contribute to the community. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04170,October 2022,680,Conceptual framework of hybrid style in fashion image datasets for machine learning,"Hyosun An, Kyo Young Lee, Yerim Choi and Minjung Park","Fashion image datasets, in which each fashion image has a label indicating its design attributes and styles, have contributed to the achievement of various machine learning techniques in the fashion industry. ...",https://www.springeropen.com//fashionandtextiles.springeropen.com/articles/10.1186/s40691-023-00338-8,15 May 2023,
681,0.000678989392134464,681,Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP,"Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, Diana Marculescu","Open-vocabulary semantic segmentation aims to segment an image into semantic regions according to text descriptions, which may not have been seen during training. Recent two-stage methods first generate class-agnostic mask proposals and then leverage pre-trained vision-language models, e.g., CLIP, to classify masked regions. We identify the performance bottleneck of this paradigm to be the pre-trained CLIP model, since it does not perform well on masked images. To address this, we propose to finetune CLIP on a collection of masked image regions and their corresponding text descriptions. We collect training data by mining an existing image-caption dataset (e.g., COCO Captions), using CLIP to match masked image regions to nouns in the image captions. Compared with the more precise and manually annotated segmentation labels with fixed classes (e.g., COCO-Stuff), we find our noisy but diverse dataset can better retain CLIP's generalization ability. Along with finetuning the entire model, we utilize the ""blank"" areas in masked images using a method we dub mask prompt tuning. Experiments demonstrate mask prompt tuning brings significant improvement without modifying any weights of CLIP, and it can further improve a fully finetuned model. In particular, when trained on COCO and evaluated on ADE20K-150, our best model achieves 29.6% mIoU, which is +8.5% higher than the previous state-of-the-art. For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04150,October 2022,681,Gender gaps in the performance of Norwegian biology students: the roles of test anxiety and science confidence,"Sehoya Cotner, Lucas M. Jeno, J. D. Walker, Christian Jørgensen and Vigdis Vandvik","Understanding student motivational factors such as test anxiety and science confidence is important for increasing retention in science, technology, engineering, and math (STEM), especially for underrepresente...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00252-1,27 October 2020,
682,0.000678989392134464,682,Deep Clustering: A Comprehensive Survey,"Yazhou Ren, Jingyu Pu, Zhimeng Yang, Jie Xu, Guofeng Li, Xiaorong Pu, Philip S. Yu, Lifang He","Cluster analysis plays an indispensable role in machine learning and data mining. Learning a good data representation is crucial for clustering algorithms. Recently, deep clustering, which can learn clustering-friendly representations using deep neural networks, has been broadly applied in a wide range of clustering tasks. Existing surveys for deep clustering mainly focus on the single-view fields and the network architectures, ignoring the complex application scenarios of clustering. To address this issue, in this paper we provide a comprehensive survey for deep clustering in views of data sources. With different data sources and initial conditions, we systematically distinguish the clustering methods in terms of methodology, prior knowledge, and architecture. Concretely, deep clustering methods are introduced according to four categories, i.e., traditional single-view deep clustering, semi-supervised deep clustering, deep multi-view clustering, and deep transfer clustering. Finally, we discuss the open challenges and potential future opportunities in different fields of deep clustering. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04142,October 2022,682,An omnibus permutation test on ensembles of two-locus analyses can detect pure epistasis and genetic heterogeneity in genome-wide association studies,"Damrongrit Setsirichok, Phuwadej Tienboon, Nattapong Jaroonruang, Somkit Kittichaijaroen, Waranyu Wongseree, Theera Piroonratana, Touchpong Usavanarong, Chanin Limwongse, Chatchawit Aporntewan, Marong Phadoongsidhi and Nachol Chaiyaratana",This article presents the ability of an omnibus permutation test on ensembles of two-locus analyses (2LOmb) to detect pure epistasis in the presence of genetic heterogeneity. The performance of 2LOmb is evalua...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-2-230,19 May 2013,
683,0.000678989392134464,683,"Less is More: SlimG for Accurate, Robust, and Interpretable Graph Mining","Jaemin Yoo, Meng-Chieh Lee, Shubhranshu Shekhar, Christos Faloutsos","How can we solve semi-supervised node classification in various graphs possibly with noisy features and structures? Graph neural networks (GNNs) have succeeded in many graph mining tasks, but their generalizability to various graph scenarios is limited due to the difficulty of training, hyperparameter tuning, and the selection of a model itself. Einstein said that we should ""make everything as simple as possible, but not simpler."" We rephrase it into the careful simplicity principle: a carefully-designed simple model can surpass sophisticated ones in real-world graphs. Based on the principle, we propose SlimG for semi-supervised node classification, which exhibits four desirable properties: It is (a) accurate, winning or tying on 10 out of 13 real-world datasets; (b) robust, being the only one that handles all scenarios of graph data (homophily, heterophily, random structure, noisy features, etc.); (c) fast and scalable, showing up to 18 times faster training in million-scale graphs; and (d) interpretable, thanks to the linearity and sparsity. We explain the success of SlimG through a systematic study of the designs of existing GNNs, sanity checks, and comprehensive ablation studies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.04081,October 2022,683,Correction to: Cooperative co‑evolution for feature selection in Big Data with random feature grouping,"A. N. M. Bazlur Rashid, Mohiuddin Ahmed, Leslie F. Sikos and Paul Haskell‑Dowland",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00403-9,28 December 2020,
684,0.000678989392134464,684,Kernel-based Substructure Exploration for Next POI Recommendation,"Wei Ju, Yifang Qin, Ziyue Qiao, Xiao Luo, Yifan Wang, Yanjie Fu, Ming Zhang","Point-of-Interest (POI) recommendation, which benefits from the proliferation of GPS-enabled devices and location-based social networks (LBSNs), plays an increasingly important role in recommender systems. It aims to provide users with the convenience to discover their interested places to visit based on previous visits and current status. Most existing methods usually merely leverage recurrent neural networks (RNNs) to explore sequential influences for recommendation. Despite the effectiveness, these methods not only neglect topological geographical influences among POIs, but also fail to model high-order sequential substructures. To tackle the above issues, we propose a Kernel-Based Graph Neural Network (KBGNN) for next POI recommendation, which combines the characteristics of both geographical and sequential influences in a collaborative way. KBGNN consists of a geographical module and a sequential module. On the one hand, we construct a geographical graph and leverage a message passing neural network to capture the topological geographical influences. On the other hand, we explore high-order sequential substructures in the user-aware sequential graph using a graph kernel neural network to capture user preferences. Finally, a consistency learning framework is introduced to jointly incorporate geographical and sequential information extracted from two separate graphs. In this way, the two modules effectively exchange knowledge to mutually enhance each other. Extensive experiments conducted on two real-world LBSN datasets demonstrate the superior performance of our proposed method over the state-of-the-arts. Our codes are available at https://github.com/Fang6ang/KBGNN. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.03969,October 2022,684,Subpixel Registration Directly from the Phase Difference,Murat Balci and Hassan Foroosh,"This paper proposes a new approach to subpixel registration, under local/global shifts or rotation, using the phase-difference matrix. We establish the exact relationship between the continuous and the discret...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/60796,1 December 2006,
685,0.000678989392134464,685,Unsupervised hyperspectral data mining and bioimaging by information entropy and self-modeling curve resolution,"Simon Vilms Pedersen, Anders R. Walther, Anthony Callanan, Molly M. Stevens, Martin A. B. Hedegaard, Eva C. Arnspang","Unsupervised estimation of the dimensionality of hyperspectral microspectroscopy datasets containing pure and mixed spectral features, and extraction of their representative endmember spectra, remains a challenge in biochemical data mining. We report a new versatile algorithm building on semi-nonnegativity constrained self-modeling curve resolution and information entropy, to estimate the quantity of separable biochemical species from hyperspectral microspectroscopy, and extraction of their representative spectra. The algorithm is benchmarked with established methods from satellite remote sensing, spectral unmixing, and clustering. To demonstrate the widespread applicability of the developed algorithm, we collected hyperspectral datasets using spontaneous Raman, Coherent Anti-stokes Raman Scattering and Fourier Transform IR, of seven reference compounds, an oil-in-water emulsion, and tissue-engineered extracellular matrices on poly-L-lactic acid and porcine jejunum-derived small intestine submucosa scaffolds seeded with bovine chondrocytes. We show the potential of the developed algorithm by consolidating hyperspectral molecular information with sample microstructure, pertinent to fields ranging from gastrophysics to regenerative medicine. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.03238,October 2022,685,ECR 2015 Book of Abstracts - A - Postgraduate Educational Programme,Unknown,"This article is part of a Supplement:Volume 6
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1007/s13244-015-0386-0,24 February 2015,
686,0.000678989392134464,686,"Dominance-based Rough Set Approach, basic ideas and main trends","Jerzy Błaszczyński, Salvatore Greco, Benedetto Matarazzo, Marcin Szeląg","Dominance-based Rough Approach (DRSA) has been proposed as a machine learning and knowledge discovery methodology to handle Multiple Criteria Decision Aiding (MCDA). Due to its capacity of asking the decision maker (DM) for simple preference information and supplying easily understandable and explainable recommendations, DRSA gained much interest during the years and it is now one of the most appreciated MCDA approaches. In fact, it has been applied also beyond MCDA domain, as a general knowledge discovery and data mining methodology for the analysis of monotonic (and also non-monotonic) data. In this contribution, we recall the basic principles and the main concepts of DRSA, with a general overview of its developments and software. We present also a historical reconstruction of the genesis of the methodology, with a specific focus on the contribution of Roman Słowiński. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.03233,October 2022,686,The network of international trade in services,"Lucia Tajoli, Federico Airoldi and Carlo Piccardi","While the share of services in international trade has been increasing very slowly over the years, oscillating around 20 per cent since the 1990s, their role has constantly gained importance. Trade in services...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-021-00407-1,6 September 2021,
687,0.000678989392134464,687,On Explaining Confounding Bias,"Brit Youngmann, Michael Cafarella, Yuval Moskovitch, Babak Salimi","When analyzing large datasets, analysts are often interested in the explanations for surprising or unexpected results produced by their queries. In this work, we focus on aggregate SQL queries that expose correlations in the data. A major challenge that hinders the interpretation of such queries is confounding bias, which can lead to an unexpected correlation. We generate explanations in terms of a set of confounding variables that explain the unexpected correlation observed in a query. We propose to mine candidate confounding variables from external sources since, in many real-life scenarios, the explanations are not solely contained in the input data. We present an efficient algorithm that finds the optimal subset of attributes (mined from external sources and the input dataset) that explain the unexpected correlation. This algorithm is embodied in a system called MESA. We demonstrate experimentally over multiple real-life datasets and through a user study that our approach generates insightful explanations, outperforming existing methods that search for explanations only in the input data. We further demonstrate the robustness of our system to missing data and the ability of MESA to handle input datasets containing millions of tuples and an extensive search space of candidate confounding attributes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.02943,October 2022,687,Welcome to the new Journal of Cloud Computing by Springer,Chunming Rong and Zhiming Zhao,Unknown,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-021-00263-5,10 September 2021,
688,0.000678989392134464,688,Spatial-Temporal Graph Convolutional Gated Recurrent Network for Traffic Forecasting,"Le Zhao, Mingcai Chen, Yuntao Du, Haiyang Yang, Chongjun Wang","As an important part of intelligent transportation systems, traffic forecasting has attracted tremendous attention from academia and industry. Despite a lot of methods being proposed for traffic forecasting, it is still difficult to model complex spatial-temporal dependency. Temporal dependency includes short-term dependency and long-term dependency, and the latter is often overlooked. Spatial dependency can be divided into two parts: distance-based spatial dependency and hidden spatial dependency. To model complex spatial-temporal dependency, we propose a novel framework for traffic forecasting, named Spatial-Temporal Graph Convolutional Gated Recurrent Network (STGCGRN). We design an attention module to capture long-term dependency by mining periodic information in traffic data. We propose a Double Graph Convolution Gated Recurrent Unit (DGCGRU) to capture spatial dependency, which integrates graph convolutional network and GRU. The graph convolution part models distance-based spatial dependency with the distance-based predefined adjacency matrix and hidden spatial dependency with the self-adaptive adjacency matrix, respectively. Specially, we employ the multi-head mechanism to capture multiple hidden dependencies. In addition, the periodic pattern of each prediction node may be different, which is often ignored, resulting in mutual interference of periodic information among nodes when modeling spatial dependency. For this, we explore the architecture of model and improve the performance. Experiments on four datasets demonstrate the superior performance of our model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.02737,October 2022,688,ESICM LIVES 2022: part 1,Unknown,"This article is part of a Supplement:Volume 10
                                        Supplement 2",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-022-00468-1,19 October 2022,
689,0.000678989392134464,689,Join-Chain Network: A Logical Reasoning View of the Multi-head Attention in Transformer,"Jianyi Zhang, Yiran Chen, Jianshu Chen","Developing neural architectures that are capable of logical reasoning has become increasingly important for a wide range of applications (e.g., natural language processing). Towards this grand objective, we propose a symbolic reasoning architecture that chains many join operators together to model output logical expressions. In particular, we demonstrate that such an ensemble of join-chains can express a broad subset of ''tree-structured'' first-order logical expressions, named FOET, which is particularly useful for modeling natural languages. To endow it with differentiable learning capability, we closely examine various neural operators for approximating the symbolic join-chains. Interestingly, we find that the widely used multi-head self-attention module in transformer can be understood as a special neural operator that implements the union bound of the join operator in probabilistic predicate space. Our analysis not only provides a new perspective on the mechanism of the pretrained models such as BERT for natural language understanding but also suggests several important future improvement directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.02729,October 2022,689,Do private and portable web browsers leave incriminating evidence?: a forensic analysis of residual artifacts from private and portable web browsing sessions,Donny J Ohana and Narasimha Shashidhar,"The Internet is an essential tool for everyday tasks. Aside from common use, the option to browse the Internet privately is a desirable attribute. However, this can create a problem when private Internet sessi...",https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1186/1687-417X-2013-6,21 November 2013,
690,0.000678989392134464,690,Applicability of Hubbert model to global mining industry: Interpretations and insights,"Lucas Riondet, Daniel Suchet, Olivier Vidal, José Halloy","The Hubert's model has been introduced in 1956 as a phenomenological description of the time evolution of US oil fields production. It has since then acquired a vast notoriety as a conceptual approach to resource depletion. It is often invoked nowadays in the context of the energy transition to question the limitations induced by the finitude of mineral stocks. Yet, its validity is often controversial despite its popularity. This paper offers a pedagogical introduction to the model, assesses its ability to describe the current evolution of 20 mining elements, and discusses the nature and robustness of conclusions drawn from Hubbert's model considered either as a for cast or as a foresight tool. We propose a novel way to represent graphically these conclusions as a ""Hubbert's map"" which offers direct visualization of their main features. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.02298,October 2022,690,"Environmentally data-driven smart sustainable cities: applied innovative solutions for energy efficiency, pollution reduction, and urban metabolism",Simon Elias Bibri and John Krogstie,"The IoT and big data technologies have become essential to the functioning of both smart cities and sustainable cities, and thus, urban operational functioning and planning are becoming highly responsive to a ...",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-020-00130-8,23 November 2020,
691,0.000678989392134464,691,DISCOVER: Deep identification of symbolically concise open-form PDEs via enhanced reinforcement-learning,"Mengge Du, Yuntian Chen, Dongxiao Zhang","The working mechanisms of complex natural systems tend to abide by concise and profound partial differential equations (PDEs). Methods that directly mine equations from data are called PDE discovery, which reveals consistent physical laws and facilitates our adaptive interaction with the natural world. In this paper, an enhanced deep reinforcement-learning framework is proposed to uncover symbolically concise open-form PDEs with little prior knowledge. Particularly, based on a symbol library of basic operators and operands, a structure-aware recurrent neural network agent is designed and seamlessly combined with the sparse regression method to generate concise and open-form PDE expressions. All of the generated PDEs are evaluated by a meticulously designed reward function by balancing fitness to data and parsimony, and updated by the model-based reinforcement learning in an efficient way. Customized constraints and regulations are formulated to guarantee the rationality of PDEs in terms of physics and mathematics. The experiments demonstrate that our framework is capable of mining open-form governing equations of several dynamic systems, even with compound equation terms, fractional structure, and high-order derivatives, with excellent efficiency. Without the need for prior knowledge, this method shows great potential for knowledge discovery in more complicated circumstances with exceptional efficiency and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.02181,October 2022,691,Learning analytics in virtual laboratories: a systematic literature review of empirical research,"Ramy Elmoazen, Mohammed Saqr, Mohammad Khalil and Barbara Wasson","Remote learning has advanced from the theoretical to the practical sciences with the advent of virtual labs. Although virtual labs allow students to conduct their experiments remotely, it is a challenge to eva...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00244-y,9 March 2023,
692,0.000678989392134464,692,Link Partitioning on Simplicial Complexes Using Higher-Order Laplacians,"Xinyi Wu, Arnab Sarker, Ali Jadbabaie","Link partitioning is a popular approach in network science used for discovering overlapping communities by identifying clusters of strongly connected links. Current link partitioning methods are specifically designed for networks modelled by graphs representing pairwise relationships. Therefore, these methods are incapable of utilizing higher-order information about group interactions in network data which is increasingly available. Simplicial complexes extend the dyadic model of graphs and can model polyadic relationships which are ubiquitous and crucial in many complex social and technological systems. In this paper, we introduce a link partitioning method that leverages higher-order (i.e. triadic and higher) information in simplicial complexes for better community detection. Our method utilizes a novel random walk on links of simplicial complexes defined by the higher-order Laplacian--a generalization of the graph Laplacian that incorporates polyadic relationships of the network. We transform this random walk into a graph-based random walk on a lifted line graph--a dual graph in which links are nodes while nodes and higher-order connections are links--and optimize for the standard notion of modularity. We show that our method is guaranteed to provide interpretable link partitioning results under mild assumptions. We also offer new theoretical results on the spectral properties of simplicial complexes by studying the spectrum of the link random walk. Experiment results on real-world community detection tasks show that our higher-order approach significantly outperforms existing graph-based link partitioning methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01849,October 2022,692,Survey on computational 3D visual optical art design,"Kang Wu, Xiao-Ming Fu, Renjie Chen and Ligang Liu",Visual arts refer to art experienced primarily through vision. 3D visual optical art is one of them. Artists use their rich imagination and experience to combine light and objects to give viewers an unforgetta...,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-022-00126-z,19 December 2022,
693,0.000678989392134464,693,Robust self-healing prediction model for high dimensional data,"Anirudha Rayasam, Nagamma Patil","Owing to the advantages of increased accuracy and the potential to detect unseen patterns, provided by data mining techniques they have been widely incorporated for standard classification problems. They have often been used for high precision disease prediction in the medical field, and several hybrid prediction models capable of achieving high accuracies have been proposed. Though this stands true most of the previous models fail to efficiently address the recurring issue of bad data quality which plagues most high dimensional data, and especially proves troublesome in the highly sensitive medical data. This work proposes a robust self healing (RSH) hybrid prediction model which functions by using the data in its entirety by removing errors and inconsistencies from it rather than discarding any data. Initial processing involves data preparation followed by cleansing or scrubbing through context-dependent attribute correction, which ensures that there is no significant loss of relevant information before the feature selection and prediction phases. An ensemble of heterogeneous classifiers, subjected to local boosting, is utilized to build the prediction model and genetic algorithm based wrapper feature selection technique wrapped on the respective classifiers is employed to select the corresponding optimal set of features, which warrant higher accuracy. The proposed method is compared with some of the existing high performing models and the results are analyzed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01788,October 2022,693,A systematic review of Mobile-Assisted Vocabulary Learning research,Kübra Okumuş Dağdeler,"With rapid development of mobile technology, the number of researches on its use in language education process has increased. This growing body of interest has led to a need for review studies that are expecte...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-023-00235-z,28 February 2023,
694,0.000678989392134464,694,DIAGNOSE: Avoiding Out-of-distribution Data using Submodular Information Measures,"Suraj Kothawade, Akshit Srivastava, Venkat Iyer, Ganesh Ramakrishnan, Rishabh Iyer","Avoiding out-of-distribution (OOD) data is critical for training supervised machine learning models in the medical imaging domain. Furthermore, obtaining labeled medical data is difficult and expensive since it requires expert annotators like doctors, radiologists, etc. Active learning (AL) is a well-known method to mitigate labeling costs by selecting the most diverse or uncertain samples. However, current AL methods do not work well in the medical imaging domain with OOD data. We propose Diagnose (avoiDing out-of-dIstribution dAta usinG submodular iNfOrmation meaSurEs), a novel active learning framework that can jointly model similarity and dissimilarity, which is crucial in mining in-distribution data and avoiding OOD data at the same time. Particularly, we use a small number of data points as exemplars that represent a query set of in-distribution data points and a private set of OOD data points. We illustrate the generalizability of our framework by evaluating it on a wide variety of real-world OOD scenarios. Our experiments verify the superiority of Diagnose over the state-of-the-art AL methods across multiple domains of medical imaging. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01526,October 2022,694,Identifying middle school students’ challenges in computational thinking-based science learning,"Satabdi Basu, Gautam Biswas, Pratim Sengupta, Amanda Dickes, John S. Kinnebrew and Douglas Clark","Computational thinking (CT) parallels the core practices of science, technology, engineering, and mathematics (STEM) education and is believed to effectively support students’ learning of science and math conc...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-016-0036-2,21 May 2016,
695,0.000678989392134464,695,CLINICAL: Targeted Active Learning for Imbalanced Medical Image Classification,"Suraj Kothawade, Atharv Savarkar, Venkat Iyer, Lakshman Tamil, Ganesh Ramakrishnan, Rishabh Iyer","Training deep learning models on medical datasets that perform well for all classes is a challenging task. It is often the case that a suboptimal performance is obtained on some classes due to the natural class imbalance issue that comes with medical data. An effective way to tackle this problem is by using targeted active learning, where we iteratively add data points to the training data that belong to the rare classes. However, existing active learning methods are ineffective in targeting rare classes in medical datasets. In this work, we propose Clinical (targeted aCtive Learning for ImbalaNced medICal imAge cLassification) a framework that uses submodular mutual information functions as acquisition functions to mine critical data points from rare classes. We apply our framework to a wide-array of medical imaging datasets on a variety of real-world class imbalance scenarios - namely, binary imbalance and long-tail imbalance. We show that Clinical outperforms the state-of-the-art active learning methods by acquiring a diverse set of data points that belong to the rare classes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01520,October 2022,695,"ScrumOntoBDD: Agile software development based on scrum, ontologies and behaviour-driven development","Pedro Lopes de Souza, Wanderley Lopes de Souza and Luís Ferreira Pires","When developing a Learning Management System (LMS) using Scrum, we noticed that it was quite often necessary to redefine some system behaviour scenarios, due to ambiguities in the requirement specifications, o...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00114-w,13 June 2021,
696,0.000678989392134464,696,Estimating productivity gains in digital automation,"Mauricio Jacobo-Romero, Danilo S. Carvalho, André Freitas","This paper proposes a novel productivity estimation model to evaluate the effects of adopting Artificial Intelligence (AI) components in a production chain. Our model provides evidence to address the ""AI's"" Solow's Paradox. We provide (i) theoretical and empirical evidence to explain Solow's dichotomy; (ii) a data-driven model to estimate and asses productivity variations; (iii) a methodology underpinned on process mining datasets to determine the business process, BP, and productivity; (iv) a set of computer simulation parameters; (v) and empirical analysis on labour-distribution. These provide data on why we consider AI Solow's paradox a consequence of metric mismeasurement. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01252,October 2022,696,Exploration of a virtual restoration practice route for architectural heritage based on evidence-based design: a case study of the Bagong House,"Ziyi Zhang, Yiquan Zou and Wei Xiao","Architectural heritage is a testament to human and natural development, and the process of human social development can be glimpsed through the study and exploration of heritage. However, in the long history o...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-023-00878-8,20 February 2023,
697,0.000678989392134464,697,"Process Modeling, Hidden Markov Models, and Non-negative Tensor Factorization with Model Selection","Erik Skau, Andrew Hollis, Stephan Eidenbenz, Kim Rasmussen, Boian Alexandrov","Monitoring of industrial processes is a critical capability in industry and in government to ensure reliability of production cycles, quick emergency response, and national security. Process monitoring allows users to gauge the involvement of an organization in an industrial process or predict the degradation or aging of machine parts in processes taking place at a remote location. Similar to many data science applications, we usually only have access to limited raw data, such as satellite imagery, short video clips, some event logs, and signatures captured by a small set of sensors. To combat data scarcity, we leverage the knowledge of subject matter experts (SMEs) who are familiar with the process. Various process mining techniques have been developed for this type of analysis; typically such approaches combine theoretical process models built based on domain expert insights with ad-hoc integration of available pieces of raw data. Here, we introduce a novel mathematically sound method that integrates theoretical process models (as proposed by SMEs) with interrelated minimal Hidden Markov Models (HMM), built via non-negative tensor factorization and discrete model simulations. Our method consolidates: (a) Theoretical process models development, (b) Discrete model simulations (c) HMM, (d) Joint Non-negative Matrix Factorization (NMF) and Non-negative Tensor Factorization (NTF), and (e) Custom model selection. To demonstrate our methodology and its abilities, we apply it on simple synthetic and real world process models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.01060,October 2022,697,The measure of socio-economic status in PISA: a review and some suggested improvements,Francesco Avvisati,"This article reviews the history of the measure of socio-economic status in PISA and identifies theoretical underpinnings of the index of economic, social and cultural status (ESCS). It then highlights multipl...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00086-x,6 June 2020,
698,0.000678989392134464,698,Data-mining of In-Situ TEM Experiments: on the Dynamics of Dislocations in CoCrFeMnNi Alloys,"Chen Zhang, Hengxu Song, Daniela Oliveros, Anna Fraczkiewicz, Marc Legros, Stefan Sandfeld","High entropy alloys are a class of materials with many significant improvements in terms of mechanical properties as compared to ``classical'' alloys. The corresponding structure-property relations are not yet entirely clear, but it is commonly believed that the good mechanical performance is strongly related to dislocation interactions with the complex energy landscape formed due to alloying. Although in-situ Transmission Electron Microscopy (TEM) allows high-resolution studies of the structure and dynamics of moving dislocations and makes the local obstacle/energy ``landscape'' directly visible in the geometry of dislocations; such observation, however, are merely qualitative, and detailed three-dimensional analyses of the interaction between dislocations and the energy landscape is still missing. In this work, we utilized dislocations as ``probes'' for the local energy maxima which play the role of pinning points for the dislocation movement. To this end, we developed a unique data-mining approach that can perform coarse-grained spatio-temporal analysis, making ensemble averaging of a considerable number of snapshots possible. We investigate the effect of pinning points on the dislocation gliding behavior of CoCrFeMnNi alloy during in-situ TEM straining and find that (i) the pinning point strength changes when dislocations glide through and (ii) the pinning point moves along the direction close to the Burgers vector direction. Our data-mining method can be applied to dislocation motion in general, making it a useful tool for dislocation research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.00478,October 2022,698,Multiple imputation using chained equations for missing data in TIMSS: a case study,Donia Smaali Bouhlila and Fethi Sellaouti,"In this paper, we document a study that involved applying a multiple imputation technique with chained equations to data drawn from the 2007 iteration of the TIMSS database. More precisely, we imputed missing ...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/2196-0739-1-4,16 September 2013,
699,0.000678989392134464,699,Citation Trajectory Prediction via Publication Influence Representation Using Temporal Knowledge Graph,"Chang Zong, Yueting Zhuang, Weiming Lu, Jian Shao, Siliang Tang","Predicting the impact of publications in science and technology has become an important research area, which is useful in various real world scenarios such as technology investment, research direction selection, and technology policymaking. Citation trajectory prediction is one of the most popular tasks in this area. Existing approaches mainly rely on mining temporal and graph data from academic articles. Some recent methods are capable of handling cold-start prediction by aggregating metadata features of new publications. However, the implicit factors causing citations and the richer information from handling temporal and attribute features still need to be explored. In this paper, we propose CTPIR, a new citation trajectory prediction framework that is able to represent the influence (the momentum of citation) of either new or existing publications using the history information of all their attributes. Our framework is composed of three modules: difference-preserved graph embedding, fine-grained influence representation, and learning-based trajectory calculation. To test the effectiveness of our framework in more situations, we collect and construct a new temporal knowledge graph dataset from the real world, named AIPatent, which stems from global patents in the field of artificial intelligence. Experiments are conducted on both the APS academic dataset and our contributed AIPatent dataset. The results demonstrate the strengths of our approach in the citation trajectory prediction task. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.00450,October 2022,699,Accuracy of MFCC-Based Speaker Recognition in Series 60 Device,"Juhani Saastamoinen, Evgeny Karpov, Ville Hautamäki and Pasi Fränti",A fixed point implementation of speaker recognition based on MFCC signal processing is considered. We analyze the numerical error of the MFCC and its effect on the recognition accuracy. Techniques to reduce th...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2816,23 October 2005,
700,0.000678989392134464,700,Contrastive Graph Few-Shot Learning,"Chunhui Zhang, Hongfu Liu, Jundong Li, Yanfang Ye, Chuxu Zhang","Prevailing deep graph learning models often suffer from label sparsity issue. Although many graph few-shot learning (GFL) methods have been developed to avoid performance degradation in face of limited annotated data, they excessively rely on labeled data, where the distribution shift in the test phase might result in impaired generalization ability. Additionally, they lack a general purpose as their designs are coupled with task or data-specific characteristics. To this end, we propose a general and effective Contrastive Graph Few-shot Learning framework (CGFL). CGFL leverages a self-distilled contrastive learning procedure to boost GFL. Specifically, our model firstly pre-trains a graph encoder with contrastive learning using unlabeled data. Later, the trained encoder is frozen as a teacher model to distill a student model with a contrastive loss. The distilled model is finally fed to GFL. CGFL learns data representation in a self-supervised manner, thus mitigating the distribution shift impact for better generalization and making model task and data-independent for a general graph mining purpose. Furthermore, we introduce an information-based method to quantitatively measure the capability of CGFL. Comprehensive experiments demonstrate that CGFL outperforms state-of-the-art baselines on several graph mining tasks in the few-shot scenario. We also provide quantitative measurement of CGFL's success. △ Less",https://arxiv.orghttps://arxiv.org/abs/2210.00084,October 2022,700,Partitioning and Scheduling DSP Applications with Maximal Memory Access Hiding,"Zhong Wang, Edwin Hsing-Mean Sha and Yuke Wang",This paper presents an iteration space partitioning scheme to reduce the CPU idle time due to the long memory access latency. We take into consideration both the data accesses of intermediate and initial data....,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702205041,1 September 2002,
701,0.00673579331412596,701,Learning Second Order Local Anomaly for General Face Forgery Detection,"Jianwei Fei, Yunshu Dai, Peipeng Yu, Tianrun Shen, Zhihua Xia, Jian Weng","In this work, we propose a novel method to improve the generalization ability of CNN-based face forgery detectors. Our method considers the feature anomalies of forged faces caused by the prevalent blending operations in face forgery algorithms. Specifically, we propose a weakly supervised Second Order Local Anomaly (SOLA) learning module to mine anomalies in local regions using deep feature maps. SOLA first decomposes the neighborhood of local features by different directions and distances and then calculates the first and second order local anomaly maps which provide more general forgery traces for the classifier. We also propose a Local Enhancement Module (LEM) to improve the discrimination between local features of real and forged regions, so as to ensure accuracy in calculating anomalies. Besides, an improved Adaptive Spatial Rich Model (ASRM) is introduced to help mine subtle noise features via learnable high pass filters. With neither pixel level annotations nor external synthetic data, our method using a simple ResNet18 backbone achieves competitive performances compared with state-of-the-art works when evaluated on unseen forgeries. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.15490,September 2022,701,Collaborative concept mapping with reciprocal kit-build: a practical use in linear algebra course,"Lia Sadita, Tsukasa Hirashima, Yusuke Hayashi, Warunya Wunnasri, Jaruwat Pailai, Kasiyah Junus and Harry Budi Santoso",The design of interactions among peers plays a key role in collaborative learning. Various strategies have been applied to assist learners in collaborating and creating a continuous effort to construct and mai...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-020-00136-6,31 July 2020,
702,0.000678989392134464,702,OAK4XAI: Model towards Out-Of-Box eXplainable Artificial Intelligence for Digital Agriculture,"Quoc Hung Ngo, Tahar Kechadi, Nhien-An Le-Khac","Recent machine learning approaches have been effective in Artificial Intelligence (AI) applications. They produce robust results with a high level of accuracy. However, most of these techniques do not provide human-understandable explanations for supporting their results and decisions. They usually act as black boxes, and it is not easy to understand how decisions have been made. Explainable Artificial Intelligence (XAI), which has received much interest recently, tries to provide human-understandable explanations for decision-making and trained AI models. For instance, in digital agriculture, related domains often present peculiar or input features with no link to background knowledge. The application of the data mining process on agricultural data leads to results (knowledge), which are difficult to explain. In this paper, we propose a knowledge map model and an ontology design as an XAI framework (OAK4XAI) to deal with this issue. The framework does not only consider the data analysis part of the process, but it takes into account the semantics aspect of the domain knowledge via an ontology and a knowledge map model, provided as modules of the framework. Many ongoing XAI studies aim to provide accurate and verbalizable accounts for how given feature values contribute to model decisions. The proposed approach, however, focuses on providing consistent information and definitions of concepts, algorithms, and values involved in the data mining models. We built an Agriculture Computing Ontology (AgriComO) to explain the knowledge mined in agriculture. AgriComO has a well-designed structure and includes a wide range of concepts and transformations suitable for agriculture and computing domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.15104,September 2022,702,"Information overload in the information age: a review of the literature from business administration, business psychology, and related disciplines with a bibliometric approach and framework development",Peter Gordon Roetzel,"In the light of the information age, information overload research in new areas (e.g., social media, virtual collaboration) rises rapidly in many fields of research in business administration with a variety of...",https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40685-018-0069-z,6 July 2018,
703,0.000678989392134464,703,Perturbations and Subpopulations for Testing Robustness in Token-Based Argument Unit Recognition,"Jonathan Kamp, Lisa Beinborn, Antske Fokkens",Argument Unit Recognition and Classification aims at identifying argument units from text and classifying them as pro or against. One of the design choices that need to be made when developing systems for this task is what the unit of classification should be: segments of tokens or full sentences. Previous research suggests that fine-tuning language models on the token-level yields more robust results for classifying sentences compared to training on sentences directly. We reproduce the study that originally made this claim and further investigate what exactly token-based systems learned better compared to sentence-based ones. We develop systematic tests for analysing the behavioural differences between the token-based and the sentence-based system. Our results show that token-based models are generally more robust than sentence-based models both on manually perturbed examples and on specific subpopulations of the data. △ Less,https://arxiv.orghttps://arxiv.org/abs/2209.14780,September 2022,703,Face Recognition Using Local and Global Features,"Jian Huang, Pong C. Yuen, J. H. Lai and Chun-hung Li",The combining classifier approach has proved to be a proper way for improving recognition performance in the last two decades. This paper proposes to combine local and global facial features for face recogniti...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704312187,21 April 2004,
704,0.000678989392134464,704,Intrinsic Dimensionality Estimation within Tight Localities: A Theoretical and Experimental Analysis,"Laurent Amsaleg, Oussama Chelly, Michael E. Houle, Ken-ichi Kawarabayashi, Miloš Radovanović, Weeris Treeratanajaru","Accurate estimation of Intrinsic Dimensionality (ID) is of crucial importance in many data mining and machine learning tasks, including dimensionality reduction, outlier detection, similarity search and subspace clustering. However, since their convergence generally requires sample sizes (that is, neighborhood sizes) on the order of hundreds of points, existing ID estimation methods may have only limited usefulness for applications in which the data consists of many natural groups of small size. In this paper, we propose a local ID estimation strategy stable even for `tight' localities consisting of as few as 20 sample points. The estimator applies MLE techniques over all available pairwise distances among the members of the sample, based on a recent extreme-value-theoretic model of intrinsic dimensionality, the Local Intrinsic Dimension (LID). Our experimental results show that our proposed estimation technique can achieve notably smaller variance, while maintaining comparable levels of bias, at much smaller sample sizes than state-of-the-art estimators. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.14475,September 2022,704,Understanding interactions in face-to-face and remote undergraduate science laboratories: a literature review,"Jianye Wei, David F. Treagust, Mauro Mocerino, Anthony D. Lucey, Marjan G. Zadnik and Euan D. Lindsay","This paper reviews the ways in which interactions have been studied, and the findings of such studies, in science education in both face-to-face and remote laboratories. Guided by a systematic selection proces...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-019-0015-8,3 December 2019,
705,0.000678989392134464,705,Weighted Contrastive Hashing,"Jiaguo Yu, Huming Qiu, Dubing Chen, Haofeng Zhang","The development of unsupervised hashing is advanced by the recent popular contrastive learning paradigm. However, previous contrastive learning-based works have been hampered by (1) insufficient data similarity mining based on global-only image representations, and (2) the hash code semantic loss caused by the data augmentation. In this paper, we propose a novel method, namely Weighted Contrative Hashing (WCH), to take a step towards solving these two problems. We introduce a novel mutual attention module to alleviate the problem of information asymmetry in network features caused by the missing image structure during contrative augmentation. Furthermore, we explore the fine-grained semantic relations between images, i.e., we divide the images into multiple patches and calculate similarities between patches. The aggregated weighted similarities, which reflect the deep image relations, are distilled to facilitate the hash codes learning with a distillation loss, so as to obtain better retrieval performance. Extensive experiments show that the proposed WCH significantly outperforms existing unsupervised hashing methods on three benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.14099,September 2022,705,A critical time in computational cognitive science,David M W Powers,"Understanding how people tick is an endeavour that has challenged us for millennia, both in informal settings and in increasingly formalized and scientific disciplines. Some are interested in the biology and o...",https://www.springeropen.com//computationalcognitivescience.springeropen.com/articles/10.1186/s40469-015-0005-x,7 July 2015,
706,0.000678989392134464,706,Signed Latent Factors for Spamming Activity Detection,Yuli Liu,"Due to the increasing trend of performing spamming activities (e.g., Web spam, deceptive reviews, fake followers, etc.) on various online platforms to gain undeserved benefits, spam detection has emerged as a hot research issue. Previous attempts to combat spam mainly employ features related to metadata, user behaviors, or relational ties. These works have made considerable progress in understanding and filtering spamming campaigns. However, this problem remains far from fully solved. Almost all the proposed features focus on a limited number of observed attributes or explainable phenomena, making it difficult for existing methods to achieve further improvement. To broaden the vision about solving the spam problem and address long-standing challenges (class imbalance and graph incompleteness) in the spam detection area, we propose a new attempt of utilizing signed latent factors to filter fraudulent activities. The spam-contaminated relational datasets of multiple online applications in this scenario are interpreted by the unified signed network. Two competitive and highly dissimilar algorithms of latent factors mining (LFM) models are designed based on multi-relational likelihoods estimation (LFM-MRLE) and signed pairwise ranking (LFM-SPR), respectively. We then explore how to apply the mined latent factors to spam detection tasks. Experiments on real-world datasets of different kinds of Web applications (social media and Web forum) indicate that LFM models outperform state-of-the-art baselines in detecting spamming activities. By specifically manipulating experimental data, the effectiveness of our methods in dealing with incomplete and imbalanced challenges is valida △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13814,September 2022,706,Gender and researchers with institutional affiliations in the global south/north in social network science,Alejandro Espinosa-Rada and Francisca Ortiz,The following article aims to understand the prevalence of ascribed social characteristics such as the role of gender and the country of institutional affiliation of the authors in two prominent journals of so...,https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-022-00478-8,15 June 2022,
707,0.000678989392134464,707,Contrast Pattern Mining: A Survey,"Yao Chen, Wensheng Gan, Yongdong Wu, Philip S. Yu","Contrast pattern mining (CPM) is an important and popular subfield of data mining. Traditional sequential patterns cannot describe the contrast information between different classes of data, while contrast patterns involving the concept of contrast can describe the significant differences between datasets under different contrast conditions. Based on the number of papers published in this field, we find that researchers' interest in CPM is still active. Since CPM has many research questions and research methods. It is difficult for new researchers in the field to understand the general situation of the field in a short period of time. Therefore, the purpose of this article is to provide an up-to-date comprehensive and structured overview of the research direction of contrast pattern mining. First, we present an in-depth understanding of CPM, including basic concepts, types, mining strategies, and metrics for assessing discriminative ability. Then we classify CPM methods according to their characteristics into boundary-based algorithms, tree-based algorithms, evolutionary fuzzy system-based algorithms, decision tree-based algorithms, and other algorithms. In addition, we list the classical algorithms of these methods and discuss their advantages and disadvantages. Advanced topics in CPM are presented. Finally, we conclude our survey with a discussion of the challenges and opportunities in this field. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13556,September 2022,707,Calibrating Distributed Camera Networks Using Belief Propagation,Dhanya Devarajan and Richard J. Radke,"We discuss how to obtain the accurate and globally consistent self-calibration of a distributed camera network, in which camera nodes with no centralized processor may be spread over a wide geographical area. ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/60696,1 December 2006,
708,0.000678989392134464,708,VDDB: a comprehensive resource and machine learning platform for antiviral drug discovery,"Shunming Tao, Yihao Chen, Jingxing Wu, Duancheng Zhao, Hanxuan Cai, Ling Wang","Virus infection is one of the major diseases that seriously threaten human health. To meet the growing demand for mining and sharing data resources related to antiviral drugs and to accelerate the design and discovery of new antiviral drugs, we presented an open-access antiviral drug resource and machine learning platform (VDDB), which, to the best of our knowledge, is the first comprehensive dedicated resource for experimentally verified potential drugs/molecules based on manually curated data. Currently, VDDB highlights 848 clinical vaccines, 199 clinical antibodies, as well as over 710,000 small molecules targeting 39 medically important viruses including SARS-CoV-2. Furthermore, VDDB stores approximately 3 million records of pharmacological data for these collected potential antiviral drugs/molecules, involving 314 cell infection-based phenotypic and 234 target-based genotypic assays. Based on these annotated pharmacological data, VDDB allows users to browse, search and download reliable information about these collects for various viruses of interest. In particular, VDDB also integrates 57 cell infection- and 117 target-based associated high-accuracy machine learning models to support various antivirals identification-related tasks, such as compound activity prediction, virtual screening, drug repositioning and target fishing. VDDB is freely accessible at http://vddb.idruglab.cn. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13521,September 2022,708,The key characteristics of project-based learning: how teachers implement projects in K-12 science education,Anette Markula and Maija Aksela,The aim of this multiple-case study was to research the key characteristics of project-based learning (PBL) and how teachers implement them within the context of science education. K-12 science teachers and th...,https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-021-00042-x,6 January 2022,
709,0.000678989392134464,709,Totally-ordered Sequential Rules for Utility Maximization,"Chunkai Zhang, Maohua Lyu, Wensheng Gan, Philip S. Yu","High utility sequential pattern mining (HUSPM) is a significant and valuable activity in knowledge discovery and data analytics with many real-world applications. In some cases, HUSPM can not provide an excellent measure to predict what will happen. High utility sequential rule mining (HUSRM) discovers high utility and high confidence sequential rules, allowing it to solve the problem in HUSPM. All existing HUSRM algorithms aim to find high-utility partially-ordered sequential rules (HUSRs), which are not consistent with reality and may generate fake HUSRs. Therefore, in this paper, we formulate the problem of high utility totally-ordered sequential rule mining and propose two novel algorithms, called TotalSR and TotalSR+, which aim to identify all high utility totally-ordered sequential rules (HTSRs). TotalSR creates a utility table that can efficiently calculate antecedent support and a utility prefix sum list that can compute the remaining utility in O(1) time for a sequence. We also introduce a left-first expansion strategy that can utilize the anti-monotonic property to use a confidence pruning strategy. TotalSR can also drastically reduce the search space with the help of utility upper bounds pruning strategies, avoiding much more meaningless computation. In addition, TotalSR+ uses an auxiliary antecedent record table to more efficiently discover HTSRs. Finally, there are numerous experimental results on both real and synthetic datasets demonstrating that TotalSR is significantly more efficient than algorithms with fewer pruning strategies, and TotalSR+ is significantly more efficient than TotalSR in terms of running time and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13501,September 2022,709,Calculation Scheme Based on a Weighted Primitive: Application to Image Processing Transforms,"María Teresa Signes Pont, Juan Manuel García Chamizo, Higinio Mora Mora and Gregorio de Miguel Casado",This paper presents a method to improve the calculation of functions which specially demand a great amount of computing resources. The method is based on the choice of a weighted primitive which enables the ca...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/45321,1 December 2007,
710,0.000678989392134464,710,Mine yOur owN Anatomy: Revisiting Medical Image Segmentation with Extremely Limited Labels,"Chenyu You, Weicheng Dai, Fenglin Liu, Yifei Min, Haoran Su, Xiaoran Zhang, Xiaoxiao Li, David A. Clifton, Lawrence Staib, James S. Duncan","Recent studies on contrastive learning have achieved remarkable performance solely by leveraging few labels in the context of medical image segmentation. Existing methods mainly focus on instance discrimination and invariant mapping. However, they face three common pitfalls: (1) tailness: medical image data usually follows an implicit long-tail class distribution. Blindly leveraging all pixels in training hence can lead to the data imbalance issues, and cause deteriorated performance; (2) consistency: it remains unclear whether a segmentation model has learned meaningful and yet consistent anatomical features due to the intra-class variations between different anatomical features; and (3) diversity: the intra-slice correlations within the entire dataset have received significantly less attention. This motivates us to seek a principled approach for strategically making use of the dataset itself to discover similar yet distinct samples from different anatomical views. In this paper, we introduce a novel semi-supervised 2D medical image segmentation framework termed Mine yOur owN Anatomy (MONA), and make three contributions. First, prior work argues that every pixel equally matters to the model training; we observe empirically that this alone is unlikely to define meaningful anatomical features, mainly due to lacking the supervision signal. We show two simple solutions towards learning invariances - through the use of stronger data augmentations and nearest neighbors. Second, we construct a set of objectives that encourage the model to be capable of decomposing medical images into a collection of anatomical features in an unsupervised manner. Lastly, we both empirically and theoretically, demonstrate the efficacy of our MONA on three benchmark datasets with different labeled settings, achieving new state-of-the-art under different labeled semi-supervised settings △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13476,September 2022,710,Estimating Intrinsic Camera Parameters from the Fundamental Matrix Using an Evolutionary Approach,Anthony Whitehead and Gerhard Roth,Calibration is the process of computing the intrinsic (internal) camera parameters from a series of images. Normally calibration is done by placing predefined targets in the scene or by having special camera m...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704401024,8 July 2004,
711,0.000678989392134464,711,Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations,"Vy Vo, Trung Le, Van Nguyen, He Zhao, Edwin Bonilla, Gholamreza Haffari, Dinh Phung","Interpretable machine learning seeks to understand the reasoning process of complex black-box systems that are long notorious for lack of explainability. One flourishing approach is through counterfactual explanations, which provide suggestions on what a user can do to alter an outcome. Not only must a counterfactual example counter the original prediction from the black-box classifier but it should also satisfy various constraints for practical applications. Diversity is one of the critical constraints that however remains less discussed. While diverse counterfactuals are ideal, it is computationally challenging to simultaneously address some other constraints. Furthermore, there is a growing privacy concern over the released counterfactual data. To this end, we propose a feature-based learning framework that effectively handles the counterfactual constraints and contributes itself to the limited pool of private explanation models. We demonstrate the flexibility and effectiveness of our method in generating diverse counterfactuals of actionability and plausibility. Our counterfactual engine is more efficient than counterparts of the same capacity while yielding the lowest re-identification risks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13446,September 2022,711,Welcome to the new Journal of Software Engineering Research and Development (JSERD),"Itana MS Gimenes, Claudia Werner and Andre van der Hoek",Unknown,https://www.springeropen.com//jserd.springeropen.com/articles/10.1186/2195-1721-1-1,29 October 2013,
712,0.000678989392134464,712,A Survey on Graph Neural Networks and Graph Transformers in Computer Vision: A Task-Oriented Perspective,"Chaoqi Chen, Yushuang Wu, Qiyuan Dai, Hong-Yu Zhou, Mutian Xu, Sibei Yang, Xiaoguang Han, Yizhou Yu","Graph Neural Networks (GNNs) have gained momentum in graph representation learning and boosted the state of the art in a variety of areas, such as data mining (\emph{e.g.,} social network analysis and recommender systems), computer vision (\emph{e.g.,} object detection and point cloud learning), and natural language processing (\emph{e.g.,} relation extraction and sequence learning), to name a few. With the emergence of Transformers in natural language processing and computer vision, graph Transformers embed a graph structure into the Transformer architecture to overcome the limitations of local neighborhood aggregation while avoiding strict structural inductive biases. In this paper, we present a comprehensive review of GNNs and graph Transformers in computer vision from a task-oriented perspective. Specifically, we divide their applications in computer vision into five categories according to the modality of input data, \emph{i.e.,} 2D natural images, videos, 3D data, vision + language, and medical images. In each category, we further divide the applications according to a set of vision tasks. Such a task-oriented taxonomy allows us to examine how each task is tackled by different GNN-based approaches and how well these approaches perform. Based on the necessary preliminaries, we provide the definitions and challenges of the tasks, in-depth coverage of the representative approaches, as well as discussions regarding insights, limitations, and future directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13232,September 2022,712,Fast Registration of Remotely Sensed Images for Earthquake Damage Estimation,"Arash Abadpour, Shohreh Kasaei and S. Mohsen Amiri",Analysis of the multispectral remotely sensed images of the areas destroyed by an earthquake is proved to be a helpful tool for destruction assessments. The performance of such methods is highly dependant on t...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/76462,1 December 2006,
713,0.000678989392134464,713,PARSRec: Explainable Personalized Attention-fused Recurrent Sequential Recommendation Using Session Partial Actions,"Ehsan Gholami, Mohammad Motamedi, Ashwin Aravindakshan","The emerging meta- and multi-verse landscape is yet another step towards the more prevalent use of already ubiquitous online markets. In such markets, recommender systems play critical roles by offering items of interest to the users, thereby narrowing down a vast search space that comprises hundreds of thousands of products. Recommender systems are usually designed to learn common user behaviors and rely on them for inference. This approach, while effective, is oblivious to subtle idiosyncrasies that differentiate humans from each other. Focusing on this observation, we propose an architecture that relies on common patterns as well as individual behaviors to tailor its recommendations for each person. Simulations under a controlled environment show that our proposed model learns interpretable personalized user behaviors. Our empirical results on Nielsen Consumer Panel dataset indicate that the proposed approach achieves up to 27.9% performance improvement compared to the state-of-the-art. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.13015,September 2022,713,Towards behaviour based testing to understand the black box of autonomous cars,"Fabian Utesch, Alexander Brandies, Paulin Pekezou Fouopi and Caroline Schießl","Autonomous cars could make traffic safer, more convenient, efficient and sustainable. They promise the convenience of a personal taxi, without the need for a human driver. Artificial intelligence would operate...",https://www.springeropen.com//etrr.springeropen.com/articles/10.1186/s12544-020-00438-2,29 July 2020,
714,0.000678989392134464,714,TGLib: An Open-Source Library for Temporal Graph Analysis,"Lutz Oettershagen, Petra Mutzel","We initiate an open-source library for the efficient analysis of temporal graphs. We consider one of the standard models of dynamic networks in which each edge has a discrete timestamp and transition time. Recently there has been a massive interest in analyzing such temporal graphs. Common computational data mining and analysis tasks include the computation of temporal distances, centrality measures, and network statistics like topological overlap, burstiness, or temporal diameter. To fulfill the increasing demand for efficient and easy-to-use implementations of temporal graph algorithms, we introduce the open-source library TGLib, which integrates efficient data structures and algorithms for temporal graph analysis. TGLib is highly efficient and versatile, providing simple and convenient C++ and Python interfaces, targeting computer scientists, practitioners, students, and the (temporal) network research community. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.12587,September 2022,714,Terrorist networks and the lethality of attacks: an illustrative agent based model on evolutionary principles,Paul Ormerod,A data base developed from the Memorial Institute for the Prevention of Terrorism’s (MIPT) Terrorism Knowledge Base for the years 1998–2005 was provided to participants in the workshop. The distribution of fat...,https://www.springeropen.com//security-informatics.springeropen.com/articles/10.1186/2190-8532-1-16,20 November 2012,
715,0.000678989392134464,715,5-Star Hotel Customer Satisfaction Analysis Using Hybrid Methodology,"Yongmin Yoo, Yeongjoon Park, Dongjin Lim, Deaho Seo","Due to the rapid development of non-face-to-face services due to the corona virus, commerce through the Internet, such as sales and reservations, is increasing very rapidly. Consumers also post reviews, suggestions, or judgments about goods or services on the website. The review data directly used by consumers provides positive feedback and nice impact to consumers, such as creating business value. Therefore, analysing review data is very important from a marketing point of view. Our research suggests a new way to find factors for customer satisfaction through review data. We applied a method to find factors for customer satisfaction by mixing and using the data mining technique, which is a big data analysis method, and the natural language processing technique, which is a language processing method, in our research. Unlike many studies on customer satisfaction that have been conducted in the past, our research has a novelty of the thesis by using various techniques. And as a result of the analysis, the results of our experiments were very accurate. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.12417,September 2022,715,Teaching Mathematics Online: Emergent Technologies and Methodologies,Hans Cuypers,"The following text reviews the bookTeaching Mathematics Online: Emergent Technologies and Methodologies, recently published by IGI Global. This book brings together experiences and best practices related to the ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.7238/rusc.v9i1.1432,15 January 2012,
716,0.000678989392134464,716,Integrity 2022: Integrity in Social Networks and Media,"Lluís Garcia-Pueyo, Panayiotis Tsaparas, Anand Bhaskar, Prathyusha Senthil Kumar, Roelof van Zwol, Timos Sellis, Anthony McCosker, Paolo Papotti","This is the proposal for the third edition of the Workshop on Integrity in Social Networks and Media, Integrity 2022, following the success of the first two Workshops held in conjunction with the 13th & 14th ACM Conference on Web Search and Data Mining (WSDM) in 2020 and 2021. The goal of the workshop is to bring together researchers and practitioners to discuss content and interaction integrity challenges in social networks and social media platforms. The event consists of (1) a series of invited talks by reputed members of the Integrity community from both academia and industry, (2) a call-for-papers for contributed talks and posters, and (3) a panel with the speakers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.11867,September 2022,716,State-of-the-art and annual progress of bridge engineering in 2020,"Renda Zhao, Kaifeng Zheng, Xing Wei, Hongyu Jia, Haili Liao, Xiaozhen Li, Kai Wei, Yulin Zhan, Qinghua Zhang, Lin Xiao, Lingyuan Zhou, Ruili Shen, Hongye Gou, Qianhui Pu, Fang Zhang, Ziyi Xu…","Bridge construction is one of the cores of traffic infrastructure construction. To better develop relevant bridge science, this paper introduces the main research progress in China and abroad in 2020 from 16 a...",https://www.springeropen.com//aben.springeropen.com/articles/10.1186/s43251-021-00050-x,30 December 2021,
717,0.000678989392134464,717,Artificial Intelligence and Advanced Materials,Cefe López,"Artificial intelligence is gaining strength and materials science can both contribute to and profit from it. In a simultaneous progress race, new materials, systems and processes can be devised and optimized thanks to machine learning techniques and such progress can be turned into in-novative computing platforms. Future materials scientists will profit from understanding how machine learning can boost the conception of advanced materials. This review covers aspects of computation from the fundamentals to directions taken and repercussions produced by compu-tation to account for the origins, procedures and applications of artificial intelligence. Machine learning and its methods are reviewed to provide basic knowledge on its implementation and its potential. The materials and systems used to implement artificial intelligence with electric charges are finding serious competition from other information carrying and processing agents. The impact these techniques are having on the inception of new advanced materials is so deep that a new paradigm is developing where implicit knowledge is being mined to conceive materi-als and systems for functions instead of finding applications to found materials. How far this trend can be carried is hard to fathom as exemplified by the power to discover unheard of mate-rials or physical laws buried in data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.11618,September 2022,717,"The exploration of continuous learning intention in STEAM education through attitude, motivation, and cognitive load","Chih-Hung Wu, Chih-Hsing Liu and Yueh-Min Huang","This study proposes a learning cycle and a comprehensive research framework that integrates Bloom’s taxonomy: the cognitive domain (cognitive load), affective domain (attitude and motivation) and psychomotor d...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00346-y,11 May 2022,
718,0.000678989392134464,718,Benchmarking Apache Spark and Hadoop MapReduce on Big Data Classification,"Taha Tekdogan, Ali Cakmak","Most of the popular Big Data analytics tools evolved to adapt their working environment to extract valuable information from a vast amount of unstructured data. The ability of data mining techniques to filter this helpful information from Big Data led to the term Big Data Mining. Shifting the scope of data from small-size, structured, and stable data to huge volume, unstructured, and quickly changing data brings many data management challenges. Different tools cope with these challenges in their own way due to their architectural limitations. There are numerous parameters to take into consideration when choosing the right data management framework based on the task at hand. In this paper, we present a comprehensive benchmark for two widely used Big Data analytics tools, namely Apache Spark and Hadoop MapReduce, on a common data mining task, i.e., classification. We employ several evaluation metrics to compare the performance of the benchmarked frameworks, such as execution time, accuracy, and scalability. These metrics are specialized to measure the performance for classification task. To the best of our knowledge, there is no previous study in the literature that employs all these metrics while taking into consideration task-specific concerns. We show that Spark is 5 times faster than MapReduce on training the model. Nevertheless, the performance of Spark degrades when the input workload gets larger. Scaling the environment by additional clusters significantly improves the performance of Spark. However, similar enhancement is not observed in Hadoop. Machine learning utility of MapReduce tend to have better accuracy scores than that of Spark, like around 3%, even in small size data sets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.10637,September 2022,718,Mixed-State Models for Nonstationary Multiobject Activities,Naresh P Cuntoor and Rama Chellappa,We present a mixed-state space approach for modeling and segmenting human activities. The discrete-valued component of the mixed state represents higher-level behavior while the continuous state models the dyn...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/65989,1 December 2006,
719,0.000678989392134464,719,SMTCE: A Social Media Text Classification Evaluation Benchmark and BERTology Models for Vietnamese,"Luan Thanh Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen","Text classification is a typical natural language processing or computational linguistics task with various interesting applications. As the number of users on social media platforms increases, data acceleration promotes emerging studies on Social Media Text Classification (SMTC) or social media text mining on these valuable resources. In contrast to English, Vietnamese, one of the low-resource languages, is still not concentrated on and exploited thoroughly. Inspired by the success of the GLUE, we introduce the Social Media Text Classification Evaluation (SMTCE) benchmark, as a collection of datasets and models across a diverse set of SMTC tasks. With the proposed benchmark, we implement and analyze the effectiveness of a variety of multilingual BERT-based models (mBERT, XLM-R, and DistilmBERT) and monolingual BERT-based models (PhoBERT, viBERT, vELECTRA, and viBERT4news) for tasks in the SMTCE benchmark. Monolingual models outperform multilingual models and achieve state-of-the-art results on all text classification tasks. It provides an objective assessment of multilingual and monolingual BERT-based models on the benchmark, which will benefit future studies about BERTology in the Vietnamese language. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.10482,September 2022,719,Science identity development: an interactionist approach,Ann Y. Kim and Gale M. Sinatra,"In this introduction for this Special Issue we discuss the need for the investigation of science identity with an emphasis on the environment. As such, we propose taking an interactionist approach; one that ex...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-018-0149-9,30 November 2018,
720,0.000678989392134464,720,Exact and Sampling Methods for Mining Higher-Order Motifs in Large Hypergraphs,"Quintino Francesco Lotito, Federico Musciotto, Federico Battiston, Alberto Montresor","Network motifs are patterns of interactions occurring among a small set of nodes in a graph. They highlight fundamental aspects of the interplay between the topology and the dynamics of complex networks and have a wide range of real-world applications. Motif analysis has been extended to a variety of network models that allow for a richer description of the interactions of a system, including weighted, temporal, multilayer, and, more recently, higher-order networks. Generalizing network motifs to capture patterns of group interactions is not only interesting from the fundamental perspective of understanding complex systems, but also proposes unprecedented computational challenges. In this work, we focus on the problem of counting occurrences of sub-hypergraph patterns in very large higher-order networks. We show that, by directly exploiting higher-order structures, we speed up the counting process compared to applying traditional data mining techniques for network motifs. Moreover, by including hyperedge sampling techniques, computational complexity is further reduced at the cost of small errors in the estimation of motif frequency. We evaluate our algorithms on several real-world datasets describing face-to-face interactions, co-authorship and human communication. We show that our approximated algorithm not only allows to speed up the performance, but also to extract larger higher-order motifs beyond the computational limits of an exact approach. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.10241,September 2022,720,Environmental rehabilitation of damaged land,Mike Mentis,"Much land is subject to damage by construction, development and exploitation with consequent loss of environmental function and services. How might the loss be recovered?",https://www.springeropen.com//forestecosyst.springeropen.com/articles/10.1186/s40663-020-00233-4,6 April 2020,
721,0.000678989392134464,721,Rethinking Data Augmentation in Knowledge Distillation for Object Detection,"Jiawei Liang, Siyuan Liang, Aishan Liu, Mingli Zhu, Danni Yuan, Chenye Xu, Xiaochun Cao","Knowledge distillation (KD) has shown its effectiveness for object detection, where it trains a compact object detector under the supervision of both AI knowledge (teacher detector) and human knowledge (human expert). However, existing studies treat the AI knowledge and human knowledge consistently and adopt a uniform data augmentation strategy during learning, which would lead to the biased learning of multi-scale objects and insufficient learning for the teacher detector causing unsatisfactory distillation performance. To tackle these problems, we propose the sample-specific data augmentation and adversarial feature augmentation. Firstly, to mitigate the impact incurred by multi-scale objects, we propose an adaptive data augmentation based on our observations from the Fourier perspective. Secondly, we propose a feature augmentation method based on adversarial examples for better mimicking AI knowledge to make up for the insufficient information mining of the teacher detector. Furthermore, our proposed method is unified and easily extended to other KD methods. Extensive experiments demonstrate the effectiveness of our framework and improve the performance of state-of-the-art methods in one-stage and two-stage detectors, bringing at most 0.5 mAP gains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09841,September 2022,721,Developing an online learner satisfaction framework in higher education through a systematic review of research,Florence Martin and Doris U. Bolliger,"Satisfaction is a critical aspect of student success in online education. In this systematic review, we examine 98 articles which studied various aspects of online learner satisfaction. We specifically analyze...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00355-5,20 September 2022,
722,0.000678989392134464,722,OC-PM: Analyzing Object-Centric Event Logs and Process Models,"Alessandro Berti, Wil van der Aalst","Object-centric process mining is a novel branch of process mining that aims to analyze event data from mainstream information systems (such as SAP) more naturally, without being forced to form mutually exclusive groups of events with the specification of a case notion. The development of object-centric process mining is related to exploiting object-centric event logs, which includes exploring and filtering the behavior contained in the logs and constructing process models which can encode the behavior of different classes of objects and their interactions (which can be discovered from object-centric event logs). This paper aims to provide a broad look at the exploration and processing of object-centric event logs to discover information related to the lifecycle of the different objects composing the event log. Also, comprehensive tool support (OC-PM) implementing the proposed techniques is described in the paper. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09725,September 2022,722,Cryptanalysis of the Two-Dimensional Circulation Encryption Algorithm,"Christophe De Cannière, Joseph Lano and Bart Preneel","We analyze the security of the two-dimensional circulation encryption algorithm (TDCEA), recently published by Chen et al. in this journal. We show that there are several flaws in the algorithm and describe so...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1923,28 July 2005,
723,0.000678989392134464,723,Personal Attribute Prediction from Conversations,"Yinan Liu, Hu Chen, Wei Shen","Personal knowledge bases (PKBs) are critical to many applications, such as Web-based chatbots and personalized recommendation. Conversations containing rich personal knowledge can be regarded as a main source to populate the PKB. Given a user, a user attribute, and user utterances from a conversational system, we aim to predict the personal attribute value for the user, which is helpful for the enrichment of PKBs. However, there are three issues existing in previous studies: (1) manually labeled utterances are required for model training; (2) personal attribute knowledge embedded in both utterances and external resources is underutilized; (3) the performance on predicting some difficult personal attributes is unsatisfactory. In this paper, we propose a framework DSCGN based on the pre-trained language model with a noise-robust loss function to predict personal attributes from conversations without requiring any labeled utterances. We yield two categories of supervision, i.e., document-level supervision via a distant supervision strategy and contextualized word-level supervision via a label guessing method, by mining the personal attribute knowledge embedded in both unlabeled utterances and external resources to fine-tune the language model. Extensive experiments over two real-world data sets (i.e., a profession data set and a hobby data set) show our framework obtains the best performance compared with all the twelve baselines in terms of nDCG and MRR. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09619,September 2022,723,Image and Video Processing for Cultural Heritage,"Vincent Charvillat, Anna Tonazzini, Luc Van Gool and Nikos Nikolaidis",Unknown,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2009/163064,30 March 2010,
724,0.000678989392134464,724,Portable Resistive Plate Chambers for Muography in confined environments,"R. M. I. D Gamage, Samip Basnet, Eduardo Cortina Gil, Andrea Giammanco, Pavel Demin, Marwa Moussawi, Amrutha Samalan, Michael Tytgat, Raveendrababu Karnam, Ayman Youssef","Muography (or muon radiography) is an imaging technique that relies on the use of cosmogenic muons as a free and safe radiation source. It can be applied in various fields such as archaeology, civil engineering, geology, nuclear reactor monitoring, nuclear waste characterization, underground surveys, etc. In such applications, sometimes deploying muon detectors is challenging due to logistics, e.g. in a narrow underground tunnel or mine. Therefore, we are developing muon detectors whose design goals include portability, robustness, autonomy, versatility, and safety. Our portable muon detectors (or ``muoscopes'') are based on Resistive Plate Chambers (RPC), planar detectors that use ionization in a thin gas gap to detect cosmic muons. Prototype RPCs of active area $16 \times 16~cm^2$ and $28 \times 28~cm^2$ were built in our laboratories at Louvain-la-Neuve (UCLouvain) and Ghent (UGent) to test and compare various design options. Benefiting from the experience gained in building and operating these prototypes, we are proceeding towards the development of improved prototypes with more advanced technical layout and readiness. In this paper we provide the status of our performance studies, including the cross-validation of the two types of prototypes in a joint data taking, and an outline of the direction ahead. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09560,September 2022,724,The marginal value of increased testing: An empirical analysis using four code coverage measures,Swapna S. Gokhale and Robert E. Mullen,"This paper presents an empirical comparison of the growth characteristics of four code coverage measures, block, decision, c-use and p-use, as testing is increased. Due to the theoretical foundations underlyin...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194493,October 2006,
725,0.000678989392134464,725,A Framework for Benchmarking Clustering Algorithms,Marek Gagolewski,"The evaluation of clustering algorithms can involve running them on a variety of benchmark problems, and comparing their outputs to the reference, ground-truth groupings provided by experts. Unfortunately, many research papers and graduate theses consider only a small number of datasets. Also, the fact that there can be many equally valid ways to cluster a given problem set is rarely taken into account. In order to overcome these limitations, we have developed a framework whose aim is to introduce a consistent methodology for testing clustering algorithms. Furthermore, we have aggregated, polished, and standardised many clustering benchmark dataset collections referred to across the machine learning and data mining literature, and included new datasets of different dimensionalities, sizes, and cluster types. An interactive datasets explorer, the documentation of the Python API, a description of the ways to interact with the framework from other programming languages such as R or MATLAB, and other details are all provided at <https://clustering-benchmarks.gagolewski.com>. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09493,September 2022,725,The National University of Singapore and what it does,Barry Halliwell,Unknown,https://www.springeropen.com//biointerphases.springeropen.com/articles/10.1116/1.3519904,September 2010,
726,0.000678989392134464,726,Interpreting mechanism of Synergism of drug combinations using attention based hierarchical graph pooling,"Zehao Dong, Yixin Chen, Philip Payne, Fuhai Li","The synergistic drug combinations provide huge potentials to enhance therapeutic efficacy and to reduce adverse reactions. However, effective and synergistic drug combination prediction remains an open question because of the unknown causal disease signaling pathways. Though various deep learning (AI) models have been proposed to quantitatively predict the synergism of drug combinations. The major limitation of existing deep learning methods is that they are inherently not interpretable, which makes the conclusion of AI models un-transparent to human experts, henceforth limiting the robustness of the model conclusion and the implementation ability of these models in the real-world human-AI healthcare. In this paper, we develop an interpretable graph neural network (GNN) that reveals the underlying essential therapeutic targets and mechanism of the synergy (MoS) by mining the sub-molecular network of great importance. The key point of the interpretable GNN prediction model is a novel graph pooling layer, Self-Attention based Node and Edge pool (henceforth SANEpool), that can compute the attention score (importance) of nodes and edges based on the node features and the graph topology. As such, the proposed GNN model provides a systematic way to predict and interpret the drug combination synergism based on the detected crucial sub-molecular network. We evaluate SANEpool on molecular networks formulated by genes from 46 core cancer signaling pathways and drug combinations from NCI ALMANAC drug combination screening data. The experimental results indicate that 1) SANEpool can achieve the current state-of-art performance among other popular graph neural networks; and 2) the sub-molecular network detected by SANEpool are self-explainable and salient for identifying synergistic drug combinations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09245,September 2022,726,Topological Active Volumes,N. Barreira and M. G. Penedo,The topological active volumes (TAVs) model is a general model for 3D image segmentation. It is based on deformable models and integrates features of region-based and boundary-based segmentation techniques. Be...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1939,15 August 2005,
727,0.000678989392134464,727,Fairness in Face Presentation Attack Detection,"Meiling Fang, Wufei Yang, Arjan Kuijper, Vitomir Struc, Naser Damer","Face presentation attack detection (PAD) is critical to secure face recognition (FR) applications from presentation attacks. FR performance has been shown to be unfair to certain demographic and non-demographic groups. However, the fairness of face PAD is an understudied issue, mainly due to the lack of appropriately annotated data. To address this issue, this work first presents a Combined Attribute Annotated PAD Dataset (CAAD-PAD) by combining several well-known PAD datasets where we provide seven human-annotated attribute labels. This work then comprehensively analyses the fairness of a set of face PADs and its relation to the nature of training data and the Operational Decision Threshold Assignment (ODTA) on different data groups by studying four face PAD approaches on our CAAD-PAD. To simultaneously represent both the PAD fairness and the absolute PAD performance, we introduce a novel metric, namely the Accuracy Balanced Fairness (ABF). Extensive experiments on CAAD-PAD show that the training data and ODTA induce unfairness on gender, occlusion, and other attribute groups. Based on these analyses, we propose a data augmentation method, FairSWAP, which aims to disrupt the identity/semantic information and guide models to mine attack cues rather than attribute-related information. Detailed experimental results demonstrate that FairSWAP generally enhances both the PAD performance and the fairness of face PAD. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.09035,September 2022,727,Letter from the guest editors,"Jaime Simão Sichman, Virginia Dignum and Cristiano Castelfranchi",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192367,February 2005,
728,0.000678989392134464,728,Adopting Automated Bug Assignment in Practice: A Longitudinal Case Study at Ericsson,"Markus Borg, Leif Jonsson, Emelie Engström, Béla Bartalos, Attila Szabó","The continuous inflow of bug reports is a considerable challenge in large development projects. Inspired by contemporary work on mining software repositories, we designed a prototype bug assignment solution based on machine learning in 2011-2016. The prototype evolved into an internal Ericsson product, TRR, in 2017-2018. TRR's first bug assignment without human intervention happened in April 2019. Our study evaluates the adoption of TRR within its industrial context at Ericsson. Moreover, we investigate 1) how TRR performs in the field, 2) what value TRR provides to Ericsson, and 3) how TRR has influenced the ways of working. We conduct an industrial case study combining interviews with TRR stakeholders, minutes from sprint planning meetings, and bug tracking data. The data analysis includes thematic analysis, descriptive statistics, and Bayesian causal analysis. TRR is now an incorporated part of the bug assignment process. Considering the abstraction levels of the telecommunications stack, high-level modules are more positive while low-level modules experienced some drawbacks. On average, TRR automatically assigns 30% of the incoming bug reports with an accuracy of 75%. Auto-routed TRs are resolved around 21% faster within Ericsson, and TRR has saved highly seasoned engineers many hours of work. Indirect effects of adopting TRR include process improvements, process awareness, increased communication, and higher job satisfaction. TRR has saved time at Ericsson, but the adoption of automated bug assignment was more intricate compared to similar endeavors reported from other companies. We primarily attribute the difference to the very large size of the organization and the complex products. Key facilitators in the successful adoption include a gradual introduction, product champions, and careful stakeholder analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.08955,September 2022,728,Correction to: Cooperative co‑evolution for feature selection in Big Data with random feature grouping,"A. N. M. Bazlur Rashid, Mohiuddin Ahmed, Leslie F. Sikos and Paul Haskell‑Dowland",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00403-9,28 December 2020,
729,0.000678989392134464,729,OPR-Miner: Order-preserving rule mining for time series,"Youxi Wu, Xiaoqian Zhao, Yan Li, Lei Guo, Xingquan Zhu, Philippe Fournier-Viger, Xindong Wu","Discovering frequent trends in time series is a critical task in data mining. Recently, order-preserving matching was proposed to find all occurrences of a pattern in a time series, where the pattern is a relative order (regarded as a trend) and an occurrence is a sub-time series whose relative order coincides with the pattern. Inspired by the order-preserving matching, the existing order-preserving pattern (OPP) mining algorithm employs order-preserving matching to calculate the support, which leads to low efficiency. To address this deficiency, this paper proposes an algorithm called efficient frequent OPP miner (EFO-Miner) to find all frequent OPPs. EFO-Miner is composed of four parts: a pattern fusion strategy to generate candidate patterns, a matching process for the results of sub-patterns to calculate the support of super-patterns, a screening strategy to dynamically reduce the size of prefix and suffix arrays, and a pruning strategy to further dynamically prune candidate patterns. Moreover, this paper explores the order-preserving rule (OPR) mining and proposes an algorithm called OPR-Miner to discover strong rules from all frequent OPPs using EFO-Miner. Experimental results verify that OPR-Miner gives better performance than other competitive algorithms. More importantly, clustering and classification experiments further validate that OPR-Miner achieves good performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.08932,September 2022,729,An information retrieval application using ontologies,"Christian Paz-Trillo, Renata Wassermann and Paula P. Braga","Searching for information in long videos can be a time-consuming experience. In this paper, we describe OnAIR, an ontology-aided information retrieval system applied to retrieve clips from video collections.",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192373,June 2005,
730,0.000678989392134464,730,Improving Fake News Detection of Influential Domain via Domain- and Instance-Level Transfer,"Qiong Nan, Danding Wang, Yongchun Zhu, Qiang Sheng, Yuhui Shi, Juan Cao, Jintao Li","Both real and fake news in various domains, such as politics, health, and entertainment are spread via online social media every day, necessitating fake news detection for multiple domains. Among them, fake news in specific domains like politics and health has more serious potential negative impacts on the real world (e.g., the infodemic led by COVID-19 misinformation). Previous studies focus on multi-domain fake news detection, by equally mining and modeling the correlation between domains. However, these multi-domain methods suffer from a seesaw problem: the performance of some domains is often improved at the cost of hurting the performance of other domains, which could lead to an unsatisfying performance in specific domains. To address this issue, we propose a Domain- and Instance-level Transfer Framework for Fake News Detection (DITFEND), which could improve the performance of specific target domains. To transfer coarse-grained domain-level knowledge, we train a general model with data of all domains from the meta-learning perspective. To transfer fine-grained instance-level knowledge and adapt the general model to a target domain, we train a language model on the target domain to evaluate the transferability of each data instance in source domains and re-weigh each instance's contribution. Offline experiments on two datasets demonstrate the effectiveness of DITFEND. Online experiments show that DITFEND brings additional improvements over the base models in a real-world scenario. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.08902,September 2022,730,A critical time in computational cognitive science,David M W Powers,"Understanding how people tick is an endeavour that has challenged us for millennia, both in informal settings and in increasingly formalized and scientific disciplines. Some are interested in the biology and o...",https://www.springeropen.com//computationalcognitivescience.springeropen.com/articles/10.1186/s40469-015-0005-x,7 July 2015,
731,0.000678989392134464,731,Quantum Algorithm for Anomaly Detection of Sequences,"Ming-Chao Guo, Hai-Ling Liu, Shi-Jie Pan, Wen-Min Li, Su-Juan Qin, Xin-Yi Huang, Fei Gao, Qiao-Yan Wen","Anomaly detection of sequences is a hot topic in data mining. Anomaly Detection using Piecewise Aggregate approximation in the Amplitude Domain (called ADPAAD) is one of the widely used methods in anomaly detection of sequences. The core step in the classical algorithm for performing ADPAAD is to construct an approximate representation of the subsequence, where the elements of each subsequence are divided into several subsections according to the amplitude domain and then the average of the subsections is computed. It is computationally expensive when processing large-scale sequences. In this paper, we propose a quantum algorithm for ADPAAD, which can divide the subsequence elements and compute the average in parallel. Our quantum algorithm can achieve polynomial speedups on the number of subsequences and the length of subsequences over its classical counterpart. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.08594,September 2022,731,Music Information Retrieval Based on Signal Processing,"Ichiro Fujinaga, Masataka Goto and George Tzanetakis",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/86874,1 December 2007,
732,0.000678989392134464,732,HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions,"Lingjiao Chen, Zhihua Jin, Sabri Eyuboglu, Christopher Ré, Matei Zaharia, James Zou","Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoption in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performance. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and a widespread way to consume machine learning, it is critical to systematically study and compare different APIs with each other and to characterize how APIs change over time. However, this topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API's output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML-as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs' performance change substantially over time--several APIs' accuracies dropped on specific benchmark datasets. Even when the API's aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs' performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.08443,September 2022,732,CinBalada: A multiagent rhythm factory,"Pablo Azevedo Sampaio, Geber Ramalho and Patrícia Tedesco",CinBalada is a system for automatic creation of polyphonic rhythmic performances by mixing elements from different musical styles. This system is based on agents that act as musicians playing percussion instru...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192563,September 2008,
733,0.000678989392134464,733,Topological Structure Learning for Weakly-Supervised Out-of-Distribution Detection,"Rundong He, Rongxue Li, Zhongyi Han, Yilong Yin","Out-of-distribution (OOD) detection is the key to deploying models safely in the open world. For OOD detection, collecting sufficient in-distribution (ID) labeled data is usually more time-consuming and costly than unlabeled data. When ID labeled data is limited, the previous OOD detection methods are no longer superior due to their high dependence on the amount of ID labeled data. Based on limited ID labeled data and sufficient unlabeled data, we define a new setting called Weakly-Supervised Out-of-Distribution Detection (WSOOD). To solve the new problem, we propose an effective method called Topological Structure Learning (TSL). Firstly, TSL uses a contrastive learning method to build the initial topological structure space for ID and OOD data. Secondly, TSL mines effective topological connections in the initial topological space. Finally, based on limited ID labeled data and mined topological connections, TSL reconstructs the topological structure in a new topological space to increase the separability of ID and OOD instances. Extensive studies on several representative datasets show that TSL remarkably outperforms the state-of-the-art, verifying the validity and robustness of our method in the new setting of WSOOD. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.07837,September 2022,733,A privacy service for location-based collaboration among mobile users,"Vagner Sacramento, Markus Endler and Clarisse de Souza","The potential loss of privacy due to the use of location based applications may be one of the greatest obstacles to their wider acceptance. Nevertheless, most research about privacy management to-date has not ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192571,December 2008,
734,0.000678989392134464,734,Serialized Interacting Mixed Membership Stochastic Block Model,"Gaël Poux-Médard, Julien Velcin, Sabine Loudcher","Last years have seen a regain of interest for the use of stochastic block modeling (SBM) in recommender systems. These models are seen as a flexible alternative to tensor decomposition techniques that are able to handle labeled data. Recent works proposed to tackle discrete recommendation problems via SBMs by considering larger contexts as input data and by adding second order interactions between contexts' related elements. In this work, we show that these models are all special cases of a single global framework: the Serialized Interacting Mixed membership Stochastic Block Model (SIMSBM). It allows to model an arbitrarily large context as well as an arbitrarily high order of interactions. We demonstrate that SIMSBM generalizes several recent SBM-based baselines. Besides, we demonstrate that our formulation allows for an increased predictive power on six real-world datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.07813,September 2022,734,"Changing times, changing needs: enhancing the utility of international large-scale assessments",Irwin Kirsch and Henry Braun,"Mounting concerns about the levels and distributions of human capital, as well as how they are associated with outcomes for individuals and societies, have contributed to an increase in the number of national ...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-020-00088-9,8 August 2020,
735,0.000678989392134464,735,Modeling Multiple Views via Implicitly Preserving Global Consistency and Local Complementarity,"Jiangmeng Li, Wenwen Qiang, Changwen Zheng, Bing Su, Farid Razzak, Ji-Rong Wen, Hui Xiong","While self-supervised learning techniques are often used to mining implicit knowledge from unlabeled data via modeling multiple views, it is unclear how to perform effective representation learning in a complex and inconsistent context. To this end, we propose a methodology, specifically consistency and complementarity network (CoCoNet), which avails of strict global inter-view consistency and local cross-view complementarity preserving regularization to comprehensively learn representations from multiple views. On the global stage, we reckon that the crucial knowledge is implicitly shared among views, and enhancing the encoder to capture such knowledge from data can improve the discriminability of the learned representations. Hence, preserving the global consistency of multiple views ensures the acquisition of common knowledge. CoCoNet aligns the probabilistic distribution of views by utilizing an efficient discrepancy metric measurement based on the generalized sliced Wasserstein distance. Lastly on the local stage, we propose a heuristic complementarity-factor, which joints cross-view discriminative knowledge, and it guides the encoders to learn not only view-wise discriminability but also cross-view complementary information. Theoretically, we provide the information-theoretical-based analyses of our proposed CoCoNet. Empirically, to investigate the improvement gains of our approach, we conduct adequate experimental validations, which demonstrate that CoCoNet outperforms the state-of-the-art self-supervised methods by a significant margin proves that such implicit consistency and complementarity preserving regularization can enhance the discriminability of latent representations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.07811,September 2022,735,Aspects of planning support for human-agent coalitions,Clauirton de Albuquerque Siebra and Natasha Correia Queiroz Lino,"This paper analyses aspects associated with the development of joint human-agent planning agents, showing that they can be implemented, in a unified way, via a constraint-based ontology and related functions. ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194512,December 2009,
736,0.000678989392134464,736,Model Inversion Attacks against Graph Neural Networks,"Zaixi Zhang, Qi Liu, Zhenya Huang, Hao Wang, Chee-Kong Lee, Enhong Chen","Many data mining tasks rely on graphs to model relational structures among individuals (nodes). Since relational data are often sensitive, there is an urgent need to evaluate the privacy risks in graph data. One famous privacy attack against data analysis models is the model inversion attack, which aims to infer sensitive data in the training dataset and leads to great privacy concerns. Despite its success in grid-like domains, directly applying model inversion attacks on non-grid domains such as graph leads to poor attack performance. This is mainly due to the failure to consider the unique properties of graphs. To bridge this gap, we conduct a systematic study on model inversion attacks against Graph Neural Networks (GNNs), one of the state-of-the-art graph analysis tools in this paper. Firstly, in the white-box setting where the attacker has full access to the target GNN model, we present GraphMI to infer the private training graph data. Specifically, in GraphMI, a projected gradient module is proposed to tackle the discreteness of graph edges and preserve the sparsity and smoothness of graph features; a graph auto-encoder module is used to efficiently exploit graph topology, node attributes, and target model parameters for edge inference; a random sampling module can finally sample discrete edges. Furthermore, in the hard-label black-box setting where the attacker can only query the GNN API and receive the classification results, we propose two methods based on gradient estimation and reinforcement learning (RL-GraphMI). Our experimental results show that such defenses are not sufficiently effective and call for more advanced defenses against privacy attacks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.07807,September 2022,736,Compulsory Flow Q-Learning: an RL algorithm for robot navigation based on partial-policy and macro-states,Valdinei Freire da Silva and Anna Helena Reali Costa,"Reinforcement Learning is carried out on-line, through trial-and-error interactions of the agent with the environment, which can be very time consuming when considering robots. In this paper we contribute a ne...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194507,September 2009,
737,0.000678989392134464,737,On Language Clustering: A Non-parametric Statistical Approach,"Anagh Chattopadhyay, Soumya Sankar Ghosh, Samir Karmakar","Any approach aimed at pasteurizing and quantifying a particular phenomenon must include the use of robust statistical methodologies for data analysis. With this in mind, the purpose of this study is to present statistical approaches that may be employed in nonparametric nonhomogeneous data frameworks, as well as to examine their application in the field of natural language processing and language clustering. Furthermore, this paper discusses the many uses of nonparametric approaches in linguistic data mining and processing. The data depth idea allows for the centre-outward ordering of points in any dimension, resulting in a new nonparametric multivariate statistical analysis that does not require any distributional assumptions. The concept of hierarchy is used in historical language categorisation and structuring, and it aims to organise and cluster languages into subfamilies using the same premise. In this regard, the current study presents a novel approach to language family structuring based on non-parametric approaches produced from a typological structure of words in various languages, which is then converted into a Cartesian framework using MDS. This statistical-depth-based architecture allows for the use of data-depth-based methodologies for robust outlier detection, which is extremely useful in understanding the categorization of diverse borderline languages and allows for the re-evaluation of existing classification systems. Other depth-based approaches are also applied to processes such as unsupervised and supervised clustering. This paper therefore provides an overview of procedures that can be applied to nonhomogeneous language classification systems in a nonparametric framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.06720,September 2022,737,When and why do people act on flawed science? Effects of anecdotes and prior beliefs on evidence-based decision-making,"Audrey L. Michal, Yiwen Zhong and Priti Shah","Today’s citizens are expected to use evidence, frequently presented in the media, to inform decisions about health, behavior, and public policy. However, science misinformation is ubiquitous in the media, maki...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-021-00293-2,6 April 2021,
738,0.000678989392134464,738,Collaborative SQL-injections detection system with machine learning,"M Lodeiro-Santiago, C Caballero-Gil, P Caballero-Gil","Data mining and information extraction from data is a field that has gained relevance in recent years thanks to techniques based on artificial intelligence and use of machine and deep learning. The main aim of the present work is the development of a tool based on a previous behaviour study of security audit tools (oriented to SQL pentesting) with the purpose of creating testing sets capable of performing an accurate detection of a SQL attack. The study is based on the information collected through the generated web server logs in a pentesting laboratory environment. Then, making use of the common extracted patterns from the logs, each attack vector has been classified in risk levels (dangerous attack, normal attack, non-attack, etc.). Finally, a training with the generated data was performed in order to obtain a classifier system that has a variable performance between 97 and 99 percent in positive attack detection. The training data is shared to other servers in order to create a distributed network capable of deciding if a query is an attack or is a real petition and inform to connected clients in order to block the petitions from the attacker's IP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.06553,September 2022,738,A constraints-based resource discovery model for multi-provider cloud environments,"Peter Wright, Yih Leong Sun, Terence Harmer, Anthony Keenan, Alan Stewart and Ronald Perrott","Increasingly infrastructure providers are supplying the cloud marketplace with storage and on-demand compute resources to host cloud applications. From an application user’s point of view, it is desirable to i...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-1-6,21 June 2012,
739,0.000678989392134464,739,Computational Sarcasm Analysis on Social Media: A Systematic Review,"Faria Binte Kader, Nafisa Hossain Nujat, Tasmia Binte Sogir, Mohsinul Kabir, Hasan Mahmud, Kamrul Hasan","Sarcasm can be defined as saying or writing the opposite of what one truly wants to express, usually to insult, irritate, or amuse someone. Because of the obscure nature of sarcasm in textual data, detecting it is difficult and of great interest to the sentiment analysis research community. Though the research in sarcasm detection spans more than a decade, some significant advancements have been made recently, including employing unsupervised pre-trained transformers in multimodal environments and integrating context to identify sarcasm. In this study, we aim to provide a brief overview of recent advancements and trends in computational sarcasm research for the English language. We describe relevant datasets, methodologies, trends, issues, challenges, and tasks relating to sarcasm that are beyond detection. Our study provides well-summarized tables of sarcasm datasets, sarcastic features and their extraction methods, and performance analysis of various approaches which can help researchers in related domains understand current state-of-the-art practices in sarcasm detection. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.06170,September 2022,739,Editorial,Ryohei Nakatsu and Richard Reilly,Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002653,18 September 2004,
740,0.000678989392134464,740,Leveraging Language Foundation Models for Human Mobility Forecasting,"Hao Xue, Bhanu Prakash Voutharoja, Flora D. Salim","In this paper, we propose a novel pipeline that leverages language foundation models for temporal sequential pattern mining, such as for human mobility forecasting tasks. For example, in the task of predicting Place-of-Interest (POI) customer flows, typically the number of visits is extracted from historical logs, and only the numerical data are used to predict visitor flows. In this research, we perform the forecasting task directly on the natural language input that includes all kinds of information such as numerical values and contextual semantic information. Specific prompts are introduced to transform numerical temporal sequences into sentences so that existing language models can be directly applied. We design an AuxMobLCast pipeline for predicting the number of visitors in each POI, integrating an auxiliary POI category classification task with the encoder-decoder architecture. This research provides empirical evidence of the effectiveness of the proposed AuxMobLCast pipeline to discover sequential patterns in mobility forecasting tasks. The results, evaluated on three real-world datasets, demonstrate that pre-trained language foundation models also have good performance in forecasting temporal sequences. This study could provide visionary insights and lead to new research directions for predicting human mobility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.05479,September 2022,740,General detection model in cooperative multirobot localization,"Valguima Victoria Viana Aguiar Odakura, Reinaldo Augusto da Costa Bianchi and Anna Helena Reali Costa","The cooperative multirobot localization problem consists in localizing each robot in a group within the same environment, when robots share information in order to improve localization accuracy. It can be achi...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194504,September 2009,
741,0.000678989392134464,741,Doctors vs. Nurses: Understanding the Great Divide in Vaccine Hesitancy among Healthcare Workers,"Sajid Hussain Rafi Ahamed, Shahid Shakil, Hanjia Lyu, Xinping Zhang, Jiebo Luo","Healthcare workers such as doctors and nurses are expected to be trustworthy and creditable sources of vaccine-related information. Their opinions toward the COVID-19 vaccines may influence the vaccine uptake among the general population. However, vaccine hesitancy is still an important issue even among the healthcare workers. Therefore, it is critical to understand their opinions to help reduce the level of vaccine hesitancy. There have been studies examining healthcare workers' viewpoints on COVID-19 vaccines using questionnaires. Reportedly, a considerably higher proportion of vaccine hesitancy is observed among nurses, compared to doctors. We intend to verify and study this phenomenon at a much larger scale and in fine grain using social media data, which has been effectively and efficiently leveraged by researchers to address real-world issues during the COVID-19 pandemic. More specifically, we use a keyword search to identify healthcare workers and further classify them into doctors and nurses from the profile descriptions of the corresponding Twitter users. Moreover, we apply a transformer-based language model to remove irrelevant tweets. Sentiment analysis and topic modeling are employed to analyze and compare the sentiment and thematic differences in the tweets posted by doctors and nurses. We find that doctors are overall more positive toward the COVID-19 vaccines. The focuses of doctors and nurses when they discuss vaccines in a negative way are in general different. Doctors are more concerned with the effectiveness of the vaccines over newer variants while nurses pay more attention to the potential side effects on children. Therefore, we suggest that more customized strategies should be deployed when communicating with different groups of healthcare workers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.04874,September 2022,741,A constraints-based resource discovery model for multi-provider cloud environments,"Peter Wright, Yih Leong Sun, Terence Harmer, Anthony Keenan, Alan Stewart and Ronald Perrott","Increasingly infrastructure providers are supplying the cloud marketplace with storage and on-demand compute resources to host cloud applications. From an application user’s point of view, it is desirable to i...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-1-6,21 June 2012,
742,0.000678989392134464,742,Temporal Pattern Mining for Analysis of Longitudinal Clinical Data: Identifying Risk Factors for Alzheimer's Disease,"Annette Spooner, Gelareh Mohammadi, Perminder S. Sachdev, Henry Brodaty, Arcot Sowmya","A novel framework is proposed for handling the complex task of modelling and analysis of longitudinal, multivariate, heterogeneous clinical data. This method uses temporal abstraction to convert the data into a more appropriate form for modelling, temporal pattern mining, to discover patterns in the complex, longitudinal data and machine learning models of survival analysis to select the discovered patterns. The method is applied to a real-world study of Alzheimer's disease (AD), a progressive neurodegenerative disease that has no cure. The patterns discovered were predictive of AD in survival analysis models with a Concordance index of up to 0.8. This is the first work that performs survival analysis of AD data using temporal data collections for AD. A visualisation module also provides a clear picture of the discovered patterns for ease of interpretability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.04793,September 2022,742,Editorial,Ryohei Nakatsu and Richard Reilly,Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002653,18 September 2004,
743,0.000678989392134464,743,Pitfalls and Guidelines for Using Time-Based Git Data,"Samuel W. Flint, Jigyasa Chauhan, Robert Dyer","Many software engineering research papers rely on time-based data (e.g., commit timestamps, issue report creation/update/close dates, release dates). Like most real-world data however, time-based data is often dirty. To date, there are no studies that quantify how frequently such data is used by the software engineering research community, or investigate sources of and quantify how often such data is dirty. Depending on the research task and method used, including such dirty data could affect the research results. This paper presents an extended survey of papers that utilize time-based data, published in the Mining Software Repositories (MSR) conference series. Out of the 754 technical track and data papers published in MSR 2004--2021, we saw at least 290 (38%) papers utilized time-based data. We also observed that most time-based data used in research papers comes in the form of Git commits, often from GitHub. Based on those results, we then used the Boa and Software Heritage infrastructures to help identify and quantify several sources of dirty Git timestamp data. Finally we provide guidelines/best practices for researchers utilizing time-based data from Git repositories. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.04511,September 2022,743,General detection model in cooperative multirobot localization,"Valguima Victoria Viana Aguiar Odakura, Reinaldo Augusto da Costa Bianchi and Anna Helena Reali Costa","The cooperative multirobot localization problem consists in localizing each robot in a group within the same environment, when robots share information in order to improve localization accuracy. It can be achi...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194504,September 2009,
744,0.000678989392134464,744,Mining and evaluation of patients' diagnostic therapeutic paths through state sequences analysis,"Laura Savaré, Francesca Ieva, Giovanni Corrao, Antonio Lora","The concept of care pathways is increasingly being used to enhance the quality of care and to optimize the use of resources for health care. Nevertheless, recommendations regarding the sequence of care are mostly based on consensus-based decisions as there is a lack of evidence on effective treatment sequences. In a real-world setting, classical statistical tools resulted to be insufficient to adequately consider a phenomenon with such high variability and has to be integrated with novel data mining techniques suitable of identifying patterns in complex data structures. Data-driven techniques can potentially support the empirical identification of effective care sequences by extracting them from data collected routinely. The purpose of this study is to perform sequence analysis to identify different patterns of treatment and to assess the most efficient in preventing adverse events. The clinical application that motivated the study of this method concerns the several problems frequently encountered in the quality of care provided in the mental health field. In particular, we analyzed administrative data provided by Regione Lombardia related to all the beneficiaries of the National Health Service with a diagnosis of schizophrenia from 2015 to 2018 resident in Lombardy, a region of northern Italy. This methodology considers the patient's therapeutic path as a conceptual unit, i.e., a sequence, composed of a succession of different states that can describe longitudinal patient's status. This kind of information, such as common patterns of care that allowed us to risk profile patients, can provide health policymakers an opportunity to plan optimum and individualized patient care by allocating appropriate resources, analyzing trends in the health status of a population, and finding the risk factors that can be leveraged to prevent the decline of mental health status at the population level. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.04384,September 2022,744,The effectiveness of digital storytelling in the classrooms: a comprehensive study,"Najat Smeda, Eva Dakich and Nalin Sharda","In recent years the use of new technologies in educational systems has increased worldwide as digital cameras, personal computers, scanners, and easy-to-use software have become available to educators to harne...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-014-0006-3,3 December 2014,
745,0.00514534320972017,745,MIntRec: A New Dataset for Multimodal Intent Recognition,"Hanlei Zhang, Hua Xu, Xin Wang, Qianrui Zhou, Shaojie Zhao, Jiayan Teng","Multimodal intent recognition is a significant task for understanding human language in real-world multimodal scenes. Most existing intent recognition methods have limitations in leveraging the multimodal information due to the restrictions of the benchmark datasets with only text information. This paper introduces a novel dataset for multimodal intent recognition (MIntRec) to address this issue. It formulates coarse-grained and fine-grained intent taxonomies based on the data collected from the TV series Superstore. The dataset consists of 2,224 high-quality samples with text, video, and audio modalities and has multimodal annotations among twenty intent categories. Furthermore, we provide annotated bounding boxes of speakers in each video segment and achieve an automatic process for speaker annotation. MIntRec is helpful for researchers to mine relationships between different modalities to enhance the capability of intent recognition. We extract features from each modality and model cross-modal interactions by adapting three powerful multimodal fusion methods to build baselines. Extensive experiments show that employing the non-verbal modalities achieves substantial improvements compared with the text-only modality, demonstrating the effectiveness of using multimodal information for intent recognition. The gap between the best-performing methods and humans indicates the challenge and importance of this task for the community. The full dataset and codes are available for use at https://github.com/thuiar/MIntRec. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.04355,September 2022,745,ECR 2015 Book of Abstracts - B - Scientific Sessions and Late-Breaking Clinical Trials,Unknown,"This article is part of a Supplement:Volume 6
                                        Supplement 1",https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1007/s13244-015-0387-z,24 February 2015,
746,0.000678989392134464,746,Transformer-based classification of premise in tweets related to COVID-19,"Vadim Porvatov, Natalia Semenova","Automation of social network data assessment is one of the classic challenges of natural language processing. During the COVID-19 pandemic, mining people's stances from public messages have become crucial regarding understanding attitudes towards health orders. In this paper, the authors propose the predictive model based on transformer architecture to classify the presence of premise in Twitter texts. This work is completed as part of the Social Media Mining for Health (SMM4H) Workshop 2022. We explored modern transformer-based classifiers in order to construct the pipeline efficiently capturing tweets semantics. Our experiments on a Twitter dataset showed that RoBERTa is superior to the other transformer models in the case of the premise prediction task. The model achieved competitive performance with respect to ROC AUC value 0.807, and 0.7648 for the F1 score. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.03851,September 2022,746,A generalized model for distributed comparison-based system-level diagnosis,"Luiz Carlos Pessoa Albini, Elias Procópio Duarte and Roverli Pereira Ziwich",This work introduces a new system-level diagnosis model and an algorithm based on this model: Hi-Comp (Hierarchical Comparison-based Adaptive Distributed System-Level Diagnosis algorithm). This algorithm allow...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192365,October 2004,
747,0.000678989392134464,747,A Survey of Recent Advances in Deep Learning Models for Detecting Malware in Desktop and Mobile Platforms,"Pascal Maniriho, Abdun Naser Mahmood, Mohammad Jabed Morshed Chowdhury","Malware is one of the most common and severe cyber-attack today. Malware infects millions of devices and can perform several malicious activities including mining sensitive data, encrypting data, crippling system performance, and many more. Hence, malware detection is crucial to protect our computers and mobile devices from malware attacks. Deep learning (DL) is one of the emerging and promising technologies for detecting malware. The recent high production of malware variants against desktop and mobile platforms makes DL algorithms powerful approaches for building scalable and advanced malware detection models as they can handle big datasets. This work explores current deep learning technologies for detecting malware attacks on the Windows, Linux, and Android platforms. Specifically, we present different categories of DL algorithms, network optimizers, and regularization methods. Different loss functions, activation functions, and frameworks for implementing DL models are presented. We also present feature extraction approaches and a review of recent DL-based models for detecting malware attacks on the above platforms. Furthermore, this work presents major research issues on malware detection including future directions to further advance knowledge and research in this field. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.03622,September 2022,747,An important component to investigating STEM persistence: the development and validation of the science identity (SciID) scale,"Mary Elizabeth Lockhart, Oi-Man Kwok, Myeongsun Yoon and Raymond Wong","Science, technology, engineering, and mathematics (STEM) influence almost every aspect of our daily lives. However, despite the high demand for STEM occupational talent, the STEM pipeline continues leaking, wi...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-022-00351-1,2 May 2022,
748,0.000678989392134464,748,"Nonoverlapping (delta, gamma)-approximate pattern matching","Youxi Wu, Bojing Jian, Yan Li, He Jiang, Xindong Wu","Pattern matching can be used to calculate the support of patterns, and is a key issue in sequential pattern mining (or sequence pattern mining). Nonoverlapping pattern matching means that two occurrences cannot use the same character in the sequence at the same position. Approximate pattern matching allows for some data noise, and is more general than exact pattern matching. At present, nonoverlapping approximate pattern matching is based on Hamming distance, which cannot be used to measure the local approximation between the subsequence and pattern, resulting in large deviations in matching results. To tackle this issue, we present a Nonoverlapping Delta and gamma approximate Pattern matching (NDP) scheme that employs the (delta, gamma)-distance to give an approximate pattern matching, where the local and the global distances do not exceed delta and gamma, respectively. We first transform the NDP problem into a local approximate Nettree and then construct an efficient algorithm, called the local approximate Nettree for NDP (NetNDP). We propose a new approach called the Minimal Root Distance which allows us to determine whether or not a node has root paths that satisfy the global constraint and to prune invalid nodes and parent-child relationships. NetNDP finds the rightmost absolute leaf of the max root, searches for the rightmost occurrence from the rightmost absolute leaf, and deletes this occurrence. We iterate the above steps until there are no new occurrences. Numerous experiments are used to verify the performance of the proposed algorithm. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02949,September 2022,748,Correction to: Emergency remote teaching in higher education: mapping the first global online semester,"Melissa Bond, Svenja Bedenlier, Victoria I. Marín and Marion Händel",Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education202118:50,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00298-3,22 September 2021,
749,0.000678989392134464,749,Assessing Software Privacy using the Privacy Flow-Graph,"Feiyang Tang, Bjarte M. Østvold","We increasingly rely on digital services and the conveniences they provide. Processing of personal data is integral to such services and thus privacy and data protection are a growing concern, and governments have responded with regulations such as the EU's GDPR. Following this, organisations that make software have legal obligations to document the privacy and data protection of their software. This work must involve both software developers that understand the code and the organisation's data protection officer or legal department that understands privacy and the requirements of a Data Protection and Impact Assessment (DPIA). To help developers and non-technical people such as lawyers document the privacy and data protection behaviour of software, we have developed an automatic software analysis technique. This technique is based on static program analysis to characterise the flow of privacy-related data. The results of the analysis can be presented as a graph of privacy flows and operations - that is understandable also for non-technical people. We argue that our technique facilitates collaboration between technical and non-technical people in documenting the privacy behaviour of the software. We explain how to use the results produced by our technique to answer a series of privacy-relevant questions needed for a DPIA. To illustrate our work, we show both detailed and abstract analysis results from applying our analysis technique to the secure messaging service Signal and to the client of the cloud service NextCloud and show how their privacy flow-graphs inform the writing of a DPIA. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02948,September 2022,749,Guest editorial—second part of special issue on cloud computing,"Dilma Da Silva, Xu Dongyan and Dan Reed",Unknown,https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-011-0041-z,5 November 2011,
750,0.000678989392134464,750,Code Code Evolution: Understanding How People Change Data Science Notebooks Over Time,"Deepthi Raghunandan, Aayushi Roy, Shenzhi Shi, Niklas Elmqvist, Leilani Battle","Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the ""sensemaking loop."" Although recent work observes snapshots of the sensemaking loop within computational notebooks, none measure shifts in sensemaking behaviors over time -- between exploration and explanation. This gap limits our ability to understand the full scope of the sensemaking process and thus our ability to design tools to fully support sensemaking. We contribute the first quantitative method to characterize how sensemaking evolves within data science computational notebooks. To this end, we conducted a quantitative study of 2,574 Jupyter notebooks mined from GitHub. First, we identify data science-focused notebooks that have undergone significant iterations. Second, we present regression models that automatically characterize sensemaking activity within individual notebooks by assigning them a score representing their position within the sensemaking spectrum. Finally, we use our regression models to calculate and analyze shifts in notebook scores across GitHub versions. Our results show that notebook authors participate in a diverse range of sensemaking tasks over time, such as annotation, branching analysis, and documentation. Finally, we propose design recommendations for extending notebook environments to support the sensemaking behaviors we observed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02851,September 2022,750,A large scale study of reader interactions with images on Wikipedia,"Daniele Rama, Tiziano Piccardi, Miriam Redi and Rossano Schifanella","Wikipedia is the largest source of free encyclopedic knowledge and one of the most visited sites on the Web. To increase reader understanding of the article, Wikipedia editors add images within the text of the...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-021-00312-8,3 January 2022,
751,0.000678989392134464,751,An IoT-Enriched Event Log for Process Mining in Smart Factories,"Lukas Malburg, Joscha Grüger, Ralph Bergmann","Modern technologies such as the Internet of Things (IoT) are becoming increasingly important in various domains, including Business Process Management (BPM) research. One main research area in BPM is process mining, which can be used to analyze event logs, e.g., for checking the conformance of running processes. However, there are only a few IoT-based event logs available for research purposes. Some of them are artificially generated and the problem occurs that they do not always completely reflect the actual physical properties of smart environments. In this paper, we present an IoT-enriched XES event log that is generated by a physical smart factory. For this purpose, we create the SensorStream XES extension for representing IoT-data in event logs. Finally, we present some preliminary analysis and properties of the log. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02702,September 2022,751,Common coupling as a measure of reuse effort in kernel-based software with case studies on the creation of MkLinux and Darwin,Liguo Yu,"An obstacle to software reuse is the large number of major modifications that frequently have to be made as a consequence of dependencies within the reused software components. In this paper, common coupling i...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192551,March 2008,
752,0.000678989392134464,752,The Outcome of the 2022 Landslide4Sense Competition: Advanced Landslide Detection from Multi-Source Satellite Imagery,"Omid Ghorbanzadeh, Yonghao Xu, Hengwei Zhao, Junjue Wang, Yanfei Zhong, Dong Zhao, Qi Zang, Shuang Wang, Fahong Zhang, Yilei Shi, Xiao Xiang Zhu, Lin Bai, Weile Li, Weihang Peng, Pedram Ghamisi","The scientific outcomes of the 2022 Landslide4Sense (L4S) competition organized by the Institute of Advanced Research in Artificial Intelligence (IARAI) are presented here. The objective of the competition is to automatically detect landslides based on large-scale multiple sources of satellite imagery collected globally. The 2022 L4S aims to foster interdisciplinary research on recent developments in deep learning (DL) models for the semantic segmentation task using satellite imagery. In the past few years, DL-based models have achieved performance that meets expectations on image interpretation, due to the development of convolutional neural networks (CNNs). The main objective of this article is to present the details and the best-performing algorithms featured in this competition. The winning solutions are elaborated with state-of-the-art models like the Swin Transformer, SegFormer, and U-Net. Advanced machine learning techniques and strategies such as hard example mining, self-training, and mix-up data augmentation are also considered. Moreover, we describe the L4S benchmark data set in order to facilitate further comparisons, and report the results of the accuracy assessment online. The data is accessible on \textit{Future Development Leaderboard} for future evaluation at \url{https://www.iarai.ac.at/landslide4sense/challenge/}, and researchers are invited to submit more prediction results, evaluate the accuracy of their methods, compare them with those of other users, and, ideally, improve the landslide detection results reported in this article. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02556,September 2022,752,A novel risk-based analysis for the production system under epistemic uncertainty,"Mehran Khalaj, Fereshteh Khalaj and Amineh Khalaj","Risk analysis of production system, while the actual and appropriate data is not available, will cause wrong system parameters prediction and wrong decision making. In uncertainty condition, there are no appro...",https://www.springeropen.comhttps://link.springer.com/article/10.1186/2251-712X-9-35,26 November 2013,
753,0.000678989392134464,753,Multi-class Classifier based Failure Prediction with Artificial and Anonymous Training for Data Privacy,"Dibakar Das, Vikram Seshasai, Vineet Sudhir Bhat, Pushkal Juneja, Jyotsna Bapat, Debabrata Das","This paper proposes a novel non-intrusive system failure prediction technique using available information from developers and minimal information from raw logs (rather than mining entire logs) but keeping the data entirely private with the data owners. A neural network based multi-class classifier is developed for failure prediction, using artificially generated anonymous data set, applying a combination of techniques, viz., genetic algorithm (steps), pattern repetition, etc., to train and test the network. The proposed mechanism completely decouples the data set used for training process from the actual data which is kept private. Moreover, multi-criteria decision making (MCDM) schemes are used to prioritize failures meeting business requirements. Results show high accuracy in failure prediction under different parameter configurations. On a broader context, any classification problem, beyond failure prediction, can be performed using the proposed mechanism with artificially generated data set without looking into the actual data as long as the input features can be translated to binary values (e.g. output from private binary classifiers) and can provide classification-as-a-service. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02275,September 2022,753,Editorial,"Jitendra K Tugnait, Hui Liu, Guang Gong and Tongtong Li",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/S1687147204004001,29 July 2004,
754,0.000678989392134464,754,Data-driven prediction of room temperature density for multicomponent silicate-based glasses,"Kai Gong, Elsa Olivetti","Density is one of the most commonly measured or estimated materials properties, especially for glasses and melts that are of significant interest to many fields, including metallurgy, geology, materials science and sustainable cements. Here, two types of machine learning (ML) models (i.e., random forest (RF) and artificial neural network (ANN)) have been developed to predict the room-temperature density of glasses in the compositional space of CaO-MgO-Al2O3-SiO2-TiO2-FeO-Fe2O3-Na2O-K2O-MnO (CMASTFNKM), based on ~2100 data points mined from ~140 literature studies. The results show that the RF and ANN models give accurate predictions of glass density with R2 values, RMSE, and MAPE of ~0.96-0.98, ~0.02-0.03 g/cm3 and ~0.59-0.79%, respectively, for the 15% testing set, which are more accurate compared with empirical density models based on ionic packing ratio (with R2 values, RMSE, and MAPE of ~0.28-0.91, ~0.05-0.15 g/cm3, and ~1.40-4.61%, respectively). Furthermore, glass density is shown to be a reliable reactivity indicator for a range of CaO-Al2O3-SiO2 (CAS) and volcanic glasses due to its strong correlation (R2 values above ~0.90) with the average metal-oxygen dissociation energy (a structural descriptor) of these glasses. Analysis of the predicted density-composition relationships from these models (for selected compositional subspaces) suggests that the ANN model exhibits a certain level of transferability (i.e., ability to extrapolate to compositional space not (or less) covered in the database) and captures known features including the mixed alkaline earth effects for (CaO-MgO)0.5-(Al2O3-SiO2)0.5 glasses. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.02046,September 2022,754,Global software engineering and the Brazilian perspective,"Rafael Prikladnicki, Erran Carmel and Jorge Luis Nicolas Audy",Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-010-0012-4,3 June 2010,
755,0.000678989392134464,755,Ensemble of Pre-Trained Neural Networks for Segmentation and Quality Detection of Transmission Electron Microscopy Images,"Arun Baskaran, Yulin Lin, Jianguo Wen, Maria K. Y. Chan","Automated analysis of electron microscopy datasets poses multiple challenges, such as limitation in the size of the training dataset, variation in data distribution induced by variation in sample quality and experiment conditions, etc. It is crucial for the trained model to continue to provide acceptable segmentation/classification performance on new data, and quantify the uncertainty associated with its predictions. Among the broad applications of machine learning, various approaches have been adopted to quantify uncertainty, such as Bayesian modeling, Monte Carlo dropout, ensembles, etc. With the aim of addressing the challenges specific to the data domain of electron microscopy, two different types of ensembles of pre-trained neural networks were implemented in this work. The ensembles performed semantic segmentation of ice crystal within a two-phase mixture, thereby tracking its phase transformation to water. The first ensemble (EA) is composed of U-net style networks having different underlying architectures, whereas the second series of ensembles (ER-i) are composed of randomly initialized U-net style networks, wherein each base learner has the same underlying architecture 'i'. The encoders of the base learners were pre-trained on the Imagenet dataset. The performance of EA and ER were evaluated on three different metrics: accuracy, calibration, and uncertainty. It is seen that EA exhibits a greater classification accuracy and is better calibrated, as compared to ER. While the uncertainty quantification of these two types of ensembles are comparable, the uncertainty scores exhibited by ER were found to be dependent on the specific architecture of its base member ('i') and not consistently better than EA. Thus, the challenges posed for the analysis of electron microscopy datasets appear to be better addressed by an ensemble design like EA, as compared to an ensemble design like ER. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01908,September 2022,755,A Survey of Architecture and Function of the Primary Visual Cortex (V1),"Jeffrey Ng, Anil A. Bharath and Li Zhaoping","The largest visual area, known as the primary visual cortex or V1, has greatly contributed to the current understanding of mammalian and human visual pathways and their role in visual perception. The initial d...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/97961,1 December 2006,
756,0.000678989392134464,756,DMiner: Dashboard Design Mining and Recommendation,"Yanna Lin, Haotian Li, Aoyu Wu, Yong Wang, Huamin Qu","Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01599,September 2022,756,South African and international legislature with relevance to the application of electronic documentation in medicolegal autopsies for practice and research purposes,Salona Prahladh and Jacqueline Van Wyk,"Forensic and legal medicine requires all documentation to be recorded in a manner that is admissible in court. Issues surrounding privacy, confidentiality, and security mar the implementation of electronic doc...",https://www.springeropen.com//ejfs.springeropen.com/articles/10.1186/s41935-021-00261-3,3 January 2022,
757,0.000678989392134464,757,Conditional Independence Testing via Latent Representation Learning,"Bao Duong, Thin Nguyen","Detecting conditional independencies plays a key role in several statistical and machine learning tasks, especially in causal discovery algorithms. In this study, we introduce LCIT (Latent representation based Conditional Independence Test)-a novel non-parametric method for conditional independence testing based on representation learning. Our main contribution involves proposing a generative framework in which to test for the independence between X and Y given Z, we first learn to infer the latent representations of target variables X and Y that contain no information about the conditioning variable Z. The latent variables are then investigated for any significant remaining dependencies, which can be performed using the conventional partial correlation test. The empirical evaluations show that LCIT outperforms several state-of-the-art baselines consistently under different evaluation metrics, and is able to adapt really well to both non-linear and high-dimensional settings on a diverse collection of synthetic and real data sets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01547,September 2022,757,Iterative Approximation of Empirical Grey-Level Distributions for Precise Segmentation of Multimodal Images,"Ayman El-Baz, Aly A. Farag and Georgy Gimel'farb","A new algorithm for segmenting a multimodal grey-scale image is proposed. The image is described as a sample of a joint Gibbs random field of region labels and grey levels. To initialize the model, a mixed mul...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1969,15 August 2005,
758,0.000678989392134464,758,Cross-Network Social User Embedding with Hybrid Differential Privacy Guarantees,"Jiaqian Ren, Lei Jiang, Hao Peng, Lingjuan Lyu, Zhiwei Liu, Chaochao Chen, Jia Wu, Xu Bai, Philip S. Yu","Integrating multiple online social networks (OSNs) has important implications for many downstream social mining tasks, such as user preference modelling, recommendation, and link prediction. However, it is unfortunately accompanied by growing privacy concerns about leaking sensitive user information. How to fully utilize the data from different online social networks while preserving user privacy remains largely unsolved. To this end, we propose a Cross-network Social User Embedding framework, namely DP-CroSUE, to learn the comprehensive representations of users in a privacy-preserving way. We jointly consider information from partially aligned social networks with differential privacy guarantees. In particular, for each heterogeneous social network, we first introduce a hybrid differential privacy notion to capture the variation of privacy expectations for heterogeneous data types. Next, to find user linkages across social networks, we make unsupervised user embedding-based alignment in which the user embeddings are achieved by the heterogeneous network embedding technology. To further enhance user embeddings, a novel cross-network GCN embedding model is designed to transfer knowledge across networks through those aligned users. Extensive experiments on three real-world datasets demonstrate that our approach makes a significant improvement on user interest prediction tasks as well as defending user attribute inference attacks from embedding. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01539,September 2022,758,Text Data Augmentation for Deep Learning,"Connor Shorten, Taghi M. Khoshgoftaar and Borko Furht","Natural Language Processing (NLP) is one of the most captivating applications of Deep Learning. In this survey, we consider how the Data Augmentation training strategy can aid in its development. We begin with...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00492-0,19 July 2021,
759,0.000678989392134464,759,LDP-FPMiner: FP-Tree Based Frequent Itemset Mining with Local Differential Privacy,"Zhili Chen, Jiali Wang","Data aggregation in the setting of local differential privacy (LDP) guarantees strong privacy by providing plausible deniability of sensitive data. Existing works on this issue mostly focused on discovering heavy hitters, leaving the task of frequent itemset mining (FIM) as an open problem. To the best of our knowledge, the-state-of-the-art LDP solution to FIM is the SVSM protocol proposed recently. The SVSM protocol is mainly based on the padding and sampling based frequency oracle (PSFO) protocol, and regarded an itemset as an independent item without considering the frequency consistency among itemsets. In this paper, we propose a novel LDP approach to FIM called LDP-FPMiner based on frequent pattern tree (FP-tree). Our proposal exploits frequency consistency among itemsets by constructing and optimizing a noisy FP-tree with LDP. Specifically, it works as follows. First, the most frequent items are identified, and the item domain is cut down accordingly. Second, the maximum level of the FP-tree is estimated. Third, a noisy FP-tree is constructed and optimized by using itemset frequency consistency, and then mined to obtain the k most frequent itemsets. Experimental results show that the LDP-FPMiner significantly improves over the state-of-the-art approach, SVSM, especially in the case of a high privacy level. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01333,September 2022,759,Cost-Effective Video Filtering Solution for Real-Time Vision Systems,"Viktor Fischer, Rastislav Lukac and Karl Martin","This paper presents an efficient video filtering scheme and its implementation in a field-programmable logic device (FPLD). Since the proposed nonlinear, spatiotemporal filtering scheme is based on order stati...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2026,15 August 2005,
760,0.000678989392134464,760,Deep Stable Representation Learning on Electronic Health Records,"Yingtao Luo, Zhaocheng Liu, Qiang Liu","Deep learning models have achieved promising disease prediction performance of the Electronic Health Records (EHR) of patients. However, most models developed under the I.I.D. hypothesis fail to consider the agnostic distribution shifts, diminishing the generalization ability of deep learning models to Out-Of-Distribution (OOD) data. In this setting, spurious statistical correlations that may change in different environments will be exploited, which can cause sub-optimal performances of deep learning models. The unstable correlation between procedures and diagnoses existed in the training distribution can cause spurious correlation between historical EHR and future diagnosis. To address this problem, we propose to use a causal representation learning method called Causal Healthcare Embedding (CHE). CHE aims at eliminating the spurious statistical relationship by removing the dependencies between diagnoses and procedures. We introduce the Hilbert-Schmidt Independence Criterion (HSIC) to measure the degree of independence between the embedded diagnosis and procedure features. Based on causal view analyses, we perform the sample weighting technique to get rid of such spurious relationship for the stable learning of EHR across different environments. Moreover, our proposed CHE method can be used as a flexible plug-and-play module that can enhance existing deep learning models on EHR. Extensive experiments on two public datasets and five state-of-the-art baselines unequivocally show that CHE can improve the prediction accuracy of deep learning models on out-of-distribution data by a large margin. In addition, the interpretability study shows that CHE could successfully leverage causal structures to reflect a more reasonable contribution of historical records for predictions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01321,September 2022,760,Are primary education teachers trained for the use of the technology with disabled students?,"José María Fernández-Batanero, Marta Montenegro-Rueda and José Fernández-Cerero","Incorporating information and communication technology (ICT) in inclusive classrooms requires competent teachers, both technological and pedagogical. To contrast these theoretical assumptions, this study aims ...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00195-x,18 May 2022,
761,0.000678989392134464,761,DPXPlain: Privately Explaining Aggregate Query Answers,"Yuchao Tao, Amir Gilad, Ashwin Machanavajjhala, Sudeepa Roy","Differential privacy (DP) is the state-of-the-art and rigorous notion of privacy for answering aggregate database queries while preserving the privacy of sensitive information in the data. In today's era of data analysis, however, it poses new challenges for users to understand the trends and anomalies observed in the query results: Is the unexpected answer due to the data itself, or is it due to the extra noise that must be added to preserve DP? In the second case, even the observation made by the users on query results may be wrong. In the first case, can we still mine interesting explanations from the sensitive data while protecting its privacy? To address these challenges, we present a three-phase framework DPXPlain, which is the first system to the best of our knowledge for explaining group-by aggregate query answers with DP. In its three phases, DPXPlain (a) answers a group-by aggregate query with DP, (b) allows users to compare aggregate values of two groups and with high probability assesses whether this comparison holds or is flipped by the DP noise, and (c) eventually provides an explanation table containing the approximately `top-k' explanation predicates along with their relative influences and ranks in the form of confidence intervals, while guaranteeing DP in all steps. We perform an extensive experimental analysis of DPXPlain with multiple use-cases on real and synthetic data showing that DPXPlain efficiently provides insightful explanations with good accuracy and utility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01286,September 2022,761,Caenorhabditis elegansEgg-Laying Detection and Behavior Study Using Image Analysis,"Wei Geng, Pamela Cosman, Megan Palm and William R. Schafer",Egg laying is an important phase of the life cycle of the nematodeCaenorhabditis elegans (C. elegans). Previous studies examined egg-laying events manually. This paper presents a method for automatic detection o...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2229,25 August 2005,
762,0.000678989392134464,762,A Framework for Extracting and Encoding Features from Object-Centric Event Data,"Jan Niklas Adams, Gyunam Park, Sergej Levich, Daniel Schuster, Wil M. P. van der Aalst","Traditional process mining techniques take event data as input where each event is associated with exactly one object. An object represents the instantiation of a process. Object-centric event data contain events associated with multiple objects expressing the interaction of multiple processes. As traditional process mining techniques assume events associated with exactly one object, these techniques cannot be applied to object-centric event data. To use traditional process mining techniques, the object-centric event data are flattened by removing all object references but one. The flattening process is lossy, leading to inaccurate features extracted from flattened data. Furthermore, the graph-like structure of object-centric event data is lost when flattening. In this paper, we introduce a general framework for extracting and encoding features from object-centric event data. We calculate features natively on the object-centric event data, leading to accurate measures. Furthermore, we provide three encodings for these features: tabular, sequential, and graph-based. While tabular and sequential encodings have been heavily used in process mining, the graph-based encoding is a new technique preserving the structure of the object-centric event data. We provide six use cases: a visualization and a prediction use case for each of the three encodings. We use explainable AI in the prediction use cases to show the utility of both the object-centric features and the structure of the sequential and graph-based encoding for a predictive model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.01219,September 2022,762,Lip print evaluation of Indian and Malaysian-Chinese subjects by manual and digital methods: a correlational study with gender and ethnicity,"Aditi Chadha, Ravindranath Vineetha, Mathangi Kumar, Divyansh Bansal, Keerthilatha M. Pai and Prakash K Aithal","Cheiloscopy is a reliable method of personal identification which may augment the established methods like dactylography, DNA (deoxyribonucleic acid) profiling, and dental records.",https://www.springeropen.com//ejfs.springeropen.com/articles/10.1186/s41935-022-00273-7,9 March 2022,
763,0.000678989392134464,763,Spatiotemporal statistics of the turbulent piston-removed phase and Zernike coefficients for two distinct beams,"Cédric Plantet, Giulia Carlà, Guido Agapito, Lorenzo Busoni","In the context of adaptive optics for astronomy, one can rely on the statistics of the turbulent phase to assess a part of the system's performance. Temporal statistics with one source and spatial statistics with two sources are well-known and are widely used for classical adaptive optics systems. A more general framework, including both spatial and temporal statistics, can be useful for the analysis of the existing systems and to support the design of the future ones. In this paper, we propose an expression of the temporal cross power spectral densities of the turbulent phases in two distinct beams, that is from two different sources to two different apertures. We either consider the phase as it is, without piston, or as its decomposition on Zernike modes. The general formulas allow to cover a wide variety of configurations, from single-aperture to interferometric telescopes equipped with adaptive optics, with the possibility to consider apertures of different sizes and/or sources at a finite distance. The presented approach should lead to similar results with respect to existing methods in the Fourier domain, but it is focused on temporal frequencies rather than spatial ones, which might be convenient for some aspects such as control optimization. To illustrate this framework with a simple application, we demonstrate that the wavefront residual due to the anisoplanatism error in a single-conjugated adaptive optics system is overestimated when it is computed from covariances without taking into account the temporal filtering of the adaptive optics loop. We also show this overestimation in the case of a small-baseline interferometer, for which the two beams are significantly correlated. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.00931,September 2022,763,Discussing human values in digital immortality: towards a value-oriented perspective,"Vinícius Ferreira Galvão, Cristiano Maciel, Roberto Pereira, Isabela Gasparini, José Viterbo and Ana Cristina Bicharra Garcia","Intense social media interaction, wearable devices, mobile applications, and pervasive use of sensors have created a personal information ecosystem for gathering traces of individual behavior. These traces are...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00121-x,26 November 2021,
764,0.000678989392134464,764,Structure-Preserving Graph Representation Learning,"Ruiyi Fang, Liangjian Wen, Zhao Kang, Jianzhuang Liu","Though graph representation learning (GRL) has made significant progress, it is still a challenge to extract and embed the rich topological structure and feature information in an adequate way. Most existing methods focus on local structure and fail to fully incorporate the global topological structure. To this end, we propose a novel Structure-Preserving Graph Representation Learning (SPGRL) method, to fully capture the structure information of graphs. Specifically, to reduce the uncertainty and misinformation of the original graph, we construct a feature graph as a complementary view via k-Nearest Neighbor method. The feature graph can be used to contrast at node-level to capture the local relation. Besides, we retain the global topological structure information by maximizing the mutual information (MI) of the whole graph and feature embeddings, which is theoretically reduced to exchanging the feature embeddings of the feature and the original graphs to reconstruct themselves. Extensive experiments show that our method has quite superior performance on semi-supervised node classification task and excellent robustness under noise perturbation on graph structure or node features. △ Less",https://arxiv.orghttps://arxiv.org/abs/2209.00793,September 2022,764,Industrial and OSS developers’ profiles: a family of experiments to evaluate a pioneering neuro-linguistic method for preferred representational systems automatic detection,"Methanias Colaço Júnior, Breno Santana Santos, Manoel Mendonça, Daniela Corumba and Mario André de F. Farias",Software projects use mailing lists as the primary tool for collaboration and coordination. Mailing lists can be an important source for extracting behavioral patterns in the software development. A new approa...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00107-9,1 March 2021,
765,0.000678989392134464,765,Deep Anomaly Detection and Search via Reinforcement Learning,"Chao Chen, Dawei Wang, Feng Mao, Zongzhang Zhang, Yang Yu","Semi-supervised Anomaly Detection (AD) is a kind of data mining task which aims at learning features from partially-labeled datasets to help detect outliers. In this paper, we classify existing semi-supervised AD methods into two categories: unsupervised-based and supervised-based, and point out that most of them suffer from insufficient exploitation of labeled data and under-exploration of unlabeled data. To tackle these problems, we propose Deep Anomaly Detection and Search (DADS), which applies Reinforcement Learning (RL) to balance exploitation and exploration. During the training process, the agent searches for possible anomalies with hierarchically-structured datasets and uses the searched anomalies to enhance performance, which in essence draws lessons from the idea of ensemble learning. Experimentally, we compare DADS with several state-of-the-art methods in the settings of leveraging labeled known anomalies to detect both other known anomalies and unknown anomalies. Results show that DADS can efficiently and precisely search anomalies from unlabeled data and learn from them, thus achieving good performance. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14834,August 2022,765,Basketball shooting technology based on acceleration sensor fusion motion capture technology,Binbin Zhao and Shihong Liu,"Computer vision recognition refers to the use of cameras and computers to replace the human eyes with computer vision, such as target recognition, tracking, measurement, and in-depth graphics processing, to pr...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-021-00731-9,17 May 2021,
766,0.000678989392134464,766,Ant Colony Optimization for Mining Gradual Patterns,"Dickson Odhiambo Owuor, Thomas Runkler, Anne Laurent, Joseph Orero, Edmond Menya","Gradual pattern extraction is a field in (KDD) Knowledge Discovery in Databases that maps correlations between attributes of a data set as gradual dependencies. A gradual dependency may take a form of ""the more Attribute K , the less Attribute L"". In this paper, we propose an ant colony optimization technique that uses a probabilistic approach to learn and extract frequent gradual patterns. Through computational experiments on real-world data sets, we compared the performance of our ant-based algorithm to an existing gradual item set extraction algorithm and we found out that our algorithm outperforms the later especially when dealing with large data sets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14795,August 2022,766,Computational thinking development through creative programming in higher education,"Margarida Romero, Alexandre Lepage and Benjamin Lille",Creative and problem-solving competencies are part of the so-called twenty-first century skills. The creative use of digital technologies to solve problems is also related to computational thinking as a set of...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-017-0080-z,12 December 2017,
767,0.000678989392134464,767,QuakeFlow: A Scalable Machine-learning-based Earthquake Monitoring Workflow with Cloud Computing,"Weiqiang Zhu, Alvin Brian Hou, Robert Yang, Avoy Datta, S. Mostafa Mousavi, William L. Ellsworth, Gregory C. Beroza","Earthquake monitoring workflows are designed to detect earthquake signals and to determine source characteristics from continuous waveform data. Recent developments in deep learning seismology have been used to improve tasks within earthquake monitoring workflows that allow the fast and accurate detection of up to orders of magnitude more small events than are present in conventional catalogs. To facilitate the application of machine-learning algorithms to large-volume seismic records, we developed a cloud-based earthquake monitoring workflow, QuakeFlow, that applies multiple processing steps to generate earthquake catalogs from raw seismic data. QuakeFlow uses a deep learning model, PhaseNet, for picking P/S phases and a machine learning model, GaMMA, for phase association with approximate earthquake location and magnitude. Each component in QuakeFlow is containerized, allowing straightforward updates to the pipeline with new deep learning/machine learning models, as well as the ability to add new components, such as earthquake relocation algorithms. We built QuakeFlow in Kubernetes to make it auto-scale for large datasets and to make it easy to deploy on cloud platforms, which enables large-scale parallel processing. We used QuakeFlow to process three years of continuous archived data from Puerto Rico, and found more than a factor of ten more events that occurred on much the same structures as previously known seismicity. We applied Quakeflow to monitoring frequent earthquakes in Hawaii and found over an order of magnitude more events than are in the standard catalog, including many events that illuminate the deep structure of the magmatic system. We also added Kafka and Spark streaming to deliver real-time earthquake monitoring results. QuakeFlow is an effective and efficient approach both for improving realtime earthquake monitoring and for mining archived seismic data sets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14564,August 2022,767,Interactive actuation of multiple opto-thermocapillary flow-addressed bubble microrobots,"Wenqi Hu, Qihui Fan and Aaron T Ohta",Opto-thermocapillary flow-addressed bubble (OFB) microrobots are a potential tool for the efficient transportation of micro-objects. This microrobot system uses light patterns to generate thermal gradients wit...,https://www.springeropen.com//jrobio.springeropen.com/articles/10.1186/s40638-014-0014-3,23 October 2014,
768,0.000678989392134464,768,Dual Representation Learning for One-Step Clustering of Multi-View Data,"Wei Zhang, Zhaohong Deng, Kup-Sze Choi, Jun Wang, Shitong Wang","Multi-view data are commonly encountered in data mining applications. Effective extraction of information from multi-view data requires specific design of clustering methods to cater for data with multiple views, which is non-trivial and challenging. In this paper, we propose a novel one-step multi-view clustering method by exploiting the dual representation of both the common and specific information of different views. The motivation originates from the rationale that multi-view data contain not only the consistent knowledge between views but also the unique knowledge of each view. Meanwhile, to make the representation learning more specific to the clustering task, a one-step learning framework is proposed to integrate representation learning and clustering partition as a whole. With this framework, the representation learning and clustering partition mutually benefit each other, which effectively improve the clustering performance. Results from extensive experiments conducted on benchmark multi-view datasets clearly demonstrate the superiority of the proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14450,August 2022,768,Editorial,"Gian Luca Foresti, Giovanni Ramponi, Carlo Regazzoni, Giovanni L. Sicuranza and Gianni Vernazza",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002665,29 September 2004,
769,0.000678989392134464,769,Compound Figure Separation of Biomedical Images: Mining Large Datasets for Self-supervised Learning,"Tianyuan Yao, Chang Qu, Jun Long, Quan Liu, Ruining Deng, Yuanhan Tian, Jiachen Xu, Aadarsh Jha, Zuhayr Asad, Shunxing Bao, Mengyang Zhao, Agnes B. Fogo, Bennett A. Landman, Haichun Yang, Catie Chang, Yuankai Huo","With the rapid development of self-supervised learning (e.g., contrastive learning), the importance of having large-scale images (even without annotations) for training a more generalizable AI model has been widely recognized in medical image analysis. However, collecting large-scale task-specific unannotated data at scale can be challenging for individual labs. Existing online resources, such as digital books, publications, and search engines, provide a new resource for obtaining large-scale images. However, published images in healthcare (e.g., radiology and pathology) consist of a considerable amount of compound figures with subplots. In order to extract and separate compound figures into usable individual images for downstream learning, we propose a simple compound figure separation (SimCFS) framework without using the traditionally required detection bounding box annotations, with a new loss function and a hard case simulation. Our technical contribution is four-fold: (1) we introduce a simulation-based training framework that minimizes the need for resource extensive bounding box annotations; (2) we propose a new side loss that is optimized for compound figure separation; (3) we propose an intra-class image augmentation method to simulate hard cases; and (4) to the best of our knowledge, this is the first study that evaluates the efficacy of leveraging self-supervised learning with compound image separation. From the results, the proposed SimCFS achieved state-of-the-art performance on the ImageCLEF 2016 Compound Figure Separation Database. The pretrained self-supervised learning model using large-scale mined figures improved the accuracy of downstream image classification tasks with a contrastive learning algorithm. The source code of SimCFS is made publicly available at https://github.com/hrlblab/ImageSeperation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14357,August 2022,769,Improvement of Grey System Model using Particle Swarm Optimization,"Elvis Twumasi, Emmanuel Asuming Frimpong, Daniel Kwegyir and Denis Folitse","An improvement of the traditional grey system model, GM(1,1), to enhance forecast accuracy, has been realized using the particle swarm optimization (PSO) algorithm. Unlike the GM(1,1) which uses a fixed adjace...",https://www.springeropen.com//jesit.springeropen.com/articles/10.1186/s43067-021-00036-9,1 May 2021,
770,0.000678989392134464,770,A Generic Algorithm for Top-K On-Shelf Utility Mining,"Jiahui Chen, Xu Guo, Wensheng Gan, Shichen Wan, Philip S. Yu","On-shelf utility mining (OSUM) is an emerging research direction in data mining. It aims to discover itemsets that have high relative utility in their selling time period. Compared with traditional utility mining, OSUM can find more practical and meaningful patterns in real-life applications. However, there is a major drawback to traditional OSUM. For normal users, it is hard to define a minimum threshold minutil for mining the right amount of on-shelf high utility itemsets. On one hand, if the threshold is set too high, the number of patterns would not be enough. On the other hand, if the threshold is set too low, too many patterns will be discovered and cause an unnecessary waste of time and memory consumption. To address this issue, the user usually directly specifies a parameter k, where only the top-k high relative utility itemsets would be considered. Therefore, in this paper, we propose a generic algorithm named TOIT for mining Top-k On-shelf hIgh-utility paTterns to solve this problem. TOIT applies a novel strategy to raise the minutil based on the on-shelf datasets. Besides, two novel upper-bound strategies named subtree utility and local utility are applied to prune the search space. By adopting the strategies mentioned above, the TOIT algorithm can narrow the search space as early as possible, improve the mining efficiency, and reduce the memory consumption, so it can obtain better performance than other algorithms. A series of experiments have been conducted on real datasets with different styles to compare the effects with the state-of-the-art KOSHU algorithm. The experimental results showed that TOIT outperforms KOSHU in both running time and memory consumption. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14230,August 2022,770,Abstracts from the Energy Informatics.Academy Asia 2021 conference and PhD workshop,Unknown,"This article is part of a Supplement:Volume 4
                                        Supplement 1",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-021-00145-9,24 September 2021,
771,0.000678989392134464,771,Learned k-NN Distance Estimation,"Daichi Amagata, Yusuke Arai, Sumio Fujita, Takahiro Hara","Big data mining is well known to be an important task for data science, because it can provide useful observations and new knowledge hidden in given large datasets. Proximity-based data analysis is particularly utilized in many real-life applications. In such analysis, the distances to k nearest neighbors are usually employed, thus its main bottleneck is derived from data retrieval. Much efforts have been made to improve the efficiency of these analyses. However, they still incur large costs, because they essentially need many data accesses. To avoid this issue, we propose a machine-learning technique that quickly and accurately estimates the k-NN distances (i.e., distances to the k nearest neighbors) of a given query. We train a fully connected neural network model and utilize pivots to achieve accurate estimation. Our model is designed to have useful advantages: it infers distances to the k-NNs at a time, its inference time is O(1) (no data accesses are incurred), but it keeps high accuracy. Our experimental results and case studies on real datasets demonstrate the efficiency and effectiveness of our solution. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.14210,August 2022,771,Welcome message from the organizers,"René Schumann, Roman Rudel and Khoa Nguyen","This article is part of a Supplement:Volume 3
                                        Supplement 1",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-020-00114-8,28 October 2020,
772,0.000678989392134464,772,FedEgo: Privacy-preserving Personalized Federated Graph Learning with Ego-graphs,"Taolin Zhang, Chuan Chen, Yaomin Chang, Lin Shu, Zibin Zheng","As special information carriers containing both structure and feature information, graphs are widely used in graph mining, e.g., Graph Neural Networks (GNNs). However, in some practical scenarios, graph data are stored separately in multiple distributed parties, which may not be directly shared due to conflicts of interest. Hence, federated graph neural networks are proposed to address such data silo problems while preserving the privacy of each party (or client). Nevertheless, different graph data distributions among various parties, which is known as the statistical heterogeneity, may degrade the performance of naive federated learning algorithms like FedAvg. In this paper, we propose FedEgo, a federated graph learning framework based on ego-graphs to tackle the challenges above, where each client will train their local models while also contributing to the training of a global model. FedEgo applies GraphSAGE over ego-graphs to make full use of the structure information and utilizes Mixup for privacy concerns. To deal with the statistical heterogeneity, we integrate personalization into learning and propose an adaptive mixing coefficient strategy that enables clients to achieve their optimal personalization. Extensive experimental results and in-depth analysis demonstrate the effectiveness of FedEgo. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13685,August 2022,772,Correction to: Case study of creativity in asynchronous online discussions,Timothy Corfman and Dennis Beck,Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education201916:22,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00293-8,16 November 2021,
773,0.000678989392134464,773,Detecting Surprising Situations in Event Data,"Christian Kohlschmidt, Mahnaz Sadat Qafari, Wil M. P. van der Aalst","Process mining is a set of techniques that are used by organizations to understand and improve their operational processes. The first essential step in designing any process reengineering procedure is to find process improvement opportunities. In existing work, it is usually assumed that the set of problematic process instances in which an undesirable outcome occurs is known prior or is easily detectable. So the process enhancement procedure involves finding the root causes and the treatments for the problem in those process instances. For example, the set of problematic instances is considered as those with outlier values or with values smaller/bigger than a given threshold in one of the process features. However, on various occasions, using this approach, many process enhancement opportunities, not captured by these problematic process instances, are missed. To overcome this issue, we formulate finding the process enhancement areas as a context-sensitive anomaly/outlier detection problem. We define a process enhancement area as a set of situations (process instances or prefixes of process instances) where the process performance is surprising. We aim to characterize those situations where process performance/outcome is significantly different from what was expected considering its performance/outcome in similar situations. To evaluate the validity and relevance of the proposed approach, we have implemented and evaluated it on several real-life event logs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13515,August 2022,773,An Overview of DNA Microarray Grid Alignment and Foreground Separation Approaches,Peter Bajcsy,This paper overviews DNA microarray grid alignment and foreground separation approaches. Microarray grid alignment and foreground separation are the basic processing steps of DNA microarray images that affect ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/80163,1 December 2006,
774,0.000678989392134464,774,Mining Android API Usage to Generate Unit Test Cases for Pinpointing Compatibility Issues,"Xiaoyu Sun, Xiao Chen, Yanjie Zhao, Pei Liu, John Grundy, Li Li","Despite being one of the largest and most popular projects, the official Android framework has only provided test cases for less than 30% of its APIs. Such a poor test case coverage rate has led to many compatibility issues that can cause apps to crash at runtime on specific Android devices, resulting in poor user experiences for both apps and the Android ecosystem. To mitigate this impact, various approaches have been proposed to automatically detect such compatibility issues. Unfortunately, these approaches have only focused on detecting signature-induced compatibility issues (i.e., a certain API does not exist in certain Android versions), leaving other equally important types of compatibility issues unresolved. In this work, we propose a novel prototype tool, JUnitTestGen, to fill this gap by mining existing Android API usage to generate unit test cases. After locating Android API usage in given real-world Android apps, JUnitTestGen performs inter-procedural backward data-flow analysis to generate a minimal executable code snippet (i.e., test case). Experimental results on thousands of real-world Android apps show that JUnitTestGen is effective in generating valid unit test cases for Android APIs. We show that these generated test cases are indeed helpful for pinpointing compatibility issues, including ones involving semantic code changes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13417,August 2022,774,Editorial,Maria Cristina Ferreira de Oliveira,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0090-6,19 September 2012,
775,0.000678989392134464,775,Cyber Catalysis: N$_2$ Dissociation over Ruthenium Catalyst with Strong Metal-Support Interaction,"Gerardo Valadez Huerta, Kaoru Hisama, Katsutoshi Sato, Katsutoshi Nagaoka, Michihisa Koyama","Catalysis informatics is constantly developing, and significant advances in data mining, molecular simulation, and automation for computational design and high-throughput experimentation have been achieved. However, efforts to reveal the mechanisms of complex supported nanoparticle catalysts in cyberspace have proven to be unsuccessful thus far. This study fills this gap by exploring N$_2$ dissociation on a supported Ru nanoparticle as an example using a universal neural network potential. We calculated 200 catalyst configurations considering the reduction of the support and strong metal-support interaction (SMSI), eventually performing 15,600 calculations for various N$_2$ adsorption states. After successfully validating our results with experimental IR spectral data, we clarified key N$_2$ dissociation pathways behind the high activity of the SMSI surface and disclosed the maximum activity of catalysts reduced at 650 °C. Our method is well applicable to other complex systems, and we believe it represents a key first step toward the digital transformation of investigations on heterogeneous catalysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13385,August 2022,775,Are Japanese digital natives ready for learning english online? a preliminary case study at Osaka University,"Parisa Mehran, Mehrasa Alizadeh, Ichiro Koguchi and Haruo Takemura",Assessing learner readiness for online learning is the starting point for online course design. This study thus aimed to evaluate Japanese learners’ perceived e-readiness for learning English online prior to d...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-017-0047-0,13 March 2017,
776,0.000678989392134464,776,SemanticAxis: Exploring Multi-attribute Data by Semantics Construction and Ranking Analysis,"Zeyu Li, Changhong Zhang, Yi Zhang, Jiawan Zhang","Mining the distribution of features and sorting items by combined attributes are two common tasks in exploring and understanding multi-attribute (or multivariate) data. Up to now, few have pointed out the possibility of merging these two tasks into a united exploration context and the potential benefits of doing so. In this paper, we present SemanticAxis, a technique that achieves this goal by enabling analysts to build a semantic vector in two-dimensional space interactively. Essentially, the semantic vector is a linear combination of the original attributes. It can be used to represent and explain abstract concepts implied in local (outliers, clusters) or global (general pattern) features of reduced space, as well as serving as a ranking metric for its defined concepts. In order to validate the significance of combining the above two tasks in multi-attribute data analysis, we design and implement a visual analysis system, in which several interactive components cooperate with SemanticAxis seamlessly and expand its capacity to handle complex scenarios. We prove the effectiveness of our system and the SemanticAxis technique via two practical cases. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13346,August 2022,776,Erratum to: On the reliability and availability of replicated and rejuvenating systems under stealth attacks and intrusions,Luís Teixeira d’Aguiar Norton Brandão and Alysson Neves Bessani,Theoriginal articlewas published inJournal of the Brazilian Computer Society201218:s13173-012-0062-x,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0074-6,12 May 2012,
777,0.000678989392134464,777,"Multi-dimensional Racism Classification during COVID-19: Stigmatization, Offensiveness, Blame, and Exclusion","Xin Pei, Deval Mehta","Transcending the binary categorization of racist texts, our study takes cues from social science theories to develop a multi-dimensional model for racism detection, namely stigmatization, offensiveness, blame, and exclusion. With the aid of BERT and topic modeling, this categorical detection enables insights into the underlying subtlety of racist discussion on digital platforms during COVID-19. Our study contributes to enriching the scholarly discussion on deviant racist behaviours on social media. First, a stage-wise analysis is applied to capture the dynamics of the topic changes across the early stages of COVID-19 which transformed from a domestic epidemic to an international public health emergency and later to a global pandemic. Furthermore, mapping this trend enables a more accurate prediction of public opinion evolvement concerning racism in the offline world, and meanwhile, the enactment of specified intervention strategies to combat the upsurge of racism during the global public health crisis like COVID-19. In addition, this interdisciplinary research also points out a direction for future studies on social network analysis and mining. Integration of social science perspectives into the development of computational methods provides insights into more accurate data detection and analytics. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.13318,August 2022,777,"Correction to: Prerequisites for artificial intelligence in further education: identification of drivers, barriers, and business models of educational technology companies",André Renz and Romy Hilbig,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00245-2,24 February 2021,
778,0.000678989392134464,778,Building the Intent Landscape of Real-World Conversational Corpora with Extractive Question-Answering Transformers,"Jean-Philippe Corbeil, Mia Taige Li, Hadi Abdi Ghavidel","For companies with customer service, mapping intents inside their conversational data is crucial in building applications based on natural language understanding (NLU). Nevertheless, there is no established automated technique to gather the intents from noisy online chats or voice transcripts. Simple clustering approaches are not suited to intent-sparse dialogues. To solve this intent-landscape task, we propose an unsupervised pipeline that extracts the intents and the taxonomy of intents from real-world dialogues. Our pipeline mines intent-span candidates with an extractive Question-Answering Electra model and leverages sentence embeddings to apply a low-level density clustering followed by a top-level hierarchical clustering. Our results demonstrate the generalization ability of an ELECTRA large model fine-tuned on the SQuAD2 dataset to understand dialogues. With the right prompting question, this model achieves a rate of linguistic validation on intent spans beyond 85%. We furthermore reconstructed the intent schemes of five domains from the MultiDoGo dataset with an average recall of 94.3%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12886,August 2022,778,Correction: What do we want to know about MOOCs? Results from a machine learning approach to a systematic literature mapping review,"Ignacio Despujol, Linda Castañeda, Victoria I. Marín and Carlos Turró",Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education202219:53,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00370-6,26 October 2022,
779,0.000678989392134464,779,Ammunition Component Classification Using Deep Learning,"Hadi Ghahremannezhad, Chengjun Liu, Hang Shi","Ammunition scrap inspection is an essential step in the process of recycling ammunition metal scrap. Most ammunition is composed of a number of components, including case, primer, powder, and projectile. Ammo scrap containing energetics is considered to be potentially dangerous and should be separated before the recycling process. Manually inspecting each piece of scrap is tedious and time-consuming. We have gathered a dataset of ammunition components with the goal of applying artificial intelligence for classifying safe and unsafe scrap pieces automatically. First, two training datasets are manually created from visual and x-ray images of ammo. Second, the x-ray dataset is augmented using the spatial transforms of histogram equalization, averaging, sharpening, power law, and Gaussian blurring in order to compensate for the lack of sufficient training data. Lastly, the representative YOLOv4 object detection method is applied to detect the ammo components and classify the scrap pieces into safe and unsafe classes, respectively. The trained models are tested against unseen data in order to evaluate the performance of the applied method. The experiments demonstrate the feasibility of ammo component detection and classification using deep learning. The datasets and the pre-trained models are available at https://github.com/hadi-ghnd/Scrap-Classification. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12863,August 2022,779,Perspectives on software-defined networks: interviews with five leading scientists from the networking community,"Daniel M Batista, Gordon Blair, Fabio Kon, Raouf Boutaba, David Hutchison, Raj Jain, Ramachandran Ramjee and Christian Esteve Rothenberg",Software defined Networks (SDNs) have drawn much attention both from academia and industry over the last few years. Despite the fact that underlying ideas already exist through areas such as P2P applications a...,https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1186/s13174-015-0035-3,30 October 2015,
780,0.000678989392134464,780,Effectiveness of Mining Audio and Text Pairs from Public Data for Improving ASR Systems for Low-Resource Languages,"Kaushal Santosh Bhogale, Abhigyan Raman, Tahir Javed, Sumanth Doddapaneni, Anoop Kunchukuttan, Pratyush Kumar, Mitesh M. Khapra","End-to-end (E2E) models have become the default choice for state-of-the-art speech recognition systems. Such models are trained on large amounts of labelled data, which are often not available for low-resource languages. Techniques such as self-supervised learning and transfer learning hold promise, but have not yet been effective in training accurate models. On the other hand, collecting labelled datasets on a diverse set of domains and speakers is very expensive. In this work, we demonstrate an inexpensive and effective alternative to these approaches by ``mining'' text and audio pairs for Indian languages from public sources, specifically from the public archives of All India Radio. As a key component, we adapt the Needleman-Wunsch algorithm to align sentences with corresponding audio segments given a long audio and a PDF of its transcript, while being robust to errors due to OCR, extraneous text, and non-transcribed speech. We thus create Shrutilipi, a dataset which contains over 6,400 hours of labelled audio across 12 Indian languages totalling to 4.95M sentences. On average, Shrutilipi results in a 2.3x increase over publicly available labelled data. We establish the quality of Shrutilipi with 21 human evaluators across the 12 languages. We also establish the diversity of Shrutilipi in terms of represented regions, speakers, and mentioned named entities. Significantly, we show that adding Shrutilipi to the training set of Wav2Vec models leads to an average decrease in WER of 5.8\% for 7 languages on the IndicSUPERB benchmark. For Hindi, which has the most benchmarks (7), the average WER falls from 18.8% to 13.5%. This improvement extends to efficient models: We show a 2.3% drop in WER for a Conformer model (10x smaller than Wav2Vec). Finally, we demonstrate the diversity of Shrutilipi by showing that the model trained with it is more robust to noisy input. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12666,August 2022,780,Metaverse through the prism of power and addiction: what will happen when the virtual world becomes more attractive than reality?,Ljubisa Bojic,New technologies are emerging at a fast pace without being properly analyzed in terms of their social impact or adequately regulated by societies. One of the biggest potentially disruptive technologies for the...,https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-022-00208-4,12 October 2022,
781,0.000678989392134464,781,Itemset Utility Maximization with Correlation Measure,"Jiahui Chen, Yixin Xu, Shicheng Wan, Wensheng Gan, Jerry Chun-Wei Lin","As an important data mining technology, high utility itemset mining (HUIM) is used to find out interesting but hidden information (e.g., profit and risk). HUIM has been widely applied in many application scenarios, such as market analysis, medical detection, and web click stream analysis. However, most previous HUIM approaches often ignore the relationship between items in an itemset. Therefore, many irrelevant combinations (e.g., \{gold, apple\} and \{notebook, book\}) are discovered in HUIM. To address this limitation, many algorithms have been proposed to mine correlated high utility itemsets (CoHUIs). In this paper, we propose a novel algorithm called the Itemset Utility Maximization with Correlation Measure (CoIUM), which considers both a strong correlation and the profitable values of the items. Besides, the novel algorithm adopts a database projection mechanism to reduce the cost of database scanning. Moreover, two upper bounds and four pruning strategies are utilized to effectively prune the search space. And a concise array-based structure named utility-bin is used to calculate and store the adopted upper bounds in linear time and space. Finally, extensive experimental results on dense and sparse datasets demonstrate that CoIUM significantly outperforms the state-of-the-art algorithms in terms of runtime and memory consumption. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12551,August 2022,781,Erratum to: On the reliability and availability of replicated and rejuvenating systems under stealth attacks and intrusions,Luís Teixeira d’Aguiar Norton Brandão and Alysson Neves Bessani,Theoriginal articlewas published inJournal of the Brazilian Computer Society201218:s13173-012-0062-x,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0074-6,12 May 2012,
782,0.000678989392134464,782,Temporal Fuzzy Utility Maximization with Remaining Measure,"Shicheng Wan, Zhenqiang Ye, Wensheng Gan, Jiahui Chen","High utility itemset mining approaches discover hidden patterns from large amounts of temporal data. However, an inescapable problem of high utility itemset mining is that its discovered results hide the quantities of patterns, which causes poor interpretability. The results only reflect the shopping trends of customers, which cannot help decision makers quantify collected information. In linguistic terms, computers use mathematical or programming languages that are precisely formalized, but the language used by humans is always ambiguous. In this paper, we propose a novel one-phase temporal fuzzy utility itemset mining approach called TFUM. It revises temporal fuzzy-lists to maintain less but major information about potential high temporal fuzzy utility itemsets in memory, and then discovers a complete set of real interesting patterns in a short time. In particular, the remaining measure is the first adopted in the temporal fuzzy utility itemset mining domain in this paper. The remaining maximal temporal fuzzy utility is a tighter and stronger upper bound than that of previous studies adopted. Hence, it plays an important role in pruning the search space in TFUM. Finally, we also evaluate the efficiency and effectiveness of TFUM on various datasets. Extensive experimental results indicate that TFUM outperforms the state-of-the-art algorithms in terms of runtime cost, memory usage, and scalability. In addition, experiments prove that the remaining measure can significantly prune unnecessary candidates during mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12439,August 2022,782,"Correction to: Prerequisites for artificial intelligence in further education: identification of drivers, barriers, and business models of educational technology companies",André Renz and Romy Hilbig,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00245-2,24 February 2021,
783,0.000678989392134464,783,Towards Automated Imbalanced Learning with Deep Hierarchical Reinforcement Learning,"Daochen Zha, Kwei-Herng Lai, Qiaoyu Tan, Sirui Ding, Na Zou, Xia Hu","Imbalanced learning is a fundamental challenge in data mining, where there is a disproportionate ratio of training samples in each class. Over-sampling is an effective technique to tackle imbalanced learning through generating synthetic samples for the minority class. While numerous over-sampling algorithms have been proposed, they heavily rely on heuristics, which could be sub-optimal since we may need different sampling strategies for different datasets and base classifiers, and they cannot directly optimize the performance metric. Motivated by this, we investigate developing a learning-based over-sampling algorithm to optimize the classification performance, which is a challenging task because of the huge and hierarchical decision space. At the high level, we need to decide how many synthetic samples to generate. At the low level, we need to determine where the synthetic samples should be located, which depends on the high-level decision since the optimal locations of the samples may differ for different numbers of samples. To address the challenges, we propose AutoSMOTE, an automated over-sampling algorithm that can jointly optimize different levels of decisions. Motivated by the success of SMOTE~\cite{chawla2002smote} and its extensions, we formulate the generation process as a Markov decision process (MDP) consisting of three levels of policies to generate synthetic samples within the SMOTE search space. Then we leverage deep hierarchical reinforcement learning to optimize the performance metric on the validation data. Extensive experiments on six real-world datasets demonstrate that AutoSMOTE significantly outperforms the state-of-the-art resampling algorithms. The code is at https://github.com/daochenzha/autosmote △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12433,August 2022,783,Correction: What do we want to know about MOOCs? Results from a machine learning approach to a systematic literature mapping review,"Ignacio Despujol, Linda Castañeda, Victoria I. Marín and Carlos Turró",Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education202219:53,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00370-6,26 October 2022,
784,0.000678989392134464,784,Toward Robust Graph Semi-Supervised Learning against Extreme Data Scarcity,"Kaize Ding, Elnaz Nouri, Guoqing Zheng, Huan Liu, Ryen White","The success of graph neural networks on graph-based web mining highly relies on abundant human-annotated data, which is laborious to obtain in practice. When only few labeled nodes are available, how to improve their robustness is a key to achieve replicable and sustainable graph semi-supervised learning. Though self-training has been shown to be powerful for semi-supervised learning, its application on graph-structured data may fail because (1) larger receptive fields are not leveraged to capture long-range node interactions, which exacerbates the difficulty of propagating feature-label patterns from labeled nodes to unlabeled nodes; and (2) limited labeled data makes it challenging to learn well-separated decision boundaries for different node classes without explicitly capturing the underlying semantic structure. To address the challenges of capturing informative structural and semantic knowledge, we propose a new graph data augmentation framework, AGST (Augmented Graph Self-Training), which is built with two new (i.e., structural and semantic) augmentation modules on top of a decoupled GST backbone. In this work, we investigate whether this novel framework can learn a robust graph predictive model under the low-data context. We conduct comprehensive evaluations on semi-supervised node classification under different scenarios of limited labeled-node data. The experimental results demonstrate the unique contributions of the novel data augmentation framework for node classification with few labeled data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.12422,August 2022,784,Perspectives on software-defined networks: interviews with five leading scientists from the networking community,"Daniel M Batista, Gordon Blair, Fabio Kon, Raouf Boutaba, David Hutchison, Raj Jain, Ramachandran Ramjee and Christian Esteve Rothenberg",Software defined Networks (SDNs) have drawn much attention both from academia and industry over the last few years. Despite the fact that underlying ideas already exist through areas such as P2P applications a...,https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1186/s13174-015-0035-3,30 October 2015,
785,0.000678989392134464,785,"Local Intrinsic Dimensionality Measures for Graphs, with Applications to Graph Embeddings","Miloš Savić, Vladimir Kurbalija, Miloš Radovanović","The notion of local intrinsic dimensionality (LID) is an important advancement in data dimensionality analysis, with applications in data mining, machine learning and similarity search problems. Existing distance-based LID estimators were designed for tabular datasets encompassing data points represented as vectors in a Euclidean space. After discussing their limitations for graph-structured data considering graph embeddings and graph distances, we propose NC-LID, a novel LID-related measure for quantifying the discriminatory power of the shortest-path distance with respect to natural communities of nodes as their intrinsic localities. It is shown how this measure can be used to design LID-aware graph embedding algorithms by formulating two LID-elastic variants of node2vec with personalized hyperparameters that are adjusted according to NC-LID values. Our empirical analysis of NC-LID on a large number of real-world graphs shows that this measure is able to point to nodes with high link reconstruction errors in node2vec embeddings better than node centrality metrics. The experimental evaluation also shows that the proposed LID-elastic node2vec extensions improve node2vec by better preserving graph structure in generated embeddings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.11986,August 2022,785,Metaverse through the prism of power and addiction: what will happen when the virtual world becomes more attractive than reality?,Ljubisa Bojic,New technologies are emerging at a fast pace without being properly analyzed in terms of their social impact or adequately regulated by societies. One of the biggest potentially disruptive technologies for the...,https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-022-00208-4,12 October 2022,
786,0.000678989392134464,786,ProbGraph: High-Performance and High-Accuracy Graph Mining with Probabilistic Set Representations,"Maciej Besta, Cesare Miglioli, Paolo Sylos Labini, Jakub Tětek, Patrick Iff, Raghavendra Kanakagiri, Saleh Ashkboos, Kacper Janda, Michal Podstawski, Grzegorz Kwasniewski, Niels Gleinig, Flavio Vella, Onur Mutlu, Torsten Hoefler","Important graph mining problems such as Clustering are computationally demanding. To significantly accelerate these problems, we propose ProbGraph: a graph representation that enables simple and fast approximate parallel graph mining with strong theoretical guarantees on work, depth, and result accuracy. The key idea is to represent sets of vertices using probabilistic set representations such as Bloom filters. These representations are much faster to process than the original vertex sets thanks to vectorizability and small size. We use these representations as building blocks in important parallel graph mining algorithms such as Clique Counting or Clustering. When enhanced with ProbGraph, these algorithms significantly outperform tuned parallel exact baselines (up to nearly 50x on 32 cores) while ensuring accuracy of more than 90% for many input graph datasets. Our novel bounds and algorithms based on probabilistic set representations with desirable statistical properties are of separate interest for the data analytics community. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.11469,August 2022,786,Multi-factor authentication for shibboleth identity providers,"Emerson Ribeiro de Mello, Michelle Silva Wangham, Samuel Bristot Loli, Carlos Eduardo da Silva, Gabriela Cavalcanti da Silva, Shirlei Aparecida de Chaves and Bruno Bristot Loli","The federated identity model provides a solution for user authentication across multiple administrative domains. The academic federations, such as the Brazilian federation, are examples of this model in practi...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1186/s13174-020-00128-1,2 December 2020,
787,0.000678989392134464,787,Bitext Mining for Low-Resource Languages via Contrastive Learning,"Weiting Tan, Philipp Koehn","Mining high-quality bitexts for low-resource languages is challenging. This paper shows that sentence representation of language models fine-tuned with multiple negatives ranking loss, a contrastive objective, helps retrieve clean bitexts. Experiments show that parallel data mined from our approach substantially outperform the previous state-of-the-art method on low resource languages Khmer and Pashto. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.11194,August 2022,787,Coarse Fingerprint Registration Using Orientation Fields,Neil Yager and Adnan Amin,"The majority of traditional research into automated fingerprint identification has focused on algorithms using minutiae-based features. However, shortcomings of this approach are becoming apparent due to the d...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2043,15 August 2005,
788,0.000678989392134464,788,An Evolutionary Approach for Creating of Diverse Classifier Ensembles,"Alvaro R. Ferreira Jr, Fabio A. Faria, Gustavo Carneiro, Vinicius V. de Melo","Classification is one of the most studied tasks in data mining and machine learning areas and many works in the literature have been presented to solve classification problems for multiple fields of knowledge such as medicine, biology, security, and remote sensing. Since there is no single classifier that achieves the best results for all kinds of applications, a good alternative is to adopt classifier fusion strategies. A key point in the success of classifier fusion approaches is the combination of diversity and accuracy among classifiers belonging to an ensemble. With a large amount of classification models available in the literature, one challenge is the choice of the most suitable classifiers to compose the final classification system, which generates the need of classifier selection strategies. We address this point by proposing a framework for classifier selection and fusion based on a four-step protocol called CIF-E (Classifiers, Initialization, Fitness function, and Evolutionary algorithm). We implement and evaluate 24 varied ensemble approaches following the proposed CIF-E protocol and we are able to find the most accurate approach. A comparative analysis has also been performed among the best approaches and many other baselines from the literature. The experiments show that the proposed evolutionary approach based on Univariate Marginal Distribution Algorithm (UMDA) can outperform the state-of-the-art literature approaches in many well-known UCI datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10996,August 2022,788,Secure Multimedia Authoring with Dishonest Collaborators,"Nicholas Paul Sheppard, Reihaneh Safavi-Naini and Philip Ogunbona","Many systems have been proposed for protecting the intellectual property of multimedia authors and owners from the public at large, who have access to the multimedia only after it is published. In this paper, ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704401085,28 October 2004,
789,0.000678989392134464,789,Regularized impurity reduction: Accurate decision trees with complexity guarantees,"Guangyi Zhang, Aristides Gionis","Decision trees are popular classification models, providing high accuracy and intuitive explanations. However, as the tree size grows the model interpretability deteriorates. Traditional tree-induction algorithms, such as C4.5 and CART, rely on impurity-reduction functions that promote the discriminative power of each split. Thus, although these traditional methods are accurate in practice, there has been no theoretical guarantee that they will produce small trees. In this paper, we justify the use of a general family of impurity functions, including the popular functions of entropy and Gini-index, in scenarios where small trees are desirable, by showing that a simple enhancement can equip them with complexity guarantees. We consider a general setting, where objects to be classified are drawn from an arbitrary probability distribution, classification can be binary or multi-class, and splitting tests are associated with non-uniform costs. As a measure of tree complexity, we adopt the expected cost to classify an object drawn from the input distribution, which, in the uniform-cost case, is the expected number of tests. We propose a tree-induction algorithm that gives a logarithmic approximation guarantee on the tree complexity. This approximation factor is tight up to a constant factor under mild assumptions. The algorithm recursively selects a test that maximizes a greedy criterion defined as a weighted sum of three components. The first two components encourage the selection of tests that improve the balance and the cost-efficiency of the tree, respectively, while the third impurity-reduction component encourages the selection of more discriminative tests. As shown in our empirical evaluation, compared to the original heuristics, the enhanced algorithms strike an excellent balance between predictive accuracy and tree complexity. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10949,August 2022,789,NLP-based platform as a service: a brief review,"Sebastião Pais, João Cordeiro and M. Luqman Jamil",Natural language processing (NLP) refers to the field of study that focuses on the interactions between human language and computers. It has recently gained much attention for analyzing human language computat...,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00603-5,28 April 2022,
790,0.000678989392134464,790,Evaluation of group fairness measures in student performance prediction problems,"Tai Le Quy, Thi Huyen Nguyen, Gunnar Friege, Eirini Ntoutsi","Predicting students' academic performance is one of the key tasks of educational data mining (EDM). Traditionally, the high forecasting quality of such models was deemed critical. More recently, the issues of fairness and discrimination w.r.t. protected attributes, such as gender or race, have gained attention. Although there are several fairness-aware learning approaches in EDM, a comparative evaluation of these measures is still missing. In this paper, we evaluate different group fairness measures for student performance prediction problems on various educational datasets and fairness-aware learning models. Our study shows that the choice of the fairness measure is important, likewise for the choice of the grade threshold. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10625,August 2022,790,"Correction: A systematic review of the opportunities and challenges of micro-credentials for multiple stakeholders: learners, employers, higher education institutions and government","Soovendran Varadarajan, Joyce Hwee Ling Koh and Ben Kei Daniel",Theoriginal articlewas published inInternational Journal of Educational Technology in Higher Education202320:13,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00393-7,5 April 2023,
791,0.000678989392134464,791,Shapelet-Based Counterfactual Explanations for Multivariate Time Series,"Omar Bahri, Soukaina Filali Boubrahimi, Shah Muhammad Hamdi","As machine learning and deep learning models have become highly prevalent in a multitude of domains, the main reservation in their adoption for decision-making processes is their black-box nature. The Explainable Artificial Intelligence (XAI) paradigm has gained a lot of momentum lately due to its ability to reduce models opacity. XAI methods have not only increased stakeholders' trust in the decision process but also helped developers ensure its fairness. Recent efforts have been invested in creating transparent models and post-hoc explanations. However, fewer methods have been developed for time series data, and even less when it comes to multivariate datasets. In this work, we take advantage of the inherent interpretability of shapelets to develop a model agnostic multivariate time series (MTS) counterfactual explanation algorithm. Counterfactuals can have a tremendous impact on making black-box models explainable by indicating what changes have to be performed on the input to change the final decision. We test our approach on a real-life solar flare prediction dataset and prove that our approach produces high-quality counterfactuals. Moreover, a comparison to the only MTS counterfactual generation algorithm shows that, in addition to being visually interpretable, our explanations are superior in terms of proximity, sparsity, and plausibility. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10462,August 2022,791,Expectation-Maximization Method for EEG-Based Continuous Cursor Control,"Xiaoyuan Zhu, Cuntai Guan, Jiankang Wu, Yimin Cheng and Yixiao Wang","To develop effective learning algorithms for continuous prediction of cursor movement using EEG signals is a challenging research issue in brain-computer interface (BCI). In this paper, we propose a novel stat...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/49037,1 December 2006,
792,0.000678989392134464,792,Deterministic Graph-Walking Program Mining,"Peter Belcak, Roger Wattenhofer","Owing to their versatility, graph structures admit representations of intricate relationships between the separate entities comprising the data. We formalise the notion of connection between two vertex sets in terms of edge and vertex features by introducing graph-walking programs. We give two algorithms for mining of deterministic graph-walking programs that yield programs in the order of increasing length. These programs characterise linear long-distance relationships between the given two vertex sets in the context of the whole graph. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10290,August 2022,792,School-level predictors for the use of ICT in schools and students’ CIL in international comparison,"Julia Gerick, Birgit Eickelmann and Wilfried Bos",The increasing relevance of information and communication technologies (ICT) and society’s transition towards an information or knowledge society have led to the emergence of new challenges for schools and sch...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-017-0037-7,31 January 2017,
793,0.000678989392134464,793,Improving Knowledge-aware Recommendation with Multi-level Interactive Contrastive Learning,"Ding Zou, Wei Wei, Ziyang Wang, Xian-Ling Mao, Feida Zhu, Rui Fang, Dangyang Chen","Incorporating Knowledge Graphs (KG) into recommeder system has attracted considerable attention. Recently, the technical trend of Knowledge-aware Recommendation (KGR) is to develop end-to-end models based on graph neural networks (GNNs). However, the extremely sparse user-item interactions significantly degrade the performance of the GNN-based models, as: 1) the sparse interaction, means inadequate supervision signals and limits the supervised GNN-based models; 2) the combination of sparse interactions (CF part) and redundant KG facts (KG part) results in an unbalanced information utilization. Besides, the GNN paradigm aggregates local neighbors for node representation learning, while ignoring the non-local KG facts and making the knowledge extraction insufficient. Inspired by the recent success of contrastive learning in mining supervised signals from data itself, in this paper, we focus on exploring contrastive learning in KGR and propose a novel multi-level interactive contrastive learning mechanism. Different from traditional contrastive learning methods which contrast nodes of two generated graph views, interactive contrastive mechanism conducts layer-wise self-supervised learning by contrasting layers of different parts within graphs, which is also an ""interaction"" action. Specifically, we first construct local and non-local graphs for user/item in KG, exploring more KG facts for KGR. Then an intra-graph level interactive contrastive learning is performed within each graph, which contrasts layers of the CF and KG parts, for more consistent information leveraging. Besides, an inter-graph level interactive contrastive learning is performed between the local and non-local graphs, for sufficiently and coherently extracting non-local KG signals. Extensive experiments conducted on three benchmark datasets show the superior performance of our proposed method over the state-of-the-arts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.10061,August 2022,793,A qualitative exploration of teachers’ perspective on smartphones usage in higher education in developing countries,Shakeel Iqbal and Zeeshan Ahmed Bhatti,"Recently, Smartphone sales have surpassed the sales of all other computing devices including desktops, laptops and tablet PCs. Moreover, Smartphones have become a primary device to access the Internet as well ...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00203-4,6 July 2020,
794,0.000678989392134464,794,AA-Forecast: Anomaly-Aware Forecast for Extreme Events,"Ashkan Farhangi, Jiang Bian, Arthur Huang, Haoyi Xiong, Jun Wang, Zhishan Guo","Time series models often deal with extreme events and anomalies, both prevalent in real-world datasets. Such models often need to provide careful probabilistic forecasting, which is vital in risk management for extreme events such as hurricanes and pandemics. However, it is challenging to automatically detect and learn to use extreme events and anomalies for large-scale datasets, which often require manual effort. Hence, we propose an anomaly-aware forecast framework that leverages the previously seen effects of anomalies to improve its prediction accuracy during and after the presence of extreme events. Specifically, the framework automatically extracts anomalies and incorporates them through an attention mechanism to increase its accuracy for future extreme events. Moreover, the framework employs a dynamic uncertainty optimization algorithm that reduces the uncertainty of forecasts in an online manner. The proposed framework demonstrated consistent superior accuracy with less uncertainty on three datasets with different varieties of anomalies over the current prediction models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09933,August 2022,794,Compact Visualisation of Video Summaries,Janko Ćalić and Neill W. Campbell,This paper presents a system for compact and intuitive video summarisation aimed at both high-end professional production environments and small-screen portable devices. To represent large amounts of informati...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/19496,1 December 2007,
795,0.000678989392134464,795,MentorGNN: Deriving Curriculum for Pre-Training GNNs,"Dawei Zhou, Lecheng Zheng, Dongqi Fu, Jiawei Han, Jingrui He","Graph pre-training strategies have been attracting a surge of attention in the graph mining community, due to their flexibility in parameterizing graph neural networks (GNNs) without any label information. The key idea lies in encoding valuable information into the backbone GNNs, by predicting the masked graph signals extracted from the input graphs. In order to balance the importance of diverse graph signals (e.g., nodes, edges, subgraphs), the existing approaches are mostly hand-engineered by introducing hyperparameters to re-weight the importance of graph signals. However, human interventions with sub-optimal hyperparameters often inject additional bias and deteriorate the generalization performance in the downstream applications. This paper addresses these limitations from a new perspective, i.e., deriving curriculum for pre-training GNNs. We propose an end-to-end model named MentorGNN that aims to supervise the pre-training process of GNNs across graphs with diverse structures and disparate feature spaces. To comprehend heterogeneous graph signals at different granularities, we propose a curriculum learning paradigm that automatically re-weighs graph signals in order to ensure a good generalization in the target domain. Moreover, we shed new light on the problem of domain adaption on relational data (i.e., graphs) by deriving a natural and interpretable upper bound on the generalization error of the pre-trained GNNs. Extensive experiments on a wealth of real graphs validate and verify the performance of MentorGNN. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09905,August 2022,795,Accuracy Evaluation for Region Centroid-Based Registration of Fluorescent CLSM Imagery,"Sang-Chul Lee, Peter Bajcsy, Amy Lin and Robert Folberg",We present an accuracy evaluation of a semiautomatic registration technique for 3D volume reconstruction from fluorescent confocal laser scanning microscope (CLSM) imagery. The presented semiautomatic method i...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/82480,1 December 2006,
796,0.000678989392134464,796,From Time Series to Networks in R with the ts2net Package,Leonardo N. Ferreira,"Network science established itself as a prominent tool for modeling time series and complex systems. This modeling process consists of transforming a set or a single time series into a network. Nodes may represent complete time series, segments, or single values, while links define associations or similarities between the represented parts. R is one of the main programming languages used in data science, statistics, and machine learning, with many packages available. However, no single package provides the necessary methods to transform time series into networks. This paper presents ts2net, an R package for modeling one or multiple time series into networks. The package provides the time series distance functions that can be easily computed in parallel and in supercomputers to process larger data sets and methods to transform distance matrices into networks. Ts2net also provides methods to transform a single time series into a network, such as recurrence networks, visibility graphs, and transition networks. Together with other packages, ts2net permits using network science and graph mining tools to extract information from time series. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09660,August 2022,796,A Vision Chip for Color Segmentation and Pattern Matching,"Ralph Etienne-Cummings, Philippe Pouliquen and M. Anthony Lewis",A 128(H)64(V)RGB CMOS imager is integrated with region-of-...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703302021,18 June 2003,
797,0.000678989392134464,797,Topical: Learning Repository Embeddings from Source Code using Attention,"Agathe Lherondelle, Varun Babbar, Yash Satsangi, Fran Silavong, Shaltiel Eloul, Sean Moran","Machine learning on source code (MLOnCode) promises to transform how software is delivered. By mining the context and relationship between software artefacts, MLOnCode augments the software developers capabilities with code auto-generation, code recommendation, code auto-tagging and other data-driven enhancements. For many of these tasks a script level representation of code is sufficient, however, in many cases a repository level representation that takes into account various dependencies and repository structure is imperative, for example, auto-tagging repositories with topics or auto-documentation of repository code etc. Existing methods for computing repository level representations suffer from (a) reliance on natural language documentation of code (for example, README files) (b) naive aggregation of method/script-level representation, for example, by concatenation or averaging. This paper introduces Topical a deep neural network to generate repository level embeddings of publicly available GitHub code repositories directly from source code. Topical incorporates an attention mechanism that projects the source code, the full dependency graph and the script level textual information into a dense repository-level representation. To compute the repository-level representations, Topical is trained to predict the topics associated with a repository, on a dataset of publicly available GitHub repositories that were crawled along with their ground truth topic tags. Our experiments show that the embeddings computed by Topical are able to outperform multiple baselines, including baselines that naively combine the method-level representations through averaging or concatenation at the task of repository auto-tagging. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09495,August 2022,797,Discovering Recurrent Image Semantics from Class Discrimination,Joo-Hwee Lim and Jesse S. Jin,"Supervised statistical learning has become a critical means to design and learn visual concepts (e.g., faces, foliage, buildings, etc.) in content-based indexing systems. The drawback of this approach is the n...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/76093,1 December 2006,
798,0.000678989392134464,798,Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes,"Can Zheng, Yanshan Wang, Xiaowei Jia","Semantic textual similarity (STS) in the clinical domain helps improve diagnostic efficiency and produce concise texts for downstream data mining tasks. However, given the high degree of domain knowledge involved in clinic text, it remains challenging for general language models to infer implicit medical relationships behind clinical sentences and output similarities correctly. In this paper, we present a graph-augmented cyclic learning framework for similarity estimation in the clinical domain. The framework can be conveniently implemented on a state-of-art backbone language model, and improve its performance by leveraging domain knowledge through co-training with an auxiliary graph convolution network (GCN) based network. We report the success of introducing domain knowledge in GCN and the co-training framework by improving the Bio-clinical BERT baseline by 16.3% and 27.9%, respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09437,August 2022,798,Inquiry-based learning and E-learning: how to serve high and low achievers,"Sofoklis A. Sotiriou, Angelos Lazoudis and Franz X. Bogner","Large-scale implementations of effective inquiry-based learning are rare. A European-wide initiative gave teachers access to innovative e-learning tools (ranging from virtual labs, virtual games and simulation...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00130-x,12 October 2020,
799,0.000678989392134464,799,Self-Supervised Visual Place Recognition by Mining Temporal and Feature Neighborhoods,"Chao Chen, Xinhao Liu, Xuchu Xu, Yiming Li, Li Ding, Ruoyu Wang, Chen Feng","Visual place recognition (VPR) using deep networks has achieved state-of-the-art performance. However, most of them require a training set with ground truth sensor poses to obtain positive and negative samples of each observation's spatial neighborhood for supervised learning. When such information is unavailable, temporal neighborhoods from a sequentially collected data stream could be exploited for self-supervised training, although we find its performance suboptimal. Inspired by noisy label learning, we propose a novel self-supervised framework named \textit{TF-VPR} that uses temporal neighborhoods and learnable feature neighborhoods to discover unknown spatial neighborhoods. Our method follows an iterative training paradigm which alternates between: (1) representation learning with data augmentation, (2) positive set expansion to include the current feature space neighbors, and (3) positive set contraction via geometric verification. We conduct comprehensive experiments on both simulated and real datasets, with either RGB images or point clouds as inputs. The results show that our method outperforms our baselines in recall rate, robustness, and heading diversity, a novel metric we propose for VPR. Our code and datasets can be found at https://ai4ce.github.io/TF-VPR/. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09315,August 2022,799,Privacy preserving collaborative filtering for SaaS enabling PaaS clouds,"Anirban Basu, Jaideep Vaidya, Hiroaki Kikuchi, Theo Dimitrakos and Srijith K Nair","Recommender systems use, amongst others, a mechanism called collaborative filtering (CF) to predict the rating that a user will give to an item given the ratings of other items provided by other users. While r...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-1-8,9 July 2012,
800,0.000678989392134464,800,CohortVA: A Visual Analytic System for Interactive Exploration of Cohorts based on Historical Data,"Wei Zhang, Jason K. Wong, Xumeng Wang, Youcheng Gong, Rongchen Zhu, Kai Liu, Zihan Yan, Siwei Tan, Huamin Qu, Siming Chen, Wei Chen","In history research, cohort analysis seeks to identify social structures and figure mobilities by studying the group-based behavior of historical figures. Prior works mainly employ automatic data mining approaches, lacking effective visual explanation. In this paper, we present CohortVA, an interactive visual analytic approach that enables historians to incorporate expertise and insight into the iterative exploration process. The kernel of CohortVA is a novel identification model that generates candidate cohorts and constructs cohort features by means of pre-built knowledge graphs constructed from large-scale history databases. We propose a set of coordinated views to illustrate identified cohorts and features coupled with historical events and figure profiles. Two case studies and interviews with historians demonstrate that CohortVA can greatly enhance the capabilities of cohort identifications, figure authentications, and hypothesis generation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09237,August 2022,800,Exploring the trends of educational virtual reality games: a systematic review of empirical studies,"Solomon Sunday Oyelere, Nacir Bouali, Rogers Kaliisa, George Obaido, Abdullahi Abubakar Yunusa and Ebunayo R. Jimoh","Virtual Reality (VR) and educational games are emerging technologies mediating a rapid transformation in the educational world. However, few studies have systematically analyzed Educational Virtual Reality Gam...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-020-00142-7,19 October 2020,
801,0.000678989392134464,801,Cross-Domain Evaluation of a Deep Learning-Based Type Inference System,"Bernd Gruner, Tim Sonnekalb, Thomas S. Heinze, Clemens-Alexander Brust","Optional type annotations allow for enriching dynamic programming languages with static typing features like better Integrated Development Environment (IDE) support, more precise program analysis, and early detection and prevention of type-related runtime errors. Machine learning-based type inference promises interesting results for automating this task. However, the practical usage of such systems depends on their ability to generalize across different domains, as they are often applied outside their training domain. In this work, we investigate Type4Py as a representative of state-of-the-art deep learning-based type inference systems, by conducting extensive cross-domain experiments. Thereby, we address the following problems: class imbalances, out-of-vocabulary words, dataset shifts, and unknown classes. To perform such experiments, we use the datasets ManyTypes4Py and CrossDomainTypes4Py. The latter we introduce in this paper. Our dataset enables the evaluation of type inference systems in different domains of software projects and has over 1,000,000 type annotations mined on the platforms GitHub and Libraries. It consists of data from the two domains web development and scientific calculation. Through our experiments, we detect that the shifts in the dataset and the long-tailed distribution with many rare and unknown data types decrease the performance of the deep learning-based type inference system drastically. In this context, we test unsupervised domain adaptation methods and fine-tuning to overcome these issues. Moreover, we investigate the impact of out-of-vocabulary words. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09189,August 2022,801,Adaptive Processing of Range Scanned Head: Synthesis of Personalized Animated Human Face Representation with Multiple-Level Radial Basis Function,C. Chen and Edmond C. Prakash,We propose an animation system for personalized human head. Landmarks compliant to MPEG-4 facial definition parameters (FDP) are initially labeled on both template model and any target human head model as prio...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/27658,1 December 2006,
802,0.00514534320972017,802,UniCausal: Unified Benchmark and Repository for Causal Text Mining,"Fiona Anting Tan, Xinyu Zuo, See-Kiong Ng","Current causal text mining datasets vary in objectives, data coverage, and annotation schemes. These inconsistent efforts prevent modeling capabilities and fair comparisons of model performance. Furthermore, few datasets include cause-effect span annotations, which are needed for end-to-end causal relation extraction. To address these issues, we propose UniCausal, a unified benchmark for causal text mining across three tasks: (I) Causal Sequence Classification, (II) Cause-Effect Span Detection and (III) Causal Pair Classification. We consolidated and aligned annotations of six high quality, mainly human-annotated, corpora, resulting in a total of 58,720, 12,144 and 69,165 examples for each task respectively. Since the definition of causality can be subjective, our framework was designed to allow researchers to work on some or all datasets and tasks. To create an initial benchmark, we fine-tuned BERT pre-trained language models to each task, achieving 70.10% Binary F1, 52.42% Macro F1, and 84.68% Binary F1 scores respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.09163,August 2022,802,Research and trends in STEM education: a systematic analysis of publicly funded projects,"Yeping Li, Ke Wang, Yu Xiao, Jeffrey E. Froyd and Sandra B. Nite","Taking publicly funded projects in STEM education as a special lens, we aimed to learn about research and trends in STEM education. We identified a total of 127 projects funded by the Institute of Education Sc...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00213-8,22 April 2020,
803,0.000678989392134464,803,Towards Automated Process Planning and Mining,"Peter Fettke, Alexander Rombach","AI Planning, Machine Learning and Process Mining have so far developed into separate research fields. At the same time, many interesting concepts and insights have been gained at the intersection of these areas in recent years. For example, the behavior of future processes is now comprehensively predicted with the aid of Machine Learning. For the practical application of these findings, however, it is also necessary not only to know the expected course, but also to give recommendations and hints for the achievement of goals, i.e. to carry out comprehensive process planning. At the same time, an adequate integration of the aforementioned research fields is still lacking. In this article, we present a research project in which researchers from the AI and BPM field work jointly together. Therefore, we discuss the overall research problem, the relevant fields of research and our overall research framework to automatically derive process models from executional process data, derive subsequent planning problems and conduct automated planning in order to adaptively plan and execute business processes using real-time forecasts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.08943,August 2022,803,"Quantum optimal control in quantum technologies. Strategic report on current status, visions and goals for research in Europe","Christiane P. Koch, Ugo Boscain, Tommaso Calarco, Gunther Dirr, Stefan Filipp, Steffen J. Glaser, Ronnie Kosloff, Simone Montangero, Thomas Schulte-Herbrüggen, Dominique Sugny and Frank K. Wilhelm","Quantum optimal control, a toolbox for devising and implementing the shapes of external fields that accomplish given tasks in the operation of a quantum device in the best way possible, has evolved into one of...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00138-x,20 July 2022,
804,0.000678989392134464,804,Domain-Specific Risk Minimization for Out-of-Distribution Generalization,"Yi-Fan Zhang, Jindong Wang, Jian Liang, Zhang Zhang, Baosheng Yu, Liang Wang, Dacheng Tao, Xing Xie","Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose \textbf{Domain-specific Risk Minimization (DRM)}. During training, DRM models the distributions of different source domains separately; for inference, DRM performs online model steering using the source hypothesis for each arriving target sample. Extensive experiments demonstrate the effectiveness of the proposed DRM for domain generalization with the following advantages: 1) it significantly outperforms competitive baselines on different distributional shift settings; 2) it achieves either comparable or superior accuracies on all source domains compared to vanilla empirical risk minimization; 3) it remains simple and efficient during training, and 4) it is complementary to invariant learning approaches. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.08661,August 2022,804,Adaptive UEP and Packet Size Assignment for Scalable Video Transmission over Burst-Error Channels,"Chen-Wei Lee, Chu-Sing Yang and Yih-Ching Su",This work proposes an adaptive unequal error protection (UEP) and packet size assignment scheme for scalable video transmission over a burst-error channel. An analytic model is developed to evaluate the impact...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/10131,1 December 2006,
805,0.000678989392134464,805,Domain Camera Adaptation and Collaborative Multiple Feature Clustering for Unsupervised Person Re-ID,Yuanpeng Tu,"Recently unsupervised person re-identification (re-ID) has drawn much attention due to its open-world scenario settings where limited annotated data is available. Existing supervised methods often fail to generalize well on unseen domains, while the unsupervised methods, mostly lack multi-granularity information and are prone to suffer from confirmation bias. In this paper, we aim at finding better feature representations on the unseen target domain from two aspects, 1) performing unsupervised domain adaptation on the labeled source domain and 2) mining potential similarities on the unlabeled target domain. Besides, a collaborative pseudo re-labeling strategy is proposed to alleviate the influence of confirmation bias. Firstly, a generative adversarial network is utilized to transfer images from the source domain to the target domain. Moreover, person identity preserving and identity mapping losses are introduced to improve the quality of generated images. Secondly, we propose a novel collaborative multiple feature clustering framework (CMFC) to learn the internal data structure of target domain, including global feature and partial feature branches. The global feature branch (GB) employs unsupervised clustering on the global feature of person images while the Partial feature branch (PB) mines similarities within different body regions. Finally, extensive experiments on two benchmark datasets show the competitive performance of our method under unsupervised person re-ID settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.08624,August 2022,805,Low-Complexity Versatile Finite Field Multiplier in Normal Basis,Hua Li and Chang Nian Zhang,A low-complexity VLSI array of versatile multiplier in normal basis overis presented. The finite field parameters can be changed accord...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570220414X,1 September 2002,
806,0.000678989392134464,806,A Concept and Argumentation based Interpretable Model in High Risk Domains,"Haixiao Chi, Dawei Wang, Gaojie Cui, Feng Mao, Beishui Liao","Interpretability has become an essential topic for artificial intelligence in some high-risk domains such as healthcare, bank and security. For commonly-used tabular data, traditional methods trained end-to-end machine learning models with numerical and categorical data only, and did not leverage human understandable knowledge such as data descriptions. Yet mining human-level knowledge from tabular data and using it for prediction remain a challenge. Therefore, we propose a concept and argumentation based model (CAM) that includes the following two components: a novel concept mining method to obtain human understandable concepts and their relations from both descriptions of features and the underlying data, and a quantitative argumentation-based method to do knowledge representation and reasoning. As a result of it, CAM provides decisions that are based on human-level knowledge and the reasoning process is intrinsically interpretable. Finally, to visualize the purposed interpretable model, we provide a dialogical explanation that contain dominated reasoning path within CAM. Experimental results on both open source benchmark dataset and real-word business dataset show that (1) CAM is transparent and interpretable, and the knowledge inside the CAM is coherent with human understanding; (2) Our interpretable approach can reach competitive results comparing with other state-of-art models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.08149,August 2022,806,Correction: DCAU-Net: dense convolutional attention U-Net for segmentation of intracranial aneurysm images,"Wenwen Yuan, Yanjun Peng, Yanfei Guo, Yande Ren and Qianwen Xue","Theoriginal articlewas published inVisual Computing for Industry, Biomedicine, and Art20225:9",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-022-00110-7,8 May 2022,
807,0.000678989392134464,807,A Large-Scale Dataset of Twitter Chatter about Online Learning during the Current COVID-19 Omicron Wave,Nirmalya Thakur,"The COVID-19 Omicron variant, reported to be the most immune evasive variant of COVID-19, is resulting in a surge of COVID-19 cases globally. This has caused schools, colleges, and universities in different parts of the world to transition to online learning. As a result, social media platforms such as Twitter are seeing an increase in conversations related to online learning in the form of tweets. Mining such tweets to develop a dataset can serve as a data resource for different applications and use-cases related to the analysis of interest, views, opinions, perspectives, attitudes, and feedback towards online learning during the current surge of COVID-19 cases caused by the Omicron variant. Therefore, this work presents a large-scale open-access Twitter dataset of conversations about online learning from different parts of the world since the first detected case of the COVID-19 Omicron variant in November 2021. The dataset is compliant with the privacy policy, developer agreement, and guidelines for content redistribution of Twitter, as well as with the FAIR principles (Findability, Accessibility, Interoperability, and Reusability) principles for scientific data management. The paper also briefly outlines some potential applications in the fields of Big Data, Data Mining, Natural Language Processing, and their related disciplines, with a specific focus on online learning during this Omicron wave that may be studied, explored, and investigated by using this dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.07810,August 2022,807,3D dynamic fashion design development using digital technology and its potential in online platforms,Kyung-Hee Choi,"The purpose of this study is to develop 3D dynamic fashion garments with changeable styles, colors and textile patterns, especially using a 3D virtual simulation system, and to examine their potential possibil...",https://www.springeropen.com//fashionandtextiles.springeropen.com/articles/10.1186/s40691-021-00286-1,11 March 2022,
808,0.000678989392134464,808,Identifying Source Code File Experts,"Otávio Cury, Guilherme Avelino, Pedro Santos Neto, Ricardo Britto, Marco Túlio Valente","In software development, the identification of source code file experts is an important task. Identifying these experts helps to improve software maintenance and evolution activities, such as developing new features, code reviews, and bug fixes. Although some studies have proposed repository mining techniques to automatically identify source code experts, there are still gaps in this area that can be explored. For example, investigating new variables related to source code knowledge and applying machine learning aiming to improve the performance of techniques to identify source code experts. The goal of this study is to investigate opportunities to improve the performance of existing techniques to recommend source code files experts. We built an oracle by collecting data from the development history and surveying developers of 113 software projects. Then, we use this oracle to: (i) analyze the correlation between measures extracted from the development history and the developers source code knowledge and (ii) investigate the use of machine learning classifiers by evaluating their performance in identifying source code files experts. First Authorship and Recency of Modification are the variables with the highest positive and negative correlations with source code knowledge, respectively. Machine learning classifiers outperformed the linear techniques (F-Measure = 71% to 73%) in the public dataset, but this advantage is not clear in the private dataset, with F-Measure ranging from 55% to 68% for the linear techniques and 58% to 67% for ML techniques. Overall, the linear techniques and the machine learning classifiers achieved similar performance, particularly if we analyze F-Measure. However, machine learning classifiers usually get higher precision while linear techniques obtained the highest recall values. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.07501,August 2022,808,The nature of properly human mathematics,David Ruelle,We claim that human mathematics is only a limited part of the consequences of the chosen basic axioms. Properly human mathematics varies with time but appears to have universal features which we try to analyse...,https://www.springeropen.com//advancesincontinuousanddiscretemodels.springeropen.com/articles/10.1186/s13662-023-03761-9,22 March 2023,
809,0.000678989392134464,809,Optimal LP Rounding and Linear-Time Approximation Algorithms for Clustering Edge-Colored Hypergraphs,Nate Veldt,"We study the approximability of an existing framework for clustering edge-colored hypergraphs, which is closely related to chromatic correlation clustering and is motivated by machine learning and data mining applications where the goal is to cluster a set of objects based on multiway interactions of different categories or types. We present improved approximation guarantees based on linear programming, and show they are tight by proving a matching integrality gap. Our results also include new approximation hardness results, a combinatorial 2-approximation whose runtime is linear in the hypergraph size, and several new connections to well-studied objectives such as vertex cover and hypergraph multiway cut. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.06506,August 2022,809,Accelerating CFD simulation with high order finite difference method on curvilinear coordinates for modern GPU clusters,"Chuang-Chao Ye, Peng-Jun-Yi Zhang, Zhen-Hua Wan, Rui Yan and De-Jun Sun","A high fidelity flow simulation for complex geometries for high Reynolds number (Re) flow is still very challenging, requiring a more powerful HPC system. However, the development of HPC with traditional CPU arch...",https://www.springeropen.com//AIA.Springeropen.com/articles/10.1186/s42774-021-00098-3,8 February 2022,
810,0.000678989392134464,810,Implementation of Data Mining on a Secure Cloud Computing over a Web API using Supervised Machine Learning Algorithm,"Tosin Ige, Sikiru Adewale","Ever since the era of internet had ushered in cloud computing, there had been increase in the demand for the unlimited data available through cloud computing for data analysis, pattern recognition and technology advancement. With this also bring the problem of scalability, efficiency and security threat. This research paper focuses on how data can be dynamically mine in real time for pattern detection in a secure cloud computing environment using combination of decision tree algorithm and Random Forest over a restful Application Programming Interface (API). We are able to successfully Implement data mining on cloud computing bypassing or avoiding direct interaction with data warehouse and without any terminal involve by using combination of IBM Cloud storage facility, Amazing Web Service, Application Programming Interface and Window service along with a decision tree and Random Forest algorithm for our classifier. We were able to successfully bypass direct connection with the data warehouse and cloud terminal with 94% accuracy in our model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.06433,August 2022,810,Massively parallel non-stationary EEG data processing on GPGPU platforms with Morlet continuous wavelet transform,"Ze Deng, Dan Chen, Yangyang Hu, Xiaoming Wu, Weizhou Peng and Xiaoli Li","Morlet continuous wavelet transform (MCWT) has been widely used to process non-stationary electro-encephalogram (EEG) data. Nowadays, the MCWT application for processing EEG data is time-sensitive and data-int...",https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-012-0071-1,10 November 2012,
811,0.000678989392134464,811,Facial Expression Recognition and Image Description Generation in Vietnamese,"Khang Nhut Lam, Kim-Ngoc Thi Nguyen, Loc Huu Nguy, Jugal Kalita","This paper discusses a facial expression recognition model and a description generation model to build descriptive sentences for images and facial expressions of people in images. Our study shows that YOLOv5 achieves better results than a traditional CNN for all emotions on the KDEF dataset. In particular, the accuracies of the CNN and YOLOv5 models for emotion recognition are 0.853 and 0.938, respectively. A model for generating descriptions for images based on a merged architecture is proposed using VGG16 with the descriptions encoded over an LSTM model. YOLOv5 is also used to recognize dominant colors of objects in the images and correct the color words in the descriptions generated if it is necessary. If the description contains words referring to a person, we recognize the emotion of the person in the image. Finally, we combine the results of all models to create sentences that describe the visual content and the human emotions in the images. Experimental results on the Flickr8k dataset in Vietnamese achieve BLEU-1, BLEU-2, BLEU-3, BLEU-4 scores of 0.628; 0.425; 0.280; and 0.174, respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.06117,August 2022,811,Editorial,Magdy A. Bayoumi and Bertrand Zavidovique,Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.989,22 May 2005,
812,0.000678989392134464,812,An Accelerated Doubly Stochastic Gradient Method with Faster Explicit Model Identification,"Runxue Bao, Bin Gu, Heng Huang","Sparsity regularized loss minimization problems play an important role in various fields including machine learning, data mining, and modern statistics. Proximal gradient descent method and coordinate descent method are the most popular approaches to solving the minimization problem. Although existing methods can achieve implicit model identification, aka support set identification, in a finite number of iterations, these methods still suffer from huge computational costs and memory burdens in high-dimensional scenarios. The reason is that the support set identification in these methods is implicit and thus cannot explicitly identify the low-complexity structure in practice, namely, they cannot discard useless coefficients of the associated features to achieve algorithmic acceleration via dimension reduction. To address this challenge, we propose a novel accelerated doubly stochastic gradient descent (ADSGD) method for sparsity regularized loss minimization problems, which can reduce the number of block iterations by eliminating inactive coefficients during the optimization process and eventually achieve faster explicit model identification and improve the algorithm efficiency. Theoretically, we first prove that ADSGD can achieve a linear convergence rate and lower overall computational complexity. More importantly, we prove that ADSGD can achieve a linear rate of explicit model identification. Numerically, experimental results on benchmark datasets confirm the efficiency of our proposed method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.06058,August 2022,812,A survey of automatic term extraction for Brazilian Portuguese,"Merley da Silva Conrado, Ariani Di Felippo, Thiago Alexandre Salgueiro Pardo and Solange Oliveira Rezende","Term extraction is highly relevant as it is the basis for several tasks, such as the building of dictionaries, taxonomies, and ontologies, as well as the translation and organization of text data.",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/1678-4804-20-12,30 May 2014,
813,0.000678989392134464,813,Self-supervised Multi-modal Training from Uncurated Image and Reports Enables Zero-shot Oversight Artificial Intelligence in Radiology,"Sangjoon Park, Eun Sun Lee, Kyung Sook Shin, Jeong Eun Lee, Jong Chul Ye","Oversight AI is an emerging concept in radiology where the AI forms a symbiosis with radiologists by continuously supporting radiologists in their decision-making. Recent advances in vision-language models sheds a light on the long-standing problems of the oversight AI by the understanding both visual and textual concepts and their semantic correspondences. However, there have been limited successes in the application of vision-language models in the medical domain, as the current vision-language models and learning strategies for photographic images and captions call for the web-scale data corpus of image and text pairs which was not often feasible in the medical domain. To address this, here we present a model dubbed Medical Cross-attention Vision-Language model (Medical X-VL), leveraging the key components to be tailored for the medical domain. Our medical X-VL model is based on the following components: self-supervised uni-modal models in medical domain and fusion encoder to bridge them, momentum distillation, sentence-wise contrastive learning for medical reports, and the sentence similarity-adjusted hard negative mining. We experimentally demonstrated that our model enables various zero-shot tasks for oversight AI, ranging from the zero-shot classification to zero-shot error correction. Our model outperformed the current state-of-the-art models in two different medical image database, suggesting the novel clinical usage of our oversight AI model for monitoring human errors. Our method was especially successful in the data-limited setting, which is frequently encountered in the clinics, suggesting the potential widespread applicability in medical domain. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.05140,August 2022,813,Investigating temporal access in a flipped classroom: procrastination persists,"Abeer AlJarrah, Michael K. Thomas and Mohamed Shehab",This paper reports on a study that examines the learning behaviors and characteristics of students in a mobile applications computer programming class that adopted a “flipped” learning style. By harvesting lea...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-017-0083-9,9 January 2018,
814,0.000678989392134464,814,Increasing Students' Engagement to Reminder Emails Through Multi-Armed Bandits,"Fernando J. Yanez, Angela Zavaleta-Bernuy, Ziwen Han, Michael Liut, Anna Rafferty, Joseph Jay Williams","Conducting randomized experiments in education settings raises the question of how we can use machine learning techniques to improve educational interventions. Using Multi-Armed Bandits (MAB) algorithms like Thompson Sampling (TS) in adaptive experiments can increase students' chances of obtaining better outcomes by increasing the probability of assignment to the most optimal condition (arm), even before an intervention completes. This is an advantage over traditional A/B testing, which may allocate an equal number of students to both optimal and non-optimal conditions. The problem is the exploration-exploitation trade-off. Even though adaptive policies aim to collect enough information to allocate more students to better arms reliably, past work shows that this may not be enough exploration to draw reliable conclusions about whether arms differ. Hence, it is of interest to provide additional uniform random (UR) exploration throughout the experiment. This paper shows a real-world adaptive experiment on how students engage with instructors' weekly email reminders to build their time management habits. Our metric of interest is open email rates which tracks the arms represented by different subject lines. These are delivered following different allocation algorithms: UR, TS, and what we identified as TS† - which combines both TS and UR rewards to update its priors. We highlight problems with these adaptive algorithms - such as possible exploitation of an arm when there is no significant difference - and address their causes and consequences. Future directions includes studying situations where the early choice of the optimal arm is not ideal and how adaptive algorithms can address them. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.05090,August 2022,814,Etiology of Keratoconus: proposed biomechanical pathogenesis,"Roy Asher, Amit Gefen, Elad Moisseiev and David Varssano",The etiology of keratoconus most likely involves substantial biomechanical interactions. The goal of this study was to characterize corneal biomechanics using computer modeling techniques in order to elucidate...,https://www.springeropen.com//in-silico-cell-and-tissue-science.springeropen.com/articles/10.1186/2196-050X-1-3,23 October 2014,
815,0.000678989392134464,815,Robust Scenario Interpretation from Multi-model Prediction Efforts,"Yuanhao Lu, Ajitesh Srivastava","Multi-model prediction efforts in infectious disease modeling and climate modeling involve multiple teams independently producing projections under various scenarios. Often these scenarios are produced by the presence and absence of a decision in the future, e.g., no vaccinations (scenario A) vs vaccinations (scenario B) available in the future. The models submit probabilistic projections for each of the scenarios. Obtaining a confidence interval on the impact of the decision (e.g., number of deaths averted) is important for decision making. However, obtaining tight bounds only from the probabilistic projections for the individual scenarios is difficult, as the joint probability is not known. Further, the models may not be able to generate the joint probability distribution due to various reasons including the need to rewrite simulations, and storage and transfer requirements. Without asking the submitting models for additional work, we aim to estimate a non-trivial bound on the outcomes due to the decision variable. We first prove, under a key assumption, that an $α-$confidence interval on the difference of scenario predictions can be obtained given only the quantiles of the predictions. Then we show how to estimate a confidence interval after relaxing that assumption. We use our approach to estimate confidence intervals on reduction in cases, deaths, and hospitalizations due to vaccinations based on model submissions to the US Scenario Modeling Hub. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.05075,August 2022,815,Lip print evaluation of Indian and Malaysian-Chinese subjects by manual and digital methods: a correlational study with gender and ethnicity,"Aditi Chadha, Ravindranath Vineetha, Mathangi Kumar, Divyansh Bansal, Keerthilatha M. Pai and Prakash K Aithal","Cheiloscopy is a reliable method of personal identification which may augment the established methods like dactylography, DNA (deoxyribonucleic acid) profiling, and dental records.",https://www.springeropen.com//ejfs.springeropen.com/articles/10.1186/s41935-022-00273-7,9 March 2022,
816,0.000678989392134464,816,Automated Infectious Disease Forecasting: Use-cases and Practical Considerations for Pipeline Implementation,"VP Nagraj, Chris Hulme-Lowe, Shakeel Jessa, Stephen D. Turner","Real-time forecasting of disease outbreaks requires standardized outputs generated in a timely manner. Development of pipelines to automate infectious disease forecasts can ensure that parameterization and software dependencies are common to any execution of the forecasting code. Here we present our implementation of an automated cloud computing pipeline to forecast infectious disease outcomes, with examples of usage to forecast COVID-19 and influenza targets. We also offer our perspective on the limits of automation and importance of human-in-the-loop automated infectious disease forecasting. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.05019,August 2022,816,Modularizing communication middleware concerns using aspects,Cristiano Amaral Maffort and Marco Tulio de Oliveira Valente,"Software engineers often rely on communication middleware platforms to design and implement distributed systems. However, middleware functionality is usually invasive, pervasive and tangled with business-speci...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194258,December 2007,
817,0.000678989392134464,817,"Characterizing the 2022 Russo-Ukrainian Conflict Through the Lenses of Aspect-Based Sentiment Analysis: Dataset, Methodology, and Preliminary Findings","Maurantonio Caprolu, Alireza Sadighian, Roberto Di Pietro","Online social networks (OSNs) play a crucial role in today's world. On the one hand, they allow free speech, information sharing, and social-movements organization, to cite a few. On the other hand, they are the tool of choice to spread disinformation, hate speech, and to support propaganda. For these reasons, OSNs data mining and analysis aimed at detecting disinformation campaigns that may arm the society and, more in general, poison the democratic posture of states, are essential activities during key events such as elections, pandemics, and conflicts. In this paper, we studied the 2022 Russo-Ukrainian conflict on Twitter, one of the most used OSNs. We quantitatively and qualitatively analyze a dataset of more than 5.5+ million tweets related to the subject, generated by 1.8+ million unique users. By leveraging statistical analysis techniques and aspect-based sentiment analysis (ABSA), we discover hidden insights in the collected data and abnormal patterns in the users' sentiment that in some cases confirm while in other cases disprove common beliefs on the conflict. In particular, based on our findings and contrary to what suggested in some mainstream media, there is no evidence of massive disinformation campaigns. However, we have identified several anomalies in the behavior of particular accounts and in the sentiment trend for some subjects that represent a starting point for further analysis in the field. The adopted techniques, the availability of the data, the replicability of the experiments, and the preliminary findings, other than being interesting on their own, also pave the way to further research in the domain. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.04903,August 2022,817,"An integrated role-based approach for modeling, designing and implementing multi-agent systems","Xiaoqin Zhang, Haiping Xu and Bhavesh Shrestha","To facilitate the development of multi-agent systems and improve the reusability, robustness and feasibility of these systems, we have developed a role-based agent development framework (RADE). In this paper, ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192409,June 2007,
818,0.000678989392134464,818,Mining Reaction and Diffusion Dynamics in Social Activities,"Taichi Murayama, Yasuko Matsubara, Yasushi Sakurai","Large quantifies of online user activity data, such as weekly web search volumes, which co-evolve with the mutual influence of several queries and locations, serve as an important social sensor. It is an important task to accurately forecast the future activity by discovering latent interactions from such data, i.e., the ecosystems between each query and the flow of influences between each area. However, this is a difficult problem in terms of data quantity and complex patterns covering the dynamics. To tackle the problem, we propose FluxCube, which is an effective mining method that forecasts large collections of co-evolving online user activity and provides good interpretability. Our model is the expansion of a combination of two mathematical models: a reaction-diffusion system provides a framework for modeling the flow of influences between local area groups and an ecological system models the latent interactions between each query. Also, by leveraging the concept of physics-informed neural networks, FluxCube achieves high interpretability obtained from the parameters and high forecasting performance, together. Extensive experiments on real datasets showed that FluxCube outperforms comparable models in terms of the forecasting accuracy, and each component in FluxCube contributes to the enhanced performance. We then show some case studies that FluxCube can extract useful latent interactions between queries and area groups. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.04846,August 2022,818,Using a Functional Ontology Of Reputation to interoperate different agent reputation models,Sara Casare and Jaime Simão Sichman,"This paper presents a Functional Ontology of Reputation that could be used as a common shared reputation knowledge by agents. Although there is a huge work on agent reputation, each research defines its own ba...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192377,June 2005,
819,0.000678989392134464,819,"Decolonisation, Global Data Law, and Indigenous Data Sovereignty","Jennafer Shae Roberts, Laura N Montoya","This research examines the impact of digital neo-colonialism on the Global South and encourages the development of legal and economic incentives to protect Indigenous cultures globally. Data governance is discussed in an evolutionary context while focusing on data sharing and data mining. Case studies that exemplify the need to steer global data law towards protecting the earth, while addressing issues of data access, privacy, rights, and colonialism in the global South are explored. The case studies highlight connections to indigenous people's rights, in regard to the protection of environmental ecosystems, thus establishing how data law can serve the earth from an autochthonous lens. This framework examines histories shaped by colonialism and suggests how data governance could be used to create healthier balances of power. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.04700,August 2022,819,A Lua-based AOP infrastructure,"Nélio Cacho, Thaís Batista and Fabrício Fernandes","In this paper we describe an aspect-oriented infrastructure to handle dynamic AOP based on the Lua language. This infrastructure is composed of AspectLua, a Lua extension that allows the declaration of aspects...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192379,October 2005,
820,0.000678989392134464,820,GRIT-VLP: Grouped Mini-batch Sampling for Efficient Vision and Language Pre-training,"Jaeseok Byun, Taebaek Hwang, Jianlong Fu, Taesup Moon","Most of the currently existing vision and language pre-training (VLP) methods have mainly focused on how to extract and align vision and text features. In contrast to the mainstream VLP methods, we highlight that two routinely applied steps during pre-training have crucial impact on the performance of the pre-trained model: in-batch hard negative sampling for image-text matching (ITM) and assigning the large masking probability for the masked language modeling (MLM). After empirically showing the unexpected effectiveness of above two steps, we systematically devise our GRIT-VLP, which adaptively samples mini-batches for more effective mining of hard negative samples for ITM while maintaining the computational cost for pre-training. Our method consists of three components: 1) GRouped mIni-baTch sampling (GRIT) strategy that collects similar examples in a mini-batch, 2) ITC consistency loss for improving the mining ability, and 3) enlarged masking probability for MLM. Consequently, we show our GRIT-VLP achieves a new state-of-the-art performance on various downstream tasks with much less computational cost. Furthermore, we demonstrate that our model is essentially in par with ALBEF, the previous state-of-the-art, only with one-third of training epochs on the same training data. Code is available at https://github.com/jaeseokbyun/GRIT-VLP. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.04060,August 2022,820,Automatic Target Detection Using Wavelet Transform,S. Arivazhagan and L. Ganesan,"Automatic target recognition (ATR) involves processing images for detecting, classifying, and tracking targets embedded in a background scene. This paper presents an algorithm for detecting a specified set of ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704408208,27 December 2004,
821,0.000678989392134464,821,CSSAM:Code Search via Attention Matching of Code Semantics and Structures,"Yi Hu, Bo Cai, Yaoxiang Yu","Despite the continuous efforts in improving both the effectiveness and efficiency of code search, two issues remained unsolved. First, programming languages have inherent strong structural linkages, and feature mining of code as text form would omit the structural information contained inside it. Second, there is a potential semantic relationship between code and query, it is challenging to align code and text across sequences so that vectors are spatially consistent during similarity matching. To tackle both issues, in this paper, a code search model named CSSAM (Code Semantics and Structures Attention Matching) is proposed. By introducing semantic and structural matching mechanisms, CSSAM effectively extracts and fuses multidimensional code features. Specifically, the cross and residual layer was developed to facilitate high-latitude spatial alignment of code and query at the token level. By leveraging the residual interaction, a matching module is designed to preserve more code semantics and descriptive features, that enhances the adhesion between the code and its corresponding query text. Besides, to improve the model's comprehension of the code's inherent structure, a code representation structure named CSRG (Code Semantic Representation Graph) is proposed for jointly representing abstract syntax tree nodes and the data flow of the codes. According to the experimental results on two publicly available datasets containing 540k and 330k code segments, CSSAM significantly outperforms the baselines in terms of achieving the highest SR@1/5/10, MRR, and NDCG@50 on both datasets respectively. Moreover, the ablation study is conducted to quantitatively measure the impact of each key component of CSSAM on the efficiency and effectiveness of code search, which offers the insights into the improvement of advanced code search solutions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.03922,August 2022,821,Interactive cognitive artifacts for enhancing situation awareness of incident commanders in mass casualty incidents,Tilo Mentler and Michael Herczeg,"In mass casualty incidents, several members of Emergency Medical Services have to take actions in the field in order to cope with many injured or sick people. Incident commanders are responsible for managing o...",https://www.springeropen.com//journalofinteractionscience.springeropen.com/articles/10.1186/s40166-015-0012-0,19 November 2015,
822,0.000678989392134464,822,Defining Cases and Variants for Object-Centric Event Data,"Jan Niklas Adams, Daniel Schuster, Seth Schmitz, Günther Schuh, Wil M. P. van der Aalst","The execution of processes leaves traces of event data in information systems. These event data can be analyzed through process mining techniques. For traditional process mining techniques, one has to associate each event with exactly one object, e.g., the company's customer. Events related to one object form an event sequence called a case. A case describes an end-to-end run through a process. The cases contained in event data can be used to discover a process model, detect frequent bottlenecks, or learn predictive models. However, events encountered in real-life information systems, e.g., ERP systems, can often be associated with multiple objects. The traditional sequential case concept falls short of these object-centric event data as these data exhibit a graph structure. One might force object-centric event data into the traditional case concept by flattening it. However, flattening manipulates the data and removes information. Therefore, a concept analogous to the case concept of traditional event logs is necessary to enable the application of different process mining tasks on object-centric event data. In this paper, we introduce the case concept for object-centric process mining: process executions. These are graph-based generalizations of cases as considered in traditional process mining. Furthermore, we provide techniques to extract process executions. Based on these executions, we determine equivalent process behavior with respect to an attribute using graph isomorphism. Equivalent process executions with respect to the event's activity are object-centric variants, i.e., a generalization of variants in traditional process mining. We provide a visualization technique for object-centric variants. The contribution's scalability and efficiency are extensively evaluated. Furthermore, we provide a case study showing the most frequent object-centric variants of a real-life event log. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.03235,August 2022,822,Unsung voices of technology in school education-findings using the constructivist grounded theory approach,"V. Deepa, R. Sujatha and Jitendra Mohan","Technology adoption for school education further gained momentum during the COVID-19 pandemic. However, the challenges and strategies of children belonging to the less privileged (we use ‘privileged’ in the ar...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-021-00182-7,4 January 2022,
823,0.000678989392134464,823,"Hybrid Multimodal Feature Extraction, Mining and Fusion for Sentiment Analysis","Jia Li, Ziyang Zhang, Junjie Lang, Yueqi Jiang, Liuwei An, Peng Zou, Yangyang Xu, Sheng Gao, Jie Lin, Chunxiao Fan, Xiao Sun, Meng Wang","In this paper, we present our solutions for the Multimodal Sentiment Analysis Challenge (MuSe) 2022, which includes MuSe-Humor, MuSe-Reaction and MuSe-Stress Sub-challenges. The MuSe 2022 focuses on humor detection, emotional reactions and multimodal emotional stress utilizing different modalities and data sets. In our work, different kinds of multimodal features are extracted, including acoustic, visual, text and biological features. These features are fused by TEMMA and GRU with self-attention mechanism frameworks. In this paper, 1) several new audio features, facial expression features and paragraph-level text embeddings are extracted for accuracy improvement. 2) we substantially improve the accuracy and reliability of multimodal sentiment prediction by mining and blending the multimodal features. 3) effective data augmentation strategies are applied in model training to alleviate the problem of sample imbalance and prevent the model from learning biased subject characters. For the MuSe-Humor sub-challenge, our model obtains the AUC score of 0.8932. For the MuSe-Reaction sub-challenge, the Pearson's Correlations Coefficient of our approach on the test set is 0.3879, which outperforms all other participants. For the MuSe-Stress sub-challenge, our approach outperforms the baseline in both arousal and valence on the test dataset, reaching a final combined result of 0.5151. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.03051,August 2022,823,Improved technique of personalised surgical guides generation for mandibular free flap reconstruction using an open-source tool,"Luka Šimić, Vjekoslav Kopačin, Ivan Mumlek, Josip Butković and Vedran Zubčić","With advancements in computer systems, computer graphics and medical imaging technologies, clinicians strive for a personalised approach to patient treatment. Therefore, the production of personalised surgical...",https://www.springeropen.com//eurradiolexp.springeropen.com/articles/10.1186/s41747-021-00229-x,28 July 2021,
824,0.000678989392134464,824,Twitter Attribute Classification with Q-Learning on Bitcoin Price Prediction,"Sattarov Otabek, Jaeyoung Choi","Aspiring to achieve an accurate Bitcoin price prediction based on people's opinions on Twitter usually requires millions of tweets, using different text mining techniques (preprocessing, tokenization, stemming, stop word removal), and developing a machine learning model to perform the prediction. These attempts lead to the employment of a significant amount of computer power, central processing unit (CPU) utilization, random-access memory (RAM) usage, and time. To address this issue, in this paper, we consider a classification of tweet attributes that effects on price changes and computer resource usage levels while obtaining an accurate price prediction. To classify tweet attributes having a high effect on price movement, we collect all Bitcoin-related tweets posted in a certain period and divide them into four categories based on the following tweet attributes: $(i)$ the number of followers of the tweet poster, $(ii)$ the number of comments on the tweet, $(iii)$ the number of likes, and $(iv)$ the number of retweets. We separately train and test by using the Q-learning model with the above four categorized sets of tweets and find the best accurate prediction among them. Especially, we design several reward functions to improve the prediction accuracy of the Q-leaning. We compare our approach with a classic approach where all Bitcoin-related tweets are used as input data for the model, by analyzing the CPU workloads, RAM usage, memory, time, and prediction accuracy. The results show that tweets posted by users with the most followers have the most influence on a future price, and their utilization leads to spending 80\% less time, 88.8\% less CPU consumption, and 12.5\% more accurate predictions compared with the classic approach. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.02610,August 2022,824,Editorial,"Vesa Välimäki, Augusto Sarti, Matti Karjalainen, Rudolf Rabenstein and Lauri Savioja",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002690,27 June 2004,
825,0.000678989392134464,825,TunaOil: A Tuning Algorithm Strategy for Reservoir Simulation Workloads,"Felipe Albuquerque Portella, David Buchaca Prats, José Roberto Pereira Rodrigues, Josep Lluís Berral","Reservoir simulations for petroleum fields and seismic imaging are known as the most demanding workloads for high-performance computing (HPC) in the oil and gas (O&G) industry. The optimization of the simulator numerical parameters plays a vital role as it could save considerable computational efforts. State-of-the-art optimization techniques are based on running numerous simulations, specific for that purpose, to find good parameter candidates. However, using such an approach is highly costly in terms of time and computing resources. This work presents TunaOil, a new methodology to enhance the search for optimal numerical parameters of reservoir flow simulations using a performance model. In the O&G industry, it is common to use ensembles of models in different workflows to reduce the uncertainty associated with forecasting O&G production. We leverage the runs of those ensembles in such workflows to extract information from each simulation and optimize the numerical parameters in their subsequent runs. To validate the methodology, we implemented it in a history matching (HM) process that uses a Kalman filter algorithm to adjust an ensemble of reservoir models to match the observed data from the real field. We mine past execution logs from many simulations with different numerical configurations and build a machine learning model based on extracted features from the data. These features include properties of the reservoir models themselves, such as the number of active cells, to statistics of the simulation's behavior, such as the number of iterations of the linear solver. A sampling technique is used to query the oracle to find the numerical parameters that can reduce the elapsed time without significantly impacting the quality of the results. Our experiments show that the predictions can improve the overall HM workflow runtime on average by 31%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.02606,August 2022,825,Development and validation of students’ digital competence scale (SDiCoS),"Katerina Tzafilkou, Maria Perifanou and A. A. Economides","Towards the transition to blended and remote education, evaluating the levels of students’ digital competence and designing educational programs to advance them is of paramount importance. Existing validated d...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-022-00330-0,16 May 2022,
826,0.000678989392134464,826,GROWN+UP: A Graph Representation Of a Webpage Network Utilizing Pre-training,"Benedict Yeoh, Huijuan Wang","Large pre-trained neural networks are ubiquitous and critical to the success of many downstream tasks in natural language processing and computer vision. However, within the field of web information retrieval, there is a stark contrast in the lack of similarly flexible and powerful pre-trained models that can properly parse webpages. Consequently, we believe that common machine learning tasks like content extraction and information mining from webpages have low-hanging gains that yet remain untapped. We aim to close the gap by introducing an agnostic deep graph neural network feature extractor that can ingest webpage structures, pre-train self-supervised on massive unlabeled data, and fine-tune to arbitrary tasks on webpages effectually. Finally, we show that our pre-trained model achieves state-of-the-art results using multiple datasets on two very different benchmarks: webpage boilerplate removal and genre classification, thus lending support to its potential application in diverse downstream tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.02252,August 2022,826,Meta-heuristic algorithms as tools for hydrological science,Do Guen Yoo and Joong Hoon Kim,"In this paper, meta-heuristic optimization techniques are introduced and their applications to water resources engineering, particularly in hydrological science are introduced. In recent years, meta-heuristic ...",https://www.springeropen.com//geoscienceletters.springeropen.com/articles/10.1186/2196-4092-1-4,6 March 2014,
827,0.000678989392134464,827,Quantum mean centering for block-encoding-based quantum algorithm,"Hai-Ling Liu, Chao-Hua Yu, Lin-Chun Wan, Su-Juan Qin, Fei Gao, Qiao-Yan Wen","Mean Centering (MC) is an important data preprocessing technique, which has a wide range of applications in data mining, machine learning, and multivariate statistical analysis. When the data set is large, this process will be time-consuming. In this paper, we propose an efficient quantum MC algorithm based on the block-encoding technique, which enables the existing quantum algorithms can get rid of the assumption that the original data set has been classically mean-centered. Specifically, we first adopt the strategy that MC can be achieved by multiplying by the centering matrix $C$, i.e., removing the row means, column means and row-column means of the original data matrix $X$ can be expressed as $XC$, $CX$ and $CXC$, respectively. This allows many classical problems involving MC, such as Principal Component Analysis (PCA), to directly solve the matrix algebra problems related to $XC$, $CX$ or $CXC$. Next, we can employ the block-encoding technique to realize MC. To achieve it, we first show how to construct the block-encoding of the centering matrix $C$, and then further obtain the block-encodings of $XC$, $CX$ and $CXC$. Finally, we describe one by one how to apply our MC algorithm to PCA and other algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.02143,August 2022,827,Secure Multiparty Computation between Distrusted Networks Terminals,S.-C. S. Cheung and Thinh Nguyen,One of the most important problems facing any distributed application over a heterogeneous network is the protection of private sensitive information in local terminals. A subfield of cryptography called secur...,https://www.springeropen.com//jis-eurasipjournals.springeropen.com/articles/10.1155/2007/51368,11 December 2007,
828,0.000678989392134464,828,Evaluating and improving social awareness of energy communities through semantic network analysis of online news,"C. Piselli, A. Fronzetti Colladon, L. Segneri, A. L. Pisello","The implementation of energy communities represents a cross-disciplinary phenomenon that has the potential to support the energy transition while fostering citizens' participation throughout the energy system and their exploitation of renewables. An important role is played by online information sources in engaging people in this process and increasing their awareness of associated benefits. In this view, this work analyses online news data on energy communities to understand people's awareness and the media importance of this topic. We use the Semantic Brand Score (SBS) indicator as an innovative measure of semantic importance, combining social network analysis and text mining methods. Results show different importance trends for energy communities and other energy and society-related topics, also allowing the identification of their connections. Our approach gives evidence to information gaps and possible actions that could be taken to promote a low-carbon energy transition. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01892,August 2022,828,Statistical Segmentation of Regions of Interest on a Mammographic Image,"Mouloud Adel, Monique Rasigni, Salah Bourennane and Valerie Juhan","This paper deals with segmentation of breast anatomical regions, pectoral muscle, fatty and fibroglandular regions, using a Bayesian approach. This work is a part of a computer aided diagnosis project aiming a...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/49482,1 December 2007,
829,0.000678989392134464,829,Quantifying Temporal Privacy Leakage in Continuous Event Data Publishing,"Majid Rafiei, Gamal Elkoumy, Wil M. P. van der Aalst","Process mining employs event data extracted from different types of information systems to discover and analyze actual processes. Event data often contain highly sensitive information about the people who carry out activities or the people for whom activities are performed. Therefore, privacy concerns in process mining are receiving increasing attention. To alleviate privacy-related risks, several privacy preservation techniques have been proposed. Differential privacy is one of these techniques which provides strong privacy guarantees. However, the proposed techniques presume that event data are released in only one shot, whereas business processes are continuously executed. Hence, event data are published repeatedly, resulting in additional risks. In this paper, we demonstrate that continuously released event data are not independent, and the correlation among different releases can result in privacy degradation when the same differential privacy mechanism is applied to each release. We quantify such privacy degradation in the form of temporal privacy leakages. We apply continuous event data publishing scenarios to real-life event logs to demonstrate privacy leakages. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01886,August 2022,829,"Secure, Redundant, and Fully Distributed Key Management Scheme for Mobile Ad Hoc Networks: An Analysis","Deepti Joshi, Kamesh Namuduri and Ravi Pendse","Security poses a major challenge in ad hoc networks today due to the lack of fixed or organizational infrastructure. This paper proposes a modification to the existing ""fully distributed certificate authority""...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/WCN.2005.579,8 September 2005,
830,0.000678989392134464,830,Robust Learning of Deep Time Series Anomaly Detection Models with Contaminated Training Data,"Wenkai Li, Cheng Feng, Ting Chen, Jun Zhu","Time series anomaly detection (TSAD) is an important data mining task with numerous applications in the IoT era. In recent years, a large number of deep neural network-based methods have been proposed, demonstrating significantly better performance than conventional methods on addressing challenging TSAD problems in a variety of areas. Nevertheless, these deep TSAD methods typically rely on a clean training dataset that is not polluted by anomalies to learn the ""normal profile"" of the underlying dynamics. This requirement is nontrivial since a clean dataset can hardly be provided in practice. Moreover, without the awareness of their robustness, blindly applying deep TSAD methods with potentially contaminated training data can possibly incur significant performance degradation in the detection phase. In this work, to tackle this important challenge, we firstly investigate the robustness of commonly used deep TSAD methods with contaminated training data which provides a guideline for applying these methods when the provided training data are not guaranteed to be anomaly-free. Furthermore, we propose a model-agnostic method which can effectively improve the robustness of learning mainstream deep TSAD models with potentially contaminated data. Experiment results show that our method can consistently prevent or mitigate performance degradation of mainstream deep TSAD models on widely used benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01841,August 2022,830,A systematic approach for structuring exception handling in robust component-based software,"Fernando Castor Filho, Paulo Asterio de C. Guerra, Vinicius Asta Pagano and Cecília Mary F. Rubira","Component-based development (CBD) is recognized today as the standard paradigm for structuring large software systems. However, the most popular component models and component-based development processes provi...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192362,October 2004,
831,0.000678989392134464,831,Detecting and Characterizing Propagation of Security Weaknesses in Puppet-based Infrastructure Management,"Akond Rahman, Chris Parnin","Despite being beneficial for managing computing infrastructure automatically, Puppet manifests are susceptible to security weaknesses, e.g., hard-coded secrets and use of weak cryptography algorithms. Adequate mitigation of security weaknesses in Puppet manifests is thus necessary to secure computing infrastructure that are managed with Puppet manifests. A characterization of how security weaknesses propagate and affect Puppet-based infrastructure management, can inform practitioners on the relevance of the detected security weaknesses, as well as help them take necessary actions for mitigation. To that end, we conduct an empirical study with 17,629 Puppet manifests mined from 336 open source repositories. We construct Taint Tracker for Puppet Manifests (TaintPup), for which we observe 2.4 times more precision compared to that of a state-of-the-art security static analysis tool. TaintPup leverages Puppet-specific information flow analysis using which we characterize propagation of security weaknesses. From our empirical study, we observe security weaknesses to propagate into 4,457 resources, i.e, Puppet-specific code elements used to manage infrastructure. A single instance of a security weakness can propagate into as many as 35 distinct resources. We observe security weaknesses to propagate into 7 categories of resources, which include resources used to manage continuous integration servers and network controllers. According to our survey with 24 practitioners, propagation of security weaknesses into data storage-related resources is rated to have the most severe impact for Puppet-based infrastructure management. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01242,August 2022,831,"Gender, interest, and prior experience shape opportunities to learn programming in robotics competitions","Eben B. Witherspoon, Christian D. Schunn, Ross M. Higashi and Emily C. Baehr","Robotics competitions are increasingly popular and potentially provide an on-ramp to computer science, which is currently highly gender imbalanced. However, within competitive robotics teams, student participa...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-016-0052-1,4 November 2016,
832,0.000678989392134464,832,VacciNet: Towards a Smart Framework for Learning the Distribution Chain Optimization of Vaccines for a Pandemic,"Jayeeta Mondal, Jeet Dutta, Hrishav Bakul Barua","Vaccinations against viruses have always been the need of the hour since long past. However, it is hard to efficiently distribute the vaccines (on time) to all the corners of a country, especially during a pandemic. Considering the vastness of the population, diversified communities, and demands of a smart society, it is an important task to optimize the vaccine distribution strategy in any country/state effectively. Although there is a profusion of data (Big Data) from various vaccine administration sites that can be mined to gain valuable insights about mass vaccination drives, very few attempts has been made towards revolutionizing the traditional mass vaccination campaigns to mitigate the socio-economic crises of pandemic afflicted countries. In this paper, we bridge this gap in studies and experimentation. We collect daily vaccination data which is publicly available and carefully analyze it to generate meaning-full insights and predictions. We put forward a novel framework leveraging Supervised Learning and Reinforcement Learning (RL) which we call VacciNet, that is capable of learning to predict the demand of vaccination in a state of a country as well as suggest optimal vaccine allocation in the state for minimum cost of procurement and supply. At the present, our framework is trained and tested with vaccination data of the USA. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01112,August 2022,832,Big and open linked data analytics: a study on changing roles and skills in the higher educational process,"Martin Lnenicka, Hana Kopackova, Renata Machova and Jitka Komarkova",The concept of openness and information sharing (linking) together with increasing amounts of data available significantly affect the current educational system. Institutions as well as other stakeholders are ...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00208-z,19 August 2020,
833,0.000678989392134464,833,ASTA: Learning Analytical Semantics over Tables for Intelligent Data Analysis and Visualization,"Lingbo Li, Tianle Li, Xinyi He, Mengyu Zhou, Shi Han, Dongmei Zhang","Intelligent analysis and visualization of tables use techniques to automatically recommend useful knowledge from data, thus freeing users from tedious multi-dimension data mining. While many studies have succeeded in automating recommendations through rules or machine learning, it is difficult to generalize expert knowledge and provide explainable recommendations. In this paper, we present the recommendation of conditional formatting for the first time, together with chart recommendation, to exemplify intelligent table analysis. We propose analytical semantics over tables to uncover common analysis pattern behind user-created analyses. Here, we design analytical semantics by separating data focus from user intent, which extract the user motivation from data and human perspective respectively. Furthermore, the ASTA framework is designed by us to apply analytical semantics to multiple automated recommendations. ASTA framework extracts data features by designing signatures based on expert knowledge, and enables data referencing at field- (chart) or cell-level (conditional formatting) with pre-trained models. Experiments show that our framework achieves recall at top 1 of 62.86% on public chart corpora, outperforming the best baseline about 14%, and achieves 72.31% on the collected corpus ConFormT, validating that ASTA framework is effective in providing accurate and explainable recommendations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.01043,August 2022,833,Fast Graph Partitioning Active Contours for Image Segmentation Using Histograms,Sumit K. Nath and Kannappan Palaniappan,"We present a method to improve the accuracy and speed, as well as significantly reduce the memory requirements, for the recently proposed Graph Partitioning Active Contours (GPACs) algorithm for image segmenta...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2009/820986,26 January 2010,
834,0.000678989392134464,834,MULTIPAR: Supervised Irregular Tensor Factorization with Multi-task Learning,"Yifei Ren, Jian Lou, Li Xiong, Joyce C Ho, Xiaoqian Jiang, Sivasubramanium Bhavani","Tensor factorization has received increasing interest due to its intrinsic ability to capture latent factors in multi-dimensional data with many applications such as recommender systems and Electronic Health Records (EHR) mining. PARAFAC2 and its variants have been proposed to address irregular tensors where one of the tensor modes is not aligned, e.g., different users in recommender systems or patients in EHRs may have different length of records. PARAFAC2 has been successfully applied on EHRs for extracting meaningful medical concepts (phenotypes). Despite recent advancements, current models' predictability and interpretability are not satisfactory, which limits its utility for downstream analysis. In this paper, we propose MULTIPAR: a supervised irregular tensor factorization with multi-task learning. MULTIPAR is flexible to incorporate both static (e.g. in-hospital mortality prediction) and continuous or dynamic (e.g. the need for ventilation) tasks. By supervising the tensor factorization with downstream prediction tasks and leveraging information from multiple related predictive tasks, MULTIPAR can yield not only more meaningful phenotypes but also better predictive performance for downstream tasks. We conduct extensive experiments on two real-world temporal EHR datasets to demonstrate that MULTIPAR is scalable and achieves better tensor fit with more meaningful subgroups and stronger predictive performance compared to existing state-of-the-art methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00993,August 2022,834,Comparison of Image Transform-Based Features for Visual Speech Recognition in Clean and Corrupted Videos,"Rowan Seymour, Darryl Stewart and Ji Ming",We present results of a study into the performance of a variety of different image transform-based feature types for speaker-independent visual speech recognition of isolated digits. This includes the first re...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/810362,17 December 2007,
835,0.000678989392134464,835,Large-Scale Product Retrieval with Weakly Supervised Representation Learning,"Xiao Han, Kam Woh Ng, Sauradip Nag, Zhiyu Qu","Large-scale weakly supervised product retrieval is a practically useful yet computationally challenging problem. This paper introduces a novel solution for the eBay Visual Search Challenge (eProduct) held at the Ninth Workshop on Fine-Grained Visual Categorisation workshop (FGVC9) of CVPR 2022. This competition presents two challenges: (a) E-commerce is a drastically fine-grained domain including many products with subtle visual differences; (b) A lacking of target instance-level labels for model training, with only coarse category labels and product titles available. To overcome these obstacles, we formulate a strong solution by a set of dedicated designs: (a) Instead of using text training data directly, we mine thousands of pseudo-attributes from product titles and use them as the ground truths for multi-label classification. (b) We incorporate several strong backbones with advanced training recipes for more discriminative representation learning. (c) We further introduce a number of post-processing techniques including whitening, re-ranking and model ensemble for retrieval enhancement. By achieving 71.53% MAR, our solution ""Involution King"" achieves the second position on the leaderboard. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00955,August 2022,835,"Robust, Real-Time 3D Face Tracking from a Monocular View","Wei-Kai Liao, Douglas Fidaleo and Gerard Medioni",This paper addresses the problem of 3D face tracking from a monocular view. Dominant tracking algorithms in current literature can be classified as intensity-based or feature-based methods. Intensity-based met...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2010/183605,14 October 2010,
836,0.000678989392134464,836,Data Collection and Analysis of French Dialects,"Omar Shaur Choudhry, Paul Omara Odida, Joshua Reiner, Keiron Appleyard, Danielle Kushnir, William Toon","This paper discusses creating and analysing a new dataset for data mining and text analytics research, contributing to a joint Leeds University research project for the Corpus of National Dialects. This report investigates machine learning classifiers to classify samples of French dialect text across various French-speaking countries. Following the steps of the CRISP-DM methodology, this report explores the data collection process, data quality issues and data conversion for text analysis. Finally, after applying suitable data mining techniques, the evaluation methods, best overall features and classifiers and conclusions are discussed. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00752,August 2022,836,The transfer of heritage modelling from research to practice,Jenny Richards and Peter Brimblecombe,"Heritage science is an inherently practice-oriented field that aims to support our understanding, and conservation, of heritage. Research is commonly undertaken using laboratory or field-based methodologies, b...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-022-00650-4,2 February 2022,
837,0.000678989392134464,837,RASIPAM: Interactive Pattern Mining of Multivariate Event Sequences in Racket Sports,"Jiang Wu, Dongyu Liu, Ziyang Guo, Yingcai Wu","Experts in racket sports like tennis and badminton use tactical analysis to gain insight into competitors' playing styles. Many data-driven methods apply pattern mining to racket sports data -- which is often recorded as multivariate event sequences -- to uncover sports tactics. However, tactics obtained in this way are often inconsistent with those deduced by experts through their domain knowledge, which can be confusing to those experts. This work introduces RASIPAM, a RAcket-Sports Interactive PAttern Mining system, which allows experts to incorporate their knowledge into data mining algorithms to discover meaningful tactics interactively. RASIPAM consists of a constraint-based pattern mining algorithm that responds to the analysis demands of experts: Experts provide suggestions for finding tactics in intuitive written language, and these suggestions are translated into constraints to run the algorithm. RASIPAM further introduces a tailored visual interface that allows experts to compare the new tactics with the original ones and decide whether to apply a given adjustment. This interactive workflow iteratively progresses until experts are satisfied with all tactics. We conduct a quantitative experiment to show that our algorithm supports real-time interaction. Two case studies in tennis and in badminton respectively, each involving two domain experts, are conducted to show the effectiveness and usefulness of the system. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00671,August 2022,837,Based on Color Constancy Estimation Based on Color Constancy Estimation,Anustup Choudhury and Gérard Medioni,We address the problem of contrast enhancement for color images. Our method to enhance images is inspired from the retinex theory. We try to estimate the illumination and separate it from the reflectance compo...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2010/837237,11 October 2010,
838,0.000678989392134464,838,Safer Traffic Recovery from the Pandemic in London -- Spatiotemporal Data Mining of Car Crashes,"Kejiang Qian, Yijing Li","In the aim to support London's safer recovery from the pandemic by improving road safety intelligently, this study investigated the spatiotemporal patterns of age-involved car crashes and affecting factors, upon answering two main research questions: (1)""What are the spatial and temporal patterns of car crashes as well as their changes in two typical years, 2019 and 2020, in London, and how the influential factors work?""; (2)""What are the spatiotemporal patterns of casualty by age groups, and how people's daily activities affect the patterns pre- and para- the pandemic""? Three approaches, i.e., spatial analysis (network Kernel Density Estimation, NetKDE), factor analysis, and spatiotemporal data mining (tensor decomposition), had been implemented to identify the temporal patterns of car crashes on weekly and daily basis respectively, detect the crashes' hot spots, and to gain better understanding the effect from citizens' daily activity on crashes' patterns pre- and para- the pandemic. It had been found from the study that car crashes mainly clustered in the central part of London, especially busier areas around denser hubs of point-of-interest (POIs); the POIs, as a reflector for citizens' daily activities and travel behaviours, can be of help to gain a better understanding of the crashes' patterns, upon further assessment on interactions through the geographical detector; the crashes' casualty patterns varied by age group, with distinctive relationships between POIs and crashes' pattern for corresponding age group categorised. In all, the paper provided an in-depth exploratory analysis of car crashes and their casualty patterns in London to facilitate deployment policies towards post-pandemic safer recovery upon COVID-19. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00548,August 2022,838,"Olfactory-colour crossmodal correspondences in art, science, and design",Charles Spence,The last few years have seen a rapid growth of interest amongst researchers in the crossmodal correspondences. One of the correspondences that has long intrigued artists is the putative association between col...,https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-020-00246-1,28 October 2020,
839,0.000678989392134464,839,Intelligent decision-making method of TBM operating parameters based on multiple constraints and objective optimization,"Bin Liu, Jiwen Wang, Ruirui Wang, Yaxu Wang, Guangzu Zhao","The decision-making of TBM operating parameters has an important guiding significance for TBM safe and efficient construction, and it has been one of the research hotpots in the field of TBM tunneling. For this purpose, this paper introduces rock-breaking rules into machine learning method, and a rock-machine mapping dual-driven by physical-rule and data-mining is established with high accuracy. This dual-driven mappings are subsequently used as objective function and constraints to build a decision-making method for TBM operating parameters. By searching the revolution per minute and penetration corresponding to the extremum of the objective function subject to the constraints, the optimal operating parameters can be obtained. This method is verified in the field of the Second Water Source Channel of Hangzhou, China, resulting in the average penetration rate increased by 11.3%, and the total cost decreased by 10.0%, which proves the practicability and effectiveness of the developed decision-making model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00404,August 2022,839,Virtual experimental platforms in chemistry laboratory education and its impact on experimental self-efficacy,"Vysakh Kani Kolil, Sharanya Muthupalani and Krishnashree Achuthan","Self-efficacy is an important determinant in successfully attempting a task. In the area of education, self-efficacy plays a crucial role in causing behavioral changes, resulting in enhanced performance over t...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00204-3,9 July 2020,
840,0.000678989392134464,840,Improving Distantly Supervised Relation Extraction by Natural Language Inference,"Kang Zhou, Qiao Qiao, Yuepei Li, Qi Li","To reduce human annotations for relation extraction (RE) tasks, distantly supervised approaches have been proposed, while struggling with low performance. In this work, we propose a novel DSRE-NLI framework, which considers both distant supervision from existing knowledge bases and indirect supervision from pretrained language models for other tasks. DSRE-NLI energizes an off-the-shelf natural language inference (NLI) engine with a semi-automatic relation verbalization (SARV) mechanism to provide indirect supervision and further consolidates the distant annotations to benefit multi-classification RE models. The NLI-based indirect supervision acquires only one relation verbalization template from humans as a semantically general template for each relationship, and then the template set is enriched by high-quality textual patterns automatically mined from the distantly annotated corpus. With two simple and effective data consolidation strategies, the quality of training data is substantially improved. Extensive experiments demonstrate that the proposed framework significantly improves the SOTA performance (up to 7.73\% of F1) on distantly supervised RE benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00346,August 2022,840,Robust Fusion of Irregularly Sampled Data Using Adaptive Normalized Convolution,"Tuan Q. Pham, Lucas J. van Vliet and Klamer Schutte","We present a novel algorithm for image fusion from irregularly sampled data. The method is based on the framework of normalized convolution (NC), in which the local signal is approximated through a projection ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/83268,1 December 2006,
841,0.000678989392134464,841,Doubly Deformable Aggregation of Covariance Matrices for Few-shot Segmentation,"Zhitong Xiong, Haopeng Li, Xiao Xiang Zhu","Training semantic segmentation models with few annotated samples has great potential in various real-world applications. For the few-shot segmentation task, the main challenge is how to accurately measure the semantic correspondence between the support and query samples with limited training data. To address this problem, we propose to aggregate the learnable covariance matrices with a deformable 4D Transformer to effectively predict the segmentation map. Specifically, in this work, we first devise a novel hard example mining mechanism to learn covariance kernels for the Gaussian process. The learned covariance kernel functions have great advantages over existing cosine similarity-based methods in correspondence measurement. Based on the learned covariance kernels, an efficient doubly deformable 4D Transformer module is designed to adaptively aggregate feature similarity maps into segmentation results. By combining these two designs, the proposed method can not only set new state-of-the-art performance on public benchmarks, but also converge extremely faster than existing methods. Experiments on three public datasets have demonstrated the effectiveness of our method. △ Less",https://arxiv.orghttps://arxiv.org/abs/2208.00306,August 2022,841,Theoretical background of innovation in services in small and medium-sized enterprises: literature mapping,"Saymon Ricardo de Oliveira Sousa, Wesley Vieira da Silva, Claudimar Pereira da Veiga and Roselaine Ruviaro Zanini","The aim of this study was to map the innovation of services in small and medium-sized enterprises, as reported on the Web of Science and SCOPUS databases, using a structured review, involving 121 papers publis...",https://www.springeropen.com//innovation-entrepreneurship.springeropen.com/articles/10.1186/s13731-020-00135-3,22 September 2020,
842,0.000678989392134464,842,A Data-driven Latent Semantic Analysis for Automatic Text Summarization using LDA Topic Modelling,"Daniel F. O. Onah, Elaine L. L. Pang, Mahmoud El-Haj","With the advent and popularity of big data mining and huge text analysis in modern times, automated text summarization became prominent for extracting and retrieving important information from documents. This research investigates aspects of automatic text summarization from the perspectives of single and multiple documents. Summarization is a task of condensing huge text articles into short, summarized versions. The text is reduced in size for summarization purpose but preserving key vital information and retaining the meaning of the original document. This study presents the Latent Dirichlet Allocation (LDA) approach used to perform topic modelling from summarised medical science journal articles with topics related to genes and diseases. In this study, PyLDAvis web-based interactive visualization tool was used to visualise the selected topics. The visualisation provides an overarching view of the main topics while allowing and attributing deep meaning to the prevalence individual topic. This study presents a novel approach to summarization of single and multiple documents. The results suggest the terms ranked purely by considering their probability of the topic prevalence within the processed document using extractive summarization technique. PyLDAvis visualization describes the flexibility of exploring the terms of the topics' association to the fitted LDA model. The topic modelling result shows prevalence within topics 1 and 2. This association reveals that there is similarity between the terms in topic 1 and 2 in this study. The efficacy of the LDA and the extractive summarization methods were measured using Latent Semantic Analysis (LSA) and Recall-Oriented Understudy for Gisting Evaluation (ROUGE) metrics to evaluate the reliability and validity of the model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.14687,July 2022,842,Robust Real-Time 3D Object Tracking with Interfering Background Visual Projections,Huan Jin and Gang Qian,This paper presents a robust real-time object tracking system for human computer interaction in mediated environments with interfering visual projection in the background. Two major contributions are made in o...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/638073,9 June 2008,
843,0.000678989392134464,843,Contrastive Pre-training of Spatial-Temporal Trajectory Embeddings,"Yan Lin, Huaiyu Wan, Shengnan Guo, Youfang Lin","Pre-training trajectory embeddings is a fundamental and critical procedure in spatial-temporal trajectory mining, and is beneficial for a wide range of downstream tasks. The key for generating effective trajectory embeddings is to extract high-level travel semantics from trajectories, including movement patterns and travel purposes, with consideration of the trajectories' long-term spatial-temporal correlations. Despite the existing efforts, there are still major challenges in pre-training trajectory embeddings. First, commonly used generative pretext tasks are not suitable for extracting high-level semantics from trajectories. Second, existing data augmentation methods fit badly on trajectory datasets. Third, current encoder designs fail to fully incorporate long-term spatial-temporal correlations hidden in trajectories. To tackle these challenges, we propose a novel Contrastive Spatial-Temporal Trajectory Embedding (CSTTE) model for learning comprehensive trajectory embeddings. CSTTE adopts the contrastive learning framework so that its pretext task is robust to noise. A specially designed data augmentation method for trajectories is coupled with the contrastive pretext task to preserve the high-level travel semantics. We also build an efficient spatial-temporal trajectory encoder to efficiently and comprehensively model the long-term spatial-temporal correlations in trajectories. Extensive experiments on two downstream tasks and three real-world datasets prove the superiority of our model compared with the existing trajectory embedding methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.14539,July 2022,843,Graph theory and algorithms,Celina M. H. de Figueiredo and Jayme L. Szwarcfiter,Unknown,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-012-0068-4,16 March 2012,
844,0.000678989392134464,844,Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome,"Tiffany J. Callahan, Adrianne L. Stefanski, Jin-Dong Kim, William A. Baumgartner Jr., Jordan M. Wyrwa, Lawrence E. Hunter","Preeclampsia is a leading cause of maternal and fetal morbidity and mortality. Currently, the only definitive treatment of preeclampsia is delivery of the placenta, which is central to the pathogenesis of the disease. Transcriptional profiling of human placenta from pregnancies complicated by preeclampsia has been extensively performed to identify differentially expressed genes (DEGs). The decisions to investigate DEGs experimentally are biased by many factors, causing many DEGs to remain uninvestigated. A set of DEGs which are associated with a disease experimentally, but which have no known association to the disease in the literature are known as the ignorome. Preeclampsia has an extensive body of scientific literature, a large pool of DEG data, and only one definitive treatment. Tools facilitating knowledge-based analyses, which are capable of combining disparate data from many sources in order to suggest underlying mechanisms of action, may be a valuable resource to support discovery and improve our understanding of this disease. In this work we demonstrate how a biomedical knowledge graph (KG) can be used to identify novel preeclampsia molecular mechanisms. Existing open source biomedical resources and publicly available high-throughput transcriptional profiling data were used to identify and annotate the function of currently uninvestigated preeclampsia-associated DEGs. Experimentally investigated genes associated with preeclampsia were identified from PubMed abstracts using text-mining methodologies. The relative complement of the text-mined- and meta-analysis-derived lists were identified as the uninvestigated preeclampsia-associated DEGs (n=445), i.e., the preeclampsia ignorome. Using the KG to investigate relevant DEGs revealed 53 novel clinically relevant and biologically actionable mechanistic associations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.14294,July 2022,844,Rendering-Oriented Decoding for a Distributed Multiview Coding System Using a Coset Code,Yuichi Taguchi and Takeshi Naemura,This paper discusses a system in which multiview images are captured and encoded in a distributed fashion and a viewer synthesizes a novel image from this data. We present an efficient method for such a system...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2009/251081,22 April 2009,
845,0.000678989392134464,845,Unsupervised Frequent Pattern Mining for CEP,"Guy Shapira, Assaf Schuster","Complex Event Processing (CEP) is a set of methods that allow efficient knowledge extraction from massive data streams using complex and highly descriptive patterns. Numerous applications, such as online finance, healthcare monitoring and fraud detection use CEP technologies to capture critical alerts, potential threats, or vital notifications in real time. As of today, in many fields, patterns are manually defined by human experts. However, desired patterns often contain convoluted relations that are difficult for humans to detect, and human expertise is scarce in many domains. We present REDEEMER (REinforcement baseD cEp pattErn MinER), a novel reinforcement and active learning approach aimed at mining CEP patterns that allow expansion of the knowledge extracted while reducing the human effort required. This approach includes a novel policy gradient method for vast multivariate spaces and a new way to combine reinforcement and active learning for CEP rule learning while minimizing the number of labels needed for training. REDEEMER aims to enable CEP integration in domains that could not utilize it before. To the best of our knowledge, REDEEMER is the first system that suggests new CEP rules that were not observed beforehand, and is the first method aimed for increasing pattern knowledge in fields where experts do not possess sufficient information required for CEP tools. Our experiments on diverse data-sets demonstrate that REDEEMER is able to extend pattern knowledge while outperforming several state-of-the-art reinforcement learning methods for pattern mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.14017,July 2022,845,Are the Wavelet Transforms the Best Filter Banks for Image Compression?,Ilangko Balasingham and Tor A. Ramstad,"Maximum regular wavelet filter banks have received much attention in the literature, and it is a general conception that they enjoy some type of optimality for image coding purposes. To investigate this claim,...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/287197,15 January 2008,
846,0.000678989392134464,846,Real Image Restoration via Structure-preserving Complementarity Attention,"Yuanfan Zhang, Gen Li, Lei Sun","Since convolutional neural networks perform well in learning generalizable image priors from large-scale data, these models have been widely used in image denoising tasks. However, the computational complexity increases dramatically as well on complex model. In this paper, We propose a novel lightweight Complementary Attention Module, which includes a density module and a sparse module, which can cooperatively mine dense and sparse features for feature complementary learning to build an efficient lightweight architecture. Moreover, to reduce the loss of details caused by denoising, this paper constructs a gradient-based structure-preserving branch. We utilize gradient-based branches to obtain additional structural priors for denoising, and make the model pay more attention to image geometric details through gradient loss optimization.Based on the above, we propose an efficiently Unet structured network with dual branch, the visual results show that can effectively preserve the structural details of the original image, we evaluate benchmarks including SIDD and DND, where SCANet achieves state-of-the-art performance in PSNR and SSIM while significantly reducing computational cost. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.13879,July 2022,846,Colour Vision Model-Based Approach for Segmentation of Traffic Signs,"Xiaohong Gao, Kunbin Hong, Peter Passmore, Lubov Podladchikova and Dmitry Shaposhnikov","This paper presents a new approach to segment traffic signs from the rest of a scene via CIECAM, a colour appearance model. This approach not only takes CIECAM into practical application for the first time sin...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/386705,13 December 2007,
847,0.00514534320972017,847,Challenges and Opportunities of Computational Social Science for Official Statistics,"Serena Signorelli, Matteo Fontana, Lorenzo Gabrielli, Michele Vespe","The vast amount of data produced everyday (so-called 'digital traces') and available nowadays represent a gold mine for the social sciences, especially in a computational context, that allows to fully extract their informational and knowledge value. In the latest years, statistical offices have made efforts to profit from harnessing the potential offered by these new sources of data, with promising results. But how difficult is this integration process? What are the challenges that statistical offices would likely face to profit from new data sources and analytical methods? This chapter will start by setting the scene of the current official statistics system, with a focus on its fundamental principles and dimensions relevant to the use of non-traditional data. It will then present some experiments and proofs of concept in the context of data innovation for official statistics, followed by a discussion on prospective challenges related to sustainable data access, new technical and methodological approaches and effective use of new sources of data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.13508,July 2022,847,Research and trends in STEM education: a systematic analysis of publicly funded projects,"Yeping Li, Ke Wang, Yu Xiao, Jeffrey E. Froyd and Sandra B. Nite","Taking publicly funded projects in STEM education as a special lens, we aimed to learn about research and trends in STEM education. We identified a total of 127 projects funded by the Institute of Education Sc...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-020-00213-8,22 April 2020,
848,0.000678989392134464,848,Partial Selfish Mining for More Profits,"Jiaping Yu, Shang Gao, Rui Song, Zhiping Cai, Bin Xiao","Mining attacks aim to gain an unfair share of extra rewards in the blockchain mining. Selfish mining can preserve discovered blocks and strategically release them, wasting honest miners' computing resources and getting higher profits. Previous mining attacks either conceal the mined whole blocks (hiding or discarding), or release them completely in a particular time slot (e.g., causing a fork). In this paper, we extend the mining attack's strategy space to partial block sharing, and propose a new and feasible Partial Selfish Mining (PSM) attack. We show that by releasing partial block data publicly and attracting rational miners to work on attacker's private branch, attackers and these attracted miners can gain an unfair share of mining rewards. We then propose Advanced PSM (A-PSM) attack that can further improve attackers' profits to be no less than the selfish mining. Both theoretical and experimental results show that PSM attackers can be more profitable than selfish miners under a certain range of mining power and network conditions. A-PSM attackers can gain even higher profits than both selfish mining and honest mining with attracted rational miners. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.13478,July 2022,848,Designing BEE: A Hardware Emulation Engine for Signal Processing in Low-Power Wireless Applications,"Kimmo Kuusilinna, Chen Chang, M. Josephine Ammer, Brian C. Richards and Robert W. Brodersen",This paper describes the design of a large-scale emulation engine and an application example from the field of low-power wireless devices. The primary goal of the emulator is to support design space exploratio...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703212154,20 May 2003,
849,0.000678989392134464,849,Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining,"Soumen Basu, Somanshu Singla, Mayank Gupta, Pratyaksha Rana, Pankaj Gupta, Chetan Arora","Rich temporal information and variations in viewpoints make video data an attractive choice for learning image representations using unsupervised contrastive learning (UCL) techniques. State-of-the-art (SOTA) contrastive learning techniques consider frames within a video as positives in the embedding space, whereas the frames from other videos are considered negatives. We observe that unlike multiple views of an object in natural scene videos, an Ultrasound (US) video captures different 2D slices of an organ. Hence, there is almost no similarity between the temporally distant frames of even the same US video. In this paper we propose to instead utilize such frames as hard negatives. We advocate mining both intra-video and cross-video negatives in a hardness-sensitive negative mining curriculum in a UCL framework to learn rich image representations. We deploy our framework to learn the representations of Gallbladder (GB) malignancy from US videos. We also construct the first large-scale US video dataset containing 64 videos and 15,800 frames for learning GB representations. We show that the standard ResNet50 backbone trained with our framework improves the accuracy of models pretrained with SOTA UCL techniques as well as supervised pretrained models on ImageNet for the GB malignancy detection task by 2-6%. We further validate the generalizability of our method on a publicly available lung US image dataset of COVID-19 pathologies and show an improvement of 1.5% compared to SOTA. Source code, dataset, and models are available at https://gbc-iitd.github.io/usucl. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.13148,July 2022,849,Rapid VLIW Processor Customization for Signal Processing Applications Using Combinational Hardware Functions,"Raymond R. Hoare, Alex K. Jones, Dara Kusic, Joshua Fazekas, John Foster, Shenchih Tung and Michael McCloud",This paper presents an architecture that combines VLIW (very long instruction word) processing with the capability to introduce application-specific customized instructions and highly parallel combinational ha...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/46472,1 December 2006,
850,0.000678989392134464,850,An Explainable Decision Support System for Predictive Process Analytics,"Riccardo Galanti, Massimiliano de Leoni, Merylin Monaro, Nicolò Navarin, Alan Marazzi, Brigida Di Stasi, Stéphanie Maldera","Predictive Process Analytics is becoming an essential aid for organizations, providing online operational support of their processes. However, process stakeholders need to be provided with an explanation of the reasons why a given process execution is predicted to behave in a certain way. Otherwise, they will be unlikely to trust the predictive monitoring technology and, hence, adopt it. This paper proposes a predictive analytics framework that is also equipped with explanation capabilities based on the game theory of Shapley Values. The framework has been implemented in the IBM Process Mining suite and commercialized for business users. The framework has been tested on real-life event data to assess the quality of the predictions and the corresponding evaluations. In particular, a user evaluation has been performed in order to understand if the explanations provided by the system were intelligible to process stakeholders. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12782,July 2022,850,Face morphing attacks: Investigating detection with humans and computers,"Robin S. S. Kramer, Michael O. Mireku, Tessa R. Flack and Kay L. Ritchie","In recent years, fraudsters have begun to use readily accessible digital manipulation techniques in order to carry out face morphing attacks. By submitting a morph image (a 50/50 average of two people’s faces)...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-019-0181-4,29 July 2019,
851,0.000678989392134464,851,Clustering Object-Centric Event Logs,"Anahita Farhang Ghahfarokhi, Fatemeh Akoochekian, Fareed Zandkarimi, Wil M. P. van der Aalst","Process mining provides various algorithms to analyze process executions based on event data. Process discovery, the most prominent category of process mining techniques, aims to discover process models from event logs, however, it leads to spaghetti models when working with real-life data. Therefore, several clustering techniques have been proposed on top of traditional event logs (i.e., event logs with a single case notion) to reduce the complexity of process models and discover homogeneous subsets of cases. Nevertheless, in real-life processes, particularly in the context of Business-to-Business (B2B) processes, multiple objects are involved in a process. Recently, Object-Centric Event Logs (OCELs) have been introduced to capture the information of such processes, and several process discovery techniques have been developed on top of OCELs. Yet, the output of the proposed discovery techniques on real OCELs leads to more informative but also more complex models. In this paper, we propose a clustering-based approach to cluster similar objects in OCELs to simplify the obtained process models. Using a case study of a real B2B process, we demonstrate that our approach reduces the complexity of the process models and generates coherent subsets of objects which help the end-users gain insights into the process. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12764,July 2022,851,Systematic review with radiomics quality score of cholangiocarcinoma: an EuSoMII Radiomics Auditing Group Initiative,"Roberto Cannella, Federica Vernuccio, Michail E. Klontzas, Andrea Ponsiglione, Ekaterina Petrash, Lorenzo Ugga, Daniel Pinto dos Santos and Renato Cuocolo",To systematically review current research applications of radiomics in patients with cholangiocarcinoma and to assess the quality of CT and MRI radiomics studies.,https://www.springeropen.com//insightsimaging.springeropen.com/articles/10.1186/s13244-023-01365-1,1 February 2023,
852,0.000678989392134464,852,Active Learning of Ordinal Embeddings: A User Study on Football Data,"Christoffer Loeffler, Kion Fallah, Stefano Fenu, Dario Zanca, Bjoern Eskofier, Christopher John Rozell, Christopher Mutschler","Humans innately measure distance between instances in an unlabeled dataset using an unknown similarity function. Distance metrics can only serve as proxy for similarity in information retrieval of similar instances. Learning a good similarity function from human annotations improves the quality of retrievals. This work uses deep metric learning to learn these user-defined similarity functions from few annotations for a large football trajectory dataset. We adapt an entropy-based active learning method with recent work from triplet mining to collect easy-to-answer but still informative annotations from human participants and use them to train a deep convolutional network that generalizes to unseen samples. Our user study shows that our approach improves the quality of the information retrieval compared to a previous deep metric learning approach that relies on a Siamese network. Specifically, we shed light on the strengths and weaknesses of passive sampling heuristics and active learners alike by analyzing the participants' response efficacy. To this end, we collect accuracy, algorithmic time complexity, the participants' fatigue and time-to-response, qualitative self-assessment and statements, as well as the effects of mixed-expertise annotators and their consistency on model performance and transfer-learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12710,July 2022,852,Recommender systems to support learners’ Agency in a Learning Context: a systematic review,Michelle Deschênes,"Recommender systems for technology-enhanced learning are examined in relation to learners’ agency, that is, their ability to define and pursue learning goals. These systems make it easier for learners to acces...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00219-w,21 October 2020,
853,0.000678989392134464,853,Graph Neural Network and Spatiotemporal Transformer Attention for 3D Video Object Detection from Point Clouds,"Junbo Yin, Jianbing Shen, Xin Gao, David Crandall, Ruigang Yang","Previous works for LiDAR-based 3D object detection mainly focus on the single-frame paradigm. In this paper, we propose to detect 3D objects by exploiting temporal information in multiple frames, i.e., the point cloud videos. We empirically categorize the temporal information into short-term and long-term patterns. To encode the short-term data, we present a Grid Message Passing Network (GMPNet), which considers each grid (i.e., the grouped points) as a node and constructs a k-NN graph with the neighbor grids. To update features for a grid, GMPNet iteratively collects information from its neighbors, thus mining the motion cues in grids from nearby frames. To further aggregate the long-term frames, we propose an Attentive Spatiotemporal Transformer GRU (AST-GRU), which contains a Spatial Transformer Attention (STA) module and a Temporal Transformer Attention (TTA) module. STA and TTA enhance the vanilla GRU to focus on small objects and better align the moving objects. Our overall framework supports both online and offline video object detection in point clouds. We implement our algorithm based on prevalent anchor-based and anchor-free detectors. The evaluation results on the challenging nuScenes benchmark show the superior performance of our method, achieving the 1st on the leaderboard without any bells and whistles, by the time the paper is submitted. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12659,July 2022,853,Correction: Adaptive optical quantitative phase imaging based on annular illumination Fourier ptychographic microscopy,"Yefeng Shu, Jiasong Sun, Jiaming Lyu, Yao Fan, Ning Zhou, Ran Ye, Guoan Zheng, Qian Chen and Chao Zuo",Theoriginal articlewas published inPhotoniX20223:24,https://www.springeropen.com//photoniX.springeropen.com/articles/10.1186/s43074-022-00073-1,14 November 2022,
854,0.000678989392134464,854,Graph Querying for Semantic Annotations,"Maxime Amblard, Bruno Guillaume, Siyana Pavlova, Guy Perrier","This paper presents how the online tool GREW-MATCH can be used to make queries and visualise data from existing semantically annotated corpora. A dedicated syntax is available to construct simple to complex queries and execute them against a corpus. Such queries give transverse views of the annotated data, these views can help for checking the consistency of annotations in one corpus or across several corpora. GREW-MATCH can then be seen as an error mining tool: when inconsistencies are detected, it helps finding the sentences which should be fixed. Finally, GREW-MATCH can also be used as a side tool to assist annotation tasks helping to find annotation examples in existing corpora to be compared to the data to be annotated. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12166,July 2022,854,"Logarithmic Adaptive Neighborhood Image Processing (LANIP): Introduction, Connections to Human Brightness Perception, and Application Issues",J-C Pinoli and J Debayle,"A new framework for image representation, processing, and analysis is introduced and exposed through practical applications. The proposed approach is called logarithmic adaptive neighborhood image processing (...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/36105,1 December 2006,
855,0.000678989392134464,855,A Reference Data Model for Process-Related User Interaction Logs,"Luka Abb, Jana-Rebecca Rehse","User interaction (UI) logs are high-resolution event logs that record low-level activities performed by a user during the execution of a task in an information system. Each event in a UI log corresponds to a single interaction between the user and the interface, such as clicking a button or entering a string into a text field. UI logs are used for purposes like task mining or robotic process automation (RPA), but each study and tool relies on a different conceptualization and implementation of the elements and attributes that constitute user interactions. This lack of standardization makes it difficult to integrate UI logs from different sources and to combine tools for UI data collection with downstream analytics or automation solutions. To address this, we propose a universally applicable reference data model for process-related UI logs. Based on a review of scientific literature and industry solutions, this model includes the core attributes of UI logs, but remains flexible with regard to the scope, level of abstraction, and case notion. We provide an implementation of the model as an extension to the XES interchange standard for event logs and demonstrate its practical applicability in a real-life RPA scenario. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.12054,July 2022,855,PIAAC: a new design for a new era,Irwin Kirsch and Mary Louise Lennon,"As the largest and most innovative international assessment of adults, PIAAC marks an inflection point in the evolution of large-scale comparative assessments. PIAAC grew from the foundation laid by surveys th...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-017-0046-6,18 April 2017,
856,0.000678989392134464,856,One-off Negative Sequential Pattern Mining,"Youxi Wu, Mingjie Chen, Yan Li, Jing Liu, Zhao Li, Jinyan Li, Xindong Wu","Negative sequential pattern mining (SPM) is an important SPM research topic. Unlike positive SPM, negative SPM can discover events that should have occurred but have not occurred, and it can be used for financial risk management and fraud detection. However, existing methods generally ignore the repetitions of the pattern and do not consider gap constraints, which can lead to mining results containing a large number of patterns that users are not interested in. To solve this problem, this paper discovers frequent one-off negative sequential patterns (ONPs). This problem has the following two characteristics. First, the support is calculated under the one-off condition, which means that any character in the sequence can only be used once at most. Second, the gap constraint can be given by the user. To efficiently mine patterns, this paper proposes the ONP-Miner algorithm, which employs depth-first and backtracking strategies to calculate the support. Therefore, ONP-Miner can effectively avoid creating redundant nodes and parent-child relationships. Moreover, to effectively reduce the number of candidate patterns, ONP-Miner uses pattern join and pruning strategies to generate and further prune the candidate patterns, respectively. Experimental results show that ONP-Miner not only improves the mining efficiency, but also has better mining performance than the state-of-the-art algorithms. More importantly, ONP mining can find more interesting patterns in traffic volume data to predict future traffic. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11950,July 2022,856,Emergent categorization in the recognition of black and white paintings through conditional discrimination,"Paulo Roberto dos Santos Ferreira, Diana Rasteli Santos, Waldir Monteiro Sampaio, Antonio Carlos Leme Jr. and Felipe Maciel dos Santos Souza","The emergent categorization involving paintings by renowned painters and their corresponding names was demonstrated by previous studies. However, the results of these studies suggest that the colors of the pic...",https://www.springeropen.com//prc.springeropen.com/articles/10.1186/s41155-021-00191-y,30 July 2021,
857,0.000678989392134464,857,Spatial-Temporal Federated Learning for Lifelong Person Re-identification on Distributed Edges,"Lei Zhang, Guanyu Gao, Huaizheng Zhang","Data drift is a thorny challenge when deploying person re-identification (ReID) models into real-world devices, where the data distribution is significantly different from that of the training environment and keeps changing. To tackle this issue, we propose a federated spatial-temporal incremental learning approach, named FedSTIL, which leverages both lifelong learning and federated learning to continuously optimize models deployed on many distributed edge clients. Unlike previous efforts, FedSTIL aims to mine spatial-temporal correlations among the knowledge learnt from different edge clients. Specifically, the edge clients first periodically extract general representations of drifted data to optimize their local models. Then, the learnt knowledge from edge clients will be aggregated by centralized parameter server, where the knowledge will be selectively and attentively distilled from spatial- and temporal-dimension with carefully designed mechanisms. Finally, the distilled informative spatial-temporal knowledge will be sent back to correlated edge clients to further improve the recognition accuracy of each edge client with a lifelong learning method. Extensive experiments on a mixture of five real-world datasets demonstrate that our method outperforms others by nearly 4% in Rank-1 accuracy, while reducing communication cost by 62%. All implementation codes are publicly available on https://github.com/MSNLAB/Federated-Lifelong-Person-ReID △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11759,July 2022,857,"Gender, interest, and prior experience shape opportunities to learn programming in robotics competitions","Eben B. Witherspoon, Christian D. Schunn, Ross M. Higashi and Emily C. Baehr","Robotics competitions are increasingly popular and potentially provide an on-ramp to computer science, which is currently highly gender imbalanced. However, within competitive robotics teams, student participa...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-016-0052-1,4 November 2016,
858,0.000678989392134464,858,Explored An Effective Methodology for Fine-Grained Snake Recognition,"Yong Huang, Aderon Huang, Wei Zhu, Yanming Fang, Jinghua Feng","Fine-Grained Visual Classification (FGVC) is a longstanding and fundamental problem in computer vision and pattern recognition, and underpins a diverse set of real-world applications. This paper describes our contribution at SnakeCLEF2022 with FGVC. Firstly, we design a strong multimodal backbone to utilize various meta-information to assist in fine-grained identification. Secondly, we provide new loss functions to solve the long tail distribution with dataset. Then, in order to take full advantage of unlabeled datasets, we use self-supervised learning and supervised learning joint training to provide pre-trained model. Moreover, some effective data process tricks also are considered in our experiments. Last but not least, fine-tuned in downstream task with hard mining, ensambled kinds of model performance. Extensive experiments demonstrate that our method can effectively improve the performance of fine-grained recognition. Our method can achieve a macro f1 score 92.7% and 89.4% on private and public dataset, respectively, which is the 1st place among the participators on private leaderboard. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11637,July 2022,858,On the design of ontology-driven workflow flexibilization mechanisms,"Tatiana A. S. C. Vieira, Marco A. Casanova and Luis G. Ferrão","Workflow management systems usually interpret a workflow definition rigidly. However, there are real life situations where users should be allowed to deviate from the prescribed static workflow definition for ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192374,June 2005,
859,0.000678989392134464,859,Deep Pneumonia: Attention-Based Contrastive Learning for Class-Imbalanced Pneumonia Lesion Recognition in Chest X-rays,"Xinxu Wei, Haohan Bai, Xianshi Zhang, Yongjie Li","Computer-aided X-ray pneumonia lesion recognition is important for accurate diagnosis of pneumonia. With the emergence of deep learning, the identification accuracy of pneumonia has been greatly improved, but there are still some challenges due to the fuzzy appearance of chest X-rays. In this paper, we propose a deep learning framework named Attention-Based Contrastive Learning for Class-Imbalanced X-Ray Pneumonia Lesion Recognition (denoted as Deep Pneumonia). We adopt self-supervised contrastive learning strategy to pre-train the model without using extra pneumonia data for fully mining the limited available dataset. In order to leverage the location information of the lesion area that the doctor has painstakingly marked, we propose mask-guided hard attention strategy and feature learning with contrastive regulation strategy which are applied on the attention map and the extracted features respectively to guide the model to focus more attention on the lesion area where contains more discriminative features for improving the recognition performance. In addition, we adopt Class-Balanced Loss instead of traditional Cross-Entropy as the loss function of classification to tackle the problem of serious class imbalance between different classes of pneumonia in the dataset. The experimental results show that our proposed framework can be used as a reliable computer-aided pneumonia diagnosis system to assist doctors to better diagnose pneumonia cases accurately. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11393,July 2022,859,"The comparative effect of computerized dynamic assessment and rater mediated assessment on EFL learners’ oral proficiency, writing performance, and test anxiety","Nasiba Sherkuziyeva, Farida Imamutdinovna Gabidullina, Khaled Ahmed Abdel-Al Ibrahim and Sania Bayat","This study aimed to examine the impacts of computerized dynamic assessment (C-DA) and rater-mediated assessment on the test anxiety, writing performance, and oral proficiency of Iranian EFL learners. Based on ...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-023-00227-3,15 March 2023,
860,0.000678989392134464,860,METER-ML: A Multi-Sensor Earth Observation Benchmark for Automated Methane Source Mapping,"Bryan Zhu, Nicholas Lui, Jeremy Irvin, Jimmy Le, Sahil Tadwalkar, Chenghao Wang, Zutao Ouyang, Frankie Y. Liu, Andrew Y. Ng, Robert B. Jackson","Reducing methane emissions is essential for mitigating global warming. To attribute methane emissions to their sources, a comprehensive dataset of methane source infrastructure is necessary. Recent advancements with deep learning on remotely sensed imagery have the potential to identify the locations and characteristics of methane sources, but there is a substantial lack of publicly available data to enable machine learning researchers and practitioners to build automated mapping approaches. To help fill this gap, we construct a multi-sensor dataset called METER-ML containing 86,599 georeferenced NAIP, Sentinel-1, and Sentinel-2 images in the U.S. labeled for the presence or absence of methane source facilities including concentrated animal feeding operations, coal mines, landfills, natural gas processing plants, oil refineries and petroleum terminals, and wastewater treatment plants. We experiment with a variety of models that leverage different spatial resolutions, spatial footprints, image products, and spectral bands. We find that our best model achieves an area under the precision recall curve of 0.915 for identifying concentrated animal feeding operations and 0.821 for oil refineries and petroleum terminals on an expert-labeled test set, suggesting the potential for large-scale mapping. We make METER-ML freely available at https://stanfordmlgroup.github.io/projects/meter-ml/ to support future work on automated methane source mapping. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11166,July 2022,860,Correction: DCAU-Net: dense convolutional attention U-Net for segmentation of intracranial aneurysm images,"Wenwen Yuan, Yanjun Peng, Yanfei Guo, Yande Ren and Qianwen Xue","Theoriginal articlewas published inVisual Computing for Industry, Biomedicine, and Art20225:9",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-022-00110-7,8 May 2022,
861,0.000678989392134464,861,Learning from what we know: How to perform vulnerability prediction using noisy historical data,"Aayush Garg, Renzo Degiovanni, Matthieu Jimenez, Maxime Cordy, Mike Papadakis, Yves LeTraon","Vulnerability prediction refers to the problem of identifying system components that are most likely to be vulnerable. Typically, this problem is tackled by training binary classifiers on historical data. Unfortunately, recent research has shown that such approaches underperform due to the following two reasons: a) the imbalanced nature of the problem, and b) the inherently noisy historical data, i.e., most vulnerabilities are discovered much later than they are introduced. This misleads classifiers as they learn to recognize actual vulnerable components as non-vulnerable. To tackle these issues, we propose TROVON, a technique that learns from known vulnerable components rather than from vulnerable and non-vulnerable components, as typically performed. We perform this by contrasting the known vulnerable, and their respective fixed components. This way, TROVON manages to learn from the things we know, i.e., vulnerabilities, hence reducing the effects of noisy and unbalanced data. We evaluate TROVON by comparing it with existing techniques on three security-critical open source systems, i.e., Linux Kernel, OpenSSL, and Wireshark, with historical vulnerabilities that have been reported in the National Vulnerability Database (NVD). Our evaluation demonstrates that the prediction capability of TROVON significantly outperforms existing vulnerability prediction techniques such as Software Metrics, Imports, Function Calls, Text Mining, Devign, LSTM, and LSTM-RF with an improvement of 40.84% in Matthews Correlation Coefficient (MCC) score under Clean Training Data Settings, and an improvement of 35.52% under Realistic Training Data Settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11018,July 2022,861,Model-based reuse for crosscutting frameworks: assessing reuse and maintenance effort,"Thiago Gottardi, Rafael Serapilha Durelli, Óscar Pastor López and Valter Vieira de Camargo","Over the last years, a number of researchers have investigated how to improve the reuse of crosscutting concerns. New possibilities have emerged with the advent of aspect-oriented programming, and many framewo...",https://www.springeropen.com//jserd.springeropen.com/articles/10.1186/2195-1721-1-4,29 October 2013,
862,0.00461519317491824,862,POP: Mining POtential Performance of new fashion products via webly cross-modal query expansion,"Christian Joppi, Geri Skenderi, Marco Cristani","We propose a data-centric pipeline able to generate exogenous observation data for the New Fashion Product Performance Forecasting (NFPPF) problem, i.e., predicting the performance of a brand-new clothing probe with no available past observations. Our pipeline manufactures the missing past starting from a single, available image of the clothing probe. It starts by expanding textual tags associated with the image, querying related fashionable or unfashionable images uploaded on the web at a specific time in the past. A binary classifier is robustly trained on these web images by confident learning, to learn what was fashionable in the past and how much the probe image conforms to this notion of fashionability. This compliance produces the POtential Performance (POP) time series, indicating how performing the probe could have been if it were available earlier. POP proves to be highly predictive for the probe's future performance, ameliorating the sales forecasts of all state-of-the-art models on the recent VISUELLE fast-fashion dataset. We also show that POP reflects the ground-truth popularity of new styles (ensembles of clothing items) on the Fashion Forward benchmark, demonstrating that our webly-learned signal is a truthful expression of popularity, accessible by everyone and generalizable to any time of analysis. Forecasting code, data and the POP time series are available at: https://github.com/HumaticsLAB/POP-Mining-POtential-Performance △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.11001,July 2022,862,"Intelligent metasurfaces: control, communication and computing","Lianlin Li, Hanting Zhao, Che Liu, Long Li and Tie Jun Cui",Controlling electromagnetic waves and information simultaneously by information metasurfaces is of central importance in modern society. Intelligent metasurfaces are smart platforms to manipulate the wave–info...,https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-022-00013-3,6 May 2022,
863,0.000678989392134464,863,Session-based Cyberbullying Detection in Social Media: A Survey,"Peiling Yi, Arkaitz Zubiaga","Cyberbullying is a pervasive problem in online social media, where a bully abuses a victim through a social media session. By investigating cyberbullying perpetrated through social media sessions, recent research has looked into mining patterns and features for modeling and understanding the two defining characteristics of cyberbullying: repetitive behavior and power imbalance. In this survey paper, we define the Session-based Cyberbullying Detection framework that encapsulates the different steps and challenges of the problem. Based on this framework, we provide a comprehensive overview of session-based cyberbullying detection in social media, delving into existing efforts from a data and methodological perspective. Our review leads us to propose evidence-based criteria for a set of best practices to create session-based cyberbullying datasets. In addition, we perform benchmark experiments comparing the performance of state-of-the-art session-based cyberbullying detection models as well as large pre-trained language models across two different datasets. Through our review, we also put forth a set of open challenges as future research directions. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10639,July 2022,863,Meta-heuristic algorithms as tools for hydrological science,Do Guen Yoo and Joong Hoon Kim,"In this paper, meta-heuristic optimization techniques are introduced and their applications to water resources engineering, particularly in hydrological science are introduced. In recent years, meta-heuristic ...",https://www.springeropen.com//geoscienceletters.springeropen.com/articles/10.1186/2196-4092-1-4,6 March 2014,
864,0.000678989392134464,864,Democratizing Ethical Assessment of Natural Language Generation Models,"Amin Rasekh, Ian Eisenberg","Natural language generation models are computer systems that generate coherent language when prompted with a sequence of words as context. Despite their ubiquity and many beneficial applications, language generation models also have the potential to inflict social harms by generating discriminatory language, hateful speech, profane content, and other harmful material. Ethical assessment of these models is therefore critical. But it is also a challenging task, requiring an expertise in several specialized domains, such as computational linguistics and social justice. While significant strides have been made by the research community in this domain, accessibility of such ethical assessments to the wider population is limited due to the high entry barriers. This article introduces a new tool to democratize and standardize ethical assessment of natural language generation models: Tool for Ethical Assessment of Language generation models (TEAL), a component of Credo AI Lens, an open-source assessment framework. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10576,July 2022,864,Performance characteristic of a PV module as influenced by dust accumulation: theory versus experiment,"Reham Kamal, Mazen Abdel-Salam and Mohamed Nayel",This paper is aimed at assessing by theory and experiment the current–voltage and power-voltage characteristics of a PV module as influenced by dust accumulation. A method is proposed in a computer code to fol...,https://www.springeropen.com//jeas.springeropen.com/articles/10.1186/s44147-023-00181-0,21 February 2023,
865,0.000678989392134464,865,Big Data and Education: using big data analytics in language learning,Vahid Ashrafimoghari,"Working with big data using data mining tools is rapidly becoming a trend in education industry. The combination of the current capacity to collect, store, manage and process data in a timely manner, and data from online educational platforms represents an unprecedented opportunity for educational institutes, learners, educators, and researchers. In this position paper, we consider some basic concepts as well as most popular tools, methods and techniques regarding Educational Data Mining and Learning Analytics, and discuss big data applications in language learning, in particular. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10572,July 2022,865,Cultural heritage preservation by using blockchain technologies,Denis Trček,"Ubiquitous digitization enables promising options for cultural heritage preservation. Therefore, a new approach is presented that considers deployment scenarios by linking heritage science to tourism. Such an ...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-021-00643-9,10 January 2022,
866,0.000678989392134464,866,ProMix: Combating Label Noise via Maximizing Clean Sample Utility,"Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, Junbo Zhao","The ability to train deep neural networks under label noise is appealing, as imperfectly annotated data are relatively cheaper to obtain. State-of-the-art approaches are based on semi-supervised learning(SSL), which selects small loss examples as clean and then applies SSL techniques for boosted performance. However, the selection step mostly provides a medium-sized and decent-enough clean subset, which overlooks a rich set of clean samples. In this work, we propose a novel noisy label learning framework ProMix that attempts to maximize the utility of clean samples for boosted performance. Key to our method, we propose a matched high-confidence selection technique that selects those examples having high confidence and matched prediction with its given labels. Combining with the small-loss selection, our method is able to achieve a precision of 99.27 and a recall of 98.22 in detecting clean samples on the CIFAR-10N dataset. Based on such a large set of clean data, ProMix improves the best baseline method by +2.67% on CIFAR-10N and +1.61% on CIFAR-100N datasets. The code and data are available at https://github.com/Justherozen/ProMix △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10276,July 2022,866,Research on Chinese traditional opera costume recognition based on improved YOLOv5,"Kaixuan Liu, Kai Lin and Chun Zhu","In order to protect the cultural heritage of opera costumes, establish visual labels for opera costumes, accelerate the establishment of a database for opera costumes, and increase the dissemination of opera c...",https://www.springeropen.com//heritagesciencejournal.springeropen.com/articles/10.1186/s40494-023-00883-x,25 February 2023,
867,0.000678989392134464,867,Digraphwave: Scalable Extraction of Structural Node Embeddings via Diffusion on Directed Graphs,"Ciwan Ceylan, Kambiz Ghoorchian, Danica Kragic","Structural node embeddings, vectors capturing local connectivity information for each node in a graph, have many applications in data mining and machine learning, e.g., network alignment and node classification, clustering and anomaly detection. For the analysis of directed graphs, e.g., transactions graphs, communication networks and social networks, the capability to capture directional information in the structural node embeddings is highly desirable, as is scalability of the embedding extraction method. Most existing methods are nevertheless only designed for undirected graph. Therefore, we present Digraphwave -- a scalable algorithm for extracting structural node embeddings on directed graphs. The Digraphwave embeddings consist of compressed diffusion pattern signatures, which are twice enhanced to increase their discriminate capacity. By proving a lower bound on the heat contained in the local vicinity of a diffusion initialization node, theoretically justified diffusion timescale values are established, and Digraphwave is left with only two easy-to-interpret hyperparameters: the embedding dimension and a neighbourhood resolution specifier. In our experiments, the two embedding enhancements, named transposition and aggregation, are shown to lead to a significant increase in macro F1 score for classifying automorphic identities, with Digraphwave outperforming all other structural embedding baselines. Moreover, Digraphwave either outperforms or matches the performance of all baselines on real graph datasets, displaying a particularly large performance gain in a network alignment task, while also being scalable to graphs with millions of nodes and edges, running up to 30x faster than a previous diffusion pattern based method and with a fraction of the memory consumption. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10149,July 2022,867,Solving the large-scale knapsack feasibility problem using a distributed computation approach to integer programming,"Zhengtian Wu, Fuyuan Hu and Baochuan Fu","The knapsack feasibility problems have been intensively studied both because of their immediate applications in industry and financial management, but more pronounced for theoretical reasons, as knapsack probl...",https://www.springeropen.com//applied-informatics-j.springeropen.com/articles/10.1186/s40535-017-0047-0,20 December 2017,
868,0.000678989392134464,868,Traditional kriging versus modern Gaussian processes for large-scale mining data,"Ryan B. Christianson, Ryan M. Pollyea, Robert B. Gramacy","The canonical technique for nonlinear modeling of spatial/point-referenced data is known as kriging in geostatistics, and as Gaussian Process (GP) regression for surrogate modeling and statistical learning. This article reviews many similarities shared between kriging and GPs, but also highlights some important differences. One is that GPs impose a process that can be used to automate kernel/variogram inference, thus removing the human from the loop. The GP framework also suggests a probabilistically valid means of scaling to handle a large corpus of training data, i.e., an alternative to so-called ordinary kriging. Finally, recent GP implementations are tailored to make the most of modern computing architectures such as multi-core workstations and multi-node supercomputers. We argue that such distinctions are important even in classically geostatistical settings. To back that up, we present out-of-sample validation exercises using two, real, large-scale borehole data sets involved in the mining of gold and other minerals. We pit classic kriging against the modern GPs in several variations and conclude that the latter can more economical (fewer human and compute resources), more accurate and offer better uncertainty quantification. We go on to show how the fully generative modeling apparatus provided by GPs can gracefully accommodate left-censoring of small measurements, as commonly occurs in mining data and other borehole assays. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.10138,July 2022,868,An analysis of internal and external feedback in self-regulated learning activities mediated by self-regulated learning tools and open learner models,Chih-Yueh Chou and Nian-Bao Zou,"In self-regulated learning (SRL), students organize, monitor, direct, and regulate their learning. In SRL, monitoring plays a critical role in generating internal feedback and thus adopting appropriate regulat...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00233-y,9 December 2020,
869,0.000678989392134464,869,DESCN: Deep Entire Space Cross Networks for Individual Treatment Effect Estimation,"Kailiang Zhong, Fengtong Xiao, Yan Ren, Yaorong Liang, Wenqing Yao, Xiaofeng Yang, Ling Cen","Causal Inference has wide applications in various areas such as E-commerce and precision medicine, and its performance heavily relies on the accurate estimation of the Individual Treatment Effect (ITE). Conventionally, ITE is predicted by modeling the treated and control response functions separately in their individual sample spaces. However, such an approach usually encounters two issues in practice, i.e. divergent distribution between treated and control groups due to treatment bias, and significant sample imbalance of their population sizes. This paper proposes Deep Entire Space Cross Networks (DESCN) to model treatment effects from an end-to-end perspective. DESCN captures the integrated information of the treatment propensity, the response, and the hidden treatment effect through a cross network in a multi-task learning manner. Our method jointly learns the treatment and response functions in the entire sample space to avoid treatment bias and employs an intermediate pseudo treatment effect prediction network to relieve sample imbalance. Extensive experiments are conducted on a synthetic dataset and a large-scaled production dataset from the E-commerce voucher distribution business. The results indicate that DESCN can successfully enhance the accuracy of ITE estimation and improve the uplift ranking performance. A sample of the production dataset and the source code are released to facilitate future research in the community, which is, to the best of our knowledge, the first large-scale public biased treatment dataset for causal inference. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.09920,July 2022,869,An isogeometric b-rep mortar-based mapping method for non-matching grids in fluid-structure interaction,"Andreas Apostolatos, Altuğ Emiroğlu, Shahrokh Shayegan, Fabien Péan, Kai-Uwe Bletzinger and Roland Wüchner",In this study the isogeometric B-Rep mortar-based mapping method for geometry models stemming directly fromComputer-Aided Design(CAD) is systematically augmented and applied to partitionedFluid-Structure Inter...,https://www.springeropen.com//amses-journal.springeropen.com/articles/10.1186/s40323-021-00190-9,27 April 2021,
870,0.000678989392134464,870,HICF: Hyperbolic Informative Collaborative Filtering,"Menglin Yang, Zhihao Li, Min Zhou, Jiahong Liu, Irwin King","Considering the prevalence of the power-law distribution in user-item networks, hyperbolic space has attracted considerable attention and achieved impressive performance in the recommender system recently. The advantage of hyperbolic recommendation lies in that its exponentially increasing capacity is well-suited to describe the power-law distributed user-item network whereas the Euclidean equivalent is deficient. Nonetheless, it remains unclear which kinds of items can be effectively recommended by the hyperbolic model and which cannot. To address the above concerns, we take the most basic recommendation technique, collaborative filtering, as a medium, to investigate the behaviors of hyperbolic and Euclidean recommendation models. The results reveal that (1) tail items get more emphasis in hyperbolic space than that in Euclidean space, but there is still ample room for improvement; (2) head items receive modest attention in hyperbolic space, which could be considerably improved; (3) and nonetheless, the hyperbolic models show more competitive performance than Euclidean models. Driven by the above observations, we design a novel learning method, named hyperbolic informative collaborative filtering (HICF), aiming to compensate for the recommendation effectiveness of the head item while at the same time improving the performance of the tail item. The main idea is to adapt the hyperbolic margin ranking learning, making its pull and push procedure geometric-aware, and providing informative guidance for the learning of both head and tail items. Extensive experiments back up the analytic findings and also show the effectiveness of the proposed method. The work is valuable for personalized recommendations since it reveals that the hyperbolic space facilitates modeling the tail item, which often represents user-customized preferences or new products. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.09051,July 2022,870,The effects of using exploratory computerized environments in grades 1 to 8 mathematics: a meta-analysis of research,"Andrzej Sokolowski, Yeping Li and Victor Willson","The process of problem solving is difficult for students; thus, mathematics educators have made multiple attempts to seek ways of making this process more accessible to learners. The purpose of this study was ...",https://www.springeropen.com//stemeducationjournal.springeropen.com/articles/10.1186/s40594-015-0022-z,20 May 2015,
871,0.000678989392134464,871,PackCache: An Online Cost-driven Data Caching Algorithm in the Cloud,"Jiashu Wu, Hao Dai, Yang Wang, Yong Zhang, Dong Huang, Chengzhong Xu","In this paper, we study a data caching problem in the cloud environment, where multiple frequently co-utilised data items could be packed as a single item being transferred to serve a sequence of data requests dynamically with reduced cost. To this end, we propose an online algorithm with respect to a homogeneous cost model, called PackCache, that can leverage the FP-Tree technique to mine those frequently co-utilised data items for packing whereby the incoming requests could be cost-effectively served online by exploiting the concept of anticipatory caching. We show the algorithm is 2αcompetitive, reaching the lower bound of the competitive ratio for any deterministic online algorithm on the studied caching problem, and also time and space efficient to serve the requests. Finally, we evaluate the performance of the algorithm via experimental studies to show its actual cost-effectiveness and scalability. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.09035,July 2022,871,Modified spectral format based on probability level using site-specific uniform hazard spectrum,Ali Ahmed,"Deficiencies of the four spectral formats i.e., 2%/50-yr, 5%/50-yr, 10%/50-yr and AASHTO (American Association of State Highway Officials) 2009 demand modification of the spectral formats for bridge design app...",https://www.springeropen.com//aben.springeropen.com/articles/10.1186/s43251-021-00033-y,5 March 2021,
872,0.000678989392134464,872,Research Trends and Applications of Data Augmentation Algorithms,"Joao Fonseca, Fernando Bacao","In the Machine Learning research community, there is a consensus regarding the relationship between model complexity and the required amount of data and computation power. In real world applications, these computational requirements are not always available, motivating research on regularization methods. In addition, current and past research have shown that simpler classification algorithms can reach state-of-the-art performance on computer vision tasks given a robust method to artificially augment the training dataset. Because of this, data augmentation techniques became a popular research topic in recent years. However, existing data augmentation methods are generally less transferable than other regularization methods. In this paper we identify the main areas of application of data augmentation algorithms, the types of algorithms used, significant research trends, their progression over time and research gaps in data augmentation literature. To do this, the related literature was collected through the Scopus database. Its analysis was done following network science, text mining and exploratory analysis approaches. We expect readers to understand the potential of data augmentation, as well as identify future research directions and open questions within data augmentation research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.08817,July 2022,872,Blind Separation of Acoustic Signals Combining SIMO-Model-Based Independent Component Analysis and Binary Masking,"Yoshimitsu Mori, Hiroshi Saruwatari, Tomoya Takatani, Satoshi Ukai, Kiyohiro Shikano, Takashi Hiekata, Youhei Ikeda, Hiroshi Hashimoto and Takashi Morita","A new two-stage blind source separation (BSS) method for convolutive mixtures of speech is proposed, in which a single-input multiple-output (SIMO)-model-based independent component analysis (ICA) and a new SI...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/34970,1 December 2006,
873,0.00673579331412596,873,A transportable clock laser system with an instability of $1.6 \times 10^{-16}$,"Sofia Herbers, Sebastian Häfner, Sören Dörscher, Tim Lücke, Uwe Sterr, Christian Lisdat","We present a transportable ultra-stable clock laser system based on a Fabry-Pérot cavity with crystalline Al$_{0.92}$Ga$_{0.08}$As/GaAs mirror coatings, fused silica (FS) mirror substrates and a 20~cm-long ultra-low expansion (ULE\textsuperscript{\textregistered}) glass spacer with a predicted thermal noise floor of $\mathrm{mod}\,σ_\mathrm{y} = 7 \times 10^{-17}$ in modified Allan deviation at one second averaging time. The cavity has a cylindrical shape and is mounted at ten points. Its measured sensitivity of the fractional frequency to acceleration for the three Cartesian directions are $2(1) \times 10^{-12}$/(ms$^{-2}$), $3(3) \times 10^{-12}$/(ms$^{-2}$) and $3(1) \times 10^{-12}$/(ms$^{-2}$), which belong to the lowest acceleration sensitivities published for transportable systems. The laser system's instability reaches down to $\mathrm{mod}\,σ_\mathrm{y} = 1.6 \times 10^{-16}$ △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.08679,July 2022,873,Secure semantic expansion based search over encrypted cloud data supporting similarity ranking,"Zhihua Xia, Yanling Zhu, Xingming Sun and Lihong Chen","With the advent of cloud computing, more and more information data are outsourced to the public cloud for economic savings and ease of access. However, the privacy information has to be encrypted to guarantee ...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-014-0008-2,2 July 2014,
874,0.000678989392134464,874,Software Artifact Mining in Software Engineering Conferences: A Meta-Analysis,"Zeinab Abou Khalil, Stefano Zacchiroli","Background: Software development results in the production of various types of artifacts: source code, version control system metadata, bug reports, mailing list conversations, test data, etc. Empirical software engineering (ESE) has thrived mining those artifacts to uncover the inner workings of software development and improve its practices. But which artifacts are studied in the field is a moving target, which we study empirically in this paper.Aims: We quantitatively characterize the most frequently mined and co-mined software artifacts in ESE research and the research purposes they support.Method: We conduct a meta-analysis of artifact mining studies published in 11 top conferences in ESE, for a total of 9621 papers. We use natural language processing (NLP) techniques to characterize the types of software artifacts that are most often mined and their evolution over a 16-year period (2004-2020). We analyze the combinations of artifact types that are most often mined together, as well as the relationship between study purposes and mined artifacts.Results: We find that: (1) mining happens in the vast majority of analyzed papers, (2) source code and test data are the most mined artifacts, (3) there is an increasing interest in mining novel artifacts, together with source code, (4) researchers are most interested in the evaluation of software systems and use all possible empirical signals to support that goal. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.08436,July 2022,874,A comparative analysis of the skilled use of automated feedback tools through the lens of teacher feedback literacy,"Simon Buckingham Shum, Lisa-Angelique Lim, David Boud, Margaret Bearman and Phillip Dawson","Effective learning depends on effective feedback, which in turn requires a set of skills, dispositions and practices on the part of both students and teachers which have been termedfeedback literacy.A previousl...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00410-9,12 July 2023,
875,0.000678989392134464,875,Understanding Influence Maximization via Higher-Order Decomposition,"Zonghan Zhang, Zhiqian Chen","Given its vast application on online social networks, Influence Maximization (IM) has garnered considerable attention over the last couple of decades. Due to the intricacy of IM, most current research concentrates on estimating the first-order contribution of the nodes to select a seed set, disregarding the higher-order interplay between different seeds. Consequently, the actual influence spread frequently deviates from expectations, and it remains unclear how the seed set quantitatively contributes to this deviation. To address this deficiency, this work dissects the influence exerted on individual seeds and their higher-order interactions utilizing the Sobol index, a variance-based sensitivity analysis. To adapt to IM contexts, seed selection is phrased as binary variables and split into distributions of varying orders. Based on our analysis with various Sobol indices, an IM algorithm dubbed SIM is proposed to improve the performance of current IM algorithms by over-selecting nodes followed by strategic pruning. A case study is carried out to demonstrate that the explanation of the impact effect can dependably identify the key higher-order interactions among seeds. SIM is empirically proved to be superior in effectiveness and competitive in efficiency by experiments on synthetic and real-world graphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07833,July 2022,875,Artificial intelligence in higher education: the state of the field,Helen Crompton and Diane Burke,"This systematic review provides unique findings with an up-to-date examination of artificial intelligence (AI) in higher education (HE) from 2016 to 2022. Using PRISMA principles and protocol, 138 articles wer...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-023-00392-8,24 April 2023,
876,0.000678989392134464,876,Greykite: Deploying Flexible Forecasting at Scale at LinkedIn,"Reza Hosseini, Albert Chen, Kaixu Yang, Sayan Patra, Yi Su, Saad Eddin Al Orjany, Sishi Tang, Parvez Ahammad","Forecasts help businesses allocate resources and achieve objectives. At LinkedIn, product owners use forecasts to set business targets, track outlook, and monitor health. Engineers use forecasts to efficiently provision hardware. Developing a forecasting solution to meet these needs requires accurate and interpretable forecasts on diverse time series with sub-hourly to quarterly frequencies. We present Greykite, an open-source Python library for forecasting that has been deployed on over twenty use cases at LinkedIn. Its flagship algorithm, Silverkite, provides interpretable, fast, and highly flexible univariate forecasts that capture effects such as time-varying growth and seasonality, autocorrelation, holidays, and regressors. The library enables self-serve accuracy and trust by facilitating data exploration, model configuration, execution, and interpretation. Our benchmark results show excellent out-of-the-box speed and accuracy on datasets from a variety of domains. Over the past two years, Greykite forecasts have been trusted by Finance, Engineering, and Product teams for resource planning and allocation, target setting and progress tracking, anomaly detection and root cause analysis. We expect Greykite to be useful to forecast practitioners with similar applications who need accurate, interpretable forecasts that capture complex dynamics common to time series related to human activity. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07788,July 2022,876,Streamline pair selection for comparative flow field visualization,"Shoko Sawada, Takayuki Itoh, Takashi Misaka, Shigeru Obayashi, Tobias Czauderna and Kingsley Stephens","Fluid dynamics simulation is often repeated under varying conditions. This leads to a generation of large amounts of results, which are difficult to compare. To compare results under different conditions, it i...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-020-00056-8,27 August 2020,
877,0.000678989392134464,877,Subgroup Discovery in Unstructured Data,"Ali Arab, Dev Arora, Jialin Lu, Martin Ester","Subgroup discovery is a descriptive and exploratory data mining technique to identify subgroups in a population that exhibit interesting behavior with respect to a variable of interest. Subgroup discovery has numerous applications in knowledge discovery and hypothesis generation, yet it remains inapplicable for unstructured, high-dimensional data such as images. This is because subgroup discovery algorithms rely on defining descriptive rules based on (attribute, value) pairs, however, in unstructured data, an attribute is not well defined. Even in cases where the notion of attribute intuitively exists in the data, such as a pixel in an image, due to the high dimensionality of the data, these attributes are not informative enough to be used in a rule. In this paper, we introduce the subgroup-aware variational autoencoder, a novel variational autoencoder that learns a representation of unstructured data which leads to subgroups with higher quality. Our experimental results demonstrate the effectiveness of the method at learning subgroups with high quality while supporting the interpretability of the concepts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07781,July 2022,877,Integrating machine learning and blockchain to develop a system to veto the forgeries and provide efficient results in education sector,"Dhruvil Shah, Devarsh Patel, Jainish Adesara, Pruthvi Hingu and Manan Shah","Although the education sector is improving more quickly than ever with the help of advancing technologies, there are still many areas yet to be discovered, and there will always be room for further enhancement...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00084-y,21 June 2021,
878,0.000678989392134464,878,Knowledge Representation in Digital Agriculture: A Step Towards Standardised Model,"Quoc Hung Ngo, Tahar Kechadi, Nhien-An Le-Khac","In recent years, data science has evolved significantly. Data analysis and mining processes become routines in all sectors of the economy where datasets are available. Vast data repositories have been collected, curated, stored, and used for extracting knowledge. And this is becoming commonplace. Subsequently, we extract a large amount of knowledge, either directly from the data or through experts in the given domain. The challenge now is how to exploit all this large amount of knowledge that is previously known for efficient decision-making processes. Until recently, much of the knowledge gained through a number of years of research is stored in static knowledge bases or ontologies, while more diverse and dynamic knowledge acquired from data mining studies is not centrally and consistently managed. In this research, we propose a novel model called ontology-based knowledge map to represent and store the results (knowledge) of data mining in crop farming to build, maintain, and enrich the process of knowledge discovery. The proposed model consists of six main sets: concepts, attributes, relations, transformations, instances, and states. This model is dynamic and facilitates the access, updates, and exploitation of the knowledge at any time. This paper also proposes an architecture for handling this knowledge-based model. The system architecture includes knowledge modelling, extraction, assessment, publishing, and exploitation. This system has been implemented and used in agriculture for crop management and monitoring. It is proven to be very effective and promising for its extension to other domains. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07740,July 2022,878,An Efficient Feature Extraction Method with Pseudo-Zernike Moment in RBF Neural Network-Based Human Face Recognition System,"Javad Haddadnia, Majid Ahmadi and Karim Faez",This paper introduces a novel method for the recognition of human faces in digital images using a new feature extraction method that combines the global and local information in frontal view of facial images. ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703305128,18 August 2003,
879,0.000678989392134464,879,Contrastive Brain Network Learning via Hierarchical Signed Graph Pooling Model,"Haoteng Tang, Guixiang Ma, Lei Guo, Xiyao Fu, Heng Huang, Liang Zhang","Recently brain networks have been widely adopted to study brain dynamics, brain development and brain diseases. Graph representation learning techniques on brain functional networks can facilitate the discovery of novel biomarkers for clinical phenotypes and neurodegenerative diseases. However, current graph learning techniques have several issues on brain network mining. Firstly, most current graph learning models are designed for unsigned graph, which hinders the analysis of many signed network data (e.g., brain functional networks). Meanwhile, the insufficiency of brain network data limits the model performance on clinical phenotypes predictions. Moreover, few of current graph learning model is interpretable, which may not be capable to provide biological insights for model outcomes. Here, we propose an interpretable hierarchical signed graph representation learning model to extract graph-level representations from brain functional networks, which can be used for different prediction tasks. In order to further improve the model performance, we also propose a new strategy to augment functional brain network data for contrastive learning. We evaluate this framework on different classification and regression tasks using the data from HCP and OASIS. Our results from extensive experiments demonstrate the superiority of the proposed model compared to several state-of-the-art techniques. Additionally, we use graph saliency maps, derived from these prediction tasks, to demonstrate detection and interpretation of phenotypic biomarkers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07650,July 2022,879,Applying authentication and network security to in-cloud enterprise resource planning system,"Bao Rong Chang, Hsiu-Fen Tsai, Yun-Che Tsai and Yi-Sheng Chang",The service-oriented hosts in enterprises like enterprise resources planning (ERP) system have always encountered the crucial problem of unexpected down-time or system failure that will cause data loss and sys...,https://www.springeropen.comhttps://link.springer.com/article/10.1007/s40595-014-0015-8,15 February 2014,
880,0.000678989392134464,880,OASYS: Domain-Agnostic Automated System for Constructing Knowledge Base from Unstructured Text,"Minsang Kim, Sang-hyun Je, Eunjoo Park","In recent years, creating and managing knowledge bases have become crucial to the retail product and enterprise domains. We present an automatic knowledge base construction system that mines data from documents. This system can generate training data during the training process without human intervention. Therefore, it is domain-agnostic trainable using only the target domain text corpus and a pre-defined knowledge base. This system is called OASYS and is the first system built with the Korean language in mind. In addition, we also have constructed a new human-annotated benchmark dataset of the Korean Wikipedia corpus paired with a Korean DBpedia to aid system evaluation. The system performance results on human-annotated benchmark test dataset are meaningful and show that the generated knowledge base from OASYS trained on only auto-generated data is useful. We provide both a human-annotated test dataset and an auto-generated dataset. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07597,July 2022,880,Virtual Reality System with Integrated Sound Field Simulation and Reproduction,"Tobias Lentz, Dirk Schröder, Michael Vorländer and Ingo Assenmacher","A real-time audio rendering system is introduced which combines a full room-specific simulation, dynamic crosstalk cancellation, and multitrack binaural synthesis for virtual acoustical imaging. The system is ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/70540,1 December 2007,
881,0.000678989392134464,881,A Consensus Algorithm Based on Risk Assessment Model for Permissioned Blockchain,"Xiaohui Zhang, Mingying Xue, Xianghua Miao","Blockchain technology enables stakeholders to conduct trusted data sharing and exchange without a trusted centralized institution. These features make blockchain applications attractive to enhance trustworthiness in very different contexts. Due to unique design concepts and outstanding performance, blockchain has become a popular research topic in industry and academia in recent years. Every participant is anonymous in a permissionless blockchain represented by cryptocurrency applications such as Bitcoin. In this situation, some special incentive mechanisms are applied to permissionless blockchain, such as mined native cryptocurrency to solve the trust issues of permissionless blockchain. In many use cases, permissionless blockchain has bottlenecks in transaction throughput performance, which restricts further application in the real world. A permissioned blockchain can reach a consensus among a group of entities that do not establish an entire trust relationship. Unlike permissionless blockchains, the participants must be identified in permissioned blockchains. By relying on the traditional crash fault-tolerant consensus protocols, permissioned blockchains can achieve high transaction throughput and low latency without sacrificing security. However, how to balance the security and consensus efficiency is still the issue that needs to be solved urgently in permissioned blockchains. As the core module of blockchain technology, the consensus algorithm plays a vital role in the performance of the blockchain system. Thus, this paper proposes a new consensus algorithm for permissioned blockchain, the Risk Assessment-based Consensus protocol (RAC), combined with the decentralized design concept and the risk-node assessment mechanism to address the unbalance issues of performance in speed, scalability, and security. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07453,July 2022,881,An Efficient Feature Extraction Method with Pseudo-Zernike Moment in RBF Neural Network-Based Human Face Recognition System,"Javad Haddadnia, Majid Ahmadi and Karim Faez",This paper introduces a novel method for the recognition of human faces in digital images using a new feature extraction method that combines the global and local information in frontal view of facial images. ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703305128,18 August 2003,
882,0.000678989392134464,882,Recent arrivals to the main asteroid belt,"C. de la Fuente Marcos, R. de la Fuente Marcos","The region where the main asteroid belt is now located may have started empty, to become populated early in the history of the Solar system with material scattered outward by the terrestrial planets and inward by the giant planets. These dynamical pathways toward the main belt may still be active today. Here, we present results from a data mining experiment aimed at singling out present-day members of the main asteroid belt that may have reached the belt during the last few hundred years. Probable newcomers include 2003 BM1, 2007 RS62, 457175 (2008 GO98), 2010 BG18, 2010 JC58, 2010 JV52, 2010 KS6, 2010 LD74, 2010 OX38, 2011 QQ99, 2013 HT149, 2015 BH103, 2015 BU525, 2015 RO127, 2015 RS139, 2016 PC41, 2016 UU231, 2020 SA75, 2020 UO43, and 2021 UJ5, all of them in the outer belt. Some of these candidates may have been inserted in their current orbits after experiencing relatively recent close encounters with Jupiter. We also investigated the likely source regions of such new arrivals. Asteroid 2020 UO43, if real, has a non-negligible probability of having an origin in the Oort cloud or even interstellar space. Asteroid 2003 BM1 may have come from the neighborhood of Uranus. However, most newcomers -- including 457175, 2011 QQ99, and 2021 UJ5 -- might have had an origin in Centaur orbital space. The reliability of these findings is assessed within the context of the uncertainties of the available orbit determinations. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.07013,July 2022,882,Virtual Reality System with Integrated Sound Field Simulation and Reproduction,"Tobias Lentz, Dirk Schröder, Michael Vorländer and Ingo Assenmacher","A real-time audio rendering system is introduced which combines a full room-specific simulation, dynamic crosstalk cancellation, and multitrack binaural synthesis for virtual acoustical imaging. The system is ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/70540,1 December 2007,
883,0.000678989392134464,883,GrabQC: Graph based Query Contextualization for automated ICD coding,"Jeshuren Chelladurai, Sudarsun Santhiappan, Balaraman Ravindran","Automated medical coding is a process of codifying clinical notes to appropriate diagnosis and procedure codes automatically from the standard taxonomies such as ICD (International Classification of Diseases) and CPT (Current Procedure Terminology). The manual coding process involves the identification of entities from the clinical notes followed by querying a commercial or non-commercial medical codes Information Retrieval (IR) system that follows the Centre for Medicare and Medicaid Services (CMS) guidelines. We propose to automate this manual process by automatically constructing a query for the IR system using the entities auto-extracted from the clinical notes. We propose \textbf{GrabQC}, a \textbf{Gra}ph \textbf{b}ased \textbf{Q}uery \textbf{C}ontextualization method that automatically extracts queries from the clinical text, contextualizes the queries using a Graph Neural Network (GNN) model and obtains the ICD Codes using an external IR system. We also propose a method for labelling the dataset for training the model. We perform experiments on two datasets of clinical text in three different setups to assert the effectiveness of our approach. The experimental results show that our proposed method is better than the compared baselines in all three settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.06802,July 2022,883,Physically Informed Signal Processing Methods for Piano Sound Synthesis: A Research Overview,"Balázs Bank, Federico Avanzini, Gianpaolo Borin, Giovanni De Poli, Federico Fontana and Davide Rocchesso","This paper reviews recent developments in physics-based synthesis of piano. The paper considers the main components of the instrument, that is, the hammer, the string, and the soundboard. Modeling techniques a...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703304093,8 September 2003,
884,0.000678989392134464,884,Towards Knowledge-based Mining of Mental Disorder Patterns from Textual Data,Maryam Shahabikargar,"Mental health disorders may cause severe consequences on all the countries' economies and health. For example, the impacts of the COVID-19 pandemic, such as isolation and travel ban, can make us feel depressed. Identifying early signs of mental health disorders is vital. For example, depression may increase an individual's risk of suicide. The state-of-the-art research in identifying mental disorder patterns from textual data, uses hand-labelled training sets, especially when a domain expert's knowledge is required to analyse various symptoms. This task could be time-consuming and expensive. To address this challenge, in this paper, we study and analyse the various clinical and non-clinical approaches to identifying mental health disorders. We leverage the domain knowledge and expertise in cognitive science to build a domain-specific Knowledge Base (KB) for the mental health disorder concepts and patterns. We present a weaker form of supervision by facilitating the generating of training data from a domain-specific Knowledge Base (KB). We adopt a typical scenario for analysing social media to identify major depressive disorder symptoms from the textual content generated by social users. We use this scenario to evaluate how our knowledge-based approach significantly improves the quality of results. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.06254,July 2022,884,Conceptualization of a new generation of smart energy systems and the transition toward them using anticipatory systems,"Zahra Heidari Darani, Mohsen Taheri Demne, Darush Mohammadi Zanjirani and Ali Zackery","Emerging energy systems are inherently different from their conventional counter-parts. To address all issues of these systems, comprehensive approaches of transdisciplinary and post-normal sciences are needed...",https://www.springeropen.com//eujournalfuturesresearch.springeropen.com/articles/10.1186/s40309-021-00184-1,8 November 2021,
885,0.000678989392134464,885,Social Network Mining (SNM): A Definition of Relation between the Resources and SNA,Mahyuddin K. M. Nasution,"Social Network Mining (SNM) has become one of the main themes in big data agenda. As a resultant network, we can extract social network from different sources of information, but the information sources were growing dynamically require a flexible approach. To determine the appropriate approach needs the data engineering in order to get the behavior associated with the data. Each social network has the resources and the information source, but the relationship between resources and information sources requires explanation. This paper aimed to address the behavior of the resource as a part of social network analysis (SNA) in the growth of social networks by using the statistical calculations to explain the evolutionary mechanisms. To represent the analysis unit of the SNA, this paper only considers the degree of a vertex, where it is the core of all the analysis in the SNA and it is basic for defining the relation between resources and SNA in SNM. There is a strong effect on the growth of the resources of social networks. In total, the behavior of resources has positive effects. Thus, different information sources behave similarly and have relations with SNA. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.06234,July 2022,885,Correction to: Arabic text summarization using deep learning approach,Molham Al-Maleh and Said Desouki,An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00440-y,9 April 2021,
886,0.000678989392134464,886,Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings,"Xiaoyi Qin, Na Li, Chao Weng, Dan Su, Ming Li","Automatic speaker verification has achieved remarkable progress in recent years. However, there is little research on cross-age speaker verification (CASV) due to insufficient relevant data. In this paper, we mine cross-age test sets based on the VoxCeleb dataset and propose our age-invariant speaker representation(AISR) learning method. Since the VoxCeleb is collected from the YouTube platform, the dataset consists of cross-age data inherently. However, the meta-data does not contain the speaker age label. Therefore, we adopt the face age estimation method to predict the speaker age value from the associated visual data, then label the audio recording with the estimated age. We construct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately select the positive trials with large age-gap. Also, the effect of nationality and gender is considered in selecting negative pairs to align with Vox-H cases. The baseline system performance drops from 1.939\% EER on the Vox-H test set to 10.419\% on the Vox-CA20 test set, which indicates how difficult the cross-age scenario is. Consequently, we propose an age-decoupling adversarial learning (ADAL) method to alleviate the negative effect of the age gap and reduce intra-class variance. Our method outperforms the baseline system by over 10\% related EER reduction on the Vox-CA20 test set. The source code and trial resources are available on https://github.com/qinxiaoyi/Cross-Age_Speaker_Verification △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.05929,July 2022,886,Welcome message from the organizers,"Anke Weidlich, Dirk Neumann, Philipp Staudt, Gunther Gust and Mirko Schäfer","This article is part of a Supplement:Volume 5
                                        Supplement 1",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-022-00196-6,7 September 2022,
887,0.000678989392134464,887,Revealing Unfair Models by Mining Interpretable Evidence,"Mohit Bajaj, Lingyang Chu, Vittorio Romaniello, Gursimran Singh, Jian Pei, Zirui Zhou, Lanjun Wang, Yong Zhang","The popularity of machine learning has increased the risk of unfair models getting deployed in high-stake applications, such as justice system, drug/vaccination design, and medical diagnosis. Although there are effective methods to train fair models from scratch, how to automatically reveal and explain the unfairness of a trained model remains a challenging task. Revealing unfairness of machine learning models in interpretable fashion is a critical step towards fair and trustworthy AI. In this paper, we systematically tackle the novel task of revealing unfair models by mining interpretable evidence (RUMIE). The key idea is to find solid evidence in the form of a group of data instances discriminated most by the model. To make the evidence interpretable, we also find a set of human-understandable key attributes and decision rules that characterize the discriminated data instances and distinguish them from the other non-discriminated data. As demonstrated by extensive experiments on many real-world data sets, our method finds highly interpretable and solid evidence to effectively reveal the unfairness of trained models. Moreover, it is much more scalable than all of the baseline methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.05811,July 2022,887,Fractal dimension analysis as an easy computational approach to improve breast cancer histopathological diagnosis,"Lucas Glaucio da Silva, Waleska Rayanne Sizinia da Silva Monteiro, Tiago Medeiros de Aguiar Moreira, Maria Aparecida Esteves Rabelo, Emílio Augusto Campos Pereira de Assis and Gustavo Torres de Souza","Histopathology is a well-established standard diagnosis employed for the majority of malignancies, including breast cancer. Nevertheless, despite training and standardization, it is considered operator-depende...",https://www.springeropen.com//appmicro.springeropen.com/articles/10.1186/s42649-021-00055-w,30 April 2021,
888,0.000678989392134464,888,Few-Shot Semantic Relation Prediction across Heterogeneous Graphs,"Pengfei Ding, Yan Wang, Guanfeng Liu, Xiaofang Zhou","Semantic relation prediction aims to mine the implicit relationships between objects in heterogeneous graphs, which consist of different types of objects and different types of links. In real-world scenarios, new semantic relations constantly emerge and they typically appear with only a few labeled data. Since a variety of semantic relations exist in multiple heterogeneous graphs, the transferable knowledge can be mined from some existing semantic relations to help predict the new semantic relations with few labeled data. This inspires a novel problem of few-shot semantic relation prediction across heterogeneous graphs. However, the existing methods cannot solve this problem because they not only require a large number of labeled samples as input, but also focus on a single graph with a fixed heterogeneity. Targeting this novel and challenging problem, in this paper, we propose a Meta-learning based Graph neural network for Semantic relation prediction, named MetaGS. Firstly, MetaGS decomposes the graph structure between objects into multiple normalized subgraphs, then adopts a two-view graph neural network to capture local heterogeneous information and global structure information of these subgraphs. Secondly, MetaGS aggregates the information of these subgraphs with a hyper-prototypical network, which can learn from existing semantic relations and adapt to new semantic relations. Thirdly, using the well-initialized two-view graph neural network and hyper-prototypical network, MetaGS can effectively learn new semantic relations from different graphs while overcoming the limitation of few labeled data. Extensive experiments on three real-world datasets have demonstrated the superior performance of MetaGS over the state-of-the-art methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.05068,July 2022,888,Correction: Using EV charging control to provide building load flexibility,"Harsimrat Singh Bhundar, Lukasz Golab and Srinivasan Keshav",Theoriginal articlewas published inEnergy Informatics20236:5,https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-023-00265-4,27 March 2023,
889,0.000678989392134464,889,"DUG Insight: A software package for big-data analysis and visualisation, and its demonstration for passive radar space situational awareness using radio telescopes","Dylan Grigg, Steven Tingay, Marcin Sokolowski, Randall Wayth","As the demand for software to support the processing and analysis of massive radio astronomy datasets increases in the era of the SKA, we demonstrate the interactive workflow building, data mining, processing, and visualisation capabilities of DUG Insight. We test the performance and flexibility of DUG Insight by processing almost 68,000 full sky radio images produced from the Engineering Development Array (EDA2) over the course of a three day period. The goal of the processing was to passively detect and identify known Resident Space Objects (RSOs: satellites and debris in orbit) and investigate how radio interferometry could be used to passively monitor aircraft traffic. These signals are observable due to both terrestrial FM radio signals reflected back to Earth and out-of-band transmission from RSOs. This surveillance of the low Earth orbit and airspace environment is useful as a contribution to space situational awareness and aircraft tracking technology. From the observations, we made 40 detections of 19 unique RSOs within a range of 1,500 km from the EDA2. This is a significant improvement on a previously published study of the same dataset and showcases the flexible features of DUG Insight that allow the processing of complex datasets at scale. Future enhancements of our DUG Insight workflow will aim to realise real-time acquisition, detect unknown RSOs, and continue to process data from SKA-relevant facilities. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04728,July 2022,889,A Human Body Analysis System,"Vincent Girondel, Laurent Bonnaud and Alice Caplier","This paper describes a system for human body analysis (segmentation, tracking, face/hands localisation, posture recognition) from a single view that is fast and completely automatic. The system first extracts ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/61927,1 December 2006,
890,0.00461519317491824,890,No Language Left Behind: Scaling Human-Centered Machine Translation,"NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran , et al. (14 additional authors not shown)","Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today. However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the 200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety. Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system. Finally, we open source all contributions described in this work, accessible at https://github.com/facebookresearch/fairseq/tree/nllb. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04672,July 2022,890,"Intelligent metasurfaces: control, communication and computing","Lianlin Li, Hanting Zhao, Che Liu, Long Li and Tie Jun Cui",Controlling electromagnetic waves and information simultaneously by information metasurfaces is of central importance in modern society. Intelligent metasurfaces are smart platforms to manipulate the wave–info...,https://www.springeropen.com//elight.springeropen.com/articles/10.1186/s43593-022-00013-3,6 May 2022,
891,0.000678989392134464,891,Multi-task Envisioning Transformer-based Autoencoder for Corporate Credit Rating Migration Early Prediction,"Han Yue, Steve Xia, Hongfu Liu","Corporate credit ratings issued by third-party rating agencies are quantified assessments of a company's creditworthiness. Credit Ratings highly correlate to the likelihood of a company defaulting on its debt obligations. These ratings play critical roles in investment decision-making as one of the key risk factors. They are also central to the regulatory framework such as BASEL II in calculating necessary capital for financial institutions. Being able to predict rating changes will greatly benefit both investors and regulators alike. In this paper, we consider the corporate credit rating migration early prediction problem, which predicts the credit rating of an issuer will be upgraded, unchanged, or downgraded after 12 months based on its latest financial reporting information at the time. We investigate the effectiveness of different standard machine learning algorithms and conclude these models deliver inferior performance. As part of our contribution, we propose a new Multi-task Envisioning Transformer-based Autoencoder (META) model to tackle this challenging problem. META consists of Positional Encoding, Transformer-based Autoencoder, and Multi-task Prediction to learn effective representations for both migration prediction and rating prediction. This enables META to better explore the historical data in the training stage for one-year later prediction. Experimental results show that META outperforms all baseline models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04539,July 2022,891,Design and Performance Evaluation of an Adaptive Resource Management Framework for Distributed Real-Time and Embedded Systems,"Nishanth Shankaran, Nilabja Roy, Douglas C Schmidt, Xenofon D Koutsoukos, Yingming Chen and Chenyang Lu",Achieving end-to-end quality of service (QoS) in distributed real-time embedded (DRE) systems require QoS support and enforcement from their underlying operating platforms that integrates many real-time capabi...,https://www.springeropen.com//jes-eurasipjournals.springeropen.com/articles/10.1155/2008/250895,8 January 2008,
892,0.000678989392134464,892,NGAME: Negative Mining-aware Mini-batching for Extreme Classification,"Kunal Dahiya, Nilesh Gupta, Deepak Saini, Akshay Soni, Yajun Wang, Kushal Dave, Jian Jiao, Gururaj K, Prasenjit Dey, Amit Singh, Deepesh Hada, Vidit Jain, Bhawna Paliwal, Anshul Mittal, Sonu Mehta, Ramachandran Ramjee, Sumeet Agarwal, Purushottam Kar, Manik Varma","Extreme Classification (XC) seeks to tag data points with the most relevant subset of labels from an extremely large label set. Performing deep XC with dense, learnt representations for data points and labels has attracted much attention due to its superiority over earlier XC methods that used sparse, hand-crafted features. Negative mining techniques have emerged as a critical component of all deep XC methods that allow them to scale to millions of labels. However, despite recent advances, training deep XC models with large encoder architectures such as transformers remains challenging. This paper identifies that memory overheads of popular negative mining techniques often force mini-batch sizes to remain small and slow training down. In response, this paper introduces NGAME, a light-weight mini-batch creation technique that offers provably accurate in-batch negative samples. This allows training with larger mini-batches offering significantly faster convergence and higher accuracies than existing negative sampling techniques. NGAME was found to be up to 16% more accurate than state-of-the-art methods on a wide array of benchmark datasets for extreme classification, as well as 3% more accurate at retrieving search engine queries in response to a user webpage visit to show personalized ads. In live A/B tests on a popular search engine, NGAME yielded up to 23% gains in click-through-rates. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04452,July 2022,892,A New Repeating Color Watermarking Scheme Based on Human Visual Model,Chwei-Shyong Tsai and Chin-Chen Chang,"This paper proposes a human-visual-model-based scheme that effectively protects the intellectual copyright of digital images. In the proposed method, the theory of the visual secret sharing scheme is used to c...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704405071,5 October 2004,
893,0.000678989392134464,893,Domain Adaptation Under Behavioral and Temporal Shifts for Natural Time Series Mobile Activity Recognition,"Garrett Wilson, Janardhan Rao Doppa, Diane J. Cook","Increasingly, human behavior is captured on mobile devices, leading to an increased interest in automated human activity recognition. However, existing datasets typically consist of scripted movements. Our long-term goal is to perform mobile activity recognition in natural settings. We collect a dataset to support this goal with activity categories that are relevant for downstream tasks such as health monitoring and intervention. Because of the large variations present in human behavior, we collect data from many participants across two different age groups. Because human behavior can change over time, we also collect data from participants over a month's time to capture the temporal drift. We hypothesize that mobile activity recognition can benefit from unsupervised domain adaptation algorithms. To address this need and test this hypothesis, we analyze the performance of domain adaptation across people and across time. We then enhance unsupervised domain adaptation with contrastive learning and with weak supervision when label proportions are available. The dataset is available at https://github.com/WSU-CASAS/smartwatch-data △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04367,July 2022,893,Correction to: Data and optimization model of an industrial heat transfer station to increase energy flexibility,"Thomas Kohne, Lukas Theisinger, Jan Scherff and Matthias Weigold",Theoriginal articlewas published inEnergy Informatics20214:24,https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-021-00185-1,21 October 2021,
894,0.000678989392134464,894,Adaptive Structural Similarity Preserving for Unsupervised Cross Modal Hashing,"Liang Li, Baihua Zheng, Weiwei Sun","Cross-modal hashing is an important approach for multimodal data management and application. Existing unsupervised cross-modal hashing algorithms mainly rely on data features in pre-trained models to mine their similarity relationships. However, their optimization objectives are based on the static metric between the original uni-modal features, without further exploring data correlations during the training. In addition, most of them mainly focus on association mining and alignment among pairwise instances in continuous space but ignore the latent structural correlations contained in the semantic hashing space. In this paper, we propose an unsupervised hash learning framework, namely Adaptive Structural Similarity Preservation Hashing (ASSPH), to solve the above problems. Firstly, we propose an adaptive learning scheme, with limited data and training batches, to enrich semantic correlations of unlabeled instances during the training process and meanwhile to ensure a smooth convergence of the training process. Secondly, we present an asymmetric structural semantic representation learning scheme. We introduce structural semantic metrics based on graph adjacency relations during the semantic reconstruction and correlation mining stage and meanwhile align the structure semantics in the hash space with an asymmetric binary optimization process. Finally, we conduct extensive experiments to validate the enhancements of our work in comparison with existing works. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.04214,July 2022,894,Neural-Network-Based Smart Sensor Framework Operating in a Harsh Environment,"Jagdish C. Patra, Ee Luang Ang, Narendra S. Chaudhari and Amitabha Das",We present an artificial neural-network- (NN-) based smart interface framework for sensors operating in harsh environments. The NN-based sensor can automatically compensate for the nonlinear response character...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.558,30 March 2005,
895,0.000678989392134464,895,Predicting Opinion Dynamics via Sociologically-Informed Neural Networks,"Maya Okawa, Tomoharu Iwata","Opinion formation and propagation are crucial phenomena in social networks and have been extensively studied across several disciplines. Traditionally, theoretical models of opinion dynamics have been proposed to describe the interactions between individuals (i.e., social interaction) and their impact on the evolution of collective opinions. Although these models can incorporate sociological and psychological knowledge on the mechanisms of social interaction, they demand extensive calibration with real data to make reliable predictions, requiring much time and effort. Recently, the widespread use of social media platforms provides new paradigms to learn deep learning models from a large volume of social media data. However, these methods ignore any scientific knowledge about the mechanism of social interaction. In this work, we present the first hybrid method called Sociologically-Informed Neural Network (SINN), which integrates theoretical models and social media data by transporting the concepts of physics-informed neural networks (PINNs) from natural science (i.e., physics) into social science (i.e., sociology and social psychology). In particular, we recast theoretical models as ordinary differential equations (ODEs). Then we train a neural network that simultaneously approximates the data and conforms to the ODEs that represent the social scientific knowledge. In addition, we extend PINNs by integrating matrix factorization and a language model to incorporate rich side information (e.g., user profiles) and structural knowledge (e.g., cluster structure of the social interaction network). Moreover, we develop an end-to-end training procedure for SINN, which involves Gumbel-Softmax approximation to include stochastic mechanisms of social interaction. Extensive experiments on real-world and synthetic datasets show SINN outperforms six baseline methods in predicting opinion dynamics. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.03990,July 2022,895,Welcome message from the organizers,"Anke Weidlich, Dirk Neumann, Philipp Staudt, Gunther Gust and Mirko Schäfer","This article is part of a Supplement:Volume 4
                                        Supplement 3",https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-021-00169-1,13 September 2021,
896,0.000678989392134464,896,Nonparametric Embeddings of Sparse High-Order Interaction Events,"Zheng Wang, Yiming Xu, Conor Tillinghast, Shibo Li, Akil Narayan, Shandian Zhe","High-order interaction events are common in real-world applications. Learning embeddings that encode the complex relationships of the participants from these events is of great importance in knowledge mining and predictive tasks. Despite the success of existing approaches, e.g. Poisson tensor factorization, they ignore the sparse structure underlying the data, namely the occurred interactions are far less than the possible interactions among all the participants. In this paper, we propose Nonparametric Embeddings of Sparse High-order interaction events (NESH). We hybridize a sparse hypergraph (tensor) process and a matrix Gaussian process to capture both the asymptotic structural sparsity within the interactions and nonlinear temporal relationships between the participants. We prove strong asymptotic bounds (including both a lower and an upper bound) of the sparsity ratio, which reveals the asymptotic properties of the sampled structure. We use batch-normalization, stick-breaking construction, and sparse variational GP approximations to develop an efficient, scalable model inference algorithm. We demonstrate the advantage of our approach in several real-world applications. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.03639,July 2022,896,Correction to: Multi-criteria collaborative filtering recommender by fusing deep neural network and matrix factorization,"Nour Nassar, Assef Jafar and Yasser Rahhal",An amendment to this paper has been published and can be accessed via the original article.,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-020-00314-9,7 July 2020,
897,0.000678989392134464,897,Word Embedding for Social Sciences: An Interdisciplinary Survey,"Akira Matsui, Emilio Ferrara","To extract essential information from complex data, computer scientists have been developing machine learning models that learn low-dimensional representation mode. From such advances in machine learning research, not only computer scientists but also social scientists have benefited and advanced their research because human behavior or social phenomena lies in complex data. To document this emerging trend, we survey the recent studies that apply word embedding techniques to human behavior mining, building a taxonomy to illustrate the methods and procedures used in the surveyed papers and highlight the recent emerging trends applying word embedding models to non-textual human behavior data. This survey conducts a simple experiment to warn that common similarity measurements used in the literature could yield different results even if they return consistent results at an aggregate level. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.03086,July 2022,897,Publisher Correction: Towards reinforcement learning for vulnerability analysis in power-economic systems,"Thomas Wolgast, Eric M. S. P. Veith and Astrid Nieße",Theoriginal articlewas published inEnergy Informatics20214:21,https://www.springeropen.com//energyinformatics.springeropen.com/articles/10.1186/s42162-023-00264-5,9 May 2023,
898,0.000678989392134464,898,Planning Courses for Student Success at the American College of Greece,"Ioannis T. Christou, Evgenia Vagianou, George Vardoulias","We model the problem of optimizing the schedule of courses a student at the American College of Greece will need to take to complete their studies. We model all constraints set forth by the institution and the department, so that we guarantee the validity of all produced schedules. We formulate several different objectives to optimize in the resulting schedule, including fastest completion time, course difficulty balance, and so on, with a very important objective our model is capable of capturing being the maximization of the expected student GPA given their performance on passed courses using Machine Learning and Data Mining techniques. All resulting problems are Mixed Integer Linear Programming problems with a number of binary variables that is in the order of the maximum number of terms times the number of courses available for the student to take. The resulting Mathematical Programming problem is always solvable by the GUROBI solver in less than 10 seconds on a modern commercial off-the-self PC, whereas the manual process that was installed before used to take department heads that are designated as student advisors more than one hour of their time for every student and was resulting in sub-optimal schedules as measured by the objectives set forth. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.02659,July 2022,898,"Special issue “Photoacoustic imaging: microscopy, tomography, and their recent applications in biomedicine” in visual computation for industry, biomedicine, and art","Puxiang Lai, Liming Nie and Lidai Wang",Unknown,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-021-00082-0,31 May 2021,
899,0.000678989392134464,899,Modeling and Analysis of Utilizing Cryptocurrency Mining for Demand Flexibility in Electric Energy Systems: A Synthetic Texas Grid Case Study,"Ali Menati, Kiyeob Lee, Le Xie","The electricity sector is facing the dual challenge of supporting increasing level of demand electrification while substantially reducing its carbon footprint. Among electricity demands, the energy consumption of cryptocurrency mining data centers has witnessed significant growth worldwide. If well-coordinated, these data centers could be tailor-designed to aggressively absorb the increasing uncertainties of energy supply and, in turn, provide valuable grid-level services in the electricity market. In this paper, we study the impact of integrating new cryptocurrency mining loads into Texas power grid and the potential profit of utilizing demand flexibility from cryptocurrency mining facilities in the electricity market. We investigate different demand response programs available for data centers and quantify the annual profit of cryptocurrency mining units participating in these programs. We perform our simulations using a synthetic 2000 bus ERCOT grid model, along with added cryptocurrency mining loads on top of the real-world demand profiles in Texas. Our preliminary results show that depending on the size and location of these new loads, we observe different impacts on the ERCOT electricity market, where they could increase the electricity prices and incur more fluctuations in a highly non-uniform manner. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.02428,July 2022,899,Formal Specification and Verification of Real-Time Systems using Graph Grammars,"Leonardo Michelon, Simone André da Costa and Leila Ribeiro","The importance of real-time systems has enormously increased in the last decade. Application areas that typically need real-time models include railroad systems, intelligent vehicle highway systems, avionics, ...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03194256,December 2007,
900,0.000678989392134464,900,A Tutorial on the Spectral Theory of Markov Chains,"Eddie Seabrook, Laurenz Wiskott","Markov chains are a class of probabilistic models that have achieved widespread application in the quantitative sciences. This is in part due to their versatility, but is compounded by the ease with which they can be probed analytically. This tutorial provides an in-depth introduction to Markov chains, and explores their connection to graphs and random walks. We utilize tools from linear algebra and graph theory to describe the transition matrices of different types of Markov chains, with a particular focus on exploring properties of the eigenvalues and eigenvectors corresponding to these matrices. The results presented are relevant to a number of methods in machine learning and data mining, which we describe at various stages. Rather than being a novel academic study in its own right, this text presents a collection of known results, together with some new concepts. Moreover, the tutorial focuses on offering intuition to readers rather than formal understanding, and only assumes basic exposure to concepts from linear algebra and probability theory. It is therefore accessible to students and researchers from a wide variety of disciplines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.02296,July 2022,900,Behavioural specification of middleware systems,Nelson Souto Rosa and Paulo Roberto Freire Cunha,"The number of open specifications of middleware systems and middleware services is increasing. Despite their complexity, they are traditionally described through APIs (the operation signatures) and informal pr...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192395,June 2006,
901,0.000678989392134464,901,PRoA: A Probabilistic Robustness Assessment against Functional Perturbations,"Tianle Zhang, Wenjie Ruan, Jonathan E. Fieldsend","In safety-critical deep learning applications robustness measurement is a vital pre-deployment phase. However, existing robustness verification methods are not sufficiently practical for deploying machine learning systems in the real world. On the one hand, these methods attempt to claim that no perturbations can ``fool'' deep neural networks (DNNs), which may be too stringent in practice. On the other hand, existing works rigorously consider $L_p$ bounded additive perturbations on the pixel space, although perturbations, such as colour shifting and geometric transformations, are more practically and frequently occurring in the real world. Thus, from the practical standpoint, we present a novel and general {\it probabilistic robustness assessment method} (PRoA) based on the adaptive concentration, and it can measure the robustness of deep learning models against functional perturbations. PRoA can provide statistical guarantees on the probabilistic robustness of a model, \textit{i.e.}, the probability of failure encountered by the trained model after deployment. Our experiments demonstrate the effectiveness and flexibility of PRoA in terms of evaluating the probabilistic robustness against a broad range of functional perturbations, and PRoA can scale well to various large-scale deep neural networks compared to existing state-of-the-art baselines. For the purpose of reproducibility, we release our tool on GitHub: \url{ https://github.com/TrustAI/PRoA}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.02036,July 2022,901,"Score equivalence of paper-, tablet-, and interactive voice response system-based versions of PROMIS, PRO-CTCAE, and numerical rating scales among cancer patients","Minji K. Lee, Timothy J. Beebe, Kathleen J. Yost, David T. Eton, Paul J. Novotny, Amylou C. Dueck, Marlene Frost and Jeff A. Sloan","The study tests the effects of data collection modes on patient responses associated with the multi-item measures such as Patient-Reported Outcomes Measurement System (PROMIS®), and single-item measures such as P...",https://www.springeropen.com//jpro.springeropen.com/articles/10.1186/s41687-021-00368-0,17 September 2021,
902,0.000678989392134464,902,GSMFlow: Generation Shifts Mitigating Flow for Generalized Zero-Shot Learning,"Zhi Chen, Yadan Luo, Sen Wang, Jingjing Li, Zi Huang","Generalized Zero-Shot Learning (GZSL) aims to recognize images from both the seen and unseen classes by transferring semantic knowledge from seen to unseen classes. It is a promising solution to take the advantage of generative models to hallucinate realistic unseen samples based on the knowledge learned from the seen classes. However, due to the generation shifts, the synthesized samples by most existing methods may drift from the real distribution of the unseen data. To address this issue, we propose a novel flow-based generative framework that consists of multiple conditional affine coupling layers for learning unseen data generation. Specifically, we discover and address three potential problems that trigger the generation shifts, i.e., semantic inconsistency, variance collapse, and structure disorder. First, to enhance the reflection of the semantic information in the generated samples, we explicitly embed the semantic information into the transformation in each conditional affine coupling layer. Second, to recover the intrinsic variance of the real unseen features, we introduce a boundary sample mining strategy with entropy maximization to discover more difficult visual variants of semantic prototypes and hereby adjust the decision boundary of the classifiers. Third, a relative positioning strategy is proposed to revise the attribute embeddings, guiding them to fully preserve the inter-class geometric structure and further avoid structure disorder in the semantic space. Extensive experimental results on four GZSL benchmark datasets demonstrate that GSMFlow achieves the state-of-the-art performance on GZSL. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01798,July 2022,902,Real-Time Multiple Moving Targets Detection from Airborne IR Imagery by Dynamic Gabor Filter and Dynamic Gaussian Detector,"Fenghui Yao, Guifeng Shao, Ali Sekmen and Mohan Malkani",This paper presents a robust approach to detect multiple moving targets from aerial infrared (IR) image sequences. The proposed novel method is based on dynamic Gabor filter and dynamic Gaussian detector. Firs...,https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2010/124681,18 July 2010,
903,0.000678989392134464,903,Object-Level Targeted Selection via Deep Template Matching,"Suraj Kothawade, Donna Roy, Michele Fenzi, Elmar Haussmann, Jose M. Alvarez, Christoph Angerer","Retrieving images with objects that are semantically similar to objects of interest (OOI) in a query image has many practical use cases. A few examples include fixing failures like false negatives/positives of a learned model or mitigating class imbalance in a dataset. The targeted selection task requires finding the relevant data from a large-scale pool of unlabeled data. Manual mining at this scale is infeasible. Further, the OOI are often small and occupy less than 1% of image area, are occluded, and co-exist with many semantically different objects in cluttered scenes. Existing semantic image retrieval methods often focus on mining for larger sized geographical landmarks, and/or require extra labeled data, such as images/image-pairs with similar objects, for mining images with generic objects. We propose a fast and robust template matching algorithm in the DNN feature space, that retrieves semantically similar images at the object-level from a large unlabeled pool of data. We project the region(s) around the OOI in the query image to the DNN feature space for use as the template. This enables our method to focus on the semantics of the OOI without requiring extra labeled data. In the context of autonomous driving, we evaluate our system for targeted selection by using failure cases of object detectors as OOI. We demonstrate its efficacy on a large unlabeled dataset with 2.2M images and show high recall in mining for images with small-sized OOI. We compare our method against a well-known semantic image retrieval method, which also does not require extra labeled data. Lastly, we show that our method is flexible and retrieves images with one or more semantically different co-occurring OOI seamlessly. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01778,July 2022,903,Real-timein silicoexperiments on gene regulatory networks and surgery simulation on handheld devices,"Icíar Alfaro, David González, Felipe Bordeu, Adrien Leygue, Amine Ammar, Elías Cueto and Francisco Chinesta","Simulation of all phenomena taking place in a surgical procedure is a formidable task that involves, when possible, the use of supercomputing facilities over long time periods. However, decision taking in the ...",https://www.springeropen.com//computationalsurgery.springeropen.com/articles/10.1186/2194-3990-1-1,10 January 2014,
904,0.000678989392134464,904,Can Population-based Engagement Improve Personalisation? A Novel Dataset and Experiments,"Sahan Bulathwela, Meghana Verma, Maria Perez-Ortiz, Emine Yilmaz, John Shawe-Taylor","This work explores how population-based engagement prediction can address cold-start at scale in large learning resource collections. The paper introduces i) VLE, a novel dataset that consists of content and video based features extracted from publicly available scientific video lectures coupled with implicit and explicit signals related to learner engagement, ii) two standard tasks related to predicting and ranking context-agnostic engagement in video lectures with preliminary baselines and iii) a set of experiments that validate the usefulness of the proposed dataset. Our experimental results indicate that the newly proposed VLE dataset leads to building context-agnostic engagement prediction models that are significantly performant than ones based on previous datasets, mainly attributing to the increase of training examples. VLE dataset's suitability in building models towards Computer Science/ Artificial Intelligence education focused on e-learning/ MOOC use-cases is also evidenced. Further experiments in combining the built model with a personalising algorithm show promising improvements in addressing the cold-start problem encountered in educational recommenders. This is the largest and most diverse publicly available dataset to our knowledge that deals with learner engagement prediction tasks. The dataset, helper tools, descriptive statistics and example code snippets are available publicly. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01504,July 2022,904,Editorial,"Xiaodong Wang, Edward R. Dougherty, Yidong Chen and Carsten O. Peterson",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704002756,21 January 2004,
905,0.000678989392134464,905,Experts' View on Challenges and Needs for Fairness in Artificial Intelligence for Education,"Gianni Fenu, Roberta Galici, Mirko Marras","In recent years, there has been a stimulating discussion on how artificial intelligence (AI) can support the science and engineering of intelligent educational applications. Many studies in the field are proposing actionable data mining pipelines and machine-learning models driven by learning-related data. The potential of these pipelines and models to amplify unfairness for certain categories of students is however receiving increasing attention. If AI applications are to have a positive impact on education, it is crucial that their design considers fairness at every step. Through anonymous surveys and interviews with experts (researchers and practitioners) who have published their research at top-tier educational conferences in the last year, we conducted the first expert-driven systematic investigation on the challenges and needs for addressing fairness throughout the development of educational systems based on AI. We identified common and diverging views about the challenges and the needs faced by educational technologies experts in practice, that lead the community to have a clear understanding on the main questions raising doubts in this topic. Based on these findings, we highlighted directions that will facilitate the ongoing research towards fairer AI for education. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01490,July 2022,905,A survey on data‐efficient algorithms in big data era,Amina Adadi,"The leading approaches in Machine Learning are notoriously data-hungry. Unfortunately, many application domains do not have access to big data because acquiring data involves a process that is expensive or tim...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00419-9,26 January 2021,
906,0.000678989392134464,906,Generalisable Methods for Early Prediction in Interactive Simulations for Education,"Jade Maï Cock, Mirko Marras, Christian Giang, Tanja Käser","Interactive simulations allow students to discover the underlying principles of a scientific phenomenon through their own exploration. Unfortunately, students often struggle to learn effectively in these environments. Classifying students' interaction data in the simulations based on their expected performance has the potential to enable adaptive guidance and consequently improve students' learning. Previous research in this field has mainly focused on a-posteriori analyses or investigations limited to one specific predictive model and simulation. In this paper, we investigate the quality and generalisability of models for an early prediction of conceptual understanding based on clickstream data of students across interactive simulations. We first measure the students' conceptual understanding through their in-task performance. Then, we suggest a novel type of features that, starting from clickstream data, encodes both the state of the simulation and the action performed by the student. We finally propose to feed these features into GRU-based models, with and without attention, for prediction. Experiments on two different simulations and with two different populations show that our proposed models outperform shallow learning baselines and better generalise to different learning environments and populations. The inclusion of attention into the model increases interpretability in terms of effective inquiry. The source code is available on Github (https://github.com/epfl-ml4ed/beerslaw-lab.git). △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01457,July 2022,906,Clustering and Symbolic Analysis of Cardiovascular Signals: Discovery and Visualization of Medically Relevant Patterns in Long-Term Data Using Limited Prior Knowledge,"Zeeshan Syed, John Guttag and Collin Stultz",This paper describes novel fully automated techniques for analyzing large amounts of cardiovascular data. In contrast to traditional medical expert systems our techniques incorporate no a priori knowledge abou...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/67938,1 December 2007,
907,0.000678989392134464,907,Enhancing Automated Software Traceability by Transfer Learning from Open-World Data,"Jinfeng Lin, Amrit Poudel, Wenhao Yu, Qingkai Zeng, Meng Jiang, Jane Cleland-Huang","Software requirements traceability is a critical component of the software engineering process, enabling activities such as requirements validation, compliance verification, and safety assurance. However, the cost and effort of manually creating a complete set of trace links across natural language artifacts such as requirements, design, and test-cases can be prohibitively expensive. Researchers have therefore proposed automated link-generation solutions primarily based on information-retrieval (IR) techniques; however, these solutions have failed to deliver the accuracy needed for full adoption in industrial projects. Improvements can be achieved using deep-learning traceability models; however, their efficacy is impeded by the limited size and availability of project-level artifacts and links to serve as training data. In this paper, we address this problem by proposing and evaluating several deep-learning approaches for text-to-text traceability. Our method, named NLTrace, explores three transfer learning strategies that use datasets mined from open world platforms. Through pretraining Language Models (LMs) and leveraging adjacent tracing tasks, we demonstrate that NLTrace can significantly improve the performance of LM based trace models when training links are available. In such scenarios NLTrace outperforms the best performing classical IR method with an 188% improvement in F2 score and 94.01% in Mean Average Precision (MAP). It also outperforms the general LM based trace model by 7% and 23% for F2 and MAP respectively. In addition, NLTrace can adapt to low-resource tracing scenarios where other LM models can not. The knowledge learned from adjacent tasks enables NLTrace to outperform VSM models by 28% F2 on generation challenges when presented with a small number of training examples. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01084,July 2022,907,In silico design and validation of a highly degenerate primer pair: a systematic approach,"Prosper Obed Chukwuemeka, Haruna Isiyaku Umar, Oluwatoyin Folake Olukunle, Oluwaseyi Matthew Oretade, Christopher Busayo Olowosoke, Emmanuel Oluwasegun Akinsola, Michael Omoniyi Elabiyi, Usman Garba Kurmi, Joy Oseme Eigbe, Bukola Rukayat Oyelere, Lucky Efe Isunu and Oyeyemi Janet Oretade","The techniques of amplifying genetic materials have enabled the extensive study of several biological activities outside the biological milieu of living systems. More recently, this approach has been extended ...",https://www.springeropen.com//jgeb.springeropen.com/articles/10.1186/s43141-020-00086-y,17 November 2020,
908,0.000678989392134464,908,Training Patch Analysis and Mining Skills for Image Restoration Deep Neural Networks,"Jae Woong Soh, Nam Ik Cho","There have been numerous image restoration methods based on deep convolutional neural networks (CNNs). However, most of the literature on this topic focused on the network architecture and loss functions, while less detailed on the training methods. Hence, some of the works are not easily reproducible because it is required to know the hidden training skills to obtain the same results. To be specific with the training dataset, few works discussed how to prepare and order the training image patches. Moreover, it requires a high cost to capture new datasets to train a restoration network for the real-world scene. Hence, we believe it is necessary to study the preparation and selection of training data. In this regard, we present an analysis of the training patches and explore the consequences of different patch extraction methods. Eventually, we propose a guideline for the patch extraction from given training images. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01075,July 2022,908,"The effect of differences in group composition on knowledge transfer, group achievement, and learners’ affective responses during reciprocal concept mapping with the Kit-Build Approach","Lia Sadita, Tsukasa Hirashima, Yusuke Hayashi, Pedro G. F. Furtado, Kasiyah Junus and Harry Budi Santoso",Collaborative learning requires a structured and open environment where individuals can actively exchange and elaborate their ideas to achieve a high-quality problem-solving solution. The use of concept map ha...,https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-020-00133-9,10 June 2020,
909,0.000678989392134464,909,FE${}^\textbf{ANN}$ $-$ An efficient data-driven multiscale approach based on physics-constrained neural networks and automated data mining,"Karl A. Kalina, Lennart Linden, Jörg Brummund, Markus Kästner","Herein, we present a new data-driven multiscale framework called FE${}^\text{ANN}$ which is based on two main keystones: the usage of physics-constrained artificial neural networks (ANNs) as macroscopic surrogate models and an autonomous data mining process. Our approach allows the efficient simulation of materials with complex underlying microstructures which reveal an overall anisotropic and nonlinear behavior on the macroscale. Thereby, we restrict ourselves to finite strain hyperelasticity problems for now. By using a set of problem specific invariants as the input of the ANN and the Helmholtz free energy density as the output, several physical principles, e.g., objectivity, material symmetry, compatibility with the balance of angular momentum and thermodynamic consistency are fulfilled a priori. The necessary data for the training of the ANN-based surrogate model, i.e., macroscopic deformations and corresponding stresses, are collected via computational homogenization of representative volume elements (RVEs). Thereby, the core feature of the approach is given by a completely autonomous mining of the required data set within an overall loop. In each iteration of the loop, new data are generated by gathering the macroscopic deformation states from the macroscopic finite element (FE) simulation and a subsequently sorting by using the anisotropy class of the considered material. Finally, all unknown deformations are prescribed in the RVE simulation to get the corresponding stresses and thus to extend the data set. The proposed framework consequently allows to reduce the number of time-consuming microscale simulations to a minimum. It is exemplarily applied to several descriptive examples, where a fiber reinforced composite with a highly nonlinear Ogden-type behavior of the individual components is considered. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01045,July 2022,909,Selective local texture features based face recognition with single sample per class,K. Jaya Priya and R. S. Rajesh,Local appearance-based methods have been successfully applied to face recognition and achieved state-of-the-art performance. In this paper we propose a local selective feature extraction approach based on Gabo...,https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/s13173-011-0049-z,30 November 2011,
910,0.000678989392134464,910,Comparative Analysis of Time Series Forecasting Approaches for Household Electricity Consumption Prediction,"Muhammad Bilal, Hyeok Kim, Muhammad Fayaz, Pravin Pawar","As a result of increasing population and globalization, the demand for energy has greatly risen. Therefore, accurate energy consumption forecasting has become an essential prerequisite for government planning, reducing power wastage and stable operation of the energy management system. In this work we present a comparative analysis of major machine learning models for time series forecasting of household energy consumption. Specifically, we use Weka, a data mining tool to first apply models on hourly and daily household energy consumption datasets available from Kaggle data science community. The models applied are: Multilayer Perceptron, K Nearest Neighbor regression, Support Vector Regression, Linear Regression, and Gaussian Processes. Secondly, we also implemented time series forecasting models, ARIMA and VAR, in python to forecast household energy consumption of selected South Korean households with and without weather data. Our results show that the best methods for the forecasting of energy consumption prediction are Support Vector Regression followed by Multilayer Perceptron and Gaussian Process Regression. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.01019,July 2022,910,CFDSD: a Communication Framework for Distributed Software Development,"Alexandre L’Erario, José Antônio Gonçalves, José Augusto Fabri, Tiago Pagotto and Rodrigo Henrique Cunha Palácios","Due to geographical and/or temporal dispersion, communication between teams in distributed software projects is a critical factor for success. Notably, distributed teams suffer adverse physical and temporal di...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-020-00101-7,21 August 2020,
911,0.000678989392134464,911,Mining Tourism Experience on Twitter: A case study,"Davide Stirparo, Beatrice Penna, Mohammad Kazemi, Ariona Shashaj","With the increase of digital data and social network platforms the impact of social media science in driving company decision related to product/service features and customer care operations is becoming more crucial. In particular, platform such as Twitter where people can share experience about almost everything can drastically impact the reputation and offering of a company as well as of a place or tourism site. Text mining tools are researched and proposed in literature in order to gain value and perform trend topics and sentiment analysis on Twitter. As data are the fuels for these models, the ""right"" ones, i.e the domain-related ones makes a difference on their accuracy. In this paper, we describe a pipeline of \textit{DataOps / MLOps} operations performed over a tourism related Twitter dataset in order to comprehend tourism motivation and interest. The gained knowledge can be exploit, by the travel/hospitality industry in order to develop data-driven strategic service, and by travelers which can consume relevant information about tourist destination. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.00816,July 2022,911,Dank or not? Analyzing and predicting the popularity of memes on Reddit,"Kate Barnes, Tiernon Riesenmy, Minh Duc Trinh, Eli Lleshi, Nóra Balogh and Roland Molontay","Internet memes have become an increasingly pervasive form of contemporary social communication that attracted a lot of research interest recently. In this paper, we analyze the data of 129,326 memes collected ...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-021-00358-7,9 March 2021,
912,0.000678989392134464,912,MIA 2022 Shared Task: Evaluating Cross-lingual Open-Retrieval Question Answering for 16 Diverse Languages,"Akari Asai, Shayne Longpre, Jungo Kasai, Chia-Hsuan Lee, Rui Zhang, Junjie Hu, Ikuya Yamada, Jonathan H. Clark, Eunsol Choi","We present the results of the Workshop on Multilingual Information Access (MIA) 2022 Shared Task, evaluating cross-lingual open-retrieval question answering (QA) systems in 16 typologically diverse languages. In this task, we adapted two large-scale cross-lingual open-retrieval QA datasets in 14 typologically diverse languages, and newly annotated open-retrieval QA data in 2 underrepresented languages: Tagalog and Tamil. Four teams submitted their systems. The best system leveraging iteratively mined diverse negative examples and larger pretrained models achieves 32.2 F1, outperforming our baseline by 4.5 points. The second best system uses entity-aware contextualized representations for document retrieval, and achieves significant improvements in Tamil (20.8 F1), whereas most of the other systems yield nearly zero scores. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.00758,July 2022,912,Feasibility analysis on the construction of a web solution for hydrometeorological forecasting considering water body management and indicators for the SARS-COV-2 pandemic,"José Roberto Dantas da Silva Júnior, Rizzieri Pedruzzi, Filipe Milani de Souza, Patrick Silva Ferraz, Daniel Guimarães Silva, Carolina Sacramento Vieira, Marcelo Romero de Moraes, Erick Giovani Sperandio Nascimento and Davidson Martins Moreira","The current scenario of a global pandemic caused by the virus SARS-CoV-2 (COVID19), highlights the importance of water studies in sewage systems. In Brazil, about 35 million Brazilians still do not have treate...",https://www.springeropen.com//aiperspectives.springeropen.com/articles/10.1186/s42467-021-00011-0,1 October 2021,
913,0.000678989392134464,913,Accelerating System-Level Debug Using Rule Learning and Subgroup Discovery Techniques,Zurab Khasidashvili,"We propose a root-causing procedure for accelerating system-level debug using rule-based techniques. We describe the procedure and how it provides high quality debug hints for reducing the debug effort. This includes the heuristics for engineering features from logs of many tests, and the data analytics techniques for generating powerful debug hints. As a case study, we used these techniques for root-causing failures of the Power Management (PM) design feature Package-C8 and showed their effectiveness. Furthermore, we propose an approach for mining the root-causing experience and results for reuse, to accelerate future debug activities and reduce dependency on validation experts. We believe that these techniques are beneficial also for other validation activities at different levels of abstraction, for complex hardware, software and firmware systems, both pre-silicon and post-silicon. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.00622,July 2022,913,Smart technology for self-organizing processes,Marlene Scardamalia and Carl Bereiter,Learning technology periodically undergoes changes in response to changes in the prevailing models of human cognition and learning. A major shift throughout the behavioral sciences that began in the 1980s is b...,https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-014-0001-8,16 October 2014,
914,0.000678989392134464,914,Time-aware Dynamic Graph Embedding for Asynchronous Structural Evolution,"Yu Yang, Hongzhi Yin, Jiannong Cao, Tong Chen, Quoc Viet Hung Nguyen, Xiaofang Zhou, Lei Chen","Dynamic graphs refer to graphs whose structure dynamically changes over time. Despite the benefits of learning vertex representations (i.e., embeddings) for dynamic graphs, existing works merely view a dynamic graph as a sequence of changes within the vertex connections, neglecting the crucial asynchronous nature of such dynamics where the evolution of each local structure starts at different times and lasts for various durations. To maintain asynchronous structural evolutions within the graph, we innovatively formulate dynamic graphs as temporal edge sequences associated with joining time of vertices (ToV) and timespan of edges (ToE). Then, a time-aware Transformer is proposed to embed vertices' dynamic connections and ToEs into the learned vertex representations. Meanwhile, we treat each edge sequence as a whole and embed its ToV of the first vertex to further encode the time-sensitive information. Extensive evaluations on several datasets show that our approach outperforms the state-of-the-art in a wide range of graph mining tasks. At the same time, it is very efficient and scalable for embedding large-scale dynamic graphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.00594,July 2022,914,"Development of a course based on BEAM robots to enhance STEM learning in electrical, electronic, and mechanical domains","Carlos Boya-Lara, Doris Saavedra, Aaron Fehrenbach and Angel Marquez-Araque","In this work, BEAM robotics is proposed to enhance the STEM knowledge and skills of engineering students in the electrical, electronic, and mechanical domains. To evaluate the proposal, a course is designed an...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00311-9,3 February 2022,
915,0.000678989392134464,915,Evaluating the Explainers: Black-Box Explainable Machine Learning for Student Success Prediction in MOOCs,"Vinitra Swamy, Bahar Radmehr, Natasa Krco, Mirko Marras, Tanja Käser","Neural networks are ubiquitous in applied machine learning for education. Their pervasive success in predictive performance comes alongside a severe weakness, the lack of explainability of their decisions, especially relevant in human-centric fields. We implement five state-of-the-art methodologies for explaining black-box machine learning models (LIME, PermutationSHAP, KernelSHAP, DiCE, CEM) and examine the strengths of each approach on the downstream task of student performance prediction for five massive open online courses. Our experiments demonstrate that the families of explainers do not agree with each other on feature importance for the same Bidirectional LSTM models with the same representative set of students. We use Principal Component Analysis, Jensen-Shannon distance, and Spearman's rank-order correlation to quantitatively cross-examine explanations across methods and courses. Furthermore, we validate explainer performance across curriculum-based prerequisite relationships. Our results come to the concerning conclusion that the choice of explainer is an important decision and is in fact paramount to the interpretation of the predictive results, even more so than the course the model is trained on. Source code and models are released at http://github.com/epfl-ml4ed/evaluating-explainers. △ Less",https://arxiv.orghttps://arxiv.org/abs/2207.00551,July 2022,915,Enabling actionable analytics for mobile devices: performance issues of distributed analytics on Hadoop mobile clusters,"Seungbae Lee, Kanika Grover and Alvin Lim",Significant innovations in mobile technologies are enabling mobile users to make real-time actionable decisions based on balancing opportunities and risks to take coordinated actions with other users in their ...,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-2-15,1 October 2013,
916,0.000678989392134464,916,MEAD: A Multi-Armed Approach for Evaluation of Adversarial Examples Detectors,"Federica Granese, Marine Picot, Marco Romanelli, Francisco Messina, Pablo Piantanida","Detection of adversarial examples has been a hot topic in the last years due to its importance for safely deploying machine learning algorithms in critical applications. However, the detection methods are generally validated by assuming a single implicitly known attack strategy, which does not necessarily account for real-life threats. Indeed, this can lead to an overoptimistic assessment of the detectors' performance and may induce some bias in the comparison between competing detection schemes. We propose a novel multi-armed framework, called MEAD, for evaluating detectors based on several attack strategies to overcome this limitation. Among them, we make use of three new objectives to generate attacks. The proposed performance metric is based on the worst-case scenario: detection is successful if and only if all different attacks are correctly recognized. Empirically, we show the effectiveness of our approach. Moreover, the poor performance obtained for state-of-the-art detectors opens a new exciting line of research. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.15415,June 2022,916,Personality-dependent content selection in natural language generation systems,"Ricelli M. S. Ramos, Danielle S. Monteiro and Ivandré Paraboni","This paper focuses on the computer side of human-computer interaction through natural language, which is the domain of natural language generation (NLG) studies. From a given (usually non-linguistic) input, NL...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-020-00096-1,29 April 2020,
917,0.000678989392134464,917,Most claimed statistical findings in cross-sectional return predictability are likely true,Andrew Y. Chen,"I present two simple bounds for the false discovery rate (FDR) that account for publication bias. The first assumes that the publication process is not worse at finding predictability than atheoretical data-mining. The second conservatively extrapolates by assuming that there are exponentially more file-drawer t-stats than published t-stats. Both methods find that at least 75% of findings in cross-sectional predictability are true. I show that, surprisingly, Harvey, Liu, and Zhu's (2016) estimates imply a similar FDR. I discuss interpretations and relate to the biostatistics literature. My analysis shows that carefully mapping multiple testing statistics to economic interpretations is important. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.15365,June 2022,917,Towards Understanding Online Question & Answer Interactions and their effects on student performance in large-scale STEM classes,"David H Smith IV, Qiang Hao, Vanessa Dennen, Michail Tsikerdekis, Bradly Barnes, Lilu Martin and Nathan Tresham",Online question & answer (Q & A) is a distinctive type of online interaction that is impactful on student learning. Prior studies on online interaction in large-scale classes mainly focused on online discussio...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00200-7,11 June 2020,
918,0.000678989392134464,918,Learning Citywide Patterns of Life from Trajectory Monitoring,"Mark Tenzer, Zeeshan Rasheed, Khurram Shafique","The recent proliferation of real-world human mobility datasets has catalyzed geospatial and transportation research in trajectory prediction, demand forecasting, travel time estimation, and anomaly detection. However, these datasets also enable, more broadly, a descriptive analysis of intricate systems of human mobility. We formally define patterns of life analysis as a natural, explainable extension of online unsupervised anomaly detection, where we not only monitor a data stream for anomalies but also explicitly extract normal patterns over time. To learn patterns of life, we adapt Grow When Required (GWR) episodic memory from research in computational biology and neurorobotics to a new domain of geospatial analysis. This biologically-inspired neural network, related to self-organizing maps (SOM), constructs a set of ""memories"" or prototype traffic patterns incrementally as it iterates over the GPS stream. It then compares each new observation to its prior experiences, inducing an online, unsupervised clustering and anomaly detection on the data. We mine patterns-of-interest from the Porto taxi dataset, including both major public holidays and newly-discovered transportation anomalies, such as festivals and concerts which, to our knowledge, have not been previously acknowledged or reported in prior work. We anticipate that the capability to incrementally learn normal and abnormal road transportation behavior will be useful in many domains, including smart cities, autonomous vehicles, and urban planning and management. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.15352,June 2022,918,Recovery and Visualization of 3D Structure of Chromosomes from Tomographic Reconstruction Images,"Sabarish Babu, Pao-Chuan Liao, Min C. Shin and Leonid V. Tsap",The objectives of this work include automatic recovery and visualization of a 3D chromosome structure from a sequence of 2D tomographic reconstruction images taken through the nucleus of a cell. Structure is v...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/45684,1 December 2006,
919,0.000678989392134464,919,Mining Seasonal Temporal Patterns in Time Series,"Van Long Ho, Nguyen Ho, Torben Bach Pedersen","Very large time series are increasingly available from an ever wider range of IoT-enabled sensors, from which significant insights can be obtained through mining temporal patterns from them. A useful type of patterns found in many real-world applications exhibits periodic occurrences, and is thus called seasonal temporal pattern (STP). Compared to regular patterns, mining seasonal temporal patterns is more challenging since traditional measures such as support and confidence do not capture the seasonality characteristics. Further, the anti-monotonicity property does not hold for STPs, and thus, resulting in an exponential search space. This paper presents our Frequent Seasonal Temporal Pattern Mining from Time Series (FreqSTPfTS) solution providing: (1) The first solution for seasonal temporal pattern mining (STPM) from time series that can mine STP at different data granularities. (2) The STPM algorithm that uses efficient data structures and two pruning techniques to reduce the search space and speed up the mining process. (3) An approximate version of STPM that uses mutual information, a measure of data correlation, to prune unpromising time series from the search space. (4) An extensive experimental evaluation showing that STPM outperforms the baseline in runtime and memory consumption, and can scale to big datasets. The approximate STPM is up to an order of magnitude faster and less memory consuming than the baseline, while maintaining high accuracy. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.14604,June 2022,919,Conceptualizing the emerging field of smart learning environments,Jonathan Michael Spector,"Learning environments have changed dramatically in the last 50 years, in large part due to information and communications technologies. From the introduction of personal computing and the Internet, there has b...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-014-0002-7,16 October 2014,
920,0.000678989392134464,920,Contributions to Context-Aware Smart Healthcare: A Security and Privacy Perspective,Edgar Batista,"The management of health data, from their gathering to their analysis, arises a number of challenging issues due to their highly confidential nature. In particular, this dissertation contributes to several security and privacy challenges within the smart health paradigm. More concretely, we firstly develop some contributions to context-aware environments enabling smart health scenarios. We present an extensive analysis on the security aspects of the underlying sensors and networks deployed in such environments, a novel user-centred privacy framework for analysing ubiquitous computing systems, and a complete analysis on the security and privacy challenges that need to be faced to implement cognitive cities properly. Second, we contribute to process mining, a popular analytical field that helps analyse business processes within organisations. Despite its popularity within the healthcare industry, we address two major issues: the high complexity of healthcare processes and the scarce research on privacy aspects. Regarding the first issue, we present a novel process discovery algorithm with a built-in heuristic that simplifies complex processes and, regarding the second, we propose two novel privacy-preserving process mining methods, which achieve a remarkable trade-off between accuracy and privacy. Last but not least, we present some smart health applications, namely a context-aware recommender system for routes, a platform supporting early mobilization programmes in hospital settings, and a health-oriented geographic information system. The results of this dissertation are intended to help the research community to enhance the security of the intelligent environments of the future as well as the privacy of the citizens regarding their personal and health data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.14567,June 2022,920,Super-Resolution Enhancement of Digital Video,"Russell C. Hardie, Richard R. Schultz and Kenneth E. Barner",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/20984,1 December 2007,
921,0.000678989392134464,921,Performance Analysis: Discovering Semi-Markov Models From Event Logs,"Anna Kalenkova, Lewis Mitchell, Matthew Roughan","Process mining is a well-established discipline of data analysis focused on the discovery of process models from information systems' event logs. Recently, an emerging subarea of process mining - stochastic process discovery has started to evolve. Stochastic process discovery considers frequencies of events in the event data and allows for more comprehensive analysis. In particular, when durations of activities are presented in the event log, performance characteristics of the discovered stochastic models can be analyzed, e.g., the overall process execution time can be estimated. Existing performance analysis techniques usually discover stochastic process models from event data and then simulate these models to evaluate their execution times. These methods rely on empirical approaches. This paper proposes analytical techniques for performance analysis allowing for the derivation of statistical characteristics of the overall processes' execution times in the presence of arbitrary time distributions of events modeled by semi-Markov processes. The proposed methods can significantly simplify the what-if analysis of processes by providing solutions without resorting to simulation. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.14415,June 2022,921,Towards Understanding Online Question & Answer Interactions and their effects on student performance in large-scale STEM classes,"David H Smith IV, Qiang Hao, Vanessa Dennen, Michail Tsikerdekis, Bradly Barnes, Lilu Martin and Nathan Tresham",Online question & answer (Q & A) is a distinctive type of online interaction that is impactful on student learning. Prior studies on online interaction in large-scale classes mainly focused on online discussio...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00200-7,11 June 2020,
922,0.000678989392134464,922,Towards Explainable Metaheuristic: Mining Surrogate Fitness Models for Importance of Variables,"Manjinder Singh, Alexander E. I. Brownlee, David Cairns","Metaheuristic search algorithms look for solutions that either maximise or minimise a set of objectives, such as cost or performance. However most real-world optimisation problems consist of nonlinear problems with complex constraints and conflicting objectives. The process by which a GA arrives at a solution remains largely unexplained to the end-user. A poorly understood solution will dent the confidence a user has in the arrived at solution. We propose that investigation of the variables that strongly influence solution quality and their relationship would be a step toward providing an explanation of the near-optimal solution presented by a metaheuristic. Through the use of four benchmark problems we use the population data generated by a Genetic Algorithm (GA) to train a surrogate model, and investigate the learning of the search space by the surrogate model. We compare what the surrogate has learned after being trained on population data generated after the first generation and contrast this with a surrogate model trained on the population data from all generations. We show that the surrogate model picks out key characteristics of the problem as it is trained on population data from each generation. Through mining the surrogate model we can build a picture of the learning process of a GA, and thus an explanation of the solution presented by the GA. The aim being to build trust and confidence in the end-user about the solution presented by the GA, and encourage adoption of the model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.14135,June 2022,922,Recovery and Visualization of 3D Structure of Chromosomes from Tomographic Reconstruction Images,"Sabarish Babu, Pao-Chuan Liao, Min C. Shin and Leonid V. Tsap",The objectives of this work include automatic recovery and visualization of a 3D chromosome structure from a sequence of 2D tomographic reconstruction images taken through the nucleus of a cell. Structure is v...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/45684,1 December 2006,
923,0.000678989392134464,923,Modeling Extraneous Activity Delays in Business Process Simulation,"David Chapela-Campa, Marlon Dumas","Business Process Simulation (BPS) is a common approach to estimate the impact of changes to a business process on its performance measures. For example, BPS allows us to estimate what would be the cycle time of a process if we automated one of its activities. The starting point of BPS is a business process model annotated with simulation parameters (a BPS model). Several studies have proposed methods to automatically discover BPS models from event logs via process mining. However, current techniques in this space discover BPS models that only capture waiting times caused by resource contention or resource unavailability. Oftentimes, a considerable portion of the waiting time in a business process is caused by extraneous delays, e.g. a resource waits for the customer to return a phone call. This paper proposes a method that discovers extraneous delays from input data, and injects timer events into a BPS model to capture the discovered delays. An empirical evaluation involving synthetic and real-life logs shows that the approach produces BPS models that better reflect the temporal dynamics of the process, relative to BPS models that do not capture extraneous delays. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.14051,June 2022,923,Conceptualizing the emerging field of smart learning environments,Jonathan Michael Spector,"Learning environments have changed dramatically in the last 50 years, in large part due to information and communications technologies. From the introduction of personal computing and the Internet, there has b...",https://www.springeropen.com//slejournal.springeropen.com/articles/10.1186/s40561-014-0002-7,16 October 2014,
924,0.000678989392134464,924,Dynamic Memory for Interpretable Sequential Optimisation,"Srivas Chennu, Andrew Maher, Jamie Martin, Subash Prabanantham","Real-world applications of reinforcement learning for recommendation and experimentation faces a practical challenge: the relative reward of different bandit arms can evolve over the lifetime of the learning agent. To deal with these non-stationary cases, the agent must forget some historical knowledge, as it may no longer be relevant to minimise regret. We present a solution to handling non-stationarity that is suitable for deployment at scale, to provide business operators with automated adaptive optimisation. Our solution aims to provide interpretable learning that can be trusted by humans, whilst responding to non-stationarity to minimise regret. To this end, we develop an adaptive Bayesian learning agent that employs a novel form of dynamic memory. It enables interpretability through statistical hypothesis testing, by targeting a set point of statistical power when comparing rewards and adjusting its memory dynamically to achieve this power. By design, the agent is agnostic to different kinds of non-stationarity. Using numerical simulations, we compare its performance against an existing proposal and show that, under multiple non-stationary scenarios, our agent correctly adapts to real changes in the true rewards. In all bandit solutions, there is an explicit trade-off between learning and achieving maximal performance. Our solution sits on a different point on this trade-off when compared to another similarly robust approach: we prioritise interpretability, which relies on more learning, at the cost of some regret. We describe the architecture of a large-scale deployment of automatic optimisation-as-a-service where our agent achieves interpretability whilst adapting to changing circumstances. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13960,June 2022,924,Super-Resolution Enhancement of Digital Video,"Russell C. Hardie, Richard R. Schultz and Kenneth E. Barner",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/20984,1 December 2007,
925,0.000678989392134464,925,Haul Road Mapping from GPS Traces,Konstantin M. Seiler,"Automation in mining requires accurate maps of road networks on site. Because roads on open-cut mines are dynamic in nature and continuously changing, manually updating road maps is tedious and error-prone. This paper investigates the possibility of automatically deriving an accurate representation of the road network using GPS data available from haul trucks operating on site. We present an overview of approaches proposed in literature and test the performance of publicly available methods on GPS data collected from trucks operating on site. Based on shortcomings seen in all tested algorithms, a post-processing step is developed which geometrically analyses the created road map for artefacts typical of free-drive areas on mine sites and significantly improves the quality of the final road network graph. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13936,June 2022,925,Ordinal Regression Based Subpixel Shift Estimation for Video Super-Resolution,"Mithun Das Gupta, Shyamsundar Rajaram, Thomas S. Huang and Nemanja Petrovic",We present a supervised learning-based approach for subpixel motion estimation which is then used to perform video super-resolution. The novelty of this work is the formulation of the problem of subpixel motio...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/85963,1 December 2007,
926,0.000678989392134464,926,POEM: Out-of-Distribution Detection with Posterior Sampling,"Yifei Ming, Ying Fan, Yixuan Li","Out-of-distribution (OOD) detection is indispensable for machine learning models deployed in the open world. Recently, the use of an auxiliary outlier dataset during training (also known as outlier exposure) has shown promising performance. As the sample space for potential OOD data can be prohibitively large, sampling informative outliers is essential. In this work, we propose a novel posterior sampling-based outlier mining framework, POEM, which facilitates efficient use of outlier data and promotes learning a compact decision boundary between ID and OOD data for improved detection. We show that POEM establishes state-of-the-art performance on common benchmarks. Compared to the current best method that uses a greedy sampling strategy, POEM improves the relative performance by 42.0% and 24.2% (FPR95) on CIFAR-10 and CIFAR-100, respectively. We further provide theoretical insights on the effectiveness of POEM for OOD detection. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13687,June 2022,926,A Frequency Domain Approach to Registration of Aliased Images with Application to Super-resolution,"Patrick Vandewalle, Sabine Süsstrunk and Martin Vetterli",Super-resolution algorithms reconstruct a high-resolution image from a set of low-resolution images of a scene. Precise alignment of the input images is an essential part of such algorithms. If the low-resolut...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/71459,1 December 2006,
927,0.00461519317491824,927,RES: A Robust Framework for Guiding Visual Explanation,"Yuyang Gao, Tong Steven Sun, Guangji Bai, Siyi Gu, Sungsoo Ray Hong, Liang Zhao","Despite the fast progress of explanation techniques in modern Deep Neural Networks (DNNs) where the main focus is handling ""how to generate the explanations"", advanced research questions that examine the quality of the explanation itself (e.g., ""whether the explanations are accurate"") and improve the explanation quality (e.g., ""how to adjust the model to generate more accurate explanations when explanations are inaccurate"") are still relatively under-explored. To guide the model toward better explanations, techniques in explanation supervision - which add supervision signals on the model explanation - have started to show promising effects on improving both the generalizability as and intrinsic interpretability of Deep Neural Networks. However, the research on supervising explanations, especially in vision-based applications represented through saliency maps, is in its early stage due to several inherent challenges: 1) inaccuracy of the human explanation annotation boundary, 2) incompleteness of the human explanation annotation region, and 3) inconsistency of the data distribution between human annotation and model explanation maps. To address the challenges, we propose a generic RES framework for guiding visual explanation by developing a novel objective that handles inaccurate boundary, incomplete region, and inconsistent distribution of human annotations, with a theoretical justification on model generalizability. Extensive experiments on two real-world image datasets demonstrate the effectiveness of the proposed framework on enhancing both the reasonability of the explanation and the performance of the backbone DNNs model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13413,June 2022,927,Fuzzy Mode Enhancement and Detection for Color Image Segmentation,"Olivier Losson, Claudine Botte-Lecocq and Ludovic Macaire","This work lies within the scope of color image segmentation by pixel classification. The classes of pixels are constructed by detecting the modes of the spatial-color compactness function, which characterizes ...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1155/2008/542378,17 February 2008,
928,0.000678989392134464,928,Local Evaluation of Time Series Anomaly Detection Algorithms,"Alexis Huet, Jose Manuel Navarro, Dario Rossi","In recent years, specific evaluation metrics for time series anomaly detection algorithms have been developed to handle the limitations of the classical precision and recall. However, such metrics are heuristically built as an aggregate of multiple desirable aspects, introduce parameters and wipe out the interpretability of the output. In this article, we first highlight the limitations of the classical precision/recall, as well as the main issues of the recent event-based metrics -- for instance, we show that an adversary algorithm can reach high precision and recall on almost any dataset under weak assumption. To cope with the above problems, we propose a theoretically grounded, robust, parameter-free and interpretable extension to precision/recall metrics, based on the concept of ``affiliation'' between the ground truth and the prediction sets. Our metrics leverage measures of duration between ground truth and predictions, and have thus an intuitive interpretation. By further comparison against random sampling, we obtain a normalized precision/recall, quantifying how much a given set of results is better than a random baseline prediction. By construction, our approach keeps the evaluation local regarding ground truth events, enabling fine-grained visualization and interpretation of algorithmic results. We compare our proposal against various public time series anomaly detection datasets, algorithms and metrics. We further derive theoretical properties of the affiliation metrics that give explicit expectations about their behavior and ensure robustness against adversary strategies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13167,June 2022,928,Dystechnia: a model of technology deficiency and implications for entrepreneurial opportunity,McDonald R Stewart and Elias G Carayannis,"Disconnects among system components preempt technology adoption by the diminution or absence of potential user's perceptions, a state of second-order ignorance (ignorance of ignorance). The condition of flawed...",https://www.springeropen.com//innovation-entrepreneurship.springeropen.com/articles/10.1186/2192-5372-2-1,18 March 2013,
929,0.000678989392134464,929,Enhancing Stochastic Petri Net-based Remaining Time Prediction using k-Nearest Neighbors,"Jarne Vandenabeele, Gilles Vermaut, Jari Peeperkorn, Jochen De Weerdt","Reliable remaining time prediction of ongoing business processes is a highly relevant topic. One example is order delivery, a key competitive factor in e.g. retailing as it is a main driver of customer satisfaction. For realising timely delivery, an accurate prediction of the remaining time of the delivery process is crucial. Within the field of process mining, a wide variety of remaining time prediction techniques have already been proposed. In this work, we extend remaining time prediction based on stochastic Petri nets with generally distributed transitions with k-nearest neighbors. The k-nearest neighbors algorithm is performed on simple vectors storing the time passed to complete previous activities. By only taking a subset of instances, a more representative and stable stochastic Petri Net is obtained, leading to more accurate time predictions. We discuss the technique and its basic implementation in Python and use different real world data sets to evaluate the predictive power of our extension. These experiments show clear advantages in combining both techniques with regard to predictive power. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.13109,June 2022,929,"Teaching students to think spatially through embodied actions: Design principles for learning environments in science, technology, engineering, and mathematics",D. DeSutter and M. Stieff,"Spatial thinking is a vital component of the science, technology, engineering, and mathematics curriculum. However, to date, broad development of learning environments that target domain-specific spatial think...",https://www.springeropen.com//cognitiveresearchjournal.springeropen.com/articles/10.1186/s41235-016-0039-y,20 March 2017,
930,0.000678989392134464,930,Spatiotemporal Data Mining: A Survey,"Arun Sharma, Zhe Jiang, Shashi Shekhar","Spatiotemporal data mining aims to discover interesting, useful but non-trivial patterns in big spatial and spatiotemporal data. They are used in various application domains such as public safety, ecology, epidemiology, earth science, etc. This problem is challenging because of the high societal cost of spurious patterns and exorbitant computational cost. Recent surveys of spatiotemporal data mining need update due to rapid growth. In addition, they did not adequately survey parallel techniques for spatiotemporal data mining. This paper provides a more up-to-date survey of spatiotemporal data mining methods. Furthermore, it has a detailed survey of parallel formulations of spatiotemporal data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.12753,June 2022,930,Intrusion detection and Big Heterogeneous Data: a Survey,"Richard Zuech, Taghi M Khoshgoftaar and Randall Wald","Intrusion Detection has been heavily studied in both industry and academia, but cybersecurity analysts still desire much more alert accuracy and overall threat analysis in order to secure their systems within ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-015-0013-4,27 February 2015,
931,0.000678989392134464,931,Protoformer: Embedding Prototypes for Transformers,"Ashkan Farhangi, Ning Sui, Nan Hua, Haiyan Bai, Arthur Huang, Zhishan Guo","Transformers have been widely applied in text classification. Unfortunately, real-world data contain anomalies and noisy labels that cause challenges for state-of-art Transformers. This paper proposes Protoformer, a novel self-learning framework for Transformers that can leverage problematic samples for text classification. Protoformer features a selection mechanism for embedding samples that allows us to efficiently extract and utilize anomalies prototypes and difficult class prototypes. We demonstrated such capabilities on datasets with diverse textual structures (e.g., Twitter, IMDB, ArXiv). We also applied the framework to several models. The results indicate that Protoformer can improve current Transformers in various empirical settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.12710,June 2022,931,Comulang: towards a collaborative e-learning system that supports student group modeling,"Christos Troussas, Maria Virvou and Efthimios Alepis",This paper describes an e-learning system that is expected to further enhance the educational process in computer-based tutoring systems by incorporating collaboration between students and work in groups. The ...,https://www.springeropen.com//springerplus.springeropen.com/articles/10.1186/2193-1801-2-387,15 August 2013,
932,0.000678989392134464,932,Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays,"Ke Yu, Shantanu Ghosh, Zhexiong Liu, Christopher Deible, Kayhan Batmanghelich","Creating a large-scale dataset of abnormality annotation on medical images is a labor-intensive and costly task. Leveraging weak supervision from readily available data such as radiology reports can compensate lack of large-scale data for anomaly detection methods. However, most of the current methods only use image-level pathological observations, failing to utilize the relevant anatomy mentions in reports. Furthermore, Natural Language Processing (NLP)-mined weak labels are noisy due to label sparsity and linguistic ambiguity. We propose an Anatomy-Guided chest X-ray Network (AGXNet) to address these issues of weak annotation. Our framework consists of a cascade of two networks, one responsible for identifying anatomical abnormalities and the second responsible for pathological observations. The critical component in our framework is an anatomy-guided attention module that aids the downstream observation network in focusing on the relevant anatomical regions generated by the anatomy network. We use Positive Unlabeled (PU) learning to account for the fact that lack of mention does not necessarily mean a negative label. Our quantitative and qualitative results on the MIMIC-CXR dataset demonstrate the effectiveness of AGXNet in disease and anatomical abnormality localization. Experiments on the NIH Chest X-ray dataset show that the learned feature representations are transferable and can achieve the state-of-the-art performances in disease classification and competitive disease localization results. Our code is available at https://github.com/batmanlab/AGXNet △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.12704,June 2022,932,Design and development of Spaiser remotely operated vehicle,Ihab ELAFF,"Design of new unmanned underwater vehicles (UUVs) is a continuous process since decades, where finding an optimal design for a specific application is still a challenging subject. New inspection class category...",https://www.springeropen.com//jeas.springeropen.com/articles/10.1186/s44147-022-00068-6,30 January 2022,
933,0.00461519317491824,933,Deep Generation of Heterogeneous Networks,"Chen Ling, Carl Yang, Liang Zhao","Heterogeneous graphs are ubiquitous data structures that can inherently capture multi-type and multi-modal interactions between objects. In recent years, research on encoding heterogeneous graph into latent representations have enjoyed a rapid increase. However, its reverse process, namely how to construct heterogeneous graphs from underlying representations and distributions have not been well explored due to several challenges in 1) modeling the local heterogeneous semantic distribution; 2) preserving the graph-structured distributions over the local semantics; and 3) characterizing the global heterogeneous graph distributions. To address these challenges, we propose a novel framework for heterogeneous graph generation (HGEN) that jointly captures the semantic, structural, and global distributions of heterogeneous graphs. Specifically, we propose a heterogeneous walk generator that hierarchically generates meta-paths and their path instances. In addition, a novel heterogeneous graph assembler is developed that can sample and combine the generated meta-path instances (e.g., walks) into heterogeneous graphs in a stratified manner. Theoretical analysis on the preservation of heterogeneous graph patterns by the proposed generation process has been performed. Extensive experiments on multiple real-world and synthetic heterogeneous graph datasets demonstrate the effectiveness of the proposed HGEN in generating realistic heterogeneous graphs. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.12336,June 2022,933,ESICM LIVES 2021: Part 1,Unknown,"This article is part of a Supplement:Volume 9
                                        Supplement 1",https://www.springeropen.com//icm-experimental.springeropen.com/articles/10.1186/s40635-021-00413-8,29 September 2021,
934,0.00514534320972017,934,DialogID: A Dialogic Instruction Dataset for Improving Teaching Effectiveness in Online Environments,"Jiahao Chen, Shuyan Huang, Zitao Liu, Weiqi Luo","Online dialogic instructions are a set of pedagogical instructions used in real-world online educational contexts to motivate students, help understand learning materials, and build effective study habits. In spite of the popularity and advantages of online learning, the education technology and educational data mining communities still suffer from the lack of large-scale, high-quality, and well-annotated teaching instruction datasets to study computational approaches to automatically detect online dialogic instructions and further improve the online teaching effectiveness. Therefore, in this paper, we present a dataset of online dialogic instruction detection, \textsc{DialogID}, which contains 30,431 effective dialogic instructions. These teaching instructions are well annotated into 8 categories. Furthermore, we utilize the prevalent pre-trained language models (PLMs) and propose a simple yet effective adversarial training learning paradigm to improve the quality and generalization of dialogic instruction detection. Extensive experiments demonstrate that our approach outperforms a wide range of baseline methods. The data and our code are available for research purposes from: https://github.com/ai4ed/DialogID. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.12034,June 2022,934,TKCA: a timely keystroke-based continuous user authentication with short keystroke sequence in uncontrolled settings,"Lulu Yang, Chen Li, Ruibang You, Bibo Tu and Linghui Li",Keystroke-based behavioral biometrics have been proven effective for continuous user authentication. Current state-of-the-art algorithms have achieved outstanding results in long text or short text collected b...,https://www.springeropen.com//cybersecurity.springeropen.com/articles/10.1186/s42400-021-00075-9,3 May 2021,
935,0.000678989392134464,935,Comparison of free-surface and conservative Allen-Cahn phase-field lattice Boltzmann method,"Christoph Schwarzmeier, Markus Holzer, Travis Mitchell, Moritz Lehmann, Fabian Häusl, Ulrich Rüde","This study compares the free-surface lattice Boltzmann method (FSLBM) with the conservative Allen-Cahn phase-field lattice Boltzmann method (PFLBM) in their ability to model two-phase flows in which the behavior of the system is dominated by the heavy phase. Both models are introduced and their individual properties, strengths and weaknesses are thoroughly discussed. Six numerical benchmark cases were simulated with both models, including (i) a standing gravity and (ii) capillary wave, (iii) an unconfined rising gas bubble in liquid, (iv) a Taylor bubble in a cylindrical tube, and (v) the vertical and (vi) oblique impact of a drop into a pool of liquid. Comparing the simulation results with either analytical models or experimental data from the literature, four major observations were made. Firstly, the PFLBM selected was able to simulate flows purely governed by surface tension with reasonable accuracy. Secondly, the FSLBM, a sharp interface model, generally requires a lower resolution than the PFLBM, a diffuse interface model. However, in the limit case of a standing wave, this was not observed. Thirdly, in simulations of a bubble moving in a liquid, the FSLBM accurately predicted the bubble's shape and rise velocity with low computational resolution. Finally, the PFLBM's accuracy is found to be sensitive to the choice of the model's mobility parameter and interface width. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11637,June 2022,935,A parameterizable spatiotemporal representation of popular dance styles for humanoid dancing characters,"João Lobato Oliveira, Luiz Naveda, Fabien Gouyon, Luis Paulo Reis, Paulo Sousa and Marc Leman",Dance movements are a complex class of human behavior which convey forms of non-verbal and subjective communication that are performed as cultural vocabularies in all human cultures. The singularity of dance f...,https://www.springeropen.com//asmp-eurasipjournals.springeropen.com/articles/10.1186/1687-4722-2012-18,19 June 2012,
936,0.00991669352293754,936,Mining Error Templates for Grammatical Error Correction,"Yue Zhang, Haochen Jiang, Zuyi Bao, Bo Zhang, Chen Li, Zhenghua Li","Some grammatical error correction (GEC) systems incorporate hand-crafted rules and achieve positive results. However, manually defining rules is time-consuming and laborious. In view of this, we propose a method to mine error templates for GEC automatically. An error template is a regular expression aiming at identifying text errors. We use the web crawler to acquire such error templates from the Internet. For each template, we further select the corresponding corrective action by using the language model perplexity as a criterion. We have accumulated 1,119 error templates for Chinese GEC based on this method. Experimental results on the newly proposed CTC-2021 Chinese GEC benchmark show that combing our error templates can effectively improve the performance of a strong GEC system, especially on two error types with very little training data. Our error templates are available at \url{https://github.com/HillZhang1999/gec_error_template}. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11569,June 2022,936,On the Performance Evaluation of 3D Reconstruction Techniques from a Sequence of Images,Ahmed Eid and Aly Farag,The performance evaluation of 3D reconstruction techniques is not a simple problem to solve. This is not only due to the increased dimensionality of the problem but also due to the lack of standardized and wid...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1948,15 August 2005,
937,0.000678989392134464,937,Prevent Car Accidents by Using AI,"Sri Siddhartha Reddy Gudemupati, Yen Ling Chao, Lakshmi Praneetha Kotikalapudi, Ebrima Ceesay","Transportation facilities are becoming more developed as society develops, and people's travel demand is increasing, but so are the traffic safety issues that arise as a result. And car accidents are a major issue all over the world. The cost of traffic fatalities and driver injuries has a significant impact on society. The use of machine learning techniques in the field of traffic accidents is becoming increasingly popular. Machine learning classifiers are used instead of traditional data mining techniques to produce better results and accuracy. As a result, this project conducts research on existing work related to accident prediction using machine learning. We will use crash data and weather data to train machine learning models to predict crash severity and reduce crashes. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11381,June 2022,937,HGM 2010 Programme / Abstract,Unknown,"This article is part of a Supplement:Volume 4
                                        Supplement 1",https://www.springeropen.com//thehugojournal.springeropen.com/articles/10.1007/s11568-010-9143-0,5 May 2010,
938,0.00673579331412596,938,Data-Mining of In-Situ TEM Experiments: Towards Understanding Nanoscale Fracture,"Dominik Steinberger, Inas Issa, Rachel Strobl, Peter J. Imrich, Daniel Kiener, Stefan Sandfeld","The lifetime and performance of any engineering component, from nanoscale sensors to macroscopic structures, are strongly influenced by fracture processes. Fracture itself is a highly localized event; originating at the atomic scale by bond breaking between individual atoms close to the crack tip. These processes, however, interact with defects such as dislocations or grain boundaries and influence phenomena on much larger length scales, ultimately giving rise to macroscopic behavior and engineering-scale fracture properties. This complex interplay is the fundamental reason why identifying the atomistic structural and energetic processes occurring at a crack tip remains a longstanding and still unsolved challenge. We develop a new analysis approach for combining quantitative in-situ observations of nanoscale deformation processes at a crack tip with three-dimensional reconstruction of the dislocation structure and advanced computational analysis to address plasticity and fracture initiation in a ductile metal. Our combinatorial approach reveals details of dislocation nucleation, their interaction process, and the local internal stress state, all of which were previously inaccessible to experiments. This enables us to describe fracture processes based on local crack driving forces on a dislocation level with a high fidelity that paves the way towards a better understanding and control of local failure processes in materials. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11355,June 2022,938,Information collection system of duck products based on IoT,"Lining Liu, Pingzeng Liu, Fujiang Wen, Chao Zhang, Rui Zhao, Maoling Yan and Xueru Yu","In view of the problems existing in the processing of duck products, such as complicated technology, difficulties in information collection and information linkage, and lack of dedicated information collection...",https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1186/s13638-018-1144-z,18 May 2018,
939,0.000678989392134464,939,AlphaMLDigger: A Novel Machine Learning Solution to Explore Excess Return on Investment,"Jimei Shen, Zhehu Yuan, Yifan Jin","How to quickly and automatically mine effective information and serve investment decisions has attracted more and more attention from academia and industry. And new challenges have arisen with the global pandemic. This paper proposes a two-phase AlphaMLDigger that effectively finds excessive returns in a highly fluctuated market. In phase 1, a deep sequential natural language processing (NLP) model is proposed to transfer Sina Microblog blogs to market sentiment. In phase 2, the predicted market sentiment is combined with social network indicator features and stock market history features to predict the stock movements with different Machine Learning models and optimizers. The results show that the ensemble models achieve an accuracy of 0.984 and significantly outperform the baseline model. In addition, we find that COVID-19 brings data shift to China's stock market. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11072,June 2022,939,Governing multi-agent systems,"Viviane Torres da Silva, Fernanda Duran, José Guedes and Carlos J. P. de Lucena","In order to cope with the heterogeneity, autonomy and diversity of interests among the different agents in open multi-agent systems, several governance mechanisms have been defined. Governance mechanism enforc...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192407,June 2007,
940,0.000678989392134464,940,Object Type Clustering using Markov Directly-Follow Multigraph in Object-Centric Process Mining,Amin Jalali,"Object-centric process mining is a new paradigm with more realistic assumptions about underlying data by considering several case notions, e.g., an order handling process can be analyzed based on order, item, package, and route case notions. Including many case notions can result in a very complex model. To cope with such complexity, this paper introduces a new approach to cluster similar case notions based on Markov Directly-Follow Multigraph, which is an extended version of the well-known Directly-Follow Graph supported by many industrial and academic process mining tools. This graph is used to calculate a similarity matrix for discovering clusters of similar case notions based on a threshold. A threshold tuning algorithm is also defined to identify sets of different clusters that can be discovered based on different levels of similarity. Thus, the cluster discovery will not rely on merely analysts' assumptions. The approach is implemented and released as a part of a python library, called processmining, and it is evaluated through a Purchase to Pay (P2P) object-centric event log file. Some discovered clusters are evaluated by discovering Directly Follow-Multigraph by flattening the log based on the clusters. The similarity between identified clusters is also evaluated by calculating the similarity between the behavior of the process models discovered for each case notion using inductive miner based on footprints conformance checking. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.11017,June 2022,940,A meta-analysis on educational technology in English language teaching,"Jafar Rahmati, Siros Izadpanah and Ali Shahnavaz","As more various types of computer-assisted language learning (CALL) programs have been incorporated into language classrooms over the recent decades, it has become more important to uncover whether, to what ex...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-021-00121-w,13 May 2021,
941,0.000678989392134464,941,SoccerCPD: Formation and Role Change-Point Detection in Soccer Matches Using Spatiotemporal Tracking Data,"Hyunsung Kim, Bit Kim, Dongwook Chung, Jinsung Yoon, Sang-Ki Ko","In fluid team sports such as soccer and basketball, analyzing team formation is one of the most intuitive ways to understand tactics from domain participants' point of view. However, existing approaches either assume that team formation is consistent throughout a match or assign formations frame-by-frame, which disagree with real situations. To tackle this issue, we propose a change-point detection framework named SoccerCPD that distinguishes tactically intended formation and role changes from temporary changes in soccer matches. We first assign roles to players frame-by-frame and perform two-step change-point detections: (1) formation change-point detection based on the sequence of role-adjacency matrices and (2) role change-point detection based on the sequence of role permutations. The evaluation of SoccerCPD using the ground truth annotated by domain experts shows that our method accurately detects the points of tactical changes and estimates the formation and role assignment per segment. Lastly, we introduce practical use-cases that domain participants can easily interpret and utilize. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10926,June 2022,941,A Statistical Approach to Automatic Speech Summarization,"Chiori Hori, Sadaoki Furui, Rob Malkin, Hua Yu and Alex Waibel","This paper proposes a statistical approach to automatic speech summarization. In our method, a set of words maximizing a summarization score indicating the appropriateness of summarization is extracted from au...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703211112,25 February 2003,
942,0.000678989392134464,942,A consistent and flexible framework for deep matrix factorizations,"Pierre De Handschutter, Nicolas Gillis","Deep matrix factorizations (deep MFs) are recent unsupervised data mining techniques inspired by constrained low-rank approximations. They aim to extract complex hierarchies of features within high-dimensional datasets. Most of the loss functions proposed in the literature to evaluate the quality of deep MF models and the underlying optimization frameworks are not consistent because different losses are used at different layers. In this paper, we introduce two meaningful loss functions for deep MF and present a generic framework to solve the corresponding optimization problems. We illustrate the effectiveness of this approach through the integration of various constraints and regularizations, such as sparsity, nonnegativity and minimum-volume. The models are successfully applied on both synthetic and real data, namely for hyperspectral unmixing and extraction of facial features. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10693,June 2022,942,A Posterior Union Model with Applications to Robust Speech and Speaker Recognition,"Ji Ming, Jie Lin and F. Jack Smith","This paper investigates speech and speaker recognition involving partial feature corruption, assuming unknown, time-varying noise characteristics. The probabilistic union model is extended from a conditional-p...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/75390,1 December 2006,
943,0.000678989392134464,943,Questions Are All You Need to Train a Dense Passage Retriever,"Devendra Singh Sachan, Mike Lewis, Dani Yogatama, Luke Zettlemoyer, Joelle Pineau, Manzil Zaheer","We introduce ART, a new corpus-level autoencoding approach for training dense retrieval models that does not require any labeled training data. Dense retrieval is a central challenge for open-domain tasks, such as Open QA, where state-of-the-art methods typically require large supervised datasets with custom hard-negative mining and denoising of positive examples. ART, in contrast, only requires access to unpaired inputs and outputs (e.g. questions and potential answer documents). It uses a new document-retrieval autoencoding scheme, where (1) an input question is used to retrieve a set of evidence documents, and (2) the documents are then used to compute the probability of reconstructing the original question. Training for retrieval based on question reconstruction enables effective unsupervised learning of both document and question encoders, which can be later incorporated into complete Open QA systems without any further finetuning. Extensive experiments demonstrate that ART obtains state-of-the-art results on multiple QA retrieval benchmarks with only generic initialization from a pre-trained language model, removing the need for labeled data and task-specific losses. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10658,June 2022,943,Deep learning in finance and banking: A literature review and classification,"Jian Huang, Junyi Chai and Stella Cho","Deep learning has been widely applied in computer vision, natural language processing, and audio-visual recognition. The overwhelming success of deep learning as a data processing technique has sparked the int...",https://www.springeropen.com//fbr.springeropen.com/articles/10.1186/s11782-020-00082-6,8 June 2020,
944,0.00991669352293754,944,Nimble GNN Embedding with Tensor-Train Decomposition,"Chunxing Yin, Da Zheng, Israt Nisa, Christos Faloutos, George Karypis, Richard Vuduc","This paper describes a new method for representing embedding tables of graph neural networks (GNNs) more compactly via tensor-train (TT) decomposition. We consider the scenario where (a) the graph data that lack node features, thereby requiring the learning of embeddings during training; and (b) we wish to exploit GPU platforms, where smaller tables are needed to reduce host-to-GPU communication even for large-memory GPUs. The use of TT enables a compact parameterization of the embedding, rendering it small enough to fit entirely on modern GPUs even for massive graphs. When combined with judicious schemes for initialization and hierarchical graph partitioning, this approach can reduce the size of node embedding vectors by 1,659 times to 81,362 times on large publicly available benchmark datasets, achieving comparable or better accuracy and significant speedups on multi-GPU systems. In some cases, our model without explicit node features on input can even match the accuracy of models that use node features. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10581,June 2022,944,Verification and Validation of a Fingerprint Image Registration Software,"Dejan Desovski, Vijai Gandikota, Yan Liu, Yue Jiang and Bojan Cukic",The need for reliable identification and authentication is driving the increased use of biometric devices and systems. Verification and validation techniques applicable to these systems are rather immature and...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/15940,1 December 2006,
945,0.000678989392134464,945,Depth-based clustering analysis of directional data,"Giuseppe Pandolfo, Antonio D'ambrosio","A new depth-based clustering procedure for directional data is proposed. Such method is fully non-parametric and has the advantages to be flexible and applicable even in high dimensions when a suitable notion of depth is adopted. The introduced technique is evaluated through an extensive simulation study. In addition, a real data example in text mining is given to explain its effectiveness in comparison with other existing directional clustering algorithms. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10447,June 2022,945,An industrial view on numerical simulation for aircraft aerodynamic design,Adel Abbas-Bayoumi and Klaus Becker,"In Airbus view, one major objective for the aircraft industry is the reduction of aircraft development lead-time and the provision of robust solutions with highly improved quality. In that context it is import...",https://www.springeropen.com//mathematicsinindustry.springeropen.com/articles/10.1186/2190-5983-1-10,12 December 2011,
946,0.000678989392134464,946,Model Joins: Enabling Analytics Over Joins of Absent Big Tables,"Ali Mohammadi Shanghooshabad, Peter Triantafillou","This work is motivated by two key facts. First, it is highly desirable to be able to learn and perform knowledge discovery and analytics (LKD) tasks without the need to access raw-data tables. This may be due to organizations finding it increasingly frustrating and costly to manage and maintain ever-growing tables, or for privacy reasons. Hence, compact models can be developed from the raw data and used instead of the tables. Second, oftentimes, LKD tasks are to be performed on a (potentially very large) table which is itself the result of joining separate (potentially very large) relational tables. But how can one do this, when the individual to-be-joined tables are absent? Here, we pose the following fundamental questions: Q1: How can one ""join models"" of (absent/deleted) tables or ""join models with other tables"" in a way that enables LKD as if it were performed on the join of the actual raw tables? Q2: What are appropriate models to use per table? Q3: As the model join would be an approximation of the actual data join, how can one evaluate the quality of the model join result? This work puts forth a framework, Model Join, addressing these challenges. The framework integrates and joins the per-table models of the absent tables and generates a uniform and independent sample that is a high-quality approximation of a uniform and independent sample of the actual raw-data join. The approximation stems from the models, but not from the Model Join framework. The sample obtained by the Model Join can be used to perform LKD downstream tasks, such as approximate query processing, classification, clustering, regression, association rule mining, visualization, and so on. To our knowledge, this is the first work with this agenda and solutions. Detailed experiments with TPC-DS data and synthetic data showcase Model Join's usefulness. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10434,June 2022,946,Using an empirical study to evaluate the feasibility of a new usability inspection technique for paper based prototypes of web applications,Luis Rivero and Tayana Conte,"Usability is one of the most important factors that determine the quality of Web applications, which can be verified performing usability inspection. This paper presents the Web Design Usability Evaluation (We...",https://www.springeropen.com//jserd.springeropen.com/articles/10.1186/2195-1721-1-2,29 October 2013,
947,0.000678989392134464,947,The Complexity of the Co-Occurrence Problem,"Philip Bille, Inge Li Gørtz, Tord Stordalen","Let $S$ be a string of length $n$ over an alphabet $Σ$ and let $Q$ be a subset of $Σ$ of size $q \geq 2$. The 'co-occurrence problem' is to construct a compact data structure that supports the following query: given an integer $w$ return the number of length-$w$ substrings of $S$ that contain each character of $Q$ at least once. This is a natural string problem with applications to, e.g., data mining, natural language processing, and DNA analysis. The state of the art is an $O(\sqrt{nq})$ space data structure that -- with some minor additions -- supports queries in $O(\log\log n)$ time [CPM 2021]. Our contributions are as follows. Firstly, we analyze the problem in terms of a new, natural parameter $d$, giving a simple data structure that uses $O(d)$ space and supports queries in $O(\log\log n)$ time. The preprocessing algorithm does a single pass over $S$, runs in expected $O(n)$ time, and uses $O(d)$ space in addition to the input. Furthermore, we show that $O(d)$ space is optimal and that $O(\log\log n)$-time queries are optimal given optimal space. Secondly, we bound $d = O(\sqrt{nq})$, giving clean bounds in terms of $n$ and $q$ that match the state of the art. Furthermore, we prove that $Ω(\sqrt{nq})$ bits of space is necessary in the worst case, meaning that the $O(\sqrt{nq})$ upper bound is tight to within polylogarithmic factors. All of our results are based on simple and intuitive combinatorial ideas that simplify the state of the art. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10383,June 2022,947,Evaluation of the trends in jobs and skill-sets using data analytics: a case study,"Armin Alibasic, Himanshu Upadhyay, Mecit Can Emre Simsekler, Thomas Kurfess, Wei Lee Woon and Mohammed Atif Omar","Fast-emerging technologies are making the job market dynamic, causing desirable skills to evolve continuously. It is therefore important to understand the transitions in the job market to proactively identify ...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00576-5,19 March 2022,
948,0.000678989392134464,948,Can process mining help in anomaly-based intrusion detection?,"Yinzheng Zhong, Alexei Lisitsa","In this paper, we consider the naive applications of process mining in network traffic comprehension, traffic anomaly detection, and intrusion detection. We standardise the procedure of transforming packet data into an event log. We mine multiple process models and analyse the process models mined with the inductive miner using ProM and the fuzzy miner using Disco. We compare the two types of process models extracted from event logs of differing sizes. We contrast the process models with the RFC TCP state transition diagram and the diagram by Bishop et al. We analyse the issues and challenges associated with process mining in intrusion detection and explain why naive process mining with network data is ineffective. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10379,June 2022,948,Network-theoretic modeling of complex activity using UK online sex advertisements,Mayank Kejriwal and Yao Gu,"Online sex has become a fast-growing business in both developing and developed network, with advertisements of (not necessarily unique) individuals numbering in the hundreds of millions across different Web po...",https://www.springeropen.com//appliednetsci.springeropen.com/articles/10.1007/s41109-020-00275-1,18 June 2020,
949,0.000678989392134464,949,Event-Case Correlation for Process Mining using Probabilistic Optimization,"Dina Bayomie, Claudio Di Ciccio, Jan Mendling","Process mining supports the analysis of the actual behavior and performance of business processes using event logs. % such as, e.g., sales transactions recorded by an ERP system. An essential requirement is that every event in the log must be associated with a unique case identifier (e.g., the order ID of an order-to-cash process). In reality, however, this case identifier may not always be present, especially when logs are acquired from different systems or extracted from non-process-aware information systems. In such settings, the event log needs to be pre-processed by grouping events into cases -- an operation known as event correlation. Existing techniques for correlating events have worked with assumptions to make the problem tractable: some assume the generative processes to be acyclic, while others require heuristic information or user input. Moreover, %these techniques' primary assumption is that they abstract the log to activities and timestamps, and miss the opportunity to use data attributes. % In this paper, we lift these assumptions and propose a new technique called EC-SA-Data based on probabilistic optimization. The technique takes as inputs a sequence of timestamped events (the log without case IDs), a process model describing the underlying business process, and constraints over the event attributes. Our approach returns an event log in which every event is associated with a case identifier. The technique allows users to incorporate rules on process knowledge and data constraints flexibly. The approach minimizes the misalignment between the generated log and the input process model, maximizes the support of the given data constraints over the correlated log, and the variance between activity durations across cases. Our experiments with various real-life datasets show the advantages of our approach over the state of the art. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.10009,June 2022,949,Discriminative Feature Selection via Multiclass Variable Memory Markov Model,"Noam Slonim, Gill Bejerano, Shai Fine and Naftali Tishby",We propose a novel feature selection method based on a variable memory Markov (VMM) model. The VMM was originally proposed as a generative model trying to preserve the original source statistics from training ...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S111086570321115X,25 February 2003,
950,0.000678989392134464,950,Multiple Fairness and Cardinality constraints for Students-Topics Grouping Problem,"Tai Le Quy, Gunnar Friege, Eirini Ntoutsi","Group work is a prevalent activity in educational settings, where students are often divided into topic-specific groups based on their preferences. The grouping should reflect the students' aspirations as much as possible. Usually, the resulting groups should also be balanced in terms of protected attributes like gender or race since studies indicate that students might learn better in a diverse group. Moreover, balancing the group cardinalities is also an essential requirement for fair workload distribution across the groups. In this paper, we introduce the multi-fair capacitated (MFC) grouping problem that fairly partitions students into non-overlapping groups while ensuring balanced group cardinalities (with a lower bound and an upper bound), and maximizing the diversity of members in terms of protected attributes. We propose two approaches: a heuristic method and a knapsack-based method to obtain the MFC grouping. The experiments on a real dataset and a semi-synthetic dataset show that our proposed methods can satisfy students' preferences well and deliver balanced and diverse groups regarding cardinality and the protected attribute, respectively. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09895,June 2022,950,A Model-Selection-Based Self-Splitting Gaussian Mixture Learning with Application to Speaker Identification,"Shih-Sian Cheng, Hsin-Min Wang and Hsin-Chia Fu",We propose a self-splitting Gaussian mixture learning (SGML) algorithm for Gaussian mixture modelling. The SGML algorithm is deterministic and is able to find an appropriate number of components of the Gaussia...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704407100,27 December 2004,
951,0.000678989392134464,951,A Novel Long-term Iterative Mining Scheme for Video Salient Object Detection,"Chenglizhao Chen, Hengsen Wang, Yuming Fang, Chong Peng","The existing state-of-the-art (SOTA) video salient object detection (VSOD) models have widely followed short-term methodology, which dynamically determines the balance between spatial and temporal saliency fusion by solely considering the current consecutive limited frames. However, the short-term methodology has one critical limitation, which conflicts with the real mechanism of our visual system -- a typical long-term methodology. As a result, failure cases keep showing up in the results of the current SOTA models, and the short-term methodology becomes the major technical bottleneck. To solve this problem, this paper proposes a novel VSOD approach, which performs VSOD in a complete long-term way. Our approach converts the sequential VSOD, a sequential task, to a data mining problem, i.e., decomposing the input video sequence to object proposals in advance and then mining salient object proposals as much as possible in an easy-to-hard way. Since all object proposals are simultaneously available, the proposed approach is a complete long-term approach, which can alleviate some difficulties rooted in conventional short-term approaches. In addition, we devised an online updating scheme that can grasp the most representative and trustworthy pattern profile of the salient objects, outputting framewise saliency maps with rich details and smoothing both spatially and temporally. The proposed approach outperforms almost all SOTA models on five widely used benchmark datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09564,June 2022,951,Secure Hashing of Dynamic Hand Signatures Using Wavelet-Fourier Compression with BioPhasor Mixing andDiscretization,"Yip Wai Kuan, Andrew B. J. Teoh and David C. L. Ngo",We introduce a novel method for secure computation of biometric hash on dynamic hand signatures using BioPhasor mixing anddiscretizatio...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/59125,1 December 2006,
952,0.000678989392134464,952,Numerical reconstruction for 3D nonlinear SAR imaging via a version of the convexification method,"Vo Anh Khoa, Michael Victor Klibanov, William Grayson Powell, Loc Hoang Nguyen","This work extends the applicability of our recent convexification-based algorithm for constructing images of the dielectric constant of buried or occluded target. We are orientated towards the detection of explosive-like targets such as antipersonnel land mines and improvised explosive devices in the non-invasive inspections of buildings. In our previous work, the method is posed in the perspective that we use multiple source locations running along a line of source to get a 2D image of the dielectric function. Mathematically, we solve a 1D coefficient inverse problem for a hyperbolic equation for each source location. Different from any conventional Born approximation-based technique for synthetic-aperture radar, this method does not need any linearization. In this paper, we attempt to verify the method using several 3D numerical tests with simulated data. We revisit the global convergence of the gradient descent method of our computational approach. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09539,June 2022,952,A Computationally Efficient Mel-Filter Bank VAD Algorithm for Distributed Speech Recognition Systems,"Damjan Vlaj, Bojan Kotnik, Bogomir Horvat and Zdravko Kačič",This paper presents a novel computationally efficient voice activity detection (VAD) algorithm and emphasizes the importance of such algorithms in distributed speech recognition (DSR) systems. When using VAD a...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.487,30 March 2005,
953,0.000678989392134464,953,Extracting Fast and Slow: User-Action Embedding with Inter-temporal Information,"Akira Matsui, Emilio Ferrara","With the recent development of technology, data on detailed human temporal behaviors has become available. Many methods have been proposed to mine those human dynamic behavior data and revealed valuable insights for research and businesses. However, most methods analyze only sequence of actions and do not study the inter-temporal information such as the time intervals between actions in a holistic manner. While actions and action time intervals are interdependent, it is challenging to integrate them because they have different natures: time and action. To overcome this challenge, we propose a unified method that analyzes user actions with intertemporal information (time interval). We simultaneously embed the user's action sequence and its time intervals to obtain a low-dimensional representation of the action along with intertemporal information. The paper demonstrates that the proposed method enables us to characterize user actions in terms of temporal context, using three real-world data sets. This paper demonstrates that explicit modeling of action sequences and inter-temporal user behavior information enable successful interpretable analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09535,June 2022,953,Probabilistic Aspects in Spoken Document Retrieval,"Wolfgang Macherey, Hans Jörg Viechtbauer and Hermann Ney","Accessing information in multimedia databases encompasses a wide range of applications in which spoken document retrieval (SDR) plays an important role. In SDR, a set of automatically transcribed speech docume...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865703210088,25 February 2003,
954,0.000678989392134464,954,Privacy-Preserving Analytics on Decentralized Social Graphs: The Case of Eigendecomposition,"Songlei Wang, Yifeng Zheng, Xiaohua Jia, Xun Yi","Analytics over social graphs allows to extract valuable knowledge and insights for many fields like community detection, fraud detection, and interest mining. In practice, decentralized social graphs frequently arise, where the social graph is not available to a single entity and is decentralized among a large number of users, each holding only a limited local view about the whole graph. Collecting the local views for analytics of decentralized social graphs raises critical privacy concerns, as they encode private information about the social interactions among individuals. In this paper, we design, implement, and evaluate PrivGED, a new system aimed at privacy-preserving analytics over decentralized social graphs. PrivGED focuses on the support for eigendecomposition, one popular and fundamental graph analytics task producing eigenvalues/eigenvectors over the adjacency matrix of a social graph and benefits various practical applications. PrivGED is built from a delicate synergy of insights on graph analytics, lightweight cryptography, and differential privacy, allowing users to securely contribute their local views on a decentralized social graph for a cloud-based eigendecomposition analytics service while gaining strong privacy protection. Extensive experiments over real-world social graph datasets demonstrate that PrivGED achieves accuracy comparable to the plaintext domain, with practically affordable performance superior to prior art. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09388,June 2022,954,Four dimensions to observe a Triple Helix: invention of ‘cored model’ and differentiation of institutional and functional spheres,Chunyan Zhou,"This investigation utilizes a matrix observation method to analyze triple helices. Firstly, by analyzing a unique Chinese academic phenomenon, the university-run enterprise (URE), a ‘cored model’ is proposed t...",https://www.springeropen.com//triplehelixjournal.springeropen.com/articles/10.1186/s40604-014-0011-0,29 October 2014,
955,0.00461519317491824,955,LogGENE: A smooth alternative to check loss for Deep Healthcare Inference Tasks,"Aryaman Jeendgar, Tanmay Devale, Soma S Dhavala, Snehanshu Saha","Mining large datasets and obtaining calibrated predictions from tem is of immediate relevance and utility in reliable deep learning. In our work, we develop methods for Deep neural networks based inferences in such datasets like the Gene Expression. However, unlike typical Deep learning methods, our inferential technique, while achieving state-of-the-art performance in terms of accuracy, can also provide explanations, and report uncertainty estimates. We adopt the Quantile Regression framework to predict full conditional quantiles for a given set of housekeeping gene expressions. Conditional quantiles, in addition to being useful in providing rich interpretations of the predictions, are also robust to measurement noise. Our technique is particularly consequential in High-throughput Genomics, an area which is ushering a new era in personalized health care, and targeted drug design and delivery. However, check loss, used in quantile regression to drive the estimation process is not differentiable. We propose log-cosh as a smooth-alternative to the check loss. We apply our methods on GEO microarray dataset. We also extend the method to binary classification setting. Furthermore, we investigate other consequences of the smoothness of the loss in faster convergence. We further apply the classification framework to other healthcare inference tasks such as heart disease, breast cancer, diabetes etc. As a test of generalization ability of our framework, other non-healthcare related data sets for regression and classification tasks are also evaluated. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09333,June 2022,955,Group-Oriented Fingerprinting for Multimedia Forensics,"Z. Jane Wang, Min Wu, Wade Trappe and K.J. Ray Liu",Digital fingerprinting of multimedia data involves embedding information in the content signal and offers protection to the digital rights of the content by allowing illegitimate usage of the content to be ide...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865704312151,28 October 2004,
956,0.000678989392134464,956,Going Green: Estimating the Potential of Green Jobs in Argentina,"Natalia Porto, Pablo de la Vega, Manuela Cerimelo","This paper aims to identify and characterize the potential of green jobs in Argentina, i.e., those that would benefit from a transition to a green economy, using occupational green potential scores calculated in US O*NET data. We apply the greenness scores to Argentine household survey data and estimate that 25% of workers are in green jobs, i.e., have a high green potential. However, when taking into account the informality dimension, we find that 15% of workers and 12% of wage earners are in formal green jobs. We then analyze the relationship between the greenness scores (with emphasis on the nexus with decent work) and various labor and demographic variables at the individual level. We find that for the full sample of workers the green potential is relatively greater for men, the elderly, those with very high qualifications, those in formal positions, and those in specific sectors such as construction, transportation, mining, and industry. These are the groups that are likely to be the most benefited by the greening of the Argentine economy. When we restrict the sample to wage earners, the green potential score is positively associated with informality. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09279,June 2022,956,Adaptive Markov Random Fields for Example-Based Super-resolution of Faces,Todd A Stephenson and Tsuhan Chen,"Image enhancement of low-resolution images can be done through methods such as interpolation, super-resolution using multiple video frames, and example-based super-resolution. Example-based super-resolution, i...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/31062,1 December 2006,
957,0.000678989392134464,957,Evolutionary Random Graph for Bitcoin Overlay and Blockchain Mining Networks,"Jacques Bou Abdo, Shuvalaxmi Dass, Basheer Qolomany, Liaquat Hossain","The world economy is experiencing the novel adoption of distributed currencies that are free from the control of central banks. Distributed currencies suffer from extreme volatility, and this can lead to catastrophic implications during future economic crisis. Understanding the dynamics of this new type of currencies is vital for empowering supervisory bodies from current reactive and manual incident responders to more proactive and well-informed planners. Bitcoin, the first and dominant distributed cryptocurrency, is still notoriously vague, especially for a financial instrument with market value exceeding 1 trillion. Modeling of bitcoin overlay network poses a number of important theoretical and methodological challenges. Current measuring approaches, for example, fail to identify the real network size of bitcoin miners. This drastically undermines the ability to predict forks, the suitable mining difficulty and most importantly the resilience of the network supporting bitcoin. In this work, we developed Evolutionary Random Graph, a theoretical model that describes the network of bitcoin miners. The correctness of this model has been validated using simulated and measure real bitcoin data. We then predicted forking, optimal mining difficulty, network size and consequently the network's inability to stand a drastic drop in bitcoin price using the current mining configuration. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.09011,June 2022,957,Robust Estimator for Non-Line-of-Sight Error Mitigation in Indoor Localization,"R. Casas, A. Marco, J. J. Guerrero and J. Falcó","Indoor localization systems are undoubtedly of interest in many application fields. Like outdoor systems, they suffer from non-line-of-sight (NLOS) errors which hinder their robustness and accuracy. Though man...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP/2006/43429,1 December 2006,
958,0.00991669352293754,958,Active Data Discovery: Mining Unknown Data using Submodular Information Measures,"Suraj Kothawade, Shivang Chopra, Saikat Ghosh, Rishabh Iyer","Active Learning is a very common yet powerful framework for iteratively and adaptively sampling subsets of the unlabeled sets with a human in the loop with the goal of achieving labeling efficiency. Most real world datasets have imbalance either in classes and slices, and correspondingly, parts of the dataset are rare. As a result, there has been a lot of work in designing active learning approaches for mining these rare data instances. Most approaches assume access to a seed set of instances which contain these rare data instances. However, in the event of more extreme rareness, it is reasonable to assume that these rare data instances (either classes or slices) may not even be present in the seed labeled set, and a critical need for the active learning paradigm is to efficiently discover these rare data instances. In this work, we provide an active data discovery framework which can mine unknown data slices and classes efficiently using the submodular conditional gain and submodular conditional mutual information functions. We provide a general algorithmic framework which works in a number of scenarios including image classification and object detection and works with both rare classes and rare slices present in the unlabeled set. We show significant accuracy and labeling efficiency gains with our approach compared to existing state-of-the-art active learning approaches for actively discovering these rare classes and slices. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.08566,June 2022,958,Rolling bearing fault diagnosis based on quantum LS-SVM,"Yuanyuan Li, Liyuan Song, Qichun Sun, Hua Xu, Xiaogang Li, Zhijun Fang and Wei Yao","Rolling bearing is an indispensable part of the contemporary industrial system, and its working conditions affect the state of the entire industrial system. Therefore, there is great engineering value to resea...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-022-00137-y,30 June 2022,
959,0.000678989392134464,959,Adversarial Robustness of Graph-based Anomaly Detection,"Yulin Zhu, Yuni Lai, Kaifa Zhao, Xiapu Luo, Mingquan Yuan, Jian Ren, Kai Zhou","Graph-based anomaly detection is becoming prevalent due to the powerful representation abilities of graphs as well as recent advances in graph mining techniques. These GAD tools, however, expose a new attacking surface, ironically due to their unique advantage of being able to exploit the relations among data. That is, attackers now can manipulate those relations (i.e., the structure of the graph) to allow target nodes to evade detection or degenerate the classification performance of the detection. In this paper, we exploit this vulnerability by designing the structural poisoning attacks to a FeXtra-based GAD system termed OddBall as well as the black box attacks against GCN-based GAD systems by attacking the imbalanced lienarized GCN ( LGCN ). Specifically, we formulate the attack against OddBall and LGCN as a one-level optimization problem by incorporating different regression techniques, where the key technical challenge is to efficiently solve the problem in a discrete domain. We propose a novel attack method termed BinarizedAttack based on gradient descent. Comparing to prior arts, BinarizedAttack can better use the gradient information, making it particularly suitable for solving discrete optimization problems, thus opening the door to studying a new type of attack against security analytic tools that rely on graph data. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.08260,June 2022,959,Overcoming Registration Uncertainty in Image Super-Resolution: Maximize or Marginalize?,"Lyndsey C Pickup, David P Capel, Stephen J Roberts and Andrew Zisserman","In multiple-image super-resolution, a high-resolution image is estimated from a number of lower-resolution images. This usually involves computing the parameters of a generative imaging model (such as geometri...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/23565,1 December 2007,
960,0.000678989392134464,960,All the World's a (Hyper)Graph: A Data Drama,"Corinna Coupette, Jilles Vreeken, Bastian Rieck","We introduce Hyperbard, a dataset of diverse relational data representations derived from Shakespeare's plays. Our representations range from simple graphs capturing character co-occurrence in single scenes to hypergraphs encoding complex communication settings and character contributions as hyperedges with edge-specific node weights. By making multiple intuitive representations readily available for experimentation, we facilitate rigorous representation robustness checks in graph learning, graph mining, and network analysis, highlighting the advantages and drawbacks of specific representations. Leveraging the data released in Hyperbard, we demonstrate that many solutions to popular graph mining problems are highly dependent on the representation choice, thus calling current graph curation practices into question. As an homage to our data source, and asserting that science can also be art, we present all our points in the form of a play. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.08225,June 2022,960,Algorithmic Aspects of Wireless Networks,"Xiuzhen Cheng, Wei Li and Taieb Znati",Unknown,https://www.springeropen.com//jwcn-eurasipjournals.springeropen.com/articles/10.1155/2007/52861,8 October 2007,
961,0.000678989392134464,961,Automated analysis of continuum fields from atomistic simulations using statistical machine learning,"Aruna Prakash, Stefan Sandfeld","Atomistic simulations of the molecular dynamics/statics kind are regularly used to study small scale plasticity. Contemporary simulations are performed with tens to hundreds of millions of atoms, with snapshots of these configurations written out at regular intervals for further analysis. Continuum scale constitutive models for material behavior can benefit from information on the atomic scale, in particular in terms of the deformation mechanisms, the accommodation of the total strain and partitioning of stress and strain fields in individual grains. In this work we develop a methodology using statistical data mining and machine learning algorithms to automate the analysis of continuum field variables in atomistic simulations. We focus on three important field variables: total strain, elastic strain and microrotation. Our results show that the elastic strain in individual grains exhibits a unimodal log-normal distribution, whilst the total strain and microrotation fields evidence a multimodal distribution. The peaks in the distribution of total strain are identified with a Gaussian mixture model and methods to circumvent overfitting problems are presented. Subsequently, we evaluate the identified peaks in terms of deformation mechanisms in a grain, which e.g., helps to quantify the strain for which individual deformation mechanisms are responsible. The overall statistics of the distributions over all grains are an important input for higher scale models, which ultimately also helps to be able to quantitatively discuss the implications for information transfer to phenomenological models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.08048,June 2022,961,Technology-supported management education: a systematic review of antecedents of learning effectiveness,Fabian Alexander Müller and Torsten Wulf,"This paper provides a systematic, multidisciplinary review of antecedents of the effectiveness of technology-supported management learning and highlights potential directions for future research. Passive knowl...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-020-00226-x,28 August 2020,
962,0.000678989392134464,962,Alexa Teacher Model: Pretraining and Distilling Multi-Billion-Parameter Encoders for Natural Language Understanding Systems,"Jack FitzGerald, Shankar Ananthakrishnan, Konstantine Arkoudas, Davide Bernardi, Abhishek Bhagia, Claudio Delli Bovi, Jin Cao, Rakesh Chada, Amit Chauhan, Luoxin Chen, Anurag Dwarakanath, Satyam Dwivedi, Turan Gojayev, Karthik Gopalakrishnan, Thomas Gueudre, Dilek Hakkani-Tur, Wael Hamza, Jonathan Hueser, Kevin Martin Jose, Haidar Khan, Beiye Liu, Jianhua Lu, Alessandro Manzotti, Pradeep Natarajan, Karolina Owczarzak , et al. (16 additional authors not shown)","We present results from a large-scale experiment on pretraining encoders with non-embedding parameter counts ranging from 700M to 9.3B, their subsequent distillation into smaller models ranging from 17M-170M parameters, and their application to the Natural Language Understanding (NLU) component of a virtual assistant system. Though we train using 70% spoken-form data, our teacher models perform comparably to XLM-R and mT5 when evaluated on the written-form Cross-lingual Natural Language Inference (XNLI) corpus. We perform a second stage of pretraining on our teacher models using in-domain data from our system, improving error rates by 3.86% relative for intent classification and 7.01% relative for slot filling. We find that even a 170M-parameter model distilled from our Stage 2 teacher model has 2.88% better intent classification and 7.69% better slot filling error rates when compared to the 2.3B-parameter teacher trained only on public data (Stage 1), emphasizing the importance of in-domain data for pretraining. When evaluated offline using labeled NLU data, our 17M-parameter Stage 2 distilled model outperforms both XLM-R Base (85M params) and DistillBERT (42M params) by 4.23% to 6.14%, respectively. Finally, we present results from a full virtual assistant experimentation platform, where we find that models trained using our pretraining and distillation pipeline outperform models distilled from 85M-parameter teachers by 3.74%-4.91% on an automatic measurement of full-system user dissatisfaction. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07808,June 2022,962,Transforming Signal Processing Applications into Parallel Implementations,"Ed F. Deprettre, Roger Woods, Ingrid Verbauwhede and Erwin de Kock",Unknown,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/95760,1 December 2007,
963,0.000678989392134464,963,Hyperparameter Sensitivity in Deep Outlier Detection: Analysis and a Scalable Hyper-Ensemble Solution,"Xueying Ding, Lingxiao Zhao, Leman Akoglu","Outlier detection (OD) literature exhibits numerous algorithms as it applies to diverse domains. However, given a new detection task, it is unclear how to choose an algorithm to use, nor how to set its hyperparameter(s) (HPs) in unsupervised settings. HP tuning is an ever-growing problem with the arrival of many new detectors based on deep learning, which usually come with a long list of HPs. Surprisingly, the issue of model selection in the outlier mining literature has been ""the elephant in the room""; a significant factor in unlocking the utmost potential of deep methods, yet little said or done to systematically tackle the issue. In the first part of this paper, we conduct the first large-scale analysis on the HP sensitivity of deep OD methods, and through more than 35,000 trained models, quantitatively demonstrate that model selection is inevitable. Next, we design a HP-robust and scalable deep hyper-ensemble model called ROBOD that assembles models with varying HP configurations, bypassing the choice paralysis. Importantly, we introduce novel strategies to speed up ensemble training, such as parameter sharing, batch/simultaneous training, and data subsampling, that allow us to train fewer models with fewer parameters. Extensive experiments on both image and tabular datasets show that ROBOD achieves and retains robust, state-of-the-art detection performance as compared to its modern counterparts, while taking only $2$-$10$\% of the time by the naive hyper-ensemble with independent training. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07647,June 2022,963,Segmentation of epithelial human type 2 cell images for the indirect immune fluorescence based on modified quantum entropy,Abu-Zinadah Hanaa and Abdel Azim Gamil,"The autoimmune disorders such as rheumatoid, arthritis, and scleroderma are connective tissue diseases (CTD). Autoimmune diseases are generally diagnosed using the antinuclear antibody (ANA) blood test. This t...",https://www.springeropen.com//jivp-eurasipjournals.springeropen.com/articles/10.1186/s13640-021-00554-6,23 April 2021,
964,0.000678989392134464,964,Sublinear Algorithms for Hierarchical Clustering,"Arpit Agarwal, Sanjeev Khanna, Huan Li, Prathamesh Patil","Hierarchical clustering over graphs is a fundamental task in data mining and machine learning with applications in domains such as phylogenetics, social network analysis, and information retrieval. Specifically, we consider the recently popularized objective function for hierarchical clustering due to Dasgupta. Previous algorithms for (approximately) minimizing this objective function require linear time/space complexity. In many applications the underlying graph can be massive in size making it computationally challenging to process the graph even using a linear time/space algorithm. As a result, there is a strong interest in designing algorithms that can perform global computation using only sublinear resources. The focus of this work is to study hierarchical clustering for massive graphs under three well-studied models of sublinear computation which focus on space, time, and communication, respectively, as the primary resources to optimize: (1) (dynamic) streaming model where edges are presented as a stream, (2) query model where the graph is queried using neighbor and degree queries, (3) MPC model where the graph edges are partitioned over several machines connected via a communication channel. We design sublinear algorithms for hierarchical clustering in all three models above. At the heart of our algorithmic results is a view of the objective in terms of cuts in the graph, which allows us to use a relaxed notion of cut sparsifiers to do hierarchical clustering while introducing only a small distortion in the objective function. Our main algorithmic contributions are then to show how cut sparsifiers of the desired form can be efficiently constructed in the query model and the MPC model. We complement our algorithmic results by establishing nearly matching lower bounds that rule out the possibility of designing better algorithms in each of these models. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07633,June 2022,964,Bootstrapping JISA—Letter from the Editors-in-Chief,Fabio Kon and Gordon Blair,Unknown,https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1007/s13174-010-0008-5,3 April 2010,
965,0.000678989392134464,965,How GNNs Facilitate CNNs in Mining Geometric Information from Large-Scale Medical Images,"Yiqing Shen, Bingxin Zhou, Xinye Xiong, Ruitian Gao, Yu Guang Wang","Gigapixel medical images provide massive data, both morphological textures and spatial information, to be mined. Due to the large data scale in histology, deep learning methods play an increasingly significant role as feature extractors. Existing solutions heavily rely on convolutional neural networks (CNNs) for global pixel-level analysis, leaving the underlying local geometric structure such as the interaction between cells in the tumor microenvironment unexplored. The topological structure in medical images, as proven to be closely related to tumor evolution, can be well characterized by graphs. To obtain a more comprehensive representation for downstream oncology tasks, we propose a fusion framework for enhancing the global image-level representation captured by CNNs with the geometry of cell-level spatial information learned by graph neural networks (GNN). The fusion layer optimizes an integration between collaborative features of global images and cell graphs. Two fusion strategies have been developed: one with MLP which is simple but turns out efficient through fine-tuning, and the other with Transformer gains a champion in fusing multiple networks. We evaluate our fusion strategies on histology datasets curated from large patient cohorts of colorectal and gastric cancers for three biomarker prediction tasks. Both two models outperform plain CNNs or GNNs, reaching a consistent AUC improvement of more than 5% on various network backbones. The experimental results yield the necessity for combining image-level morphological features with cell spatial relations in medical image analysis. Codes are available at https://github.com/yiqings/HEGnnEnhanceCnn. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07599,June 2022,965,Managerial practices and school efficiency: a data envelopment analysis across OECD and MENA countries using TIMSS 2019 data,Aditi Bhutoria and Nayyaf Aljabri,School-level inefficiencies and mismanagement can have serious repercussions for human resource development and labor market outcomes. This paper investigates the extent and consequences of existing technical ...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-022-00147-3,19 December 2022,
966,0.000678989392134464,966,READ: Aggregating Reconstruction Error into Out-of-distribution Detection,"Wenyu Jiang, Yuxin Ge, Hao Cheng, Mingcai Chen, Shuai Feng, Chongjun Wang","Detecting out-of-distribution (OOD) samples is crucial to the safe deployment of a classifier in the real world. However, deep neural networks are known to be overconfident for abnormal data. Existing works directly design score function by mining the inconsistency from classifier for in-distribution (ID) and OOD. In this paper, we further complement this inconsistency with reconstruction error, based on the assumption that an autoencoder trained on ID data can not reconstruct OOD as well as ID. We propose a novel method, READ (Reconstruction Error Aggregated Detector), to unify inconsistencies from classifier and autoencoder. Specifically, the reconstruction error of raw pixels is transformed to latent space of classifier. We show that the transformed reconstruction error bridges the semantic gap and inherits detection performance from the original. Moreover, we propose an adjustment strategy to alleviate the overconfidence problem of autoencoder according to a fine-grained characterization of OOD data. Under two scenarios of pre-training and retraining, we respectively present two variants of our method, namely READ-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED (Euclidean Distance) which retrains the classifier. Our methods do not require access to test time OOD data for fine-tuning hyperparameters. Finally, we demonstrate the effectiveness of the proposed methods through extensive comparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10 pre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8% compared with previous state-of-the-art. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07459,June 2022,966,Collision-aware interactive simulation using graph neural networks,"Xin Zhu, Yinling Qian, Qiong Wang, Ziliang Feng and Pheng-Ann Heng","Deep simulations have gained widespread attention owing to their excellent acceleration performances. However, these methods cannot provide effective collision detection and response strategies. We propose a d...",https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-022-00113-4,7 June 2022,
967,0.000678989392134464,967,"""Why Here and Not There?"" -- Diverse Contrasting Explanations of Dimensionality Reduction","André Artelt, Alexander Schulz, Barbara Hammer","Dimensionality reduction is a popular preprocessing and a widely used tool in data mining. Transparency, which is usually achieved by means of explanations, is nowadays a widely accepted and crucial requirement of machine learning based systems like classifiers and recommender systems. However, transparency of dimensionality reduction and other data mining tools have not been considered in much depth yet, still it is crucial to understand their behavior -- in particular practitioners might want to understand why a specific sample got mapped to a specific location. In order to (locally) understand the behavior of a given dimensionality reduction method, we introduce the abstract concept of contrasting explanations for dimensionality reduction, and apply a realization of this concept to the specific application of explaining two dimensional data visualization. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07391,June 2022,967,A content analysis of pre-college lesson plans on human evolution,Rebecca L. Hite,"One of the most fundamental understandings within biology is evolution, yet often ascribed as one of the most misunderstood scientific concepts by the American public. Despite not being explicitly mentioned in...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-020-00028-1,11 September 2020,
968,0.000678989392134464,968,A Fast Heuristic for Computing Geodesic Cores in Large Networks,"Florian Seiffarth, Tamás Horváth, Stefan Wrobel","Motivated by the increasing interest in applications of graph geodesic convexity in machine learning and data mining, we present a heuristic for computing the geodesic convex hull of node sets in networks. It generates a set of almost maximal outerplanar spanning subgraphs for the input graph, computes the geodesic closure in each of these graphs, and regards a node as an element of the convex hull if it belongs to the closed sets for at least a user specified number of outerplanar graphs. Our heuristic algorithm runs in time linear in the number of edges of the input graph, i.e., it is faster with one order of magnitude than the standard algorithm computing the closure exactly. Its performance is evaluated empirically by approximating convexity based core-periphery decomposition of networks. Our experimental results with large real-world networks show that for most networks, the proposed heuristic was able to produce close approximations significantly faster than the standard algorithm computing the exact convex hulls. For example, while our algorithm calculated an approximate core-periphery decomposition in 5 hours or less for networks with more than 20 million edges, the standard algorithm did not terminate within 50 days. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07350,June 2022,968,Artificial intelligence and edge computing for teaching quality evaluation based on 5G-enabled wireless communication technology,Feng Li and Caohui Wang,Cloud computing and artificial intelligence are now widely used for classroom teaching in higher learning institutes. The digital teaching supported to ICT technologies in colleges serves as a central point fo...,https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-023-00418-6,23 March 2023,
969,0.000678989392134464,969,Faster Decomposition of Weighted Graphs into Cliques using Fisher's Inequality,"Shweta Jain, Yosuke Mizutani, Blair Sullivan","Mining groups of genes that consistently co-express is an important problem in biomedical research, where it is critical for applications such as drug-repositioning and designing new disease treatments. Recently, Cooley et al. modeled this problem as Exact Weighted Clique Decomposition (EWCD) in which, given an edge-weighted graph $G$ and a positive integer $k$, the goal is to decompose $G$ into at most $k$ (overlapping) weighted cliques so that an edge's weight is exactly equal to the sum of weights for cliques it participates in. They show EWCD is fixed-parameter-tractable, giving a $4^k$-kernel alongside a backtracking algorithm (together called cricca) to iteratively build a decomposition. Unfortunately, because of inherent exponential growth in the space of potential solutions, cricca is typically able to decompose graphs only when $k \leq 11$. In this work, we establish reduction rules that exponentially decrease the size of the kernel (from $4^k$ to $k2^k$) for EWCD. In addition, we use insights about the structure of potential solutions to give new search rules that speed up the decomposition algorithm. At the core of our techniques is a result from combinatorial design theory called Fisher's inequality characterizing set systems with restricted intersections. We deploy our kernelization and decomposition algorithms (together called DeCAF) on a corpus of biologically-inspired data and obtain over two orders of magnitude speed-up over cricca. As a result, DeCAF scales to instances with $k \geq 17$. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07286,June 2022,969,Real-Time Adaptive Foreground/Background Segmentation,"Darren E. Butler, V. Michael Bove Jr. and Sridha Sridharan","The automatic analysis of digital video scenes often requires the segmentation of moving objects from a static background. Historically, algorithms developed for this purpose have been restricted to small fram...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2292,25 August 2005,
970,0.000678989392134464,970,TeKo: Text-Rich Graph Neural Networks with External Knowledge,"Zhizhi Yu, Di Jin, Jianguo Wei, Ziyang Liu, Yue Shang, Yun Xiao, Jiawei Han, Lingfei Wu","Graph Neural Networks (GNNs) have gained great popularity in tackling various analytical tasks on graph-structured data (i.e., networks). Typical GNNs and their variants follow a message-passing manner that obtains network representations by the feature propagation process along network topology, which however ignore the rich textual semantics (e.g., local word-sequence) that exist in many real-world networks. Existing methods for text-rich networks integrate textual semantics by mainly utilizing internal information such as topics or phrases/words, which often suffer from an inability to comprehensively mine the text semantics, limiting the reciprocal guidance between network structure and text semantics. To address these problems, we propose a novel text-rich graph neural network with external knowledge (TeKo), in order to take full advantage of both structural and textual information within text-rich networks. Specifically, we first present a flexible heterogeneous semantic network that incorporates high-quality entities and interactions among documents and entities. We then introduce two types of external knowledge, that is, structured triplets and unstructured entity description, to gain a deeper insight into textual semantics. We further design a reciprocal convolutional mechanism for the constructed heterogeneous semantic network, enabling network structure and textual semantics to collaboratively enhance each other and learn high-level network representations. Extensive experimental results on four public text-rich networks as well as a large-scale e-commerce searching dataset illustrate the superior performance of TeKo over state-of-the-art baselines. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.07253,June 2022,970,In-depth analysis and open challenges of Mist Computing,"Juan José López Escobar, Rebeca P. Díaz Redondo and Felipe Gil-Castiñeira","The advent and consolidation of the Massive Internet of Things (MIoT) comes with a need for new architectures to process the massive amount of generated information. A new approach, Mist Computing, entails a s...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/s13677-022-00354-x,19 November 2022,
971,0.000678989392134464,971,OSN Dashboard Tool For Sentiment Analysis,"Andreas Kilde Lien, Lars Martin Randem, Hans Petter Fauchald Taralrud, Maryam Edalati","The amount of opinionated data on the internet is rapidly increasing. More and more people are sharing their ideas and opinions in reviews, discussion forums, microblogs and general social media. As opinions are central in all human activities, sentiment analysis has been applied to gain insights in this type of data. There are proposed several approaches for sentiment classification. The major drawback is the lack of standardized solutions for classification and high-level visualization. In this study, a sentiment analyzer dashboard for online social networking analysis is proposed. This, to enable people gaining insights in topics interesting to them. The tool allows users to run the desired sentiment analysis algorithm in the dashboard. In addition to providing several visualization types, the dashboard facilitates raw data results from the sentiment classification which can be downloaded for further analysis. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06935,June 2022,971,Robust Background Subtraction with Foreground Validation for Urban Traffic Video,Sen-Ching S. Cheung and Chandrika Kamath,Identifying moving objects in a video sequence is a fundamental and critical task in many computer-vision applications. Background subtraction techniques are commonly used to separate foreground moving objects...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2330,25 August 2005,
972,0.000678989392134464,972,Temporal Multimodal Multivariate Learning,"Hyoshin Park, Justice Darko, Niharika Deshpande, Venktesh Pandey, Hui Su, Masahiro Ono, Dedrick Barkely, Larkin Folsom, Derek Posselt, Steve Chien","We introduce temporal multimodal multivariate learning, a new family of decision making models that can indirectly learn and transfer online information from simultaneous observations of a probability distribution with more than one peak or more than one outcome variable from one time stage to another. We approximate the posterior by sequentially removing additional uncertainties across different variables and time, based on data-physics driven correlation, to address a broader class of challenging time-dependent decision-making problems under uncertainty. Extensive experiments on real-world datasets ( i.e., urban traffic data and hurricane ensemble forecasting data) demonstrate the superior performance of the proposed targeted decision-making over the state-of-the-art baseline prediction methods across various settings. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06878,June 2022,972,Data science approach to stock prices forecasting in Indonesia during Covid-19 using Long Short-Term Memory (LSTM),Widodo Budiharto,"Stock market process is full of uncertainty; hence stock prices forecasting very important in finance and business. For stockbrokers, understanding trends and supported by prediction software for forecasting i...",https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00430-0,11 March 2021,
973,0.000678989392134464,973,Learning from Uncurated Regular Expressions,Michael J. Mior,"Significant work has been done on learning regular expressions from a set of data values. Depending on the domain, this approach can be very successful. However, significant time is required to learn these expressions and the resulting expressions can become either very complex or inaccurate in the presence of dirty data. The alternative of manually writing regular expressions becomes unattractive when faced with a large number of values that must be matched. As an alternative, we propose learning from a large corpus of manually authored, but uncurated regular expressions mined from a public repository. The advantage of this approach is that we are able to extract salient features from a set of strings with limited overhead to feature engineering. Since the set of regular expressions covers a wide range of application domains, we expect them to be widely applicable. To demonstrate the potential effectiveness of our approach, we train a model using the extracted corpus of regular expressions for the class of semantic type classification. While our approach yields results that are overall inferior to the state-of-the-art, our feature extraction code is an order of magnitude smaller, and our model outperforms a popular existing approach on some classes. We also demonstrate the possibility of using uncurated regular expressions for unsupervised learning. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06747,June 2022,973,Precise Image Registration with Structural Similarity Error Measurement Applied to Superresolution,"Mahmood Amintoosi, Mahmood Fathy and Nasser Mozayani",Precise image registration is a fundamental task in many computer vision algorithms including superresolution methods. The well known Lucas-Kanade (LK) algorithm is a very popular and efficient method among th...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2009/305479,30 June 2009,
974,0.000678989392134464,974,Shopping Queries Dataset: A Large-Scale ESCI Benchmark for Improving Product Search,"Chandan K. Reddy, Lluís Màrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza, Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, Karthik Subbian","Improving the quality of search results can significantly enhance users experience and engagement with search engines. In spite of several recent advancements in the fields of machine learning and data mining, correctly classifying items for a particular user search query has been a long-standing challenge, which still has a large room for improvement. This paper introduces the ""Shopping Queries Dataset"", a large dataset of difficult Amazon search queries and results, publicly released with the aim of fostering research in improving the quality of search results. The dataset contains around 130 thousand unique queries and 2.6 million manually labeled (query,product) relevance judgements. The dataset is multilingual with queries in English, Japanese, and Spanish. The Shopping Queries Dataset is being used in one of the KDDCup'22 challenges. In this paper, we describe the dataset and present three evaluation tasks along with baseline results: (i) ranking the results list, (ii) classifying product results into relevance categories, and (iii) identifying substitute products for a given query. We anticipate that this data will become the gold standard for future research in the topic of product search. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06588,June 2022,974,Raised Cosine Interpolator Filter for Digital Magnetic Recording Channel,Hui-Feng Tsai and Zang-Hao Jiang,"Interpolators have found widespread applications in communication systems such as multimedia. In this paper, the interpolated timing recovery employing raised cosine pulse for digital magnetic recording channe...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2011/651960,9 March 2011,
975,0.000678989392134464,975,Electron-phonon coupling strength from ab initio frozen-phonon approach,"Yang Sun, Feng Zhang, Cai-Zhuang Wang, Kai-Ming Ho, Igor I. Mazin, Vladimir Antropov","We propose a fast method for high-throughput screening of potential superconducting materials. The method is based on calculating metallic screening of zone-center phonon modes, which provides an accurate estimate for the electron-phonon coupling strength. This method is complementary to the recently proposed Rigid Muffin Tin (RMT) method, which amounts to integrating the electron-phonon coupling over the entire Brillouin zone (as opposed to the zone center), but in a relatively inferior approximation. We illustrate the use of this method by applying it to MgB$_\text{2}$, where the high-temperature superconductivity is known to be driven largely by the zone-center modes, and compare it to a sister compound AlB$_\text{2}$. We further illustrate the usage of this descriptor by screening a large number of binary hydrides, for which accurate first-principle calculations of electron-phonon coupling have been recently published. Together with the RMT descriptor, this method opens a way to perform initial high-throughput screening in search of conventional superconductors via machine learning or data mining. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06503,June 2022,975,Cloud computing and its interest in saving energy: the use case of a private cloud,"Robert Basmadjian, Hermann De Meer, Ricardo Lent and Giovanni Giuliani","Cloud computing data centres, due to their housing of powerful ICT equipment, are high energy consumers and therefore accountable for large quantities of emissions. Therefore, energy saving strategies applicab...",https://www.springeropen.com//journalofcloudcomputing.springeropen.com/articles/10.1186/2192-113X-1-5,8 June 2012,
976,0.000678989392134464,976,Reinforcement Learning-based Placement of Charging Stations in Urban Road Networks,"Leonie von Wahl, Nicolas Tempelmeier, Ashutosh Sao, Elena Demidova","The transition from conventional mobility to electromobility largely depends on charging infrastructure availability and optimal placement.This paper examines the optimal placement of charging stations in urban areas. We maximise the charging infrastructure supply over the area and minimise waiting, travel, and charging times while setting budget constraints. Moreover, we include the possibility of charging vehicles at home to obtain a more refined estimation of the actual charging demand throughout the urban area. We formulate the Placement of Charging Stations problem as a non-linear integer optimisation problem that seeks the optimal positions for charging stations and the optimal number of charging piles of different charging types. We design a novel Deep Reinforcement Learning approach to solve the charging station placement problem (PCRL). Extensive experiments on real-world datasets show how the PCRL reduces the waiting and travel time while increasing the benefit of the charging plan compared to five baselines. Compared to the existing infrastructure, we can reduce the waiting time by up to 97% and increase the benefit up to 497%. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.06011,June 2022,976,Hypernetwork science via high-order hypergraph walks,"Sinan G. Aksoy, Cliff Joslyn, Carlos Ortiz Marrero, Brenda Praggastis and Emilie Purvine","We propose high-order hypergraph walks as a framework to generalize graph-based network science techniques to hypergraphs. Edge incidence in hypergraphs is quantitative, yielding hypergraph walks with both len...",https://www.springeropen.com//epjdatascience.springeropen.com/articles/10.1140/epjds/s13688-020-00231-0,10 June 2020,
977,0.000678989392134464,977,Mining Multi-Label Samples from Single Positive Labels,"Youngin Cho, Daejin Kim, Mohammad Azam Khan, Jaegul Choo","Conditional generative adversarial networks (cGANs) have shown superior results in class-conditional generation tasks. To simultaneously control multiple conditions, cGANs require multi-label training datasets, where multiple labels can be assigned to each data instance. Nevertheless, the tremendous annotation cost limits the accessibility of multi-label datasets in real-world scenarios. Therefore, in this study we explore the practical setting called the single positive setting, where each data instance is annotated by only one positive label with no explicit negative labels. To generate multi-label data in the single positive setting, we propose a novel sampling approach called single-to-multi-label (S2M) sampling, based on the Markov chain Monte Carlo method. As a widely applicable ""add-on"" method, our proposed S2M sampling method enables existing unconditional and conditional GANs to draw high-quality multi-label data with a minimal annotation cost. Extensive experiments on real image datasets verify the effectiveness and correctness of our method, even when compared to a model trained with fully annotated datasets. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05764,June 2022,977,Estimating Position of Mobile Terminals with Survey Data,"Michael McGuire, Konstantinos N. Plataniotis and Anastasios N. Venetsanopoulos",Estimating the position of mobile terminals is an important problem for cellular networks. This paper describes low cost methods of locating mobile terminals in urban environments. These methods use data colle...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702000446,14 January 2002,
978,0.000678989392134464,978,DRNet: Decomposition and Reconstruction Network for Remote Physiological Measurement,"Yuhang Dong, Gongping Yang, Yilong Yin","Remote photoplethysmography (rPPG) based physiological measurement has great application values in affective computing, non-contact health monitoring, telehealth monitoring, etc, which has become increasingly important especially during the COVID-19 pandemic. Existing methods are generally divided into two groups. The first focuses on mining the subtle blood volume pulse (BVP) signals from face videos, but seldom explicitly models the noises that dominate face video content. They are susceptible to the noises and may suffer from poor generalization ability in unseen scenarios. The second focuses on modeling noisy data directly, resulting in suboptimal performance due to the lack of regularity of these severe random noises. In this paper, we propose a Decomposition and Reconstruction Network (DRNet) focusing on the modeling of physiological features rather than noisy data. A novel cycle loss is proposed to constrain the periodicity of physiological information. Besides, a plug-and-play Spatial Attention Block (SAB) is proposed to enhance features along with the spatial location information. Furthermore, an efficient Patch Cropping (PC) augmentation strategy is proposed to synthesize augmented samples with different noise and features. Extensive experiments on different public datasets as well as the cross-database testing demonstrate the effectiveness of our approach. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05687,June 2022,978,Supporting undergraduate students’ developing water literacy during a global pandemic: a longitudinal study,"Silvia-Jessica Mostacedo-Marasovic, Diane Lally, Destini N. Petitt, Holly White and Cory Forbes","To prepare students to address water-related challenges, undergraduate STEM education must provide them with opportunities to learn and reason about water issues. Water in Society is an introductory-level, inn...",https://www.springeropen.com//diser.springeropen.com/articles/10.1186/s43031-022-00049-y,7 March 2022,
979,0.000678989392134464,979,Incremental Information Gain Mining Of Temporal Relational Streams,"Ken Pu, Limin Ma","This paper studies the problem of mining for data values with high information gain in relational tables. High information gain can help data analysts and secondary data mining algorithms gain insights into strong statistical dependencies and causality relationship between key metrics. In this paper, we will study the problem of high information gain identification for scenarios involving temporal relations where new records are added continuously to the relations. We show that information gain can be efficiently maintained in an incremental fashion, making it possible to monitor continuously high information gain values. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05554,June 2022,979,Editorial: advances in deep learning techniques for biomedical imaging,Chuang Niu and Ge Wang,Unknown,https://www.springeropen.com//vciba.springeropen.com/articles/10.1186/s42492-023-00139-2,21 June 2023,
980,0.000678989392134464,980,Bilateral Dependency Optimization: Defending Against Model-inversion Attacks,"Xiong Peng, Feng Liu, Jingfen Zhang, Long Lan, Junjie Ye, Tongliang Liu, Bo Han","Through using only a well-trained classifier, model-inversion (MI) attacks can recover the data used for training the classifier, leading to the privacy leakage of the training data. To defend against MI attacks, previous work utilizes a unilateral dependency optimization strategy, i.e., minimizing the dependency between inputs (i.e., features) and outputs (i.e., labels) during training the classifier. However, such a minimization process conflicts with minimizing the supervised loss that aims to maximize the dependency between inputs and outputs, causing an explicit trade-off between model robustness against MI attacks and model utility on classification tasks. In this paper, we aim to minimize the dependency between the latent representations and the inputs while maximizing the dependency between latent representations and the outputs, named a bilateral dependency optimization (BiDO) strategy. In particular, we use the dependency constraints as a universally applicable regularizer in addition to commonly used losses for deep neural networks (e.g., cross-entropy), which can be instantiated with appropriate dependency criteria according to different tasks. To verify the efficacy of our strategy, we propose two implementations of BiDO, by using two different dependency measures: BiDO with constrained covariance (BiDO-COCO) and BiDO with Hilbert-Schmidt Independence Criterion (BiDO-HSIC). Experiments show that BiDO achieves the state-of-the-art defense performance for a variety of datasets, classifiers, and MI attacks while suffering a minor classification-accuracy drop compared to the well-trained classifier with no defense, which lights up a novel road to defend against MI attacks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05483,June 2022,980,Image Steganalysis with Binary Similarity Measures,"İsmail Avcıbaş, Mehdi Kharrazi, Nasir Memon and Bülent Sankur",We present a novel technique for steganalysis of images that have been subjected to embedding by steganographic algorithms. The seventh and eighth bit planes in an image are used for the computation of several...,https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.2749,23 October 2005,
981,0.000678989392134464,981,Sampling-based Estimation of the Number of Distinct Values in Distributed Environment,"Jiajun Li, Zhewei Wei, Bolin Ding, Xiening Dai, Lu Lu, Jingren Zhou","In data mining, estimating the number of distinct values (NDV) is a fundamental problem with various applications. Existing methods for estimating NDV can be broadly classified into two categories: i) scanning-based methods, which scan the entire data and maintain a sketch to approximate NDV; and ii) sampling-based methods, which estimate NDV using sampling data rather than accessing the entire data warehouse. Scanning-based methods achieve a lower approximation error at the cost of higher I/O and more time. Sampling-based estimation is preferable in applications with a large data volume and a permissible error restriction due to its higher scalability. However, while the sampling-based method is more effective on a single machine, it is less practical in a distributed environment with massive data volumes. For obtaining the final NDV estimators, the entire sample must be transferred throughout the distributed system, incurring a prohibitive communication cost when the sample rate is significant. This paper proposes a novel sketch-based distributed method that achieves sub-linear communication costs for distributed sampling-based NDV estimation under mild assumptions. Our method leverages a sketch-based algorithm to estimate the sample's {\em frequency of frequency} in the {\em distributed streaming model}, which is compatible with most classical sampling-based NDV estimators. Additionally, we provide theoretical evidence for our method's ability to minimize communication costs in the worst-case scenario. Extensive experiments show that our method saves orders of magnitude in communication costs compared to existing sampling- and sketch-based methods. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05476,June 2022,981,Detection and visualization of encoded local features as anatomical predictors in cross-sectional images of Lauraceae,"Sung-Wook Hwang, Kayoko Kobayashi and Junji Sugiyama",This paper describes computer vision-based quantitative microscopy and its application toward better understanding species specificity. An image dataset of the Lauraceae family that consists of nine species ac...,https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1186/s10086-020-01864-5,7 March 2020,
982,0.000678989392134464,982,Sentiment analysis on electricity twitter posts,"Pardeep Kaur, Maryam Edalati","In today's world, everyone is expressive in some way, and the focus of this project is on people's opinions about rising electricity prices in United Kingdom and India using data from Twitter, a micro-blogging platform on which people post messages, known as tweets. Because many people's incomes are not good and they have to pay so many taxes and bills, maintaining a home has become a disputed issue these days. Despite the fact that Government offered subsidy schemes to compensate people electricity bills but it is not welcomed by people. In this project, the aim is to perform sentiment analysis on people's expressions and opinions expressed on Twitter. In order to grasp the electricity prices opinion, it is necessary to carry out sentiment analysis for the government and consumers in energy market. Furthermore, text present on these medias are unstructured in nature, so to process them we firstly need to pre-process the data. There are so many feature extraction techniques such as Bag of Words, TF-IDF (Term Frequency-Inverse Document Frequency), word embedding, NLP based features like word count. In this project, we analysed the impact of feature TF-IDF word level on electricity bills dataset of sentiment analysis. We found that by using TF-IDF word level performance of sentiment analysis is 3-4 higher than using N-gram features. Analysis is done using four classification algorithms including Naive Bayes, Decision Tree, Random Forest, and Logistic Regression and considering F-Score, Accuracy, Precision, and Recall performance parameters. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.05042,June 2022,982,Systematic review of research on artificial intelligence applications in higher education – where are the educators?,"Olaf Zawacki-Richter, Victoria I. Marín, Melissa Bond and Franziska Gouverneur","According to various international reports, Artificial Intelligence in Education (AIEd) is one of the currently emerging fields in educational technology. Whilst it has been around for about 30 years, it is st...",https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-019-0171-0,28 October 2019,
983,0.000678989392134464,983,In Defense of Core-set: A Density-aware Core-set Selection for Active Learning,"Yeachan Kim, Bonggun Shin","Active learning enables the efficient construction of a labeled dataset by labeling informative samples from an unlabeled dataset. In a real-world active learning scenario, considering the diversity of the selected samples is crucial because many redundant or highly similar samples exist. Core-set approach is the promising diversity-based method selecting diverse samples based on the distance between samples. However, the approach poorly performs compared to the uncertainty-based approaches that select the most difficult samples where neural models reveal low confidence. In this work, we analyze the feature space through the lens of the density and, interestingly, observe that locally sparse regions tend to have more informative samples than dense regions. Motivated by our analysis, we empower the core-set approach with the density-awareness and propose a density-aware core-set (DACS). The strategy is to estimate the density of the unlabeled samples and select diverse samples mainly from sparse regions. To reduce the computational bottlenecks in estimating the density, we also introduce a new density approximation based on locality-sensitive hashing. Experimental results clearly demonstrate the efficacy of DACS in both classification and regression tasks and specifically show that DACS can produce state-of-the-art performance in a practical scenario. Since DACS is weakly dependent on neural architectures, we present a simple yet effective combination method to show that the existing methods can be beneficially combined with DACS. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.04838,June 2022,983,Design Theory and Method of Complex Products: A Review,"Chan Qiu, Jianrong Tan, Zhenyu Liu, Haoyang Mao and Weifei Hu","Design is a high-level and complex thinking activity of human beings, using existing knowledge and technology to solve problems and create new things. With the rise and development of intelligent manufacturing...",https://www.springeropen.com//cjme.springeropen.com/articles/10.1186/s10033-022-00779-0,11 August 2022,
984,0.000678989392134464,984,Towards Target Sequential Rules,"Wensheng Gan, Gengsen Huang, Jian Weng, Tianlong Gu, Philip S. Yu","In many real-world applications, sequential rule mining (SRM) can provide prediction and recommendation functions for a variety of services. It is an important technique of pattern mining to discover all valuable rules that belong to high-frequency and high-confidence sequential rules. Although several algorithms of SRM are proposed to solve various practical problems, there are no studies on target sequential rules. Targeted sequential rule mining aims at mining the interesting sequential rules that users focus on, thus avoiding the generation of other invalid and unnecessary rules. This approach can further improve the efficiency of users in analyzing rules and reduce the consumption of data resources. In this paper, we provide the relevant definitions of target sequential rule and formulate the problem of targeted sequential rule mining. Furthermore, we propose an efficient algorithm, called targeted sequential rule mining (TaSRM). Several pruning strategies and an optimization are introduced to improve the efficiency of TaSRM. Finally, a large number of experiments are conducted on different benchmarks, and we analyze the results in terms of their running time, memory consumption, and scalability, as well as query cases with different query rules. It is shown that the novel algorithm TaSRM and its variants can achieve better experimental performance compared to the existing baseline algorithm. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.04728,June 2022,984,Research in information systems in China (1999–2005) and international comparisons,"Shaobo Ji, Qingfei Min and Weihe Han",The purpose of this study is to review the research activities in information systems (IS) in the mainland of China. We reviewed and analyzed a total of 859 research papers in information systems published in ...,https://www.springeropen.com//fbr.springeropen.com/articles/10.1007/s11782-007-0002-3,February 2007,
985,0.000678989392134464,985,Model Degradation Hinders Deep Graph Neural Networks,"Wentao Zhang, Zeang Sheng, Ziqi Yin, Yuezihan Jiang, Yikuan Xia, Jun Gao, Zhi Yang, Bin Cui","Graph Neural Networks (GNNs) have achieved great success in various graph mining tasks.However, drastic performance degradation is always observed when a GNN is stacked with many layers. As a result, most GNNs only have shallow architectures, which limits their expressive power and exploitation of deep neighborhoods.Most recent studies attribute the performance degradation of deep GNNs to the \textit{over-smoothing} issue. In this paper, we disentangle the conventional graph convolution operation into two independent operations: \textit{Propagation} (\textbf{P}) and \textit{Transformation} (\textbf{T}).Following this, the depth of a GNN can be split into the propagation depth ($D_p$) and the transformation depth ($D_t$). Through extensive experiments, we find that the major cause for the performance degradation of deep GNNs is the \textit{model degradation} issue caused by large $D_t$ rather than the \textit{over-smoothing} issue mainly caused by large $D_p$. Further, we present \textit{Adaptive Initial Residual} (AIR), a plug-and-play module compatible with all kinds of GNN architectures, to alleviate the \textit{model degradation} issue and the \textit{over-smoothing} issue simultaneously. Experimental results on six real-world datasets demonstrate that GNNs equipped with AIR outperform most GNNs with shallow architectures owing to the benefits of both large $D_p$ and $D_t$, while the time costs associated with AIR can be ignored. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.04361,June 2022,985,Evaluating QualiCO: an ontology to facilitate qualitative methods sharing to support open science,"Julian Hocker, Taryn Bipat, David W. McDonald and Mark Zachry",Qualitative science methods have largely been omitted from discussions of open science. Platforms focused on qualitative science that support open science data and method sharing are rare. Sharing and exchangi...,https://www.springeropen.com//jisajournal.springeropen.com/articles/10.1186/s13174-021-00135-w,9 August 2021,
986,0.000678989392134464,986,Graph Attention Multi-Layer Perceptron,"Wentao Zhang, Ziqi Yin, Zeang Sheng, Yang Li, Wen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, Bin Cui","Graph neural networks (GNNs) have achieved great success in many graph-based applications. However, the enormous size and high sparsity level of graphs hinder their applications under industrial scenarios. Although some scalable GNNs are proposed for large-scale graphs, they adopt a fixed $K$-hop neighborhood for each node, thus facing the over-smoothing issue when adopting large propagation depths for nodes within sparse regions. To tackle the above issue, we propose a new GNN architecture -- Graph Attention Multi-Layer Perceptron (GAMLP), which can capture the underlying correlations between different scales of graph knowledge. We have deployed GAMLP in Tencent with the Angel platform, and we further evaluate GAMLP on both real-world datasets and large-scale industrial datasets. Extensive experiments on these 14 graph datasets demonstrate that GAMLP achieves state-of-the-art performance while enjoying high scalability and efficiency. Specifically, it outperforms GAT by 1.3\% regarding predictive accuracy on our large-scale Tencent Video dataset while achieving up to $50\times$ training speedup. Besides, it ranks top-1 on both the leaderboards of the largest homogeneous and heterogeneous graph (i.e., ogbn-papers100M and ogbn-mag) of Open Graph Benchmark. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.04355,June 2022,986,Beyond financial success: success goals of creative entrepreneurs,"Vladimír Baláž, Tomáš Jeck and Miroslav Balog","This paper presents evidence on the identities, values and success goals of creative entrepreneurs in Slovakia. The research relies on a database of creative firms supported by the Slovak Creative Voucher Sche...",https://www.springeropen.com//innovation-entrepreneurship.springeropen.com/articles/10.1186/s13731-022-00207-6,15 February 2022,
987,0.000678989392134464,987,Unsupervised Key Event Detection from Massive Text Corpora,"Yunyi Zhang, Fang Guo, Jiaming Shen, Jiawei Han","Automated event detection from news corpora is a crucial task towards mining fast-evolving structured knowledge. As real-world events have different granularities, from the top-level themes to key events and then to event mentions corresponding to concrete actions, there are generally two lines of research: (1) theme detection identifies from a news corpus major themes (e.g., ""2019 Hong Kong Protests"" vs. ""2020 U.S. Presidential Election"") that have very distinct semantics; and (2) action extraction extracts from one document mention-level actions (e.g., ""the police hit the left arm of the protester"") that are too fine-grained for comprehending the event. In this paper, we propose a new task, key event detection at the intermediate level, aiming to detect from a news corpus key events (e.g., ""HK Airport Protest on Aug. 12-14""), each happening at a particular time/location and focusing on the same topic. This task can bridge event understanding and structuring and is inherently challenging because of the thematic and temporal closeness of key events and the scarcity of labeled data due to the fast-evolving nature of news articles. To address these challenges, we develop an unsupervised key event detection framework, EvMine, that (1) extracts temporally frequent peak phrases using a novel ttf-itf score, (2) merges peak phrases into event-indicative feature sets by detecting communities from our designed peak phrase graph that captures document co-occurrences, semantic similarities, and temporal closeness signals, and (3) iteratively retrieves documents related to each key event by training a classifier with automatically generated pseudo labels from the event-indicative feature sets and refining the detected key events using the retrieved documents. Extensive experiments and case studies show EvMine outperforms all the baseline methods and its ablations on two real-world news corpora. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.04153,June 2022,987,Underwater image segmentation in the wild using deep learning,"Paulo Drews-Jr, Isadora de Souza, Igor P. Maurell, Eglen V. Protas and Silvia S. C. Botelho","Image segmentation is an important step in many computer vision and image processing algorithms. It is often adopted in tasks such as object detection, classification, and tracking. The segmentation of underwa...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1186/s13173-021-00117-7,4 October 2021,
988,0.000678989392134464,988,TSFEDL: A Python Library for Time Series Spatio-Temporal Feature Extraction and Prediction using Deep Learning (with Appendices on Detailed Network Architectures and Experimental Cases of Study),"Ignacio Aguilera-Martos, Ángel M. García-Vico, Julián Luengo, Sergio Damas, Francisco J. Melero, José Javier Valle-Alonso, Francisco Herrera","The combination of convolutional and recurrent neural networks is a promising framework that allows the extraction of high-quality spatio-temporal features together with its temporal dependencies, which is key for time series prediction problems such as forecasting, classification or anomaly detection, amongst others. In this paper, the TSFEDL library is introduced. It compiles 20 state-of-the-art methods for both time series feature extraction and prediction, employing convolutional and recurrent deep neural networks for its use in several data mining tasks. The library is built upon a set of Tensorflow+Keras and PyTorch modules under the AGPLv3 license. The performance validation of the architectures included in this proposal confirms the usefulness of this Python package. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.03179,June 2022,988,Changes in achievement on PISA: the case of Ireland and implications for international assessment practice,Jude Cosgrove and Fernando Cartwright,The PISA 2009 results for Ireland indicated a large decline in reading literacy scores since PISA 2000 (the largest of 38 countries). The decline in mathematics scores since PISA 2003 was the second largest of...,https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/2196-0739-2-2,14 January 2014,
989,0.000678989392134464,989,A Bird's-Eye Tutorial of Graph Attention Architectures,"Kaustubh D. Dhole, Carl Yang","Graph Neural Networks (GNNs) have shown tremendous strides in performance for graph-structured problems especially in the domains of natural language processing, computer vision and recommender systems. Inspired by the success of the transformer architecture, there has been an ever-growing body of work on attention variants of GNNs attempting to advance the state of the art in many of these problems. Incorporating ""attention"" into graph mining has been viewed as a way to overcome the noisiness, heterogenity and complexity associated with graph-structured data as well as to encode soft-inductive bias. It is hence crucial and advantageous to study these variants from a bird's-eye view to assess their strengths and weaknesses. We provide a systematic and focused tutorial centered around attention based GNNs in a hope to benefit researchers dealing with graph-structured problems. Our tutorial looks at GNN variants from the point of view of the attention function and iteratively builds the reader's understanding of different graph attention variants. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.02849,June 2022,989,Lost in mobile? Exploring the mobile internet digital divide among Chinese college students,Lian Wang and Chun Liu,Mobile has become the primary mode of Internet access for many people. Existing studies have generally indicated that mobile Internet represents an inferior substitute for traditional PC-based Internet. In par...,https://www.springeropen.com//educationaltechnologyjournal.springeropen.com/articles/10.1186/s41239-021-00267-w,15 June 2021,
990,0.000678989392134464,990,TransBO: Hyperparameter Optimization via Two-Phase Transfer Learning,"Yang Li, Yu Shen, Huaijun Jiang, Wentao Zhang, Zhi Yang, Ce Zhang, Bin Cui","With the extensive applications of machine learning models, automatic hyperparameter optimization (HPO) has become increasingly important. Motivated by the tuning behaviors of human experts, it is intuitive to leverage auxiliary knowledge from past HPO tasks to accelerate the current HPO task. In this paper, we propose TransBO, a novel two-phase transfer learning framework for HPO, which can deal with the complementary nature among source tasks and dynamics during knowledge aggregation issues simultaneously. This framework extracts and aggregates source and target knowledge jointly and adaptively, where the weights can be learned in a principled manner. The extensive experiments, including static and dynamic transfer learning settings and neural architecture search, demonstrate the superiority of TransBO over the state-of-the-arts. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.02663,June 2022,990,A quantum circuit to generate random numbers within a specific interval,"Francisco Orts, Ernestas Filatovas, Ester M. Garzón and Gloria Ortega","Random numbers are of vital importance in fields such as cyptography and scientific simulations. However, it is well known how difficult it is for classical computers to generate random numbers. This is not th...",https://www.springeropen.com//epjquantumtechnology.springeropen.com/articles/10.1140/epjqt/s40507-023-00174-1,25 May 2023,
991,0.000678989392134464,991,Transfer Learning based Search Space Design for Hyperparameter Tuning,"Yang Li, Yu Shen, Huaijun Jiang, Tianyi Bai, Wentao Zhang, Ce Zhang, Bin Cui","The tuning of hyperparameters becomes increasingly important as machine learning (ML) models have been extensively applied in data mining applications. Among various approaches, Bayesian optimization (BO) is a successful methodology to tune hyper-parameters automatically. While traditional methods optimize each tuning task in isolation, there has been recent interest in speeding up BO by transferring knowledge across previous tasks. In this work, we introduce an automatic method to design the BO search space with the aid of tuning history from past tasks. This simple yet effective approach can be used to endow many existing BO methods with transfer learning capabilities. In addition, it enjoys the three advantages: universality, generality, and safeness. The extensive experiments show that our approach considerably boosts BO by designing a promising and compact search space instead of using the entire space, and outperforms the state-of-the-arts on a wide range of benchmarks, including machine learning and deep learning tuning tasks, and neural architecture search. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.02511,June 2022,991,A survey on artificial intelligence assurance,"Feras A. Batarseh, Laura Freeman and Chih-Hao Huang",Artificial Intelligence (AI) algorithms are increasingly providing decision making and operational support across multiple domains. AI includes a wide (and growing) library of algorithms that could be applied ...,https://www.springeropen.com//journalofbigdata.springeropen.com/articles/10.1186/s40537-021-00445-7,26 April 2021,
992,0.000678989392134464,992,Finetuning a Kalaallisut-English machine translation system using web-crawled data,Alex Jones,"West Greenlandic, known by native speakers as Kalaallisut, is an extremely low-resource polysynthetic language spoken by around 56,000 people in Greenland. Here, we attempt to finetune a pretrained Kalaallisut-to-English neural machine translation (NMT) system using web-crawled pseudoparallel sentences from around 30 multilingual websites. We compile a corpus of over 93,000 Kalaallisut sentences and over 140,000 Danish sentences, then use cross-lingual sentence embeddings and approximate nearest-neighbors search in an attempt to mine near-translations from these corpora. Finally, we translate the Danish sentence to English to obtain a synthetic Kalaallisut-English aligned corpus. Although the resulting dataset is too small and noisy to improve the pretrained MT model, we believe that with additional resources, we could construct a better pseudoparallel corpus and achieve more promising results on MT. We also note other possible uses of the monolingual Kalaallisut data and discuss directions for future work. We make the code and data for our experiments publicly available. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.02230,June 2022,992,Rule-based process indicators of information processing explain performance differences in PIAAC web search tasks,"Carolin Hahnel, Ulf Kroehne and Frank Goldhammer","A priori assumptions about specific behavior in test items can be used to process log data in a rule-based fashion to identify the behavior of interest. In this study, we demonstrate such a top-down approach a...",https://www.springeropen.com//largescaleassessmentsineducation.springeropen.com/articles/10.1186/s40536-023-00169-5,19 May 2023,
993,0.000678989392134464,993,Chemical Short-Range Ordering in a CrCoNi Medium-Entropy Alloy,"H. W. Hsiao, R. Feng, H. Ni, K. An, J. D. Poplawsky, P. K. Liaw, J. M. Zuo","The exceptional mechanical strengths of medium and high-entropy alloys have been attributed to hardening in random solid solutions. Here, we evidence non-random chemical mixings in CrCoNi alloys, resulting from short range ordering. A novel data-mining approach of electron nanodiffraction patterns enabled the study, which is assisted by neutron scattering, atom probe tomography, and diffraction simulation using first principles theory models. Results reveal two critical types of short range orders in nanoclusters that minimize the Cr and Cr nearest neighbors (L11) or segregate Cr on alternating close-packed planes (L12). The makeup of ordering-strengthened nanoclusters can be tuned by heat treatments to affect deformation mechanisms. These findings uncover a mixture of bonding preferences and their control at the nanoscopic scale in CrCoNi and provide general opportunities for an atomistic-structure study in concentrated alloys for the design of strong and ductile materials. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.02004,June 2022,993,Creating concept maps with augmented reality: a case of eclipse of the lunar and solar topic,"Ünal Çakıroğlu, Samet Atabaş, Merve Aydın and Ilknur Özyılmaz","Concept maps are the tools used to facilitate meaningful conceptual learning. In this study, an augmented reality (AR)-based concept map (AR-ConMAP) application was developed to facilitate the concept map crea...",https://www.springeropen.com//telrp.springeropen.com/articles/10.1186/s41039-022-00191-1,20 April 2022,
994,0.000678989392134464,994,Modelling and Mining of Patient Pathways: A Scoping Review,"Caroline de Oliveira Costa Souza Rosa, Marcia Ito, Alex Borges Vieira, Antonio Tadeu Azevedo Gomes","The sequence of visits and procedures performed by the patient in the health system, also known as the patient's pathway or trajectory, can reveal important information about the clinical treatment adopted and the health service provided. The rise of electronic health data availability made it possible to assess the pathways of a large number of patients. Nevertheless, some challenges also arose concerning how to synthesize these pathways and how to mine them from the data, fostering a new field of research. The objective of this review is to survey this new field of research, highlighting representation models, mining techniques, methods of analysis, and examples of case studies. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.01980,June 2022,994,The Computation of Pitch with Vectors,Aluizio Arcela,"A pitch model is proposed which is supported by a vector representation of tones. First, an algorithm capable of performing the vector addition of the spectral components of two-tone harmonic complexes is intr...",https://www.springeropen.com//journal-bcs.springeropen.com/articles/10.1007/BF03192565,September 2008,
995,0.000678989392134464,995,Evaluation of creating scoring opportunities for teammates in soccer via trajectory prediction,"Masakiyo Teranishi, Kazushi Tsutsui, Kazuya Takeda, Keisuke Fujii","Evaluating the individual movements for teammates in soccer players is crucial for assessing teamwork, scouting, and fan engagement. It has been said that players in a 90-min game do not have the ball for about 87 minutes on average. However, it has remained difficult to evaluate an attacking player without receiving the ball, and to reveal how movement contributes to the creation of scoring opportunities for teammates. In this paper, we evaluate players who create off-ball scoring opportunities by comparing actual movements with the reference movements generated via trajectory prediction. First, we predict the trajectories of players using a graph variational recurrent neural network that can accurately model the relationship between players and predict the long-term trajectory. Next, based on the difference in the modified off-ball evaluation index between the actual and the predicted trajectory as a reference, we evaluate how the actual movement contributes to scoring opportunity compared to the predicted movement. For verification, we examined the relationship with the annual salary, the goals, and the rating in the game by experts for all games of a team in a professional soccer league in a year. The results show that the annual salary and the proposed indicator correlated significantly, which could not be explained by the existing indicators and goals. Our results suggest the effectiveness of the proposed method as an indicator for a player without the ball to create a scoring chance for teammates. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.01899,June 2022,995,Dynamic Modeling of Internet Traffic for Intrusion Detection,"Khushboo Shah, Edmond Jonckheere and Stephan Bohacek","Computer network traffic is analyzed via mutual information techniques, implemented using linear and nonlinear canonical correlation analyses, with the specific objective of detecting UDP flooding attacks. NS ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/2007/90312,1 December 2006,
996,0.000678989392134464,996,Approximate Network Motif Mining Via Graph Learning,"Carlos Oliver, Dexiong Chen, Vincent Mallet, Pericles Philippopoulos, Karsten Borgwardt","Frequent and structurally related subgraphs, also known as network motifs, are valuable features of many graph datasets. However, the high computational complexity of identifying motif sets in arbitrary datasets (motif mining) has limited their use in many real-world datasets. By automatically leveraging statistical properties of datasets, machine learning approaches have shown promise in several tasks with combinatorial complexity and are therefore a promising candidate for network motif mining. In this work we seek to facilitate the development of machine learning approaches aimed at motif mining. We propose a formulation of the motif mining problem as a node labelling task. In addition, we build benchmark datasets and evaluation metrics which test the ability of models to capture different aspects of motif discovery such as motif number, size, topology, and scarcity. Next, we propose MotiFiesta, a first attempt at solving this problem in a fully differentiable manner with promising results on challenging baselines. Finally, we demonstrate through MotiFiesta that this learning setting can be applied simultaneously to general-purpose data mining and interpretable feature extraction for graph classification tasks. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.01008,June 2022,996,Automated identification of Lauraceae by scale-invariant feature transform,"Sung-Wook Hwang, Kayoko Kobayashi, Shengcheng Zhai and Junji Sugiyama","An image dataset of the cross-sectional optical micrographs of the Lauraceae species including 39 species in 11 genera, capturing at least one full annual ring, was investigated by scale-invariant feature tran...",https://www.springeropen.com//jwoodscience.springeropen.com/articles/10.1007/s10086-017-1680-x,12 December 2017,
997,0.000678989392134464,997,Core-periphery Models for Hypergraphs,"Marios Papachristou, Jon Kleinberg","We introduce a random hypergraph model for core-periphery structure. By leveraging our model's sufficient statistics, we develop a novel statistical inference algorithm that is able to scale to large hypergraphs with runtime that is practically linear wrt. the number of nodes in the graph after a preprocessing step that is almost linear in the number of hyperedges, as well as a scalable sampling algorithm. Our inference algorithm is capable of learning embeddings that correspond to the reputation (rank) of a node within the hypergraph. We also give theoretical bounds on the size of the core of hypergraphs generated by our model. We experiment with hypergraph data that range to $\sim 10^5$ hyperedges mined from the Microsoft Academic Graph, Stack Exchange, and GitHub and show that our model outperforms baselines wrt. producing good fits. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.00783,June 2022,997,Real-timein silicoexperiments on gene regulatory networks and surgery simulation on handheld devices,"Icíar Alfaro, David González, Felipe Bordeu, Adrien Leygue, Amine Ammar, Elías Cueto and Francisco Chinesta","Simulation of all phenomena taking place in a surgical procedure is a formidable task that involves, when possible, the use of supercomputing facilities over long time periods. However, decision taking in the ...",https://www.springeropen.com//computationalsurgery.springeropen.com/articles/10.1186/2194-3990-1-1,10 January 2014,
998,0.000678989392134464,998,Photoionization cross sections of ultracold $^{88}$Sr in $^1$P$_1$ and $^3$S$_1$ states at 390 nm and the resulting blue-detuned magic wavelength optical lattice clock constraints,"Marcin Witkowski, Sławomir Bilicki, Marcin Bober, Domagoj Kovačić, Vijay Singh, Ara Tonoyan, Michał Zawada","We present the measurements of the photoionisation cross sections of the excited $^1$P$_1$ and $^3$S$_1$ states of ultracold $^{88}$Sr atoms at 389.889 nm wavelength, which is the magic wavelength of the $^{1}$S$_{0}$-${}^{3}$P${}_{0}$ clock transition. The photoionisation cross section of the $^1$P$_1$ state is determined from the measured ionisation rates of $^{88}$Sr in the magneto-optical trap in the $^1$P$_1$ state to be 2.20(50)$\times$10$^{-20}$ m$^2$, while the photoionisation cross section of $^{88}$Sr in the $^3$S$_1$ state is inferred from the photoionisation-induced reduction in the number of atoms transferred through the $^3\text{S}_1$ state in an operating optical lattice clock to be $1.38(66)\times$10$^{-18}$ m$^2$. Furthermore, the resulting limitations of employing a blue-detuned magic wavelength optical lattice in strontium optical lattice clocks are evaluated. We estimated photoionisation induced loss rates of atoms at 389.889 nm wavelength under typical experimental conditions and made several suggestions on how to mitigate these losses. In particular, the large photoionisation induced losses for the $^3$S$_1$ state would make the use of the $^3$S$_1$ state in the optical cycle in a blue-detuned optical lattice unfeasible and would instead require the less commonly used $^3$D$_{1,2}$ states during the detection part of the optical clock cycle. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.00733,June 2022,998,Chinese test takers' attitudes towards the Versant English Test: a mixed-methods approach,Jinsong Fan,"This study investigated Chinese test takers' attitudes towards the Versant English Test (VET), an automated spoken English test developed by Pearson Knowledge Technologies. Based on previous attitudinal studie...",https://www.springeropen.com//languagetestingasia.springeropen.com/articles/10.1186/s40468-014-0006-9,21 October 2014,
999,0.00514534320972017,999,The statistical nature of h-index of a network node,"Yan Liu, Mudi Jiang, Lianyu Hu, Zengyou He","Evaluating the importance of a network node is a crucial task in network science and graph data mining. H-index is a popular centrality measure for this task, however, there is still a lack of its interpretation from a rigorous statistical aspect. Here we show the statistical nature of h-index from the perspective of order statistics, and we obtain a new family of centrality indices by generalizing the h-index along this direction. The theoretical and empirical evidences show that such a statistical interpretation enables us to obtain a general and versatile framework for quantifying the importance of a network node. Under this framework, many new centrality indices can be derived and some of which can be more accurate and robust than h-index. We believe that this research opens up new avenues for developing more effective indices for node importance quantification from a viewpoint that still remains unexplored. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.00381,June 2022,999,Clipped Input RLS Applied to Vehicle Tracking,"Hadi Sadoghi Yazdi, Mojtaba Lotfizad, Ehsanollah Kabir and Mahmood Fathy","A new variation to the RLS algorithm is presented. In the clipped RLS algorithm (CRLS), proposed in updating the filter weights and computation of the inverse correlation matrix, the input signal is quantized ...",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/ASP.2005.1221,31 May 2005,
1000,0.000678989392134464,1000,Self-supervised Learning for Label Sparsity in Computational Drug Repositioning,"Xinxing Yang, Genke Yang, Jian Chu","The computational drug repositioning aims to discover new uses for marketed drugs, which can accelerate the drug development process and play an important role in the existing drug discovery system. However, the number of validated drug-disease associations is scarce compared to the number of drugs and diseases in the real world. Too few labeled samples will make the classification model unable to learn effective latent factors of drugs, resulting in poor generalization performance. In this work, we propose a multi-task self-supervised learning framework for computational drug repositioning. The framework tackles label sparsity by learning a better drug representation. Specifically, we take the drug-disease association prediction problem as the main task, and the auxiliary task is to use data augmentation strategies and contrast learning to mine the internal relationships of the original drug features, so as to automatically learn a better drug representation without supervised labels. And through joint training, it is ensured that the auxiliary task can improve the prediction accuracy of the main task. More precisely, the auxiliary task improves drug representation and serving as additional regularization to improve generalization. Furthermore, we design a multi-input decoding network to improve the reconstruction ability of the autoencoder model. We evaluate our model using three real-world datasets. The experimental results demonstrate the effectiveness of the multi-task self-supervised learning framework, and its predictive ability is superior to the state-of-the-art model. △ Less",https://arxiv.orghttps://arxiv.org/abs/2206.00262,June 2022,1000,The First 50 Years of Electronic Watermarking,Ingemar J. Cox and Matt L. Miller,"Electronic watermarking can be traced back as far as 1954. The last 10 years has seen considerable interest in digital watermarking, due, in large part, to concerns about illegal piracy of copyrighted content....",https://www.springeropen.com//asp-eurasipjournals.springeropen.com/articles/10.1155/S1110865702000525,14 February 2002,
